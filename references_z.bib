
@article{huffman_integrated_2023,
	title = {Integrated multi-satellite retrievals for {GPM} ({IMERG}), {V06B}.},
	journal = {NASA's Precipitation Processing Center, accessed 1st October 2022, https://arthurhouhttps.pps.eosdis.nasa.gov/text/gpmallversions/V06/YYYY/MM/DD/imerg/},
	author = {Huffman, G and Bolvin, D and Braithwaite, D and Hsu, K and Joyce, R and Xie, P},
	month = jan,
	year = {2023},
}

@article{efron_bootstrap_1986,
	title = {Bootstrap {Methods} for {Standard} {Errors}, {Confidence} {Intervals}, and {Other} {Measures} of {Statistical} {Accuracy}},
	volume = {1},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/2245500},
	abstract = {This is a review of bootstrap methods, concentrating on basic ideas and applications rather than theoretical considerations. It begins with an exposition of the bootstrap estimate of standard error for one-sample situations. Several examples, some involving quite complicated statistical procedures, are given. The bootstrap is then extended to other measures of statistical accuracy such as bias and prediction error, and to complicated data structures such as time series, censored data, and regression models. Several more examples are presented illustrating these ideas. The last third of the paper deals mainly with bootstrap confidence intervals.},
	number = {1},
	urldate = {2023-11-02},
	journal = {Statistical Science},
	author = {Efron, B. and Tibshirani, R.},
	year = {1986},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {54--75},
}

@article{tibshirani_discussion_1986,
	title = {Discussion: {Jackknife}, {Bootstrap} and {Other} {Resampling} {Methods} in {Regression} {Analysis}},
	volume = {14},
	issn = {0090-5364},
	shorttitle = {Discussion},
	url = {https://www.jstor.org/stable/2241470},
	number = {4},
	urldate = {2023-11-02},
	journal = {The Annals of Statistics},
	author = {Tibshirani, Robert},
	year = {1986},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1335--1339},
}

@article{selz_can_2023,
	title = {Can {Artificial} {Intelligence}-{Based} {Weather} {Prediction} {Models} {Simulate} the {Butterfly} {Effect}?},
	volume = {50},
	copyright = {© 2023. The Authors.},
	issn = {1944-8007},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2023GL105747},
	doi = {10.1029/2023GL105747},
	abstract = {We investigate error growth from small-amplitude initial condition perturbations, simulated with a recent artificial intelligence-based weather prediction model. From past simulations with standard physically-based numerical models as well as from theoretical considerations it is expected that such small-amplitude initial condition perturbations would grow very fast initially. This fast growth then sets a fixed and fundamental limit to the predictability of weather, a phenomenon known as the butterfly effect. We find however, that the AI-based model completely fails to reproduce the rapid initial growth rates and hence would incorrectly suggest an unlimited predictability of the atmosphere. In contrast, if the initial perturbations are large and comparable to current uncertainties in the estimation of the initial state, the AI-based model basically agrees with physically-based simulations, although some deficits are still present.},
	language = {en},
	number = {20},
	urldate = {2023-11-01},
	journal = {Geophysical Research Letters},
	author = {Selz, T. and Craig, G. C.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2023GL105747},
	keywords = {artificial intelligence, butterfly effect, weather prediction},
	pages = {e2023GL105747},
}

@article{noauthor_nemo_nodate,
	title = {{NEMO} ocean engine},
	url = {https://zenodo.org/records/6334656},
	doi = {10.5281/zenodo.6334656},
	abstract = {The ocean engine of NEMO is a primitive equation model adapted to regional and global ocean circulation problems. It is intended to be a flexible tool for studying the ocean and its interactions with the others components of the earth climate system over a wide range of space and time scales.},
	language = {en},
	urldate = {2023-10-31},
}

@misc{gu_matryoshka_2023,
	title = {Matryoshka {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2310.15111},
	doi = {10.48550/arXiv.2310.15111},
	abstract = {Diffusion models are the de facto approach for generating high-quality images and videos, but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space or using a downsampled latent space of a separately trained auto-encoder. In this paper, we introduce Matryoshka Diffusion Models(MDM), an end-to-end framework for high-resolution image and video synthesis. We propose a diffusion process that denoises inputs at multiple resolutions jointly and uses a NestedUNet architecture where features and parameters for small-scale inputs are nested within those of large scales. In addition, MDM enables a progressive training schedule from lower to higher resolutions, which leads to significant improvements in optimization for high-resolution generation. We demonstrate the effectiveness of our approach on various benchmarks, including class-conditioned image generation, high-resolution text-to-image, and text-to-video applications. Remarkably, we can train a single pixel-space model at resolutions of up to 1024x1024 pixels, demonstrating strong zero-shot generalization using the CC12M dataset, which contains only 12 million images.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Gu, Jiatao and Zhai, Shuangfei and Zhang, Yizhe and Susskind, Josh and Jaitly, Navdeep},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15111 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{vosper_deep_2023,
	title = {Deep {Learning} for {Downscaling} {Tropical} {Cyclone} {Rainfall} to {Hazard}-{Relevant} {Spatial} {Scales}},
	volume = {128},
	copyright = {© 2023. The Authors.},
	issn = {2169-8996},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022JD038163},
	doi = {10.1029/2022JD038163},
	abstract = {Flooding, driven in part by intense rainfall, is the leading cause of mortality and damages from the most intense tropical cyclones (TCs). With rainfall from TCs set to increase under anthropogenic climate change, it is critical to accurately estimate extreme rainfall to better support short-term and long-term resilience efforts. While high-resolution climate models capture TC statistics better than low-resolution models, they are computationally expensive. This leads to a trade-off between capturing TC features accurately, and generating large enough simulation data sets to sufficiently sample high-impact, low-probability events. Downscaling can assist by predicting high-resolution features from relatively cheap, low-resolution models. Here, we develop and evaluate a set of three deep learning models for downscaling TC rainfall to hazard-relevant spatial scales. We use rainfall from the Multi-Source Weighted-Ensemble Precipitation observational product at a coarsened resolution of ∼100 km, and apply our downscaling model to reproduce the original resolution of ∼10 km. We find that the Wasserstein Generative Adversarial Network is able to capture realistic spatial structures and power spectra and performs the best overall, with mean biases within 5\% of observations. We also show that the model can perform well at extrapolating to the most extreme storms, which were not used in training.},
	language = {en},
	number = {10},
	urldate = {2023-10-27},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Vosper, Emily and Watson, Peter and Harris, Lucy and McRae, Andrew and Santos-Rodriguez, Raul and Aitchison, Laurence and Mitchell, Dann},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2022JD038163},
	keywords = {Generative Adversarial Network, deep learning, downscaling, superresolution, tropical cyclone, tropical cyclone rainfall},
	pages = {e2022JD038163},
}

@article{bond-taylor_deep_2022,
	title = {Deep {Generative} {Modelling}: {A} {Comparative} {Review} of {VAEs}, {GANs}, {Normalizing} {Flows}, {Energy}-{Based} and {Autoregressive} {Models}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Deep {Generative} {Modelling}},
	url = {https://ieeexplore.ieee.org/document/9555209/},
	doi = {10.1109/TPAMI.2021.3116668},
	abstract = {Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing ﬂows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.},
	language = {en},
	number = {11},
	urldate = {2023-10-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
	month = nov,
	year = {2022},
	pages = {7327--7347},
}

@article{bond-taylor_deep_2022-1,
	title = {Deep {Generative} {Modelling}: {A} {Comparative} {Review} of {VAEs}, {GANs}, {Normalizing} {Flows}, {Energy}-{Based} and {Autoregressive} {Models}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Deep {Generative} {Modelling}},
	url = {https://ieeexplore.ieee.org/abstract/document/9555209},
	doi = {10.1109/TPAMI.2021.3116668},
	abstract = {Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.},
	number = {11},
	urldate = {2023-10-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
	month = nov,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {7327--7347},
}

@inproceedings{ostrovski_autoregressive_2018,
	title = {Autoregressive {Quantile} {Networks} for {Generative} {Modeling}},
	url = {https://proceedings.mlr.press/v80/ostrovski18a.html},
	abstract = {We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception scores, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.},
	language = {en},
	urldate = {2023-10-25},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ostrovski, Georg and Dabney, Will and Munos, Remi},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {3936--3945},
}

@misc{materia_artificial_2023,
	title = {Artificial {Intelligence} for {Prediction} of {Climate} {Extremes}: {State} of the art, challenges and future perspectives},
	shorttitle = {Artificial {Intelligence} for {Prediction} of {Climate} {Extremes}},
	url = {http://arxiv.org/abs/2310.01944},
	abstract = {Scientific and technological advances in numerical modelling have improved the quality of climate predictions over recent decades, but predictive skill remains limited in many aspects. Extreme events such as heat and cold waves, droughts, heavy rain and storms are particularly challenging to predict accurately due to their rarity and non-linear chaotic nature, and because of model limitations. However, recent studies have shown that predictive skill of extremes can be increased when using more sophisticated approaches, indicating that there might be systemic predictability that is not being leveraged. Recently, numerous studies have been devoted to the exploitation of Artificial Intelligence (AI) to study the predictability and make predictions of weather and climate. AI techniques have shown great potential to improve the prediction of extreme events and uncover their links to large-scale and local drivers. Machine and deep learning, causal discovery, explainable AI, are only some of the approaches that have been tested to both improve our understanding of the processes underlying predictability and enhance prediction skill of extreme events. Results are promising, especially for hybrid predictions that combine the AI, which can reveal and exploit unknown spatio-temporal connections from data, and climate models, that provide the theoretical foundation and interpretability of the physical world. On the other hand, challenges are multiple in many aspects, from data curation to model uncertainty and generalizability, to the reproducibility of methods and workflows. A few best practices are identified to increase trust in these novel techniques, and future perspectives are envisaged for further scientific development.},
	urldate = {2023-10-24},
	publisher = {arXiv},
	author = {Materia, Stefano and García, Lluís Palma and van Straaten, Chiem and O, Sungmin and Mamalakis, Antonios and Cavicchia, Leone and Coumou, Dim and De Luca, Paolo and Kretschmer, Marlene and Donat, Markus G.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01944 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics},
}

@article{dorninger_setup_2018,
	title = {The {Setup} of the {MesoVICT} {Project}},
	volume = {99},
	issn = {0003-0007, 1520-0477},
	url = {https://journals.ametsoc.org/view/journals/bams/99/9/bams-d-17-0164.1.xml},
	doi = {10.1175/BAMS-D-17-0164.1},
	abstract = {Abstract Recent advancements in numerical weather prediction (NWP) and the enhancement of model resolution have created the need for more robust and informative verification methods. In response to these needs, a plethora of spatial verification approaches have been developed in the past two decades. A spatial verification method intercomparison was established in 2007 with the aim of gaining a better understanding of the abilities of the new spatial verification methods to diagnose different types of forecast errors. The project focused on prescribed errors for quantitative precipitation forecasts over the central United States. The intercomparison led to a classification of spatial verification methods and a cataloging of their diagnostic capabilities, providing useful guidance to end users, model developers, and verification scientists. A decade later, NWP systems have continued to increase in resolution, including advances in high-resolution ensembles. This article describes the setup of a second phase of the verification intercomparison, called the Mesoscale Verification Intercomparison over Complex Terrain (MesoVICT). MesoVICT focuses on the application, capability, and enhancement of spatial verification methods to deterministic and ensemble forecasts of precipitation, wind, and temperature over complex terrain. Importantly, this phase also explores the issue of analysis uncertainty through the use of an ensemble of meteorological analyses.},
	language = {EN},
	number = {9},
	urldate = {2023-10-20},
	journal = {Bulletin of the American Meteorological Society},
	author = {Dorninger, Manfred and Gilleland, Eric and Casati, Barbara and Mittermaier, Marion P. and Ebert, Elizabeth E. and Brown, Barbara G. and Wilson, Laurence J.},
	month = sep,
	year = {2018},
	note = {Publisher: American Meteorological Society
Section: Bulletin of the American Meteorological Society},
	pages = {1887--1906},
}

@misc{baldwin_development_2003,
	title = {Development of an {Events}-{Oriented} {Verification} {System} {Using} data mining and {Image} {Processing} {Algorithms}},
	url = {https://ams.confex.com/ams/annual2003/techprogram/paper_57821.htm},
	urldate = {2023-10-20},
	author = {Baldwin, M. E. and Lakshmivarahan, S.},
	month = feb,
	year = {2003},
	note = {3rd Conference on Artificial Intelligence Applications to the Environmental Science},
}

@article{marzban_cluster_2006,
	title = {Cluster {Analysis} for {Verification} of {Precipitation} {Fields}},
	volume = {21},
	issn = {1520-0434, 0882-8156},
	url = {https://journals.ametsoc.org/view/journals/wefo/21/5/waf948_1.xml},
	doi = {10.1175/WAF948.1},
	abstract = {Abstract A statistical method referred to as cluster analysis is employed to identify features in forecast and observation fields. These features qualify as natural candidates for events or objects in terms of which verification can be performed. The methodology is introduced and illustrated on synthetic and real quantitative precipitation data. First, it is shown that the method correctly identifies clusters that are in agreement with what most experts might interpret as features or objects in the field. Then, it is shown that the verification of the forecasts can be performed within an event-based framework, with the events identified as the clusters. The number of clusters in a field is interpreted as a measure of scale, and the final “product” of the methodology is an “error surface” representing the error in the forecasts as a function of the number of clusters in the forecast and observation fields. This allows for the examination of forecast error as a function of scale.},
	language = {EN},
	number = {5},
	urldate = {2023-10-20},
	journal = {Weather and Forecasting},
	author = {Marzban, Caren and Sandgathe, Scott},
	month = oct,
	year = {2006},
	note = {Publisher: American Meteorological Society
Section: Weather and Forecasting},
	pages = {824--838},
}

@article{marzban_cluster_2008,
	title = {Cluster {Analysis} for {Object}-{Oriented} {Verification} of {Fields}: {A} {Variation}},
	volume = {136},
	issn = {1520-0493, 0027-0644},
	shorttitle = {Cluster {Analysis} for {Object}-{Oriented} {Verification} of {Fields}},
	url = {https://journals.ametsoc.org/view/journals/mwre/136/3/2007mwr1994.1.xml},
	doi = {10.1175/2007MWR1994.1},
	abstract = {Abstract In a recent paper, a statistical method referred to as cluster analysis was employed to identify clusters in forecast and observed fields. Further criteria were also proposed for matching the identified clusters in one field with those in the other. As such, the proposed methodology was designed to perform an automated form of what has been called object-oriented verification. Herein, a variation of that methodology is proposed that effectively avoids (or simplifies) the criteria for matching the objects. The basic idea is to perform cluster analysis on the combined set of observations and forecasts, rather than on the individual fields separately. This method will be referred to as combinative cluster analysis (CCA). CCA naturally lends itself to the computation of false alarms, hits, and misses, and therefore, to the critical success index (CSI). A desirable feature of the previous method—the ability to assess performance on different spatial scales—is maintained. The method is demonstrated on reflectivity data and corresponding forecasts for three dates using three mesoscale numerical weather prediction model formulations—the NCEP/NWS Nonhydrostatic Mesoscale Model (NMM) at 4-km resolution (nmm4), the University of Oklahoma’s Center for Analysis and Prediction of Storms (CAPS) Weather Research and Forecasting Model (WRF) at 2-km resolution (arw2), and the NCAR WRF at 4-km resolution (arw4). In the small demonstration sample herein, model forecast quality is efficiently differentiated when performance is assessed in terms of the CSI. In this sample, arw2 appears to outperform the other two model formulations across all scales when the cluster analysis is performed in the space of spatial coordinates and reflectivity. However, when the analysis is performed only on spatial data (i.e., when only the spatial placement of the reflectivity is assessed), the difference is not significant. This result has been verified both visually and using a standard gridpoint verification, and seems to provide a reasonable assessment of model performance. This demonstration of CCA indicates promise in quickly evaluating mesoscale model performance while avoiding the subjectivity and labor intensiveness of human evaluation or the pitfalls of non-object-oriented automated verification.},
	language = {EN},
	number = {3},
	urldate = {2023-10-20},
	journal = {Monthly Weather Review},
	author = {Marzban, Caren and Sandgathe, Scott},
	month = mar,
	year = {2008},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {1013--1025},
}

@article{marzban_verification_2009,
	title = {Verification with {Variograms}},
	volume = {24},
	issn = {1520-0434, 0882-8156},
	url = {https://journals.ametsoc.org/view/journals/wefo/24/4/2009waf2222122_1.xml},
	doi = {10.1175/2009WAF2222122.1},
	abstract = {Abstract The verification of a gridded forecast field, for example, one produced by numerical weather prediction (NWP) models, cannot be performed on a gridpoint-by-gridpoint basis; that type of approach would ignore the spatial structures present in both forecast and observation fields, leading to misinformative or noninformative verification results. A variety of methods have been proposed to acknowledge the spatial structure of the fields. Here, a method is examined that compares the two fields in terms of their variograms. Two types of variograms are examined: one examines correlation on different spatial scales and is a measure of texture; the other type of variogram is additionally sensitive to the size and location of objects in a field and can assess size and location errors. Using these variograms, the forecasts of three NWP model formulations are compared with observations/analysis, on a dataset consisting of 30 days in spring 2005. It is found that within statistical uncertainty the three formulations are comparable with one another in terms of forecasting the spatial structure of observed reflectivity fields. None, however, produce the observed structure across all scales, and all tend to overforecast the spatial extent and also forecast a smoother precipitation (reflectivity) field. A finer comparison suggests that the University of Oklahoma 2-km resolution Advanced Research Weather Research and Forecasting (WRF-ARW) model and the National Center for Atmospheric Research (NCAR) 4-km resolution WRF-ARW slightly outperform the 4.5-km WRF-Nonhydrostatic Mesoscale Model (NMM), developed by the National Oceanic and Atmospheric Administration/National Centers for Environmental Prediction (NOAA/NCEP), in terms of producing forecasts whose spatial structures are closer to that of the observed field.},
	language = {EN},
	number = {4},
	urldate = {2023-10-20},
	journal = {Weather and Forecasting},
	author = {Marzban, Caren and Sandgathe, Scott},
	month = aug,
	year = {2009},
	note = {Publisher: American Meteorological Society
Section: Weather and Forecasting},
	pages = {1102--1120},
}

@article{micheas_cell_2007,
	title = {Cell identification and verification of {QPF} ensembles using shape analysis techniques},
	volume = {343},
	issn = {0022-1694},
	url = {https://www.sciencedirect.com/science/article/pii/S0022169407002909},
	doi = {10.1016/j.jhydrol.2007.05.036},
	abstract = {This paper introduces a new verification technique designed for, but not limited to, quantitative precipitation forecasts. It is tested on, and examples are given for, an intercomparison of very short-period nowcasting schemes. One of these nowcasters is a Bayesian scheme that is used in an extensive ensemble formulation, and the verification scheme is uniquely capable of treating both the ensemble members and the mean forecast. The verification scheme uses Procrustes shape analysis methods that are well established in statistics but have not, to date, been applied to meteorological forecast assessment. The Procrustes methodology allows for a decomposition of the forecast error into any number of components such as location (displacement), shape, size, orientation and intensity. Each error component can be afforded a separate weighting such that a cost or value of the forecast can be calculated that accounts for different error types. For example, a forecaster who is concerned with the location of a storm would place greater emphasis on correct location in the forecast than other attributes. This ability to apply weights makes the system particularly suited to real-time verification applications where confidence in the performance of the forecast translates into improved dissemination to users. In addition, the decomposition of the error into parts enables diagnosis of the error sources that can lead to model adjustment and improvement.},
	number = {3},
	urldate = {2023-10-20},
	journal = {Journal of Hydrology},
	author = {Micheas, Athanasios Christou and Fox, Neil I. and Lack, Steven A. and Wikle, Christopher K.},
	month = sep,
	year = {2007},
	pages = {105--116},
}

@article{gilleland_computationally_2008,
	title = {Computationally {Efficient} {Spatial} {Forecast} {Verification} {Using} {Baddeley}’s {Delta} {Image} {Metric}},
	volume = {136},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/136/5/2007mwr2274.1.xml},
	doi = {10.1175/2007MWR2274.1},
	abstract = {Abstract An important focus of research in the forecast verification community is the development of alternative verification approaches for quantitative precipitation forecasts, as well as for other spatial forecasts. The need for information that is meaningful in an operational context and the importance of capturing the specific sources of forecast error at varying spatial scales are two primary motivating factors. In this paper, features of precipitation as identified by a convolution threshold technique are merged within fields and matched across fields in an automatic and computationally efficient manner using Baddeley’s metric for binary images. The method is carried out on 100 test cases, and 4 representative cases are shown in detail. Results of merging and matching objects are generally positive in that they are consistent with how a subjective observer might merge and match features. The results further suggest that the Baddeley metric may be useful as a computationally efficient summary metric giving information about location, shape, and size differences of individual features, which could be employed for other spatial forecast verification methods.},
	language = {EN},
	number = {5},
	urldate = {2023-10-20},
	journal = {Monthly Weather Review},
	author = {Gilleland, Eric and Lee, Thomas C. M. and Gotway, John Halley and Bullock, R. G. and Brown, Barbara G.},
	month = may,
	year = {2008},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {1747--1757},
}

@article{lemarie_simplified_2021,
	title = {A simplified atmospheric boundary layer model for an improved representation of air–sea interactions in eddying oceanic models: implementation and first evaluation in {NEMO} (4.0)},
	volume = {14},
	issn = {1991-959X},
	shorttitle = {A simplified atmospheric boundary layer model for an improved representation of air–sea interactions in eddying oceanic models},
	url = {https://gmd.copernicus.org/articles/14/543/2021/},
	doi = {10.5194/gmd-14-543-2021},
	abstract = {A simplified model of the atmospheric boundary layer (ABL) of intermediate complexity between a bulk parameterization and a three-dimensional atmospheric model is developed and integrated to the Nucleus for European Modelling of the Ocean (NEMO) general circulation model. An objective in the derivation of such a simplified model, called ABL1d, is to reach an apt representation in ocean-only numerical simulations of some of the key processes associated with air–sea interactions at the characteristic scales of the oceanic mesoscale. In this paper we describe the formulation of the ABL1d model and the strategy to constrain this model with large-scale atmospheric data available from reanalysis or real-time forecasts. A particular emphasis is on the appropriate choice and calibration of a turbulent closure scheme for the atmospheric boundary layer. This is a key ingredient to properly represent the air–sea interaction processes of interest. We also provide a detailed description of the NEMO-ABL1d coupling infrastructure and its computational efficiency. The resulting simplified model is then tested for several boundary-layer regimes relevant to either ocean–atmosphere or sea-ice–atmosphere coupling. The coupled system is also tested with a realistic 0.25∘ resolution global configuration. The numerical results are evaluated using standard metrics from the literature to quantify the wind–sea-surface-temperature (a.k.a. thermal feedback effect), wind–current (a.k.a. current feedback effect), and ABL–sea-ice couplings. With respect to these metrics, our results show very good agreement with observations and fully coupled ocean–atmosphere models for a computational overhead of about 9 \% in terms of elapsed time compared to standard uncoupled simulations. This moderate overhead, largely due to I/O operations, leaves room for further improvement to relax the assumption of horizontal homogeneity behind ABL1d and thus to further improve the realism of the coupling while keeping the flexibility of ocean-only modeling.},
	language = {English},
	number = {1},
	urldate = {2023-10-19},
	journal = {Geoscientific Model Development},
	author = {Lemarié, Florian and Samson, Guillaume and Redelsperger, Jean-Luc and Giordani, Hervé and Brivoal, Théo and Madec, Gurvan},
	month = jan,
	year = {2021},
	note = {Publisher: Copernicus GmbH},
	pages = {543--572},
}

@techreport{yang_improving_2023,
	type = {preprint},
	title = {Improving {Seasonal} {Forecast} of {Summer} {Precipitation} in {Southeastern} {China} using {CycleGAN} {Deep} {Learning} {Bias} {Correction}},
	url = {https://essopenarchive.org/users/655873/articles/661468-improving-seasonal-forecast-of-summer-precipitation-in-southeastern-china-using-cyclegan-deep-learning-bias-correction?commit=6fe5aad26bce582dd78270aa8ad3b99a0acc0f37},
	abstract = {Accurate seasonal precipitation forecasts, especially for extreme
events, are crucial to preventing meteorological hazards and its
potential impacts on national development, social stability, and
security. However, the intensity of summer precipitation is often
significantly underestimated in many current dynamical models. This
study uses a deep learning method called Cycle-Consistent Generative
Adversarial Networks (CycleGAN) to enhance the seasonal forecast skill
of the Nanjing University of Information Science \& Technology Climate
Forecast System (NUIST-CFS1.0) in predicting June-July-August
precipitation in southeastern China. The results suggest that the
CycleGAN-based model significantly improves the accuracy in predicting
the spatial-temporal distribution of summer precipitation than
traditional quantile mapping (QM) method. Due to the use of unpaired
day-to-day correction models, we can pay more attention to the
frequency, intensity, and duration of extreme precipitation events in
the climate dynamical model forecast. This study expands the potential
applications of deep learning models to improving seasonal precipitation
forecasts.},
	urldate = {2023-10-19},
	institution = {Preprints},
	author = {Yang, Song and Ling, Fenghua and Bai, Lei and Luo, Jing-Jia},
	month = aug,
	year = {2023},
	doi = {10.22541/essoar.169290554.48887852/v1},
}

@article{yang_improving_2023-1,
	title = {Improving {Seasonal} {Prediction} of {Summer} {Precipitation} in the {Middle}–{Lower} {Reaches} of the {Yangtze} {River} {Using} a {TU}-{Net} {Deep} {Learning} {Approach}},
	volume = {2},
	issn = {2769-7525},
	url = {https://journals.ametsoc.org/view/journals/aies/2/2/AIES-D-22-0078.1.xml},
	doi = {10.1175/AIES-D-22-0078.1},
	abstract = {Abstract The two-step U-Net model (TU-Net) contains a western North Pacific subtropical high (WNPSH) prediction model and a precipitation prediction model fed by the WNPSH predictions, oceanic heat content, and surface temperature. The data-driven forecast model provides improved 4-month lead predictions of the WNPSH and precipitation in the middle and lower reaches of the Yangtze River (MLYR), which has important implications for water resources management and precipitation-related disaster prevention in China. When compared with five state-of-the-art dynamical climate models including the Climate Forecast System of Nanjing University of Information Science and Technology (NUIST-CFS1.0) and four models participating in the North American Multi-Model Ensemble (NMME) project, the TU-Net produces comparable skills in forecasting 4-month lead geopotential height and winds at the 500- and 850-hPa levels. For the 4-month lead prediction of precipitation over the MLYR region, the TU-Net has the best correlation scores and mean latitude-weighted RMSE in each summer month and in boreal summer [June–August (JJA)], and pattern correlation coefficient scores are slightly lower than the dynamical models only in June and JJA. In addition, the results show that the constructed TU-Net is also superior to most of the dynamical models in predicting 2-m air temperature in the MLYR region at a 4-month lead. Thus, the deep learning-based TU-Net model can provide a rapid and inexpensive way to improve the seasonal prediction of summer precipitation and 2-m air temperature over the MLYR region. Significance Statement The purpose of this study is to examine the seasonal predictive skill of the western North Pacific subtropical high anomalies and summer rainfall anomalies over the middle and lower reaches of the Yangtze River region by means of deep learning methods. Our deep learning model provides a rapid and inexpensive way to improve the seasonal prediction of summer precipitation as well as 2-m air temperature. The work has important implications for water resources management and precipitation-related disaster prevention in China and can be extended in the future to predict other climate variables as well.},
	language = {EN},
	number = {2},
	urldate = {2023-10-19},
	journal = {Artificial Intelligence for the Earth Systems},
	author = {Yang, Shuxian and Ling, Fenghua and Li, Yue and Luo, Jing-Jia},
	month = jun,
	year = {2023},
	note = {Publisher: American Meteorological Society
Section: Artificial Intelligence for the Earth Systems},
}

@article{gilleland_new_2017,
	title = {A {New} {Characterization} within the {Spatial} {Verification} {Framework} for {False} {Alarms}, {Misses}, and {Overall} {Patterns}},
	volume = {32},
	issn = {1520-0434, 0882-8156},
	url = {https://journals.ametsoc.org/view/journals/wefo/32/1/waf-d-16-0134_1.xml},
	doi = {10.1175/WAF-D-16-0134.1},
	abstract = {Abstract This paper proposes new diagnostic plots that take advantage of the lack of symmetry in the mean-error distance measure (MED) for binary images to yield a new concept of false alarms and misses appropriate to the spatial setting where the measure does not require perfect matching to be a hit or correct negative. Additionally, three previously proposed geometric indices that provide complementary information about forecast performance are used to produce useful diagnostic plots for forecast performance. The diagnostics are applied to previously analyzed case studies from the spatial forecast verification Intercomparison Project (ICP) to facilitate a comparison with more complicated methods. Relatively new test cases from the Mesoscale Verification Intercomparison over Complex Terrain (MesoVICT) project are also employed for future comparisons. It is found that the proposed techniques provide useful information about forecast model behavior by way of a succinct, easy-to-implement method that can be complementary to other measures of forecast performance.},
	language = {EN},
	number = {1},
	urldate = {2023-10-19},
	journal = {Weather and Forecasting},
	author = {Gilleland, Eric},
	month = feb,
	year = {2017},
	note = {Publisher: American Meteorological Society
Section: Weather and Forecasting},
	pages = {187--198},
}

@article{skok_precipitation_2023,
	title = {Precipitation attribution distance},
	volume = {295},
	issn = {01698095},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169809523003952},
	doi = {10.1016/j.atmosres.2023.106998},
	abstract = {Precipitation is one of the most important meteorological parameters and is notoriously difficult to measure, predict, and also to verify. Distance measures are one of the five classes of spatial verification metrics that try to address the problems of traditionally used non-spatial methods (which only compare values at collocated grid points). Distance measures provide an estimate of spatial distance between the events in the two fields, which is relatively easy to interpret and understand. Our goal was to develop a new distance measure for precipitation that would not require thresholding and provide meaningful results that would not be too inconsistent with subjective forecast evaluation. At the same time, it would be computationally fast and numerically stable and could be used with fields provided on irregular grids. The new metric is called the Precipitation Attribution Distance or PAD and is based on a random nearest-neighbor attribution concept - it works by sequentially attributing randomly selected precipitation in one field to the closest precipitation in the other and is nonde­ terministic. We analyzed the new metric’s behavior in many idealized and real-world situations and compared it with the behavior of existing distance metrics.},
	language = {en},
	urldate = {2023-10-19},
	journal = {Atmospheric Research},
	author = {Skok, Gregor},
	month = nov,
	year = {2023},
	pages = {106998},
}

@misc{pathak_fourcastnet_2022,
	title = {{FourCastNet}: {A} {Global} {Data}-driven {High}-resolution {Weather} {Model} using {Adaptive} {Fourier} {Neural} {Operators}},
	shorttitle = {{FourCastNet}},
	url = {http://arxiv.org/abs/2202.11214},
	doi = {10.48550/arXiv.2202.11214},
	abstract = {FourCastNet, short for Fourier Forecasting Neural Network, is a global data-driven weather forecasting model that provides accurate short to medium-range global predictions at \$0.25{\textasciicircum}\{{\textbackslash}circ\}\$ resolution. FourCastNet accurately forecasts high-resolution, fast-timescale variables such as the surface wind speed, precipitation, and atmospheric water vapor. It has important implications for planning wind energy resources, predicting extreme weather events such as tropical cyclones, extra-tropical cyclones, and atmospheric rivers. FourCastNet matches the forecasting accuracy of the ECMWF Integrated Forecasting System (IFS), a state-of-the-art Numerical Weather Prediction (NWP) model, at short lead times for large-scale variables, while outperforming IFS for variables with complex fine-scale structure, including precipitation. FourCastNet generates a week-long forecast in less than 2 seconds, orders of magnitude faster than IFS. The speed of FourCastNet enables the creation of rapid and inexpensive large-ensemble forecasts with thousands of ensemble-members for improving probabilistic forecasting. We discuss how data-driven deep learning models such as FourCastNet are a valuable addition to the meteorology toolkit to aid and augment NWP models.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
	month = feb,
	year = {2022},
	note = {arXiv:2202.11214 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@misc{necker_fractions_2023,
	type = {preprint},
	title = {The fractions skill score for ensemble forecast verification},
	url = {https://www.authorea.com/users/650600/articles/659203-the-fractions-skill-score-for-ensemble-forecast-verification?commit=4bd54abb355e822fcb037ae51b827124fe7eb606},
	abstract = {The Fractions Skill Score (FSS) is a neighbourhood veriﬁcation method originally designed to verify deterministic forecasts of binary events. Previous studies employed diﬀerent approaches to computing an ensemble-based FSS for probabilistic forecast veriﬁcation. We show that the formulation of the ensemble-based FSS substantially aﬀects veriﬁcation results. Comparing four diﬀerent approaches, we determine how the ensemble-based FSS depends on ensemble size, neighbourhood size, and frequency of occurrence of the forecast event. Furthermore, we derive an empirical formula for the expected dependence of the FSS on ensemble size. Our results are based on high-resolution 1000-member ensemble precipitation forecasts over Germany for a high-impact weather period. The large ensemble enables us to study the inﬂuence of ensemble size on forecast skill in terms of a probabilistic skilful spatial scale. We demonstrate that only one form of ensemble-based FSS, which we refer to as probabilistic FSS, is well-behaved and exhibits a reasonable dependence on ensemble size.},
	language = {en},
	urldate = {2023-10-19},
	publisher = {Preprints},
	author = {Necker, Tobias and Wolfgruber, Ludwig and Kugler, Lukas and Weissmann, Martin and Dorninger, Manfred and Serafin, Stefano},
	month = aug,
	year = {2023},
	note = {Authorea},
}

@techreport{necker_fractions_2023-1,
	type = {preprint},
	title = {The fractions skill score for ensemble forecast verification},
	url = {https://www.authorea.com/users/650600/articles/659203-the-fractions-skill-score-for-ensemble-forecast-verification?commit=4bd54abb355e822fcb037ae51b827124fe7eb606},
	abstract = {The Fractions Skill Score (FSS) is a neighbourhood verification method
originally designed to verify deterministic forecasts of binary events.
Previous studies employed different approaches to computing an
ensemble-based FSS for probabilistic forecast verification. We show that
the formulation of the ensemble-based FSS substantially affects
verification results. Comparing four different approaches, we determine
how the ensemble-based FSS depends on ensemble size, neighbourhood size,
and frequency of occurrence of the forecast event. Furthermore, we
derive an empirical formula for the expected dependence of the FSS on
ensemble size. Our results are based on high-resolution 1000-member
ensemble precipitation forecasts over Germany for a high-impact weather
period. The large ensemble enables us to study the influence of ensemble
size on forecast skill in terms of a probabilistic skilful spatial
scale. We demonstrate that only one form of ensemble-based FSS, which we
refer to as probabilistic FSS, is well-behaved and exhibits a reasonable
dependence on ensemble size.},
	urldate = {2023-10-19},
	institution = {Preprints},
	author = {Necker, Tobias and Wolfgruber, Ludwig and Kugler, Lukas and Weissmann, Martin and Dorninger, Manfred and Serafin, Stefano},
	month = aug,
	year = {2023},
	doi = {10.22541/au.169169008.89657659/v1},
}

@article{deardorff_parameterization_1972,
	title = {Parameterization of the {Planetary} {Boundary} layer for {Use} in {General} {Circulation} {Models}},
	volume = {100},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/100/2/1520-0493_1972_100_0093_potpbl_2_3_co_2.xml},
	doi = {10.1175/1520-0493(1972)100<0093:POTPBL>2.3.CO;2},
	abstract = {Abstract The surface stress and fluxes of heat and moisture are parameterized for use in numerical models of the general circulation of the atmosphere. The parameterization is designed to be consistent with recent advances in knowledge of both the planetary boundary layer and the surface layer. A key quantity throughout is the height, h, of the planetary boundary layer, which appears in the governing stability parameter, a bulk Richardson number. With upward heat flux, a time-dependent prediction equation is proposed for h that incorporates penetrative convection and vertical motion. Under stable conditions, h is assumed to depart from the neutral value and to become nearly proportional to the Monin-Obukhov length. The roughness length, Zo, is incorporated in the combination h/zo, and the parameterization is consistent with h/zo affecting only the wind component in the direction of the surface velocity. The direction of the surface wind and stress is derived in a manner consistent with the known value of the surface pressure gradient and theoretical studies of the decrease of stress with height. The parameterization has been tested numerically and appears to be efficient enough to use in existing general circulation models.},
	language = {EN},
	number = {2},
	urldate = {2023-10-19},
	journal = {Monthly Weather Review},
	author = {Deardorff, James W.},
	month = feb,
	year = {1972},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {93--106},
}

@article{pelletier_two-sided_2021,
	title = {Two-sided turbulent surface-layer parameterizations for computing air–sea fluxes},
	volume = {147},
	copyright = {© 2021 Royal Meteorological Society},
	issn = {1477-870X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.3991},
	doi = {10.1002/qj.3991},
	abstract = {Standard methods for determining air–sea fluxes typically rely on bulk algorithms set in the frame of Monin–Obukhov similarity theory (MOST), using ocean surface fields and atmosphere near-surface fields. In the context of coupled ocean–atmosphere simulations, the shallowest ocean vertical level is usually used as bulk input and, by default, the turbulent closure is one-sided: it extrapolates atmosphere near-surface solution profiles (for wind speed, temperature, and humidity) to the prescribed ocean surface values. Using near-surface ocean fields as surface ones is equivalent to considering that, in the ocean surface layer, solution profiles are constant instead of also being determined by turbulent closure. Here we introduce a method for extending existing turbulent parameterizations to a two-sided framework by explicitly including the ocean surface layer within the aforementioned parameterizations. The formalism we use for this method is derived from that of classical turbulent closures, so that our novelties can easily be implemented within existing formulations. Special care is taken to ensure the smoothness of the resulting solution profiles. Other physical phenomena, such as the penetration of radiative fluxes in the ocean and the formation of waves, are then included within our formalism, and their effects are assessed. We also investigate the impact of such two-sided bulk formulations on air–sea fluxes evaluated from a setting similar to those of coupled ocean–atmosphere simulations.},
	language = {en},
	number = {736},
	urldate = {2023-10-19},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Pelletier, Charles and Lemarié, Florian and Blayo, Eric and Bouin, Marie-Noëlle and Redelsperger, Jean-Luc},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3991},
	keywords = {air–sea fluxes, bulk formulae, numerical methods, ocean surface layer, ocean–atmosphere coupling, turbulent parameterizations},
	pages = {1726--1751},
}

@misc{ecmwf_ifs_2023,
	type = {text},
	title = {{IFS} {Documentation} {CY48R1} - {Part} {IV}: {Physical} {Processes}},
	shorttitle = {{IFS} {Documentation} {CY48R1} - {Part} {IV}},
	url = {https://www.ecmwf.int/en/elibrary/81370-ifs-documentation-cy48r1-part-iv-physical-processes},
	abstract = {Chapter 1 Overview Chapter 2 Radiation Chapter 3 Turbulent transport and interactions with the surface Chapter 4 Subgrid-scale orographic drag Chapter 5 Non-orographic gravity wave drag Chapter 6 Convection Chapter 7 Clouds and large-scale precipitation Chapter 8 Surface parametrization Chapter 9 Methane oxidation Chapter 10 Ozone chemistry parametrization Chapter 11 Climatological data Chapter 12 Basic physical constants and thermodynamic functions},
	language = {en},
	urldate = {2023-10-19},
	journal = {ECMWF},
	author = {ECMWF},
	year = {2023},
}

@misc{hakim_dynamical_2023,
	title = {Dynamical {Tests} of a {Deep}-{Learning} {Weather} {Prediction} {Model}},
	url = {http://arxiv.org/abs/2309.10867},
	abstract = {Global deep-learning weather prediction models have recently been shown to produce forecasts that rival those from physics-based models run at operational centers. It is unclear whether these models have encoded atmospheric dynamics, or simply pattern matching that produces the smallest forecast error. Answering this question is crucial to establishing the utility of these models as tools for basic science. Here we subject one such model, Pangu-weather, to a set of four classical dynamical experiments that do not resemble the model training data. Localized perturbations to the model output and the initial conditions are added to steady time-averaged conditions, to assess the propagation speed and structural evolution of signals away from the local source. Perturbing the model physics by adding a steady tropical heat source results in a classical Matsuno--Gill response near the heating, and planetary waves that radiate into the extratropics. A localized disturbance on the winter-averaged North Pacific jet stream produces realistic extratropical cyclones and fronts, including the spontaneous emergence of polar lows. Perturbing the 500hPa height field alone yields adjustment from a state of rest to one of wind--pressure balance over {\textasciitilde}6 hours. Localized subtropical low pressure systems produce Atlantic hurricanes, provided the initial amplitude exceeds about 5 hPa, and setting the initial humidity to zero eliminates hurricane development. We conclude that the model encodes realistic physics in all experiments, and suggest it can be used as a tool for rapidly testing ideas before using expensive physics-based models.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Hakim, Gregory J. and Masanam, Sanjit},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10867 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@article{cooper_potential_2017,
	title = {A potential method to accelerate spin up of turbulent ocean models},
	volume = {120},
	issn = {1463-5003},
	url = {https://www.sciencedirect.com/science/article/pii/S1463500317301531},
	doi = {10.1016/j.ocemod.2017.10.008},
	abstract = {We demonstrate a simple method to quickly spin up (equilibrate) the temperature field of a turbulent idealised primitive equation ocean gyre model. We make use of the assumption that both velocity fields and non-linear eddy advection terms equilibrate on a shorter timescale than tracer fields.},
	urldate = {2023-10-17},
	journal = {Ocean Modelling},
	author = {Cooper, Fenwick C.},
	month = dec,
	year = {2017},
	keywords = {Equilibrate, Primitive equations, Spin-up, Temperature, Tracer},
	pages = {79--82},
}

@article{noauthor_technical_nodate,
	title = {Technical {Memorandum}},
	abstract = {This paper discusses the development and evaluation process followed at ECMWF to upgrade the Integrated Forecasting System (IFS), and illustrates how potential changes, developed and tested by individual scientists, are gradually merged and evaluated prior to their acceptance into the next version of the ECMWF IFS.},
	language = {en},
}

@book{sarkka_applied_2019,
	edition = {1},
	title = {Applied {Stochastic} {Differential} {Equations}},
	isbn = {978-1-108-18673-5 978-1-316-51008-7 978-1-316-64946-6},
	url = {https://www.cambridge.org/core/product/identifier/9781108186735/type/book},
	language = {en},
	urldate = {2023-10-10},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo and Solin, Arno},
	month = apr,
	year = {2019},
	doi = {10.1017/9781108186735},
}

@article{evans_introduction_nodate,
	title = {{AN} {INTRODUCTION} {TO} {STOCHASTIC} {DIFFERENTIAL} {EQUATIONS} {VERSION} 1.2},
	language = {en},
	author = {Evans, Lawrence C},
}

@article{penland_modelling_2008,
	title = {On modelling physical systems with stochastic models: diffusion versus {Lévy} processes},
	volume = {366},
	shorttitle = {On modelling physical systems with stochastic models},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2008.0051},
	doi = {10.1098/rsta.2008.0051},
	abstract = {Stochastic descriptions of multiscale interactions are more and more frequently found in numerical models of weather and climate. These descriptions are often made in terms of differential equations with random forcing components. In this article, we review the basic properties of stochastic differential equations driven by classical Gaussian white noise and compare with systems described by stable Lévy processes. We also discuss aspects of numerically generating these processes.},
	number = {1875},
	urldate = {2023-10-09},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Penland, Cécile and Ewald, Brian D},
	month = may,
	year = {2008},
	note = {Publisher: Royal Society},
	keywords = {Gaussian processes, Lévy processes, numerical methods, stochastic differential equations},
	pages = {2455--2474},
}

@incollection{chen_nino_2019,
	title = {El {Niño} and the {Southern} {Oscillation}: {Theory}},
	isbn = {978-0-12-409548-9},
	shorttitle = {El {Niño} and the {Southern} {Oscillation}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780124095489117658},
	abstract = {The El Niño Southern Oscillation (ENSO) is the most prominent interannual climate variation on Earth with large ecological and societal impacts. The ENSO results from a strong dynamical coupling between the equatorial Pacific ocean and the overlying atmosphere. This article discusses the ENSO phenomenon and the theory for its existence. A hierarchy of ENSO models, from simple linear oscillators to intermediate models and from deterministic to stochastic ones, are summarized to describe the ENSO features. These models are also utilized to emphasize the triggers, inhibitors, and nonlinearities in the ENSO. In addition, the ENSO involves various remarkable multiscale and nonlinear features that greatly increase its overall spatial and temporal complexity. The theories of ENSO asymmetry, seasonality and diversity are also discussed in this article.},
	urldate = {2023-10-09},
	booktitle = {Reference {Module} in {Earth} {Systems} and {Environmental} {Sciences}},
	publisher = {Elsevier},
	author = {Chen, Nan and Thual, Sulian and Stuecker, Malte F.},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-12-409548-9.11765-8},
	keywords = {Bjerknes feedback, Central Pacific (CP) El Niño, Discharge-recharge oscillator model, ENSO, ENSO asymmetry, ENSO diversity, Niño index, Seasonal synchronization, Wind bursts, Zebiak-Cane model, Zonal advection},
}

@article{vallis_mechanisms_nodate,
	title = {Mechanisms of {Climate} {Variability} from {Years} to {Decades}},
	abstract = {This paper discusses and reviews some of the mechanisms that may be responsible for climate variability on yearly to decadal timescales. The discussion is organized around a set of mechanisms that primarily involve the atmosphere, the ocean, or the coupling between the two. We choose an example of each, try to explain what the underlying mechanism is, and set it in the context of climate variability as a whole. All of the mechanisms are in principle deterministic, although in at least one of them we do not care about the details of the process that give rise to the variability and in that case a stochastic description may be the most economical and insightful.},
	language = {en},
	author = {Vallis, Geoffrey K},
}

@misc{kristian_mogensen_coupling_2012,
	type = {text},
	title = {Coupling of the {NEMO} and {IFS} models in a single executable},
	url = {https://www.ecmwf.int/en/elibrary/75709-coupling-nemo-and-ifs-models-single-executable},
	abstract = {A new way of coupling the Integrated Forecasting System (IFS) atmosphere model and the Nucleus for European Modelling of the Ocean (NEMO) ocean model based on an integrated single executable system with a common time step loop is presented. Details of the technical implementation are presented to give a fairly complete overview of how the IFS and the NEMO models interact in the single executable system. Initial technical performance testing is showing similar or better performance than the existing OASIS3 based system and potential for coupling at high resolution. Some potential improvements to the new system is discussed.},
	language = {eng},
	urldate = {2023-10-09},
	journal = {ECMWF},
	author = {Kristian Mogensen, Sarah Keeley},
	year = {2012},
}

@misc{mardani_generative_2023,
	title = {Generative {Residual} {Diffusion} {Modeling} for {Km}-scale {Atmospheric} {Downscaling}},
	url = {http://arxiv.org/abs/2309.15214},
	doi = {10.48550/arXiv.2309.15214},
	abstract = {The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach ({\textbackslash}textit\{ResDiff\}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. {\textbackslash}textit\{ResDiff\} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phenomena reveal appropriate multivariate relationships reminiscent of learnt physics. This includes the sharp wind and temperature variations that co-locate with intense rainfall in a cold front, and the extreme winds and rainfall bands that surround the eyewall of typhoons. Some evidence of simultaneous bias correction is found. A first attempt at downscaling directly from an operational global forecast model successfully retains many of these benefits. The implication is that a new era of fully end-to-end, global-to-regional machine learning weather prediction is likely near at hand.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Mardani, Morteza and Brenowitz, Noah and Cohen, Yair and Pathak, Jaideep and Chen, Chieh-Yu and Liu, Cheng-Chin and Vahdat, Arash and Kashinath, Karthik and Kautz, Jan and Pritchard, Mike},
	month = sep,
	year = {2023},
	note = {arXiv:2309.15214 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@misc{mangeleer_robust_2023,
	title = {Robust {Ocean} {Subgrid}-{Scale} {Parameterizations} {Using} {Fourier} {Neural} {Operators}},
	url = {http://arxiv.org/abs/2310.02691},
	doi = {10.48550/arXiv.2310.02691},
	abstract = {In climate simulations, small-scale processes shape ocean dynamics but remain computationally expensive to resolve directly. For this reason, their contributions are commonly approximated using empirical parameterizations, which lead to significant errors in long-term projections. In this work, we develop parameterizations based on Fourier Neural Operators, showcasing their accuracy and generalizability in comparison to other approaches. Finally, we discuss the potential and limitations of neural networks operating in the frequency domain, paving the way for future investigation.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Mangeleer, Victor and Louppe, Gilles},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02691 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@article{arcomano_hybrid_2023,
	title = {A {Hybrid} {Atmospheric} {Model} {Incorporating} {Machine} {Learning} {Can} {Capture} {Dynamical} {Processes} {Not} {Captured} by {Its} {Physics}-{Based} {Component}},
	volume = {50},
	copyright = {© 2023 The Authors.},
	issn = {1944-8007},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022GL102649},
	doi = {10.1029/2022GL102649},
	abstract = {It is shown that a recently developed hybrid modeling approach that combines machine learning (ML) with an atmospheric global circulation model (AGCM) can serve as a basis for capturing atmospheric processes not captured by the AGCM. This power of the approach is illustrated by three examples from a decades-long climate simulation experiment. The first example demonstrates that the hybrid model can produce sudden stratospheric warming, a dynamical process of nature not resolved by the low resolution AGCM component of the hybrid model. The second and third example show that introducing 6-hr cumulative precipitation and sea surface temperature (SST) as ML-based prognostic variables improves the precipitation climatology and leads to a realistic ENSO signal in the SST and atmospheric surface pressure.},
	language = {en},
	number = {8},
	urldate = {2023-10-06},
	journal = {Geophysical Research Letters},
	author = {Arcomano, Troy and Szunyogh, Istvan and Wikner, Alexander and Hunt, Brian R. and Ott, Edward},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2022GL102649},
	keywords = {climate, machine learning, numerical weather prediction},
	pages = {e2022GL102649},
}

@misc{mangeleer_robust_2023-1,
	title = {Robust {Ocean} {Subgrid}-{Scale} {Parameterizations} {Using} {Fourier} {Neural} {Operators}},
	url = {http://arxiv.org/abs/2310.02691},
	doi = {10.48550/arXiv.2310.02691},
	abstract = {In climate simulations, small-scale processes shape ocean dynamics but remain computationally expensive to resolve directly. For this reason, their contributions are commonly approximated using empirical parameterizations, which lead to significant errors in long-term projections. In this work, we develop parameterizations based on Fourier Neural Operators, showcasing their accuracy and generalizability in comparison to other approaches. Finally, we discuss the potential and limitations of neural networks operating in the frequency domain, paving the way for future investigation.},
	urldate = {2023-10-05},
	publisher = {arXiv},
	author = {Mangeleer, Victor and Louppe, Gilles},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02691 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@incollection{imkeller_hasselmanns_2001,
	address = {Basel},
	title = {Hasselmann’s program revisited: the analysis of stochasticity in deterministic climate models},
	isbn = {978-3-0348-9504-0 978-3-0348-8287-3},
	shorttitle = {Hasselmann’s program revisited},
	url = {http://link.springer.com/10.1007/978-3-0348-8287-3_5},
	abstract = {In his seminal 1976 paper on {\textbackslash}Stochastic Climate Models", K. Hasselmann proposed to improve deterministic models for the {\textbackslash}climate" (slow variables) by incorporating the in uence of the {\textbackslash}weather" (fast variables) in the form of random noise.},
	language = {en},
	urldate = {2023-10-05},
	booktitle = {Stochastic {Climate} {Models}},
	publisher = {Birkhäuser Basel},
	author = {Arnold, Ludwig},
	editor = {Imkeller, Peter and Von Storch, Jin-Song},
	year = {2001},
	doi = {10.1007/978-3-0348-8287-3_5},
	pages = {141--157},
}

@article{clement_atlantic_2015,
	title = {The {Atlantic} {Multidecadal} {Oscillation} without a role for ocean circulation},
	volume = {350},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aab3980},
	doi = {10.1126/science.aab3980},
	abstract = {Ocean circulation changes not needed
            
              What causes the pattern of sea surface temperature change that is seen in the North Atlantic Ocean? This naturally occurring quasi-cyclical variation, known as the Atlantic Multidecadal Oscillation (AMO), affects weather and climate. Some have suggested that the AMO is a consequence of variable large-scale ocean circulation. Clement
              et al.
              suggest otherwise. They find that the pattern of AMO variability can be produced in a model that does not include ocean circulation changes, but only the effects of changes in air temperatures and winds.
            
            
              Science
              , this issue p.
              320
            
          , 
            The Atlantic Multidecadal Oscillation does not depend on variable whole-ocean circulation.
          , 
            The Atlantic Multidecadal Oscillation (AMO) is a major mode of climate variability with important societal impacts. Most previous explanations identify the driver of the AMO as the ocean circulation, specifically the Atlantic Meridional Overturning Circulation (AMOC). Here we show that the main features of the observed AMO are reproduced in models where the ocean heat transport is prescribed and thus cannot be the driver. Allowing the ocean circulation to interact with the atmosphere does not significantly alter the characteristics of the AMO in the current generation of climate models. These results suggest that the AMO is the response to stochastic forcing from the mid-latitude atmospheric circulation, with thermal coupling playing a role in the tropics. In this view, the AMOC and other ocean circulation changes would be largely a response to, not a cause of, the AMO.},
	language = {en},
	number = {6258},
	urldate = {2023-10-04},
	journal = {Science},
	author = {Clement, Amy and Bellomo, Katinka and Murphy, Lisa N. and Cane, Mark A. and Mauritsen, Thorsten and Rädel, Gaby and Stevens, Bjorn},
	month = oct,
	year = {2015},
	pages = {320--324},
}

@article{hasselmann_pips_1988,
	title = {{PIPs} and {POPs}: {The} reduction of complex dynamical systems using principal interaction and oscillation patterns},
	volume = {93},
	copyright = {Copyright 1988 by the American Geophysical Union.},
	issn = {2156-2202},
	shorttitle = {{PIPs} and {POPs}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/JD093iD09p11015},
	doi = {10.1029/JD093iD09p11015},
	abstract = {A general method is described for constructing simple dynamical models to approximate complex dynamical systems with many degrees of freedom. The technique can be applied to interpret sets of observed time series or numerical simulations with high-resolution models, or to relate observation and simulations. The method is based on a projection of the complete system on to a smaller number of “principal interaction patterns” (PIPs). The coefficients of the PIP expansion are assumed to be governed by a dynamic model containing a small number of adjustable parameters. The optimization of the dynamical model, which in the general case can be both nonlinear and time-dependent, is carried out simultaneously with the construction of the optimal set of interaction patterns. In the linear case the PIPs reduce to the eigenoscilations of a first-order linear vector process with stochastic forcing (principal oscillation patterns, or POPs). POPs are linearly related to the “principal prediction patterns” used in linear forecasting applications. The POP analysis can also be applied as a diagnostic tool to compress the extensive information contained in the high-dimensional cross-spectral covariance matrix representing the complete second-moment structure of the system.},
	language = {en},
	number = {D9},
	urldate = {2023-10-04},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Hasselmann, K.},
	year = {1988},
	note = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/JD093iD09p11015},
	pages = {11015--11021},
}

@article{madec_nemo_2023,
	title = {{NEMO} {Ocean} {Engine} {Reference} {Manual}},
	url = {https://zenodo.org/record/8167700},
	doi = {10.5281/zenodo.8167700},
	abstract = {The ocean engine of NEMO is a primitive equation model adapted to regional and global ocean circulation problems. It is intended to be a flexible tool for studying the ocean and its interactions with the others components of the earth climate system over a wide range of space and time scales. To cite this edition: Madec, G. and the NEMO System Team, 2023. NEMO Ocean Engine Reference Manual, Zenodo, https://doi.org/10.5281/zenodo.8167700},
	language = {eng},
	urldate = {2023-10-04},
	author = {Madec, Gurvan and Bell, Mike and Blaker, Adam and Bricaud, Clément and Bruciaferri, Diego and Castrillo, Miguel and Calvert, Daley and Chanut, Jérômeme and Clementi, Emanuela and Coward, Andrew and Epicoco, Italo and Éthé, Christian and Ganderton, Jonas and Harle, James and Hutchinson, Katherine and Iovino, Doroteaciro and Lea, Dan and Lovato, Tomas and Martin, Matt and Martin, Nicolas and Mele, Francesca and Martins, Diana and Masson, Sébastien and Mathiot, Pierre and Mele, Francesca and Mocavero, Silvia and Müller, Simon and Nurser, A. J. George and Paronuzzi, Stella and Peltier, Mathieu and Person, Renaud and Rousset, Clement and Rynders, Stefanie and Samson, Guillaume and Téchené, Sibylle and Vancoppenolle, Martin and Wilson, Chris},
	month = jul,
	year = {2023},
	note = {Publisher: Zenodo},
	keywords = {modelling, nemo-ocean, ocean, ocean-modelling},
}

@misc{hakim_dynamical_2023-1,
	title = {Dynamical {Tests} of a {Deep}-{Learning} {Weather} {Prediction} {Model}},
	url = {http://arxiv.org/abs/2309.10867},
	abstract = {Global deep-learning weather prediction models have recently been shown to produce forecasts that rival those from physics-based models run at operational centers. It is unclear whether these models have encoded atmospheric dynamics, or simply pattern matching that produces the smallest forecast error. Answering this question is crucial to establishing the utility of these models as tools for basic science. Here we subject one such model, Pangu-weather, to a set of four classical dynamical experiments that do not resemble the model training data. Localized perturbations to the model output and the initial conditions are added to steady time-averaged conditions, to assess the propagation speed and structural evolution of signals away from the local source. Perturbing the model physics by adding a steady tropical heat source results in a classical Matsuno--Gill response near the heating, and planetary waves that radiate into the extratropics. A localized disturbance on the winter-averaged North Pacific jet stream produces realistic extratropical cyclones and fronts, including the spontaneous emergence of polar lows. Perturbing the 500hPa height field alone yields adjustment from a state of rest to one of wind--pressure balance over {\textasciitilde}6 hours. Localized subtropical low pressure systems produce Atlantic hurricanes, provided the initial amplitude exceeds about 5 hPa, and setting the initial humidity to zero eliminates hurricane development. We conclude that the model encodes realistic physics in all experiments, and suggest it can be used as a tool for rapidly testing ideas before using expensive physics-based models.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Hakim, Gregory J. and Masanam, Sanjit},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10867 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@article{muzy_multifractal_1993,
	title = {Multifractal formalism for fractal signals: {The} structure-function approach versus the wavelet-transform modulus-maxima method},
	volume = {47},
	issn = {1063-651X, 1095-3787},
	shorttitle = {Multifractal formalism for fractal signals},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.47.875},
	doi = {10.1103/PhysRevE.47.875},
	language = {en},
	number = {2},
	urldate = {2023-10-04},
	journal = {Physical Review E},
	author = {Muzy, J. F. and Bacry, E. and Arneodo, A.},
	month = feb,
	year = {1993},
	pages = {875--884},
}

@article{franzke_stochastic_2022,
	title = {Stochastic {Methods} and {Complexity} {Science} in {Climate} {Research} and {Modeling}},
	volume = {10},
	issn = {2296-424X},
	url = {https://www.frontiersin.org/articles/10.3389/fphy.2022.931596},
	abstract = {The 2021 Nobel prize for physics was awarded to two climate scientists, Syukuro Manabe and Klaus Hasselmann, and the physicist Giorgio Parisi. While at first sight the work of Parisi seems not to be related to climate science, this is not the case. Giorgio Parisi developed and contributed to many complexity science methods which are nowadays widely used in climate science. Giorgi Parisi also was involved in the development of the “stochastic resonance” idea to explain paleoclimate variability, while Klaus Hasselmann developed stochastic climate models. Here we review and discuss their work from a complex and stochastic systems perspective in order to highlight those aspects of their work. For instance, fractal and multi-fractal analysis of climate data is now widely used and many weather prediction and climate models contain stochastic parameterizations, topics Parisi and Hasselmann have pioneered. Furthermore, Manabe’s work was key to understanding the effects of anthropogenic climate change by the development of key advances in the parameterization of convection and radiative forcing in climate models. We discuss also how their inventive research has shaped current climate research and is still influencing climate modeling and future research directions.},
	urldate = {2023-10-04},
	journal = {Frontiers in Physics},
	author = {Franzke, Christian L. E. and Blender, Richard and O’Kane, Terence J. and Lembo, Valerio},
	year = {2022},
}

@article{acosta_balancing_2023,
	title = {Balancing {EC}-{Earth3} {Improving} the {Performance} of {EC}-{Earth} {CMIP6} {Configurations} by {Minimizing} the {Coupling} {Cost}},
	volume = {10},
	copyright = {© 2023 The Authors.},
	issn = {2333-5084},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2023EA002912},
	doi = {10.1029/2023EA002912},
	abstract = {Earth System Models (ESMs) are complex systems used in weather and climate studies generally built from different independent components responsible for simulating a specific realm (ocean, atmosphere, biosphere, etc.). To replicate the interactions between these processes, ESMs typically use coupling libraries that manage the synchronization and field exchanges between the individual components, which run in parallel as a Multi-Program, Multiple-Data application. As ESMs get more complex (increase in resolution, number of components, configurations, etc.), achieving the best performance when running in High-performance Computing platforms has become increasingly challenging and of major concern. One of the critical bottlenecks is the load-imbalance, where the fastest components will have to wait for the slower ones. Finding the optimal number of processing elements to assign to each of the multiple independent constituents to minimize the performance loss due to synchronizations and maximize the overall parallel efficiency is impossible without the right performance metrics, methodology, and tools. This paper presents the results of balancing multiple Coupled Model Intercomparison Project phase 6 configurations for the EC-Earth3 ESM. We will show that intuitive approaches can lead to suboptimal resource allocations and propose new setups up to 25\% fasters while reducing the computational cost by 72\%. We prove that new methods are needed to deal with the load-balance of ESMs and hope that our study will serve as a guide to optimize any other coupled system.},
	language = {en},
	number = {8},
	urldate = {2023-10-03},
	journal = {Earth and Space Science},
	author = {Acosta, M. C. and Palomas, S. and Tourigny, E.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2023EA002912},
	keywords = {CMIP6, EC-Earth, ESMs, HPC, performance},
	pages = {e2023EA002912},
}

@article{eisenman_westerly_2005,
	title = {Westerly {Wind} {Bursts}: {ENSO}’s {Tail} {Rather} than the {Dog}?},
	volume = {18},
	issn = {0894-8755, 1520-0442},
	shorttitle = {Westerly {Wind} {Bursts}},
	url = {https://journals.ametsoc.org/view/journals/clim/18/24/jcli3588.1.xml},
	doi = {10.1175/JCLI3588.1},
	abstract = {Abstract Westerly wind bursts (WWBs) in the equatorial Pacific occur during the development of most El Niño events and are believed to be a major factor in ENSO’s dynamics. Because of their short time scale, WWBs are normally considered part of a stochastic forcing of ENSO, completely external to the interannual ENSO variability. Recent observational studies, however, suggest that the occurrence and characteristics of WWBs may depend to some extent on the state of ENSO components, implying that WWBs, which force ENSO, are modulated by ENSO itself. Satellite and in situ observations are used here to show that WWBs are significantly more likely to occur when the warm pool is extended eastward. Based on these observations, WWBs are added to an intermediate complexity coupled ocean–atmosphere ENSO model. The representation of WWBs is idealized such that their occurrence is modulated by the warm pool extent. The resulting model run is compared with a run in which the WWBs are stochastically applied. The modulation of WWBs by ENSO results in an enhancement of the slow frequency component of the WWBs. This causes the amplitude of ENSO events forced by modulated WWBs to be twice as large as the amplitude of ENSO events forced by stochastic WWBs with the same amplitude and average frequency. Based on this result, it is suggested that the modulation of WWBs by the equatorial Pacific SST is a critical element of ENSO’s dynamics, and that WWBs should not be regarded as purely stochastic forcing. In the paradigm proposed here, WWBs are still an important aspect of ENSO’s dynamics, but they are treated as being partially stochastic and partially affected by the large-scale ENSO dynamics, rather than being completely external to ENSO. It is further shown that WWB modulation by the large-scale equatorial SST field is roughly equivalent to an increase in the ocean–atmosphere coupling strength, making the coupled equatorial Pacific effectively self-sustained.},
	language = {EN},
	number = {24},
	urldate = {2023-10-03},
	journal = {Journal of Climate},
	author = {Eisenman, Ian and Yu, Lisan and Tziperman, Eli},
	month = dec,
	year = {2005},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {5224--5238},
}

@article{neelin_enso_1998,
	title = {{ENSO} theory},
	volume = {103},
	copyright = {Copyright 1998 by the American Geophysical Union.},
	issn = {2156-2202},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/97JC03424},
	doi = {10.1029/97JC03424},
	abstract = {Beginning from the hypothesis by Bjerknes [1969] that ocean-atmosphere interaction was essential to the El Niño-Southern Oscillation (ENSO) phenomenon, the Tropical Ocean-Global Atmosphere (TOGA) decade has not only confirmed this but has supplied detailed theory for mechanisms setting the underlying period and possible mechanisms responsible for the irregularity of ENSO. Essentials of the theory of ocean dynamical adjustment are reviewed from an ENSO perspective. Approaches to simple atmospheric modeling greatly aided development of theory for ENSO atmospheric feedbacks but are critically reviewed for current stumbling blocks for applications beyond ENSO. ENSO theory has benefitted from an unusually complete hierarchy of coupled models of various levels of complexity. Most of the progress during the ENSO decade came from models of intermediate complexity, which are sufficiently detailed to compare to observations and to use in prediction but are less complex than coupled general circulation models. ENSO theory in simple models lagged behind ENSO simulation in intermediate models but has provided a useful role in uniting seemingly diverse viewpoints. The process of boiling ENSO theory down to a single consensus model of all aspects of the phenomenon is still a rapidly progressing area, and theoretical limits to ENSO predictability are still in debate, but a thorough foundation for the discussion has been established in the TOGA decade.},
	language = {en},
	number = {C7},
	urldate = {2023-10-03},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Neelin, J. David and Battisti, David S. and Hirst, Anthony C. and Jin, Fei-Fei and Wakata, Yoshinobu and Yamagata, Toshio and Zebiak, Stephen E.},
	year = {1998},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/97JC03424},
	pages = {14261--14290},
}

@article{hasselmann_stochastic_1976,
	title = {Stochastic climate models {Part} {I}. {Theory}},
	volume = {28},
	copyright = {1976 Blackwell Munksgaard},
	issn = {2153-3490},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2153-3490.1976.tb00696.x},
	doi = {10.1111/j.2153-3490.1976.tb00696.x},
	abstract = {A stochastic model of climate variability is considered in which slow changes of climate are explained as the integral response to continuous random excitation by short period “weather” disturbances. The coupled ocean-atmosphere-cryosphere-land system is divided into a rapidly varying “weather” system (essentially the atmosphere) and a slowly responding “climate” system (the ocean, cryosphere, land vegetation, etc.). In the usual Statistical Dynamical Model (SDM) only the average transport effects of the rapidly varying weather components are parameterised in the climate system. The resultant prognostic equations are deterministic, and climate variability can normally arise only through variable external conditions. The essential feature of stochastic climate models is that the non-averaged “weather” components are also retained. They appear formally as random forcing terms. The climate system, acting as an integrator of this short-period excitation, exhibits the same random-walk response characteristics as large particles interacting with an ensemble of much smaller particles in the analogous Brownian motion problem. The model predicts “red” variance spectra, in qualitative agreement with observations. The evolution of the climate probability distribution is described by a Fokker-Planck equation, in which the effect of the random weather excitation is represented by diffusion terms. Without stabilising feedback, the model predicts a continuous increase in climate variability, in analogy with the continuous, unbounded dispersion of particles in Brownian motion (or in a homogeneous turbulent fluid). Stabilising feedback yields a statistically stationary climate probability distribution. Feedback also results in a finite degree of climate predictability, but for a stationary climate the predictability is limited to maximal skill parameters of order 0.5.},
	language = {en},
	number = {6},
	urldate = {2023-10-03},
	journal = {Tellus},
	author = {Hasselmann, K.},
	year = {1976},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2153-3490.1976.tb00696.x},
	pages = {473--485},
}

@article{roberts_climate_2018,
	title = {Climate model configurations of the {ECMWF} {Integrated} {Forecasting} {System} ({ECMWF}-{IFS} cycle 43r1) for {HighResMIP}},
	volume = {11},
	issn = {1991-959X},
	url = {https://gmd.copernicus.org/articles/11/3681/2018/},
	doi = {10.5194/gmd-11-3681-2018},
	abstract = {This paper presents atmosphere-only and coupled climate model configurations of the European Centre for Medium-Range Weather Forecasts Integrated Forecasting System (ECMWF-IFS) for different combinations of ocean and atmosphere resolution. These configurations are used to perform multi-decadal ensemble experiments following the protocols of the High Resolution Model Intercomparison Project (HighResMIP) and phase 6 of the Coupled Model Intercomparison Project (CMIP6). These experiments are used to evaluate the sensitivity of major biases in the atmosphere, ocean, and cryosphere to changes in atmosphere and ocean resolution. All configurations successfully reproduce the observed long-term trends in global mean surface temperature. Furthermore, following an adjustment to account for drift in the subsurface ocean, coupled configurations of ECMWF-IFS realistically reproduce observation-based estimates of ocean heat content change since 1950. Climatological surface biases in ECMWF-IFS are relatively insensitive to an increase in atmospheric resolution from ∼ 50 to ∼ 25\&thinsp;km. However, increasing the horizontal resolution of the atmosphere while maintaining the same vertical resolution enhances the magnitude of a cold bias in the lower stratosphere. In coupled configurations, there is a strong sensitivity to an increase in ocean model resolution from 1 to 0.25°. However, this sensitivity to ocean resolution takes many years to fully manifest and is less apparent in the first year of integration. This result has implications for the ECMWF coupled model development strategy that typically relies on the analysis of biases in short ( \&lt; 1 year) ensemble (re)forecast data sets. The impacts of increased ocean resolution are particularly evident in the North Atlantic and Arctic, where they are associated with an improved Atlantic meridional overturning circulation, increased meridional ocean heat transport, and more realistic sea-ice cover. In the tropical Pacific, increased ocean resolution is associated with improvements to the magnitude and asymmetry of El Niño–Southern Oscillation (ENSO) variability and better representation of non-linear sea surface temperature (SST)–radiation feedbacks during warm events. However, increased ocean model resolution also increases the magnitude of a warm bias in the Southern Ocean. Finally, there is tentative evidence that both ocean coupling and increased atmospheric resolution can improve teleconnections between tropical Pacific rainfall and geopotential height anomalies in the North Atlantic.},
	language = {English},
	number = {9},
	urldate = {2023-10-02},
	journal = {Geoscientific Model Development},
	author = {Roberts, Christopher D. and Senan, Retish and Molteni, Franco and Boussetta, Souhail and Mayer, Michael and Keeley, Sarah P. E.},
	month = sep,
	year = {2018},
	note = {Publisher: Copernicus GmbH},
	pages = {3681--3712},
}

@inproceedings{antonio_post-processing_2023,
	title = {Post-processing {East} {African} precipitation forecasts using a generative machine learning model},
	url = {https://warwick.ac.uk/fac/sci/statistics/news/fsds/},
	urldate = {2023-09-25},
	booktitle = {{CRiSM}  {Fusing} {Simulations} with {Data} {Science}},
	author = {Antonio, Bobby and McRae, Andrew and MacLeod, Dave and Cooper, Fenwick and Marsham, John and Aitchison, Laurence and Palmer, Tim and Watson, Peter},
	month = jul,
	year = {2023},
}

@inproceedings{antonio_post-processing_2023-1,
	title = {Post-processing {East} {African} precipitation forecasts using a generative machine learning model},
	shorttitle = {{CI2023}},
	url = {https://cambridge-iccs.github.io/climate-informatics-2023/fulldetails.html},
	language = {en},
	urldate = {2023-09-25},
	booktitle = {12th {International} {Conference} on {Climate} {Informatics}, {University} of {Cambridge}},
	author = {Antonio, Bobby and McRae, Andrew and MacLeod, Dave and Cooper, Fenwick and Marsham, John and Aitchison, Laurence and Palmer, Tim and Watson, Peter},
	month = apr,
	year = {2023},
	note = {https://cambridge-iccs.github.io/climate-informatics-2023/fulldetails.html},
}

@inproceedings{antonio_improving_2023,
	title = {Improving post-processing of {East} {African} precipitation forecasts using a generative machine learning model},
	url = {https://meetingorganizer.copernicus.org/EGU23/EGU23-1365.html},
	language = {en},
	urldate = {2023-09-25},
	booktitle = {{EGU23} {General} {Assembly}, {NP5}.1 {Advances} in statistical post-processing, blending, and verification of deterministic and probabilistic forecasts},
	author = {Antonio, Bobby and McRae, Andrew and MacLeod, Dave and Cooper, Fenwick and Marsham, John and Aitchison, Laurence and Palmer, Tim and Watson, Peter},
	month = feb,
	year = {2023},
}

@misc{lessig_atmorep_2023,
	title = {{AtmoRep}: {A} stochastic model of atmosphere dynamics using large scale representation learning},
	shorttitle = {{AtmoRep}},
	url = {http://arxiv.org/abs/2308.13280},
	abstract = {The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a diverse set of applications without specifically training for them and we demonstrate this for nowcasting, temporal interpolation, model correction, and counterfactuals. We also show that AtmoRep can be improved with additional data, for example radar observations, and that it can be extended to tasks such as downscaling. Our work establishes that large-scale neural networks can provide skillful, task-independent models of atmospheric dynamics. With this, they provide a novel means to make the large record of atmospheric observations accessible for applications and for scientific inquiry, complementing existing simulations based on first principles.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Lessig, Christian and Luise, Ilaria and Gong, Bing and Langguth, Michael and Stadler, Scarlet and Schultz, Martin},
	month = sep,
	year = {2023},
	note = {arXiv:2308.13280 [physics]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics, Physics - Computational Physics},
}

@article{senior_convection-permitting_2021,
	title = {Convection-{Permitting} {Regional} {Climate} {Change} {Simulations} for {Understanding} {Future} {Climate} and {Informing} {Decision}-{Making} in {Africa}},
	volume = {102},
	issn = {0003-0007, 1520-0477},
	url = {https://journals.ametsoc.org/view/journals/bams/102/6/BAMS-D-20-0020.1.xml},
	doi = {10.1175/BAMS-D-20-0020.1},
	abstract = {Abstract Pan-Africa convection-permitting regional climate model simulations have been performed to study the impact of high resolution and the explicit representation of atmospheric moist convection on the present and future climate of Africa. These unique simulations have allowed European and African climate scientists to understand the critical role that the representation of convection plays in the ability of a contemporary climate model to capture climate and climate change, including many impact-relevant aspects such as rainfall variability and extremes. There are significant improvements in not only the small-scale characteristics of rainfall such as its intensity and diurnal cycle, but also in the large-scale circulation. Similarly, effects of explicit convection affect not only projected changes in rainfall extremes, dry spells, and high winds, but also continental-scale circulation and regional rainfall accumulations. The physics underlying such differences are in many cases expected to be relevant to all models that use parameterized convection. In some cases physical understanding of small-scale change means that we can provide regional decision-makers with new scales of information across a range of sectors. We demonstrate the potential value of these simulations both as scientific tools to increase climate process understanding and, when used with other models, for direct user applications. We describe how these ground-breaking simulations have been achieved under the U.K. Government’s Future Climate for Africa Programme. We anticipate a growing number of such simulations, which we advocate should become a routine component of climate projection, and encourage international coordination of such computationally and human-resource expensive simulations as effectively as possible.},
	language = {EN},
	number = {6},
	urldate = {2023-09-25},
	journal = {Bulletin of the American Meteorological Society},
	author = {Senior, Catherine A. and Marsham, John H. and Berthou, Ségolène and Burgin, Laura E. and Folwell, Sonja S. and Kendon, Elizabeth J. and Klein, Cornelia M. and Jones, Richard G. and Mittal, Neha and Rowell, David P. and Tomassini, Lorenzo and Vischel, Théo and Becker, Bernd and Birch, Cathryn E. and Crook, Julia and Dougill, Andrew J. and Finney, Declan L. and Graham, Richard J. and Hart, Neil C. G. and Jack, Christopher D. and Jackson, Lawrence S. and James, Rachel and Koelle, Bettina and Misiani, Herbert and Mwalukanga, Brenda and Parker, Douglas J. and Stratton, Rachel A. and Taylor, Christopher M. and Tucker, Simon O. and Wainwright, Caroline M. and Washington, Richard and Willet, Martin R.},
	month = jun,
	year = {2021},
	note = {Publisher: American Meteorological Society
Section: Bulletin of the American Meteorological Society},
	pages = {E1206--E1223},
}

@article{scheuerer_variogram-based_2015,
	title = {Variogram-{Based} {Proper} {Scoring} {Rules} for {Probabilistic} {Forecasts} of {Multivariate} {Quantities}},
	volume = {143},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/143/4/mwr-d-14-00269.1.xml},
	doi = {10.1175/MWR-D-14-00269.1},
	abstract = {Abstract Proper scoring rules provide a theoretically principled framework for the quantitative assessment of the predictive performance of probabilistic forecasts. While a wide selection of such scoring rules for univariate quantities exists, there are only few scoring rules for multivariate quantities, and many of them require that forecasts are given in the form of a probability density function. The energy score, a multivariate generalization of the continuous ranked probability score, is the only commonly used score that is applicable in the important case of ensemble forecasts, where the multivariate predictive distribution is represented by a finite sample. Unfortunately, its ability to detect incorrectly specified correlations between the components of the multivariate quantity is somewhat limited. In this paper the authors present an alternative class of proper scoring rules based on the geostatistical concept of variograms. The sensitivity of these variogram-based scoring rules to incorrectly predicted means, variances, and correlations is studied in a number of examples with simulated observations and forecasts; they are shown to be distinctly more discriminative with respect to the correlation structure. This conclusion is confirmed in a case study with postprocessed wind speed forecasts at five wind park locations in Colorado.},
	language = {EN},
	number = {4},
	urldate = {2023-09-23},
	journal = {Monthly Weather Review},
	author = {Scheuerer, Michael and Hamill, Thomas M.},
	month = apr,
	year = {2015},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {1321--1334},
}

@incollection{dinku_comparison_2010,
	address = {Dordrecht},
	title = {Comparison of {CMORPH} and {TRMM}-{3B42} over {Mountainous} {Regions} of {Africa} and {South} {America}},
	isbn = {978-90-481-2915-7},
	url = {https://doi.org/10.1007/978-90-481-2915-7_11},
	abstract = {Two satellite rainfall estimation algorithms, CMORPH and TMPA, are evaluated over two mountainous regions at daily accumulation and spatial resolution 0.25°. The evaluated TMPA products are TRMM-3B42 and TRMM-3B42RT. The first of the two validations region is located over the Ethiopian highlands in the Horn of Africa. The second is located over the highlands of Columbia in South America. Both sites are characterized by a very complex terrain. Relatively dense station networks over the two sites are used to validate the satellite products. The correlation coefficients between the reference gauge data and the satellite products were found to be low. Besides, the products underestimate both the occurrence and amount of rainfall over both validation sites. These were attributed, at least partly, to orographic warm rain process over the two regions. The performance over Colombia was better compared to that for Ethiopia. And CMORPH has exhibited better performance as compared to the two TRMM products.},
	language = {en},
	urldate = {2023-09-21},
	booktitle = {Satellite {Rainfall} {Applications} for {Surface} {Hydrology}},
	publisher = {Springer Netherlands},
	author = {Dinku, Tufa and Connor, Stephen J. and Ceccato, Pietro},
	editor = {Gebremichael, Mekonnen and Hossain, Faisal},
	year = {2010},
	doi = {10.1007/978-90-481-2915-7_11},
	keywords = {Infrared, Passive microwave, Rainfall estimation, Satellite, Validation},
	pages = {193--204},
}

@article{taylor_overview_2012,
	title = {An {Overview} of {CMIP5} and the {Experiment} {Design}},
	volume = {93},
	url = {https://journals.ametsoc.org/view/journals/bams/93/4/bams-d-11-00094.1.xml},
	doi = {10.1175/BAMS-D-11-00094.1},
	abstract = {The fifth phase of the Coupled Model Intercomparison Project (CMIP5) will produce a state-of-the- art multimodel dataset designed to advance our knowledge of climate variability and climate change. Researchers worldwide are analyzing the model output and will produce results likely to underlie the forthcoming Fifth Assessment Report by the Intergovernmental Panel on Climate Change. Unprecedented in scale and attracting interest from all major climate modeling groups, CMIP5 includes “long term” simulations of twentieth-century climate and projections for the twenty-first century and beyond. Conventional atmosphere–ocean global climate models and Earth system models of intermediate complexity are for the first time being joined by more recently developed Earth system models under an experiment design that allows both types of models to be compared to observations on an equal footing. Besides the longterm experiments, CMIP5 calls for an entirely new suite of “near term” simulations focusing on recent decades and the future to year 2035. These “decadal predictions” are initialized based on observations and will be used to explore the predictability of climate and to assess the forecast system's predictive skill. The CMIP5 experiment design also allows for participation of stand-alone atmospheric models and includes a variety of idealized experiments that will improve understanding of the range of model responses found in the more complex and realistic simulations. An exceptionally comprehensive set of model output is being collected and made freely available to researchers through an integrated but distributed data archive. For researchers unfamiliar with climate models, the limitations of the models and experiment design are described.},
	language = {EN},
	number = {4},
	urldate = {2023-09-21},
	journal = {Bulletin of the American Meteorological Society},
	author = {Taylor, Karl E. and Stouffer, Ronald J. and Meehl, Gerald A.},
	month = apr,
	year = {2012},
	note = {Publisher: American Meteorological Society
Section: Bulletin of the American Meteorological Society},
	pages = {485--498},
}

@article{ageet_validation_2022,
	title = {Validation of {Satellite} {Rainfall} {Estimates} over {Equatorial} {East} {Africa}},
	volume = {23},
	issn = {1525-7541, 1525-755X},
	url = {https://journals.ametsoc.org/view/journals/hydr/23/2/JHM-D-21-0145.1.xml},
	doi = {10.1175/JHM-D-21-0145.1},
	abstract = {Abstract Rain gauge data sparsity over Africa is known to impede the assessments of hydrometeorological risks and of the skill of numerical weather prediction models. Satellite rainfall estimates (SREs) have been used as surrogate fields for a long time and are continuously replaced by more advanced algorithms and new sensors. Using a unique daily rainfall dataset from 36 stations across equatorial East Africa for the period 2001–18, this study performs a multiscale evaluation of gauge-calibrated SREs, namely, IMERG, TMPA, CHIRPS, and MSWEP (v2.2 and v2.8). Skills were assessed from daily to annual time scales, for extreme daily precipitation, and for TMPA and IMERG near-real-time (NRT) products. Results show that 1) the SREs reproduce the annual rainfall pattern and seasonal rainfall cycle well, despite exhibiting biases of up to 9\%; 2) IMERG is the best for shorter temporal scales while MSWEPv2.2 and CHIRPS perform best at the monthly and annual time steps, respectively; 3) the performance of all the SREs varies spatially, likely due to an inhomogeneous degree of gauge calibration, with the largest variation seen in MSWEPv2.2; 4) all the SREs miss between 79\% (IMERG-NRT) and 98\% (CHIRPS) of daily extreme rainfall events recorded by the rain gauges; 5) IMERG-NRT is the best regarding extreme event detection and accuracy; and 6) for return values of extreme rainfall, IMERG, and MSWEPv2.2 have the least errors while CHIRPS and MSWEPv2.8 cannot be recommended. The study also highlights improvements of IMERG over TMPA, the decline in performance of MSWEPv2.8 compared to MSWEPv2.2, and the potential of SREs for flood risk assessment over East Africa.},
	language = {EN},
	number = {2},
	urldate = {2023-09-21},
	journal = {Journal of Hydrometeorology},
	author = {Ageet, Simon and Fink, Andreas H. and Maranan, Marlon and Diem, Jeremy E. and Hartter, Joel and Ssali, Andrew L. and Ayabagabo, Prosper},
	month = feb,
	year = {2022},
	note = {Publisher: American Meteorological Society
Section: Journal of Hydrometeorology},
	pages = {129--151},
}

@article{rowell_causes_2018,
	title = {Causes of the {Uncertainty} in {Projections} of {Tropical} {Terrestrial} {Rainfall} {Change}: {East} {Africa}},
	volume = {31},
	issn = {0894-8755, 1520-0442},
	shorttitle = {Causes of the {Uncertainty} in {Projections} of {Tropical} {Terrestrial} {Rainfall} {Change}},
	url = {https://journals.ametsoc.org/view/journals/clim/31/15/jcli-d-17-0830.1.xml},
	doi = {10.1175/JCLI-D-17-0830.1},
	abstract = {Abstract Understanding the causes of regional climate projection uncertainty is a critical component toward establishing reliability of these projections. Here, four complementary experimental and decomposition techniques are synthesized to begin to understand which mechanisms differ most between models. These tools include a variety of multimodel ensembles, a decomposition of rainfall into tropics-wide or region-specific processes, and a separation of within-domain versus remote contributions to regional model projection uncertainty. Three East African regions are identified and characterized by spatially coherent intermodel projection behavior, which interestingly differs from previously identified regions of coherent interannual behavior. For the “Short Rains” regions, uncertainty in projected seasonal mean rainfall change is primarily due to uncertainties in the regional response to both the uniform and pattern components of SST warming (but not uncertainties in the global mean warming itself) and a small direct CO2 impact. These primarily derive from uncertain regional dynamics over both African and remote regions, rather than globally coherent (thermo)dynamics. For the “Long Rains” region, results are similar, except that uncertain atmospheric responses to a fixed SST pattern change are a little less important, and some key regional uncertainties are primarily located beyond Africa. The latter reflects the behavior of two outlying models that experience exceptional warming in the southern subtropical oceans, from which large lower-tropospheric moisture anomalies are advected by the mean flow to contribute to exceptional increases in the Long Rains totals. Further research could lead to a useful assessment of the reliability of these exceptional projections.},
	language = {EN},
	number = {15},
	urldate = {2023-09-21},
	journal = {Journal of Climate},
	author = {Rowell, David P. and Chadwick, Robin},
	month = aug,
	year = {2018},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {5977--5995},
}

@article{chapman_climate_2022,
	title = {Climate {Change} {Impacts} on {Extreme} {Rainfall} in {Eastern} {Africa} in a {Convection}-{Permitting} {Climate} {Model}},
	volume = {36},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/36/1/JCLI-D-21-0851.1.xml},
	doi = {10.1175/JCLI-D-21-0851.1},
	abstract = {Abstract Climate change is expected to increase the frequency and intensity of rainfall extremes. Understanding future changes in rainfall is necessary for adaptation planning. Eastern Africa is vulnerable to rainfall extremes because of low adaptive capacity and high future population growth. Convection-permitting climate models have been found to better represent moderate (yearly) rainfall extremes than parameterized convection models, but there is limited analysis of rare extremes that occur less frequently than once per year. These events often have the largest socioeconomic impacts. We use extreme value theory and regional frequency analysis to quantify rare rainfall extremes over East Africa in a convection-permitting climate model (CP4A). We compare the results with its parameterized counterpart (P25), the Coordinated Regional Climate Downscaling Experiment for the African region (CORDEX-Africa) ensemble, and observations to understand how the convection parameterization impacts the results. We find that CP4A better matches observations than the parameterized models. With climate change, we find the parameterized convection models have unrealistically high changes in the shape parameter of the extreme value distribution, which controls the tail behavior (i.e., the most extreme events), leading to large increases in return levels of events with a return period of {\textgreater}20 years. This suggests that parameterized convection models may not be suitable for looking at relative changes in rare rainfall events with climate change and that convection-permitting models should be preferred for this type of work. With the more realistic CP4A, RCP8.5 end-of-century climate change leads to 1-in-100-yr events becoming 1-in-23-yr events, which will necessitate serious adaptation efforts to avoid devastating socioeconomic impacts. Significance Statement We use a new, high-resolution climate model to examine how rare extreme rainfall events in East Africa might change in the future with climate change and compare the results with those from standard-resolution climate models. We find that the standard-resolution models have unrealistically large increases in rainfall for events that occur less frequently than every 20 years. The high-resolution model is more realistic and is required to illustrate possible future changes in rare rainfall extremes. Extreme events will become more common with climate change, and in the more realistic model we show that a 1-in-100-yr event may become a 1-in-23-yr event by the end of the century if greenhouse gas emissions are not significantly reduced.},
	language = {EN},
	number = {1},
	urldate = {2023-09-21},
	journal = {Journal of Climate},
	author = {Chapman, Sarah and Bacon, James and Birch, Cathryn E. and Pope, Edward and Marsham, John H. and Msemo, Hellen and Nkonde, Edson and Sinachikupo, Kenneth and Vanya, Charles},
	month = dec,
	year = {2022},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {93--109},
}

@misc{antonio_cgan_2023,
	title = {{cGAN}},
	copyright = {MIT},
	shorttitle = {Conditional {GAN}},
	url = {www.github.com/bobbyantonio/downscaling-cgan},
	urldate = {2023-09-21},
	author = {Antonio, Bobby},
	month = aug,
	year = {2023},
	note = {www.github.com/bobbyantonio/downscaling-cgan},
}

@article{kimani_assessment_2017,
	title = {An {Assessment} of {Satellite}-{Derived} {Rainfall} {Products} {Relative} to {Ground} {Observations} over {East} {Africa}},
	volume = {9},
	issn = {2072-4292},
	url = {https://research.utwente.nl/en/publications/an-assessment-of-satellite-derived-rainfall-products-relative-to-},
	doi = {10.3390/rs9050430},
	language = {English},
	number = {5},
	urldate = {2023-09-21},
	journal = {Remote sensing},
	author = {Kimani, M. W. and Hoedjes, Johannes Cornelis Bernardus and Su, Z.},
	year = {2017},
	note = {Publisher: MDPI},
	pages = {430},
}

@techreport{wilson_forecast_2014,
	address = {Geneva, Switzerland},
	title = {Forecast {Verification} for the {African} {Severe} {Weather} {Forecasting} {Demonstration} {Projects}},
	institution = {World Meteorological Organization},
	author = {Wilson, Laurence},
	year = {2014},
	keywords = {SWFDP},
}

@misc{ecmwf_parameter_2023,
	title = {Parameter database},
	url = {https://codes.ecmwf.int/grib/param-db/},
	urldate = {2023-09-20},
	author = {ECMWF},
	month = feb,
	year = {2023},
	note = {https://codes.ecmwf.int/grib/param-db/, Accessed February 2023},
}

@book{maraun_statistical_2018,
	edition = {1},
	title = {Statistical {Downscaling} and {Bias} {Correction} for {Climate} {Research}},
	isbn = {978-1-107-06605-2 978-1-107-58878-3 978-1-107-68608-3},
	url = {https://www.cambridge.org/core/product/identifier/9781107588783/type/book},
	language = {en},
	urldate = {2023-09-20},
	publisher = {Cambridge University Press},
	author = {Maraun, Douglas and Widmann, Martin},
	month = jan,
	year = {2018},
	doi = {10.1017/9781107588783},
}

@article{gronquist_deep_2021,
	title = {Deep learning for post-processing ensemble weather forecasts},
	volume = {379},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0092},
	doi = {10.1098/rsta.2020.0092},
	abstract = {Quantifying uncertainty in weather forecasts is critical, especially for predicting extreme weather events. This is typically accomplished with ensemble prediction systems, which consist of many perturbed numerical weather simulations, or trajectories, run in parallel. These systems are associated with a high computational cost and often involve statistical post-processing steps to inexpensively improve their raw prediction qualities. We propose a mixed model that uses only a subset of the original weather trajectories combined with a post-processing step using deep neural networks. These enable the model to account for non-linear relationships that are not captured by current numerical models or post-processing methods. Applied to the global data, our mixed models achieve a relative improvement in ensemble forecast skill (CRPS) of over 14\%. Furthermore, we demonstrate that the improvement is larger for extreme weather events on select case studies. We also show that our post-processing can use fewer trajectories to achieve comparable results to the full ensemble. By using fewer trajectories, the computational costs of an ensemble prediction system can be reduced, allowing it to run at higher resolution and produce more accurate forecasts.

This article is part of the theme issue ‘Machine learning for weather and climate modelling’.},
	number = {2194},
	urldate = {2023-09-20},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Grönquist, Peter and Yao, Chengyuan and Ben-Nun, Tal and Dryden, Nikoli and Dueben, Peter and Li, Shigang and Hoefler, Torsten},
	month = feb,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {deep learning, ensemble post-processing, extreme weather events, weather uncertainty quantification},
	pages = {20200092},
}

@article{yuval_stable_2020,
	title = {Stable machine-learning parameterization of subgrid processes for climate modeling at a range of resolutions},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-17142-3},
	doi = {10.1038/s41467-020-17142-3},
	abstract = {Global climate models represent small-scale processes such as convection using subgrid models known as parameterizations, and these parameterizations contribute substantially to uncertainty in climate projections. Machine learning of new parameterizations from high-resolution model output is a promising approach, but such parameterizations have been prone to issues of instability and climate drift, and their performance for different grid spacings has not yet been investigated. Here we use a random forest to learn a parameterization from coarse-grained output of a three-dimensional high-resolution idealized atmospheric model. The parameterization leads to stable simulations at coarse resolution that replicate the climate of the high-resolution simulation. Retraining for different coarse-graining factors shows the parameterization performs best at smaller horizontal grid spacings. Our results yield insights into parameterization performance across length scales, and they also demonstrate the potential for learning parameterizations from global high-resolution simulations that are now emerging.},
	language = {en},
	number = {1},
	urldate = {2023-09-20},
	journal = {Nature Communications},
	author = {Yuval, Janni and O’Gorman, Paul A.},
	month = jul,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Atmospheric dynamics, Climate and Earth system modelling},
	pages = {3295},
}

@misc{noauthor_deep_nodate,
	title = {Deep learning to represent subgrid processes in climate models},
	url = {https://www.pnas.org/doi/10.1073/pnas.1810286115},
	language = {en},
	urldate = {2023-09-20},
	note = {ISBN: 9781810286112},
}

@article{krasnopolsky_using_2013,
	title = {Using {Ensemble} of {Neural} {Networks} to {Learn} {Stochastic} {Convection} {Parameterizations} for {Climate} and {Numerical} {Weather} {Prediction} {Models} from {Data} {Simulated} by a {Cloud} {Resolving} {Model}},
	volume = {2013},
	issn = {1687-7594, 1687-7608},
	url = {https://www.hindawi.com/journals/aans/2013/485913/},
	doi = {10.1155/2013/485913},
	abstract = {A novel approach based on the neural network (NN) ensemble technique is formulated and used for development of a NN stochastic convection parameterization for climate and numerical weather prediction (NWP) models. This fast parameterization is built based on learning from data simulated by a cloud-resolving model (CRM) initialized with and forced by the observed meteorological data available for 4-month boreal winter from November 1992 to February 1993. CRM-simulated data were averaged and processed to implicitly define a stochastic convection parameterization. This parameterization is learned from the data using an ensemble of NNs. The NN ensemble members are trained and tested. The inherent uncertainty of the stochastic convection parameterization derived following this approach is estimated. The newly developed NN convection parameterization has been tested in National Center of Atmospheric Research (NCAR) Community Atmospheric Model (CAM). It produced reasonable and promising decadal climate simulations for a large tropical Pacific region. The extent of the adaptive ability of the developed NN parameterization to the changes in the model environment is briefly discussed. This paper is devoted to a proof of concept and discusses methodology, initial results, and the major challenges of using the NN technique for developing convection parameterizations for climate and NWP models.},
	language = {en},
	urldate = {2023-09-20},
	journal = {Advances in Artificial Neural Systems},
	author = {Krasnopolsky, Vladimir M. and Fox-Rabinovitz, Michael S. and Belochitski, Alexei A.},
	month = may,
	year = {2013},
	pages = {1--13},
}

@inproceedings{krasnopolsky_development_2010,
	title = {Development of neural network convection parameterizations for numerical climate and weather prediction models using cloud resolving model simulations},
	doi = {10.1109/IJCNN.2010.5596766},
	abstract = {A novel approach based on the neural network (NN) technique is formulated and used for development of a NN ensemble stochastic convection parameterization for numerical climate and weather prediction models. This fast parameterization is built based on data from Cloud Resolving Model (CRM) simulations initialized with TOGA-COARE data. CRM emulated data are averaged and projected onto the General Circulation Model (GCM) space of atmospheric states to implicitly define a stochastic convection parameterization. This parameterization is comprised as an ensemble of neural networks. The developed NNs are trained and tested. The inherent uncertainty of the stochastic convection parameterization derived in such a way is estimated. The major challenges of development of stochastic NN parameterizations are discussed based on our initial results.},
	booktitle = {The 2010 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Krasnopolsky, Vladimir M. and Fox-Rabinovitz, Michael S. and Belochitski, Alexei A.},
	month = jul,
	year = {2010},
	note = {ISSN: 2161-4407},
	keywords = {Artificial neural networks, Cooling, Electricity, Heating, Numerical models},
	pages = {1--8},
}

@article{brenowitz_prognostic_2018,
	title = {Prognostic {Validation} of a {Neural} {Network} {Unified} {Physics} {Parameterization}},
	volume = {45},
	copyright = {©2018. American Geophysical Union. All Rights Reserved.},
	issn = {1944-8007},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2018GL078510},
	doi = {10.1029/2018GL078510},
	abstract = {Weather and climate models approximate diabatic and sub-grid-scale processes in terms of grid-scale variables using parameterizations. Current parameterizations are designed by humans based on physical understanding, observations, and process modeling. As a result, they are numerically efficient and interpretable, but potentially oversimplified. However, the advent of global high-resolution simulations and observations enables a more robust approach based on machine learning. In this letter, a neural network-based parameterization is trained using a near-global aqua-planet simulation with a 4-km resolution (NG-Aqua). The neural network predicts the apparent sources of heat and moisture averaged onto (160 km)2 grid boxes. A numerically stable scheme is obtained by minimizing the prediction error over multiple time steps rather than single one. In prognostic single-column model tests, this scheme matches both the fluctuations and equilibrium of NG-Aqua simulation better than the Community Atmosphere Model does.},
	language = {en},
	number = {12},
	urldate = {2023-09-20},
	journal = {Geophysical Research Letters},
	author = {Brenowitz, N. D. and Bretherton, C. S.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2018GL078510},
	keywords = {cloud-resolving model, cumulus parameterization, machine learning, neural network, single-column model},
	pages = {6289--6298},
}

@misc{ecmwf_section_2023,
	title = {Section 2 {The} {ECMWF} {Integrated} {Forecasting} {System} - {IFS} - {Forecast} {User} {Guide} - {ECMWF} {Confluence} {Wiki}},
	url = {https://confluence.ecmwf.int/display/FUG/Section+2+The+ECMWF+Integrated+Forecasting+System+-+IFS},
	urldate = {2023-09-19},
	author = {ECMWF},
	month = sep,
	year = {2023},
}

@article{bryan_resolution_2003,
	title = {Resolution {Requirements} for the {Simulation} of {Deep} {Moist} {Convection}},
	volume = {131},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/131/10/1520-0493_2003_131_2394_rrftso_2.0.co_2.xml},
	doi = {10.1175/1520-0493(2003)131<2394:RRFTSO>2.0.CO;2},
	abstract = {Abstract The spatial resolution appropriate for the simulation of deep moist convection is addressed from a turbulence perspective. To provide a clear theoretical framework for the problem, techniques for simulating turbulent flows are reviewed, and the source of the subgrid terms in the Navier–Stokes equation is clarified. For decades, cloud-resolving models have used large-eddy simulation (LES) techniques to parameterize the subgrid terms. A literature review suggests that the appropriateness of using traditional LES closures for this purpose has never been established. Furthermore, examination of the assumptions inherent in these closures suggests that grid spacing on the order of 100 m may be required for the performance of cloud models to be consistent with their design. Based on these arguments, numerical simulations of squall lines were conducted with grid spacings between 1 km and 125 m. The results reveal that simulations with 1-km grid spacing do not produce equivalent squall-line structure and evolution as compared to the higher-resolution simulations. Details of the simulated squall lines that change as resolution is increased include precipitation amount, system phase speed, cloud depth, static stability values, the size of thunderstorm cells, and the organizational mode of convective overturning (e.g., upright towers versus sloped plumes). It is argued that the ability of the higher-resolution runs to become turbulent leads directly to the differences in evolution. There appear to be no systematic trends in specific fields as resolution is increased. For example, mean vertical velocity and rainwater values increase in magnitude with increasing resolution in some environments, but decrease with increasing resolution in other environments. The statistical properties of the simulated squall lines are still not converged between the 250- and 125-m runs. Several possible explanations for the lack of convergence are offered. Nevertheless, it is clear that simulations with O(1 km) grid spacing should not be used as benchmark or control solutions for resolution sensitivity studies. The simulations also support the contention that a minimum grid spacing of O(100 m) is required for traditional LES closures to perform appropriately for their design. Specifically, only simulations with 250- and 125-m grid spacing resolve an inertial subrange. In contrast, the 1-km simulations do not even reproduce the correct magnitude or scale of the spectral kinetic energy maximum. Furthermore, the 1-km simulations contain an unacceptably large amount of subgrid turbulence kinetic energy, and do not adequately resolve turbulent fluxes of total water. A guide to resolution requirements for the operational and research communities is proposed. The proposal is based primarily on the intended use of the model output. Even though simulations with O(1 km) grid spacing display behavior that is unacceptable for the model design, it is argued that these simulations can still provide valuable information to operational forecasters. For the research community, O(100 m) grid spacing is recommended for most applications, because a modeling system that is well founded should be desired for most purposes.},
	language = {EN},
	number = {10},
	urldate = {2023-09-19},
	journal = {Monthly Weather Review},
	author = {Bryan, George H. and Wyngaard, John C. and Fritsch, J. Michael},
	month = oct,
	year = {2003},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {2394--2416},
}

@article{houze_jr_mesoscale_2004,
	title = {Mesoscale convective systems},
	volume = {42},
	copyright = {Copyright 2004 by the American Geophysical Union.},
	issn = {1944-9208},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2004RG000150},
	doi = {10.1029/2004RG000150},
	abstract = {Mesoscale convective systems (MCSs) have regions of both convective and stratiform precipitation, and they develop mesoscale circulations as they mature. The upward motion takes the form of a deep-layer ascent drawn into the MCS in response to the latent heating and cooling in the convective region. The ascending layer overturns as it rises but overall retains a coherent layer structure. A middle level layer of inflow enters the stratiform region of the MCS from a direction determined by the large-scale flow and descends in response to diabatic cooling at middle-to-low levels. A middle level mesoscale convective vortex (MCV) develops in the stratiform region, prolongs the MCS, and may contribute to tropical cyclone development. The propagation of an MCS may have a discrete component but may further be influenced by waves and disturbances generated both in response to the MCS and external to the MCS. Waves of a larger scale may affect the propagation velocity by phase locking with the MCS in a cooperative mode. The horizontal scale of an MCS may be limited either by a balance between the formation rate of convective precipitation and dissipation of stratiform precipitation or by the Rossby radius of the MCV. The vertical redistribution of momentum by an MCS depends on the size of the stratiform region, while the net vertical profile of heating of the large-scale environment depends on the amount of stratiform rain. Regional variability of the stratiform rain from MCSs affects the large-scale circulation's response to MCS heating.},
	language = {en},
	number = {4},
	urldate = {2023-09-18},
	journal = {Reviews of Geophysics},
	author = {Houze Jr., Robert A.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2004RG000150},
	keywords = {convective processes, mesoscale meteorology, precipitation},
}

@article{stratton_pan-african_2018,
	title = {A {Pan}-{African} {Convection}-{Permitting} {Regional} {Climate} {Simulation} with the {Met} {Office} {Unified} {Model}: {CP4}-{Africa}},
	volume = {31},
	issn = {0894-8755, 1520-0442},
	shorttitle = {A {Pan}-{African} {Convection}-{Permitting} {Regional} {Climate} {Simulation} with the {Met} {Office} {Unified} {Model}},
	url = {https://journals.ametsoc.org/view/journals/clim/31/9/jcli-d-17-0503.1.xml},
	doi = {10.1175/JCLI-D-17-0503.1},
	abstract = {Abstract A convection-permitting multiyear regional climate simulation using the Met Office Unified Model has been run for the first time on an Africa-wide domain. The model has been run as part of the Future Climate for Africa (FCFA) Improving Model Processes for African Climate (IMPALA) project, and its configuration, domain, and forcing data are described here in detail. The model [Pan-African Convection-Permitting Regional Climate Simulation with the Met Office UM (CP4-Africa)] uses a 4.5-km horizontal grid spacing at the equator and is run without a convection parameterization, nested within a global atmospheric model driven by observations at the sea surface, which does include a convection scheme. An additional regional simulation, with identical resolution and physical parameterizations to the global model, but with the domain, land surface, and aerosol climatologies of CP4-Africa, has been run to aid in the understanding of the differences between the CP4-Africa and global model, in particular to isolate the impact of the convection parameterization and resolution. The effect of enforcing moisture conservation in CP4-Africa is described and its impact on reducing extreme precipitation values is assessed. Preliminary results from the first five years of the CP4-Africa simulation show substantial improvements in JJA average rainfall compared to the parameterized convection models, with most notably a reduction in the persistent dry bias in West Africa, giving an indication of the benefits to be gained from running a convection-permitting simulation over the whole African continent.},
	language = {EN},
	number = {9},
	urldate = {2023-09-18},
	journal = {Journal of Climate},
	author = {Stratton, Rachel A. and Senior, Catherine A. and Vosper, Simon B. and Folwell, Sonja S. and Boutle, Ian A. and Earnshaw, Paul D. and Kendon, Elizabeth and Lock, Adrian P. and Malcolm, Andrew and Manners, James and Morcrette, Cyril J. and Short, Christopher and Stirling, Alison J. and Taylor, Christopher M. and Tucker, Simon and Webster, Stuart and Wilkinson, Jonathan M.},
	month = may,
	year = {2018},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {3485--3508},
}

@article{finney_effects_2020,
	title = {Effects of {Explicit} {Convection} on {Future} {Projections} of {Mesoscale} {Circulations}, {Rainfall}, and {Rainfall} {Extremes} over {Eastern} {Africa}},
	volume = {33},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/33/7/jcli-d-19-0328.1.xml},
	doi = {10.1175/JCLI-D-19-0328.1},
	abstract = {Abstract Eastern Africa’s fast-growing population is vulnerable to changing rainfall and extremes. Using the first pan-African climate change simulations that explicitly model the rainfall-generating convection, we investigate both the climate change response of key mesoscale drivers of eastern African rainfall, such as sea and lake breezes, and the spatial heterogeneity of rainfall responses. The explicit model shows widespread increases at the end of the century in mean ({\textasciitilde}40\%) and extreme ({\textasciitilde}50\%) rain rates, whereas the sign of changes in rainfall frequency has large spatial heterogeneity (from −50\% to over +90\%). In comparison, an equivalent parameterized simulation has greater moisture convergence and total rainfall increase over the eastern Congo and less over eastern Africa. The parameterized model also does not capture 1) the large heterogeneity of changes in rain frequency; 2) the widespread and large increases in extreme rainfall, which result from increased rainfall per humidity change; and 3) the response of rainfall to the changing sea breeze, even though the sea-breeze change is captured. Consequently, previous rainfall projections are likely inadequate for informing many climate-sensitive decisions—for example, for infrastructure in coastal cities. We consider the physics revealed here and its implications to be relevant for many other vulnerable tropical regions, especially those with coastal convection.},
	language = {EN},
	number = {7},
	urldate = {2023-09-18},
	journal = {Journal of Climate},
	author = {Finney, Declan L. and Marsham, John H. and Rowell, David P. and Kendon, Elizabeth J. and Tucker, Simon O. and Stratton, Rachel A. and Jackson, Lawrence S.},
	month = apr,
	year = {2020},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {2701--2718},
}

@misc{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	doi = {10.48550/arXiv.1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv:1502.01852},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{oshea_introduction_2015,
	title = {An {Introduction} to {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.08458},
	doi = {10.48550/arXiv.1511.08458},
	abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {O'Shea, Keiron and Nash, Ryan},
	month = dec,
	year = {2015},
	note = {arXiv:1511.08458},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@book{nielsen_neural_2015,
	title = {Neural {Networks} and {Deep} {Learning}},
	language = {en},
	publisher = {Determination Press},
	author = {Nielsen, Michael},
	year = {2015},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2023-09-15},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@article{watson_applying_2019,
	title = {Applying {Machine} {Learning} to {Improve} {Simulations} of a {Chaotic} {Dynamical} {System} {Using} {Empirical} {Error} {Correction}},
	volume = {11},
	copyright = {©2019. The Authors.},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2018MS001597},
	doi = {10.1029/2018MS001597},
	abstract = {Dynamical weather and climate prediction models underpin many studies of the Earth system and hold the promise of being able to make robust projections of future climate change based on physical laws. However, simulations from these models still show many differences compared with observations. Machine learning has been applied to solve certain prediction problems with great success, and recently, it has been proposed that this could replace the role of physically-derived dynamical weather and climate models to give better quality simulations. Here, instead, a framework using machine learning together with physically-derived models is tested, in which it is learnt how to correct the errors of the latter from time step to time step. This maintains the physical understanding built into the models, while allowing performance improvements, and also requires much simpler algorithms and less training data. This is tested in the context of simulating the chaotic Lorenz '96 system, and it is shown that the approach yields models that are stable and that give both improved skill in initialized predictions and better long-term climate statistics. Improvements in long-term statistics are smaller than for single time step tendencies, however, indicating that it would be valuable to develop methods that target improvements on longer time scales. Future strategies for the development of this approach and possible applications to making progress on important scientific problems are discussed.},
	language = {en},
	number = {5},
	urldate = {2023-09-14},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Watson, Peter A. G.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2018MS001597},
	keywords = {Lorenz '96, machine learning, modeling, neural network},
	pages = {1402--1417},
}

@article{pohl_influence_2006,
	title = {Influence of the {Madden}–{Julian} {Oscillation} on {East} {African} rainfall. {I}: {Intraseasonal} variability and regional dependency},
	volume = {132},
	copyright = {Copyright © 2006 Royal Meteorological Society},
	issn = {1477-870X},
	shorttitle = {Influence of the {Madden}–{Julian} {Oscillation} on {East} {African} rainfall. {I}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1256/qj.05.104},
	doi = {10.1256/qj.05.104},
	abstract = {The influence of the Madden–Julian Oscillation (MJO) on rainfall amounts over Equatorial East Africa (Kenya and northern Tanzania) is analysed for the period 1979–95 at the intraseasonal (pentad) time-scale. The two rainy seasons (March to May and October to December) are considered. Intraseasonal wet events in East Africa are embedded in large-scale zonal circulation anomaly patterns along the equator, showing distinct eastward propagation. It is further found that these ‘wet’ events display a clear phasing with respect to the MJO cycle. This phasing is expressed as out-of-phase variations between the Highland and the coastal areas. Such a pattern is suggested to reflect different rain-causing mechanisms. MJO phases leading to wet spells in the western (Highland) region are those associated with the development of large-scale convection in the Africa/Indian Ocean region. These events are unambiguously related to deep convection, fuelled by low-level westerly moisture advection. MJO phases leading to wet spells in the eastern (coastal) region are often those associated with overall suppressed deep convection in the Africa/Indian Ocean region. However, these phases induce moisture advection from Indian Ocean. The possible role of stratiform rainfall or relatively shallow convection in the coastal wet spells observed in this phase is discussed. The contrasting rainfall conditions found in the two regions for the two opposite MJO phases are strongly correlated with the pressure gradient between the Indian and Atlantic Oceans. Copyright © 2006 Royal Meteorological Society},
	language = {en},
	number = {621},
	urldate = {2023-09-13},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Pohl, Benjamin and Camberlin, Pierre},
	year = {2006},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1256/qj.05.104},
	keywords = {Convective rainfall, Long rains, Short rains, Wet spells},
	pages = {2521--2539},
}

@article{pohl_influence_2006-1,
	title = {Influence of the {Madden}–{Julian} {Oscillation} on {East} {African} rainfall: {II}. {March}–{May} season extremes and interannual variability},
	volume = {132},
	copyright = {Copyright © 2006 Royal Meteorological Society},
	issn = {1477-870X},
	shorttitle = {Influence of the {Madden}–{Julian} {Oscillation} on {East} {African} rainfall},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1256/qj.05.223},
	doi = {10.1256/qj.05.223},
	abstract = {The Madden–Julian Oscillation (MJO) was shown in Part I to have a significant impact on both East African rainy seasons at pentad time-scale. The case of the ‘long rains’ (March–May) is further examined by considering both shorter (daily) and longer (interannual) time-scales. Based on composite analyses, extreme daily rainfall events in the Highland (west) and in the coastal (east) regions of Equatorial East Africa (Kenya and Tanzania) are extracted. Low-level westerly wind anomalies are seen to accompany wet events in the west and easterly ones in the east. These opposite circulation anomalies preferentially occur at distinct phases of the MJO, which indicates that the latter has a major influence on rainfall at the daily time-scale. However, this influence undergoes significant year-to-year variations. It is found that the common variance between smoothed rainfall time series (5-day low-pass filter) and MJO indices varies from 5\% to 53\% in the Highland region. Significantly lower air temperatures in the upper troposphere are recorded during the MJO cycles that present the highest common variance with East African rainfall. Such a cooling is seen to be related to the Kelvin wave propagation in the upper layers which favours upward atmospheric motion over the region. At the interannual time-scale, fluctuations in MJO amplitude contribute to the March–May rainfall variability, and 44\% of the March–May seasonal rainfall variance in the 1979–95 period in East Africa is explained by this parameter. Years of high MJO amplitude are characterized by earlier onset of the rains, and higher seasonal amounts. Copyright © 2006 Royal Meteorological Society},
	language = {en},
	number = {621},
	urldate = {2023-09-13},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Pohl, Benjamin and Camberlin, Pierre},
	year = {2006},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1256/qj.05.223},
	keywords = {East African long rains, Extreme wet events, Rainfall variability},
	pages = {2541--2558},
}

@article{nicholson_long-term_2015,
	title = {Long-term variability of the {East} {African} ‘short rains’ and its links to large-scale factors},
	volume = {35},
	issn = {1097-0088},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.4259},
	doi = {10.1002/joc.4259},
	abstract = {This study utilizes a new 139-year rainfall record for East Africa to evaluate the relationship between the ‘short rains’ of October–November and four tropical indices. These indices include the zonal winds at the surface and 200 mb over the central equatorial Indian Ocean, Niño 3.4 and the Indian Ocean Zonal Mode (IOZM). The relationships with these indices are time dependent, as are the relationships among the indices. These change markedly on a decadal timescale, consistent with regime changes indicated by other authors, and the links to El Niño-Southern Oscillation (ENSO) and the IOZM appear to be weaker than those suggested by previous studies. The zonal winds show the strongest and most consistent relationships with October–November rainfall. However, the relationships are very different for wet and dry years, and the zonal winds play a stronger role in producing wet conditions. Further, several factors appear to act in tandem to produce extremely wet years, but appear to act largely independently in producing drought. The links to drought have been markedly weaker since 1982. These links were also very weak roughly between 1920 and 1960, when apparently the Walker cell over the Indian Ocean was very weak and the Pacific Walker cell particularly strong. At that time, ENSO appeared to drive most of the variability of October–November rainfall, interannual variability was weak, and the rainfall was below average during most of that period. When the zonal circulation in the Indian Ocean sector became well-developed c. 1961 and the Pacific cell weakened, both the rainfall and its interannual variability markedly increased. Overall, this study stresses the time dependence of the various relationships with East African October–November rainfall. This has strong implications for seasonal forecasting.},
	language = {en},
	number = {13},
	urldate = {2023-09-13},
	journal = {International Journal of Climatology},
	author = {Nicholson, Sharon E.},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.4259},
	keywords = {African rainfall, ENSO, East Africa, IOZM, Indian Ocean, rainfall variability, short rains},
	pages = {3979--3990},
}

@article{macleod_causal_2021,
	title = {Causal pathways linking different flavours of {ENSO} with the {Greater} {Horn} of {Africa} short rains},
	volume = {22},
	copyright = {© 2020 The Authors. Atmospheric Science Letters published by John Wiley \& Sons Ltd on behalf of Royal Meteorological Society.},
	issn = {1530-261X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asl.1015},
	doi = {10.1002/asl.1015},
	abstract = {There is a strong association between canonical El Niño and a wet Greater Horn of Africa (GHA) short rains. However, the link with Modoki El Niño events appears to be significantly weaker. In order to understand this, we present an analysis of observational data and idealised climate model experiments. Idealised atmospheric simulations isolate the direct influence of Pacific heating on the GHA and reveal that neither the longitudinal position nor the observed weaker magnitude of Modoki Pacific heating anomalies can explain the difference in teleconnections. The direct effect of canonical or Modoki Pacific heating patterns on the GHA is similar and neither reproduces the structure of the full GHA teleconnection: they both generate a wet-dry dipole over the GHA instead of large-scale single-signed wet anomalies. Our results indicate that the strong canonical ENSO influence on GHA is indirect, mediated through its strong relationship with the Indian Ocean Dipole (IOD). By contrast, Modoki ENSO is uncorrelated with the IOD, resulting in weak teleconnection to GHA. Understanding these differences aids seasonal forecast interpretation, whilst their representation in models is likely a prerequisite for making accurate projections of changes in extremes over the GHA and beyond.},
	language = {en},
	number = {2},
	urldate = {2023-09-13},
	journal = {Atmospheric Science Letters},
	author = {MacLeod, David and Graham, Richard and O'Reilly, Chris and Otieno, George and Todd, Martin},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asl.1015},
	keywords = {ENSO, Greater Horn of Africa, IOD, seasonal forecasting},
	pages = {e1015},
}

@article{pohl_intraseasonal_2011,
	title = {Intraseasonal and interannual zonal circulations over the {Equatorial} {Indian} {Ocean}},
	volume = {104},
	issn = {1434-4483},
	url = {https://doi.org/10.1007/s00704-010-0336-1},
	doi = {10.1007/s00704-010-0336-1},
	abstract = {El Niño Southern Oscillation (ENSO) and given phases of the Madden–Julian Oscillation (MJO) show similar regional signatures over the Equatorial Indian Ocean, consisting in an enhancement or reversing of the convective and dynamic zonal gradients between East Africa and the Maritime Continent of Indonesia. This study analyses how these two modes of variability add or cancel their effects at their respective timescales, through an investigation of the equatorial cellular circulations over the central Indian Ocean. Results show that (1) the wind shear between the lower and upper troposphere is related to marked regional rainfall anomalies and is embedded in larger-scale atmospheric configurations, involving the Southern Oscillation; (2) the intraseasonal (30–60 days) and interannual (4–5 years) timescales are the most energetic frequencies that modulate these circulations, confirming the implication of the MJO and ENSO; (3) extreme values of the Indian Ocean wind shear result from the combination of El Niño and the MJO phase enhancing atmospheric convection over Africa, or La Niña and the MJO phase associated with convective activity over the Maritime Continent. Consequences for regional rainfall anomalies over East Africa and Indonesia are then discussed.},
	language = {en},
	number = {1},
	urldate = {2023-09-13},
	journal = {Theoretical and Applied Climatology},
	author = {Pohl, Benjamin and Camberlin, Pierre},
	month = may,
	year = {2011},
	keywords = {Dipole Mode Index, Equatorial Indian Ocean, Indian Ocean, Outgoing Longwave Radiation, Zonal Wind Anomaly},
	pages = {175--191},
}

@article{finney_effect_2020,
	title = {The effect of westerlies on {East} {African} rainfall and the associated role of tropical cyclones and the {Madden}–{Julian} {Oscillation}},
	volume = {146},
	copyright = {© 2019 The Authors. Quarterly Journal of the Royal Meteorological Society published by John Wiley \& Sons Ltd on behalf of the Royal Meteorological Society.},
	issn = {1477-870X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.3698},
	doi = {10.1002/qj.3698},
	abstract = {Variability of rainfall in East Africa has major impacts on lives and livelihoods. From floods to droughts, this variability is important on short daily time-scales to longer decadal time-scales, as is apparent from the devastating effects of droughts in East Africa over recent decades. Past studies have highlighted the Congo airmass in enhancing East African rainfall. Our detailed analysis of the feature shows that days with a westerly moisture flow, bringing the Congo airmass, enhance rainfall by up to 100\% above the daily mean, depending on the time of year. Conversely, there is a suppression of rainfall on days with a strong easterly flow. Days with a westerly moisture flux are in a minority in all seasons but we show that long rains with more westerly days are wetter, and that during the most-recent decade which has had more frequent droughts (associated with the “Eastern African climate paradox”), there has been few days with such westerlies. We also investigate the influence of the Madden–Julian Oscillation (MJO) and tropical cyclones, and their interaction with the westerly flow. We show that days of westerly moisture flux are more likely during phases 3 and 4 of the MJO and when there are one or more tropical cyclones present. In addition, tropical cyclones are more likely to form during these phases of the MJO, and more likely to be coincident with westerlies when forming to the east of Madagascar. Overall, our analysis brings together many different processes that have been discussed in the literature but not yet considered in complete combination. The results demonstrate the importance of the Congo airmass on daily to climate time-scales, and in doing so offers useful angles of investigation for future studies into prediction of East African rainfall.},
	language = {en},
	number = {727},
	urldate = {2023-09-12},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Finney, Declan L. and Marsham, John H. and Walker, Dean P. and Birch, Cathryn E. and Woodhams, Beth J. and Jackson, Lawrence S. and Hardy, Sam},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3698},
	keywords = {Congo airmass, East Africa, Madden–Julian Oscillation, long rains, rainfall, tropical cyclones},
	pages = {647--664},
}

@article{black_observational_2003,
	title = {An {Observational} {Study} of the {Relationship} between {Excessively} {Strong} {Short} {Rains} in {Coastal} {East} {Africa} and {Indian} {Ocean} {SST}},
	volume = {131},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/131/1/1520-0493_2003_131_0074_aosotr_2.0.co_2.xml},
	doi = {10.1175/1520-0493(2003)131<0074:AOSOTR>2.0.CO;2},
	abstract = {Abstract Composites of SST, wind, rainfall, and humidity have been constructed for years of high rainfall during September, October, and November (SON) in equatorial and southern-central East Africa. These show that extreme East African short rains are associated with large-scale SST anomalies in the Indian Ocean that closely resemble those that develop during Indian Ocean dipole or zonal mode (IOZM) events. This is corroborated by the observation that strong IOZM events produce enhanced East African rainfall. However, it is also shown that the relationship between the IOZM and East African rainfall is nonlinear, with only IOZM events that reverse the zonal SST gradient for several months (extreme events) triggering high rainfall. Comparison of the wind anomalies that develop during extreme IOZM events with those that develop during weaker (moderate) events shows that strong easterly anomalies in the northern-central Indian Ocean are a persistent feature of extreme, but not of moderate, IOZM years. It is suggested that these anomalies weaken the westerly flow that normally transports moisture away from the African continent, out over the Indian Ocean. Thus, during extreme IOZM years, rainfall is enhanced over East Africa and reduced in the central and eastern Indian Ocean basin. It is also shown that the IOZM cannot be viewed in isolation from the El Niño–Southern Oscillation (ENSO). Instead it is postulated that in some years, a strong ENSO forcing can predispose the Indian Ocean coupled system to an IOZM event and is therefore a contributory factor in extreme East African rainfall. The results of this study imply that the relationship between El Niño and the IOZM explains the previously described association between El Niño and high East African rainfall. Thus, understanding the way that ENSO drives Indian Ocean dynamics may aid the development of predictive scenarios for East African climate that could have significant economic implications.},
	language = {EN},
	number = {1},
	urldate = {2023-09-12},
	journal = {Monthly Weather Review},
	author = {Black, Emily and Slingo, Julia and Sperber, Kenneth R.},
	month = jan,
	year = {2003},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {74--94},
}

@article{pinson_discrimination_nodate,
	title = {Discrimination ability of the {Energy} score},
	abstract = {Research on generating and veriﬁcation of multivariate probabilistic forecasts has gained increased interest over the last few years. Emphasis is placed here on the evaluation of forecast quality with the Energy score, which is based on a quadratic scoring rule. While this score may be seen as appealing since being proper, we show that its discrimination ability may be limited when focusing on the dependence structure of multivariate probabilistic forecasts. For the case of multivariate Gaussian process, a theoretical upper for such discrimination ability is derived and discussed. This limited discrimination ability may eventually get compromised by computational and sampling issues, as dimension increases.},
	language = {en},
	author = {Pinson, Pierre and Tastu, Julija},
}

@article{scheuerer_variogram-based_2015-1,
	title = {Variogram-{Based} {Proper} {Scoring} {Rules} for {Probabilistic} {Forecasts} of {Multivariate} {Quantities}},
	volume = {143},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/143/4/mwr-d-14-00269.1.xml},
	doi = {10.1175/MWR-D-14-00269.1},
	abstract = {Abstract Proper scoring rules provide a theoretically principled framework for the quantitative assessment of the predictive performance of probabilistic forecasts. While a wide selection of such scoring rules for univariate quantities exists, there are only few scoring rules for multivariate quantities, and many of them require that forecasts are given in the form of a probability density function. The energy score, a multivariate generalization of the continuous ranked probability score, is the only commonly used score that is applicable in the important case of ensemble forecasts, where the multivariate predictive distribution is represented by a finite sample. Unfortunately, its ability to detect incorrectly specified correlations between the components of the multivariate quantity is somewhat limited. In this paper the authors present an alternative class of proper scoring rules based on the geostatistical concept of variograms. The sensitivity of these variogram-based scoring rules to incorrectly predicted means, variances, and correlations is studied in a number of examples with simulated observations and forecasts; they are shown to be distinctly more discriminative with respect to the correlation structure. This conclusion is confirmed in a case study with postprocessed wind speed forecasts at five wind park locations in Colorado.},
	language = {EN},
	number = {4},
	urldate = {2023-09-11},
	journal = {Monthly Weather Review},
	author = {Scheuerer, Michael and Hamill, Thomas M.},
	month = apr,
	year = {2015},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {1321--1334},
}

@misc{ecmwf_operational_2023,
	title = {Operational configurations of the {ECMWF} {Integrated} {Forecasting} {System} ({IFS})},
	url = {https://confluence.ecmwf.int/pages/viewpage.action?pageId=324860211},
	urldate = {2023-01-09},
	journal = {Operational configurations of the ECMWF Integrated Forecasting System (IFS)},
	author = {{ECMWF}},
	month = feb,
	year = {2023},
	note = {https://confluence.ecmwf.int/pages/viewpage.action?pageId=324860211. Accessed 1st September},
}

@misc{ben-bouallegue_improving_2023,
	title = {Improving medium-range ensemble weather forecasts with hierarchical ensemble transformers},
	url = {http://arxiv.org/abs/2303.17195},
	doi = {10.48550/arXiv.2303.17195},
	abstract = {Statistical post-processing of global ensemble weather forecasts is revisited by leveraging recent developments in machine learning. Verification of past forecasts is exploited to learn systematic deficiencies of numerical weather predictions in order to boost post-processed forecast performance. Here, we introduce PoET, a post-processing approach based on hierarchical transformers. PoET has 2 major characteristics: 1) the post-processing is applied directly to the ensemble members rather than to a predictive distribution or a functional of it, and 2) the method is ensemble-size agnostic in the sense that the number of ensemble members in training and inference mode can differ. The PoET output is a set of calibrated members that has the same size as the original ensemble but with improved reliability. Performance assessments show that PoET can bring up to 20\% improvement in skill globally for 2m temperature and 2\% for precipitation forecasts and outperforms the simpler statistical member-by-member method, used here as a competitive benchmark. PoET is also applied to the ENS10 benchmark dataset for ensemble post-processing and provides better results when compared to other deep learning solutions that are evaluated for most parameters. Furthermore, because each ensemble member is calibrated separately, downstream applications should directly benefit from the improvement made on the ensemble forecast with post-processing.},
	urldate = {2023-08-30},
	publisher = {arXiv},
	author = {Ben-Bouallegue, Zied and Weyn, Jonathan A. and Clare, Mariana C. A. and Dramsch, Jesper and Dueben, Peter and Chantry, Matthew},
	month = aug,
	year = {2023},
	note = {arXiv:2303.17195},
	keywords = {Physics - Atmospheric and Oceanic Physics},
}

@misc{floodlist_somalia_2023,
	title = {Somalia – {Flooding} {Displaces} {Thousands}, {Prompts} {Urgent} {Humanitarian} {Response}},
	url = {https://floodlist.com/africa/somalia-floods-may-2023},
	urldate = {2023-08-04},
	author = {Floodlist},
	month = may,
	year = {2023},
	note = {https://floodlist.com/africa/somalia-floods-may-2023. Accessed May 2023.},
}

@article{dinku_validation_2007,
	title = {Validation of satellite rainfall products over {East} {Africa}'s complex topography},
	volume = {28},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160600954688},
	doi = {10.1080/01431160600954688},
	abstract = {An extensive evaluation of 10 different satellite rainfall products was performed using station network over a complex topography, where elevation varies from below sea level to 4620 m. Evaluation was for two groups of products. The first group had low spatial (2.5°) and temporal (monthly) resolution and included the Global Precipitation Climatology Project (GPCP), the National Oceanographic and Atmospheric Administration Climate Prediction Center (NOAA‐CPC) merged analysis (CMAP), and the Tropical Rainfall Measuring Mission (TRMM‐3B43). The second group comprised products with relatively high spatial (0.1° to 1°) and temporal (3‐hourly to 10‐daily) resolution. These included the NOAA‐CPC African rainfall estimation algorithm, GPCP one‐degree‐daily (1DD), TRMM‐3B42, Tropical Applications of Meteorology using SATellite and other data (TAMSAT) estimates, and the CPC morphing technique (CMORPH). These products were aggregated to a 10‐day total and remapped to spatial resolutions of 1°, 0.5° and 0.25°. TRMM‐3B43 and CMAP from the first group and CMORPH, TAMSAT and TRMM‐3B42 from the second group performed reasonably well.},
	number = {7},
	urldate = {2023-09-06},
	journal = {International Journal of Remote Sensing},
	author = {Dinku, T. and Ceccato, P. and Grover‐Kopec, E. and Lemma, M. and Connor, S. J. and Ropelewski, C. F.},
	month = apr,
	year = {2007},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431160600954688},
	pages = {1503--1526},
}

@techreport{finney_scientific_2019,
	title = {Scientific {Understanding} of {East} {African} climate change from the {HyCRISTAL} project},
	url = {http://eprints.whiterose.ac.uk/151635/},
	abstract = {Integrating Hydro-Climate Science into Policy Decisions for Climate-Resilient Infrastructure and Livelihoods in East Africa (HyCRISTAL) is a Future Climate for Africa (FCFA) project funded to deliver new understanding of East African climate change and its impacts, and to demonstrate use of climate change information in long-term decision-making in the region. Here, we briefly summarise key findings from HyCRISTAL so far on climate change, as well as key findings from the pan-African FCFA project “IMPALA” relevant to East Africa, both in the context of previous literature on the topic.},
	urldate = {2023-08-01},
	institution = {University of Leeds},
	author = {Finney, D and Marsham, J and Rowell, D and Way, C and Evans, B and Cornforth, R and Houghton-Carr, H and Mittal, N and Allan, R and Anande, D and Anyah, R and Ascott, M and Black, E and Boorman, P and Booth, B and Bornemann, J and Burgin, L and Evans, J and Gudoshava, M and Kendon, E and Kisembe, J and Kondowe, A and Lapworth, D and Lopez-Gonzalez, G and Lwiza, K and Macdonald, D and Maidment, R and Misiani, H and Nakabugo, R and Sabiiti, G and Sangalugembe, C and Scanell, C and Segele, Z and Semazzi, F and Smith, K and Tadege, A and Tesfaye, K and Waniha, P and Wainwright, C and Wilby, R and Winterbourn, B and Xia, S},
	year = {2019},
	doi = {10.5518/100/19},
}

@article{dezfuli_precipitation_2017,
	title = {Precipitation {Characteristics} in {West} and {East} {Africa} from {Satellite} and in {Situ} {Observations}},
	volume = {18},
	issn = {1525-7541, 1525-755X},
	url = {https://journals.ametsoc.org/view/journals/hydr/18/6/jhm-d-17-0068_1.xml},
	doi = {10.1175/JHM-D-17-0068.1},
	abstract = {Abstract Using in situ data, three precipitation classes are identified for rainy seasons of West and East Africa: weak convective rainfall (WCR), strong convective rainfall (SCR), and mesoscale convective systems (MCSs). Nearly 75\% of the total seasonal precipitation is produced by the SCR and MCSs, even though they represent only 8\% of the rain events. Rain events in East Africa tend to have a longer duration and lower intensity than in West Africa, reflecting different characteristics of the SCR and MCS events in these two regions. Surface heating seems to be the primary convection trigger for the SCR, particularly in East Africa, whereas the WCR requires a dynamical trigger such as low-level convergence. The data are used to evaluate the performance of the recently launched Integrated Multisatellite Retrievals for Global Precipitation Measurement (IMERG) project. The IMERG-based precipitation shows significant improvement over its predecessor, the Tropical Rainfall Measuring Mission (TRMM) Multisatellite Precipitation Analysis (TMPA), particularly in capturing the MCSs, due to its improved temporal resolution.},
	language = {EN},
	number = {6},
	urldate = {2023-09-04},
	journal = {Journal of Hydrometeorology},
	author = {Dezfuli, Amin K. and Ichoku, Charles M. and Mohr, Karen I. and Huffman, George J.},
	month = jun,
	year = {2017},
	note = {Publisher: American Meteorological Society
Section: Journal of Hydrometeorology},
	pages = {1799--1805},
}

@article{kendon_enhanced_2019,
	title = {Enhanced future changes in wet and dry extremes over {Africa} at convection-permitting scale},
	volume = {10},
	copyright = {2019 Crown},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-09776-9},
	doi = {10.1038/s41467-019-09776-9},
	abstract = {African society is particularly vulnerable to climate change. The representation of convection in climate models has so far restricted our ability to accurately simulate African weather extremes, limiting climate change predictions. Here we show results from climate change experiments with a convection-permitting (4.5 km grid-spacing) model, for the first time over an Africa-wide domain (CP4A). The model realistically captures hourly rainfall characteristics, unlike coarser resolution models. CP4A shows greater future increases in extreme 3-hourly precipitation compared to a convection-parameterised 25 km model (R25). CP4A also shows future increases in dry spell length during the wet season over western and central Africa, weaker or not apparent in R25. These differences relate to the more realistic representation of convection in CP4A, and its response to increasing atmospheric moisture and stability. We conclude that, with the more accurate representation of convection, projected changes in both wet and dry extremes over Africa may be more severe.},
	language = {en},
	number = {1},
	urldate = {2023-09-04},
	journal = {Nature Communications},
	author = {Kendon, Elizabeth J. and Stratton, Rachel A. and Tucker, Simon and Marsham, John H. and Berthou, Ségolène and Rowell, David P. and Senior, Catherine A.},
	month = apr,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Climate and Earth system modelling, Hydrology, Projection and prediction},
	pages = {1794},
}

@misc{keisler_forecasting_2022,
	title = {Forecasting {Global} {Weather} with {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2202.07575},
	doi = {10.48550/arXiv.2202.07575},
	abstract = {We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Keisler, Ryan},
	month = feb,
	year = {2022},
	note = {arXiv:2202.07575 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@misc{yang_improving_2023,
	title = {Improving {Seasonal} {Forecast} of {Summer} {Precipitation} in {Southeastern} {China} using {CycleGAN} {Deep} {Learning} {Bias} {Correction}},
	url = {https://essopenarchive.org/users/655873/articles/661468-improving-seasonal-forecast-of-summer-precipitation-in-southeastern-china-using-cyclegan-deep-learning-bias-correction?commit=6fe5aad26bce582dd78270aa8ad3b99a0acc0f37},
	abstract = {Accurate seasonal precipitation forecasts, especially for extreme events, are crucial to preventing meteorological hazards and its potential impacts on national development, social stability, and security. However, the intensity of summer precipitation is often signiﬁcantly underestimated in many current dynamical models. This study uses a deep learning method called CycleConsistent Generative Adversarial Networks (CycleGAN) to enhance the seasonal forecast skill of the Nanjing University of Information Science \& Technology Climate Forecast System (NUIST-CFS1.0) in predicting June-July-August precipitation in southeastern China. The results suggest that the CycleGAN-based model signiﬁcantly improves the accuracy in predicting the spatial-temporal distribution of summer precipitation than traditional quantile mapping (QM) method. Due to the use of unpaired day-to-day correction models, we can pay more attention to the frequency, intensity, and duration of extreme precipitation events in the climate dynamical model forecast. This study expands the potential applications of deep learning models to improving seasonal precipitation forecasts.},
	urldate = {2023-09-04},
	author = {Yang, Song and Ling, Fenghua and Bai, Lei and Luo, Jing-Jia},
	month = aug,
	year = {2023},
	note = {ESS Open Archive},
}

@techreport{wilkinson_forecasting_2018,
	title = {Forecasting hazards, averting disasters: implementing forecast-based early action at scale},
	shorttitle = {Forecasting hazards, averting disasters},
	abstract = {This report examines current pilots in the emerging forecast-based early action programming and finance and offers suggestions for scale.},
	language = {en-gb},
	urldate = {2023-08-04},
	institution = {Overseas Development Institute},
	author = {Wilkinson, Emily and Weingärtner, Lena and Choularton, Richard and Bailey, Meghan and Todd, Martin and Kniveton, Dominic and Cabot Venton, Courtenay},
	month = mar,
	year = {2018},
}

@incollection{wilcox_chapter_2022,
	title = {Chapter 3 - {Estimating} {Measures} of {Location} and {Scale}},
	isbn = {978-0-12-820098-8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128200988000099},
	abstract = {Chapter 3 describes how to estimate the measures of location and scatter that were introduced in Chapter 2. Some related estimators are described and their relative merits are summarized. Included is a summary of some methods for estimating distributions, including kernel density estimators, which have practical value for reasons illustrated in subsequent chapters. Strategies for detecting outliers are introduced. Practical concerns about detecting outliers via the mean and variance are described and illustrated. Theoretically sound methods for estimating the standard errors of robust estimators are described, which are not intuitive based on standard training in an applied statistics course. R functions for estimating robust measures of location, and their standard errors, are described and illustrated.},
	booktitle = {Introduction to {Robust} {Estimation} and {Hypothesis} {Testing} ({Fifth} {Edition})},
	publisher = {Academic Press},
	author = {Wilcox, Rand R.},
	editor = {Wilcox, Rand R.},
	year = {2022},
	doi = {https://doi.org/10.1016/B978-0-12-820098-8.00009-9},
	keywords = {Bootstrap methods, Density estimators, Finite breakdown point, M-estimators, Outlier detection methods, Quantile estimators, Robust measures of scatter, Skipped estimators, Trimmed means, Winsorization},
	pages = {45--106},
}

@misc{stanczuk_wasserstein_2021,
	title = {Wasserstein {GANs} {Work} {Because} {They} {Fail} (to {Approximate} the {Wasserstein} {Distance})},
	url = {http://arxiv.org/abs/2103.01678},
	doi = {10.48550/arXiv.2103.01678},
	abstract = {Wasserstein GANs are based on the idea of minimising the Wasserstein distance between a real and a generated distribution. We provide an in-depth mathematical analysis of differences between the theoretical setup and the reality of training Wasserstein GANs. In this work, we gather both theoretical and empirical evidence that the WGAN loss is not a meaningful approximation of the Wasserstein distance. Moreover, we argue that the Wasserstein distance is not even a desirable loss function for deep generative models, and conclude that the success of Wasserstein GANs can in truth be attributed to a failure to approximate the Wasserstein distance.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Stanczuk, Jan and Etmann, Christian and Kreusser, Lisa Maria and Schönlieb, Carola-Bibiane},
	month = oct,
	year = {2021},
	note = {arXiv:2103.01678},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{reynolds_wgne_2018,
	title = {{WGNE} {Systematic} {Error} {Survey} {Results} {Summary}, 5th {WGNE} workshop on systematic errors in weather and climate models},
	shorttitle = {Systematic {Errors} in {Weather} and {Climate} {Models}},
	url = {https://journals.ametsoc.org/view/journals/bams/99/4/bams-d-17-0287.1.xml},
	language = {en},
	urldate = {2023-08-04},
	author = {Reynolds, Carolyn and Williams, Keith and Zadra, Ayrton},
	month = apr,
	year = {2018},
}

@misc{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
}

@misc{li_seeds_2023,
	title = {{SEEDS}: {Emulation} of {Weather} {Forecast} {Ensembles} with {Diffusion} {Models}},
	shorttitle = {{SEEDS}},
	url = {http://arxiv.org/abs/2306.14066},
	doi = {10.48550/arXiv.2306.14066},
	abstract = {Probabilistic forecasting is crucial to decision-making under uncertainty about future weather. The dominant approach is to use an ensemble of forecasts to represent and quantify uncertainty in operational numerical weather prediction. However, generating ensembles is computationally costly. In this paper, we propose to generate ensemble forecasts at scale by leveraging recent advances in generative artificial intelligence. Our approach learns a data-driven probabilistic diffusion model from the 5-member ensemble GEFS reforecast dataset. The model can then be sampled efficiently to produce realistic weather forecasts, conditioned on a few members of the operational GEFS forecasting system. The generated ensembles have similar predictive skill as the full GEFS 31-member ensemble, evaluated against ERA5 reanalysis, and emulate well the statistics of large physics-based ensembles. We also apply the same methodology to developing a diffusion model for generative post-processing: the model directly learns to correct biases present in the emulated forecasting system by leveraging reanalysis data as labels during training. Ensembles from this generative post-processing model show greater reliability and accuracy, particularly in extreme event classification. In general, they are more reliable and forecast the probability of extreme weather more accurately than the GEFS operational ensemble. Our models achieve these results at less than 1/10th of the computational cost incurred by the operational GEFS system.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Li, Lizao and Carver, Rob and Lopez-Gomez, Ignacio and Sha, Fei and Anderson, John},
	month = jun,
	year = {2023},
	note = {arXiv:2306.14066},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@misc{horat_deep_2023,
	title = {Deep learning for post-processing global probabilistic forecasts on sub-seasonal time scales},
	url = {http://arxiv.org/abs/2306.15956},
	doi = {10.48550/arXiv.2306.15956},
	abstract = {Sub-seasonal weather forecasts are becoming increasingly important for a range of socio-economic activities. However, the predictive ability of physical weather models is very limited on these time scales. We propose several post-processing methods based on convolutional neural networks to improve sub-seasonal forecasts by correcting systematic errors of numerical weather prediction models. Our post-processing models operate directly on spatial input fields and are therefore able to retain spatial relationships and to generate spatially homogeneous predictions. They produce global probabilistic tercile forecasts for biweekly aggregates of temperature and precipitation for weeks 3-4 and 5-6. In a case study based on a public forecasting challenge organized by the World Meteorological Organization, our post-processing models outperform recalibrated forecasts from the European Centre for Medium-Range Weather Forecasts (ECMWF), and achieve improvements over climatological forecasts for all considered variables and lead times. We compare several model architectures and training modes and demonstrate that all approaches lead to skillful and well-calibrated probabilistic forecasts. The good calibration of the post-processed forecasts emphasizes that our post-processing models reliably quantify the forecast uncertainty based on deterministic input information in form of the ECMWF ensemble mean forecast fields only.},
	urldate = {2023-09-01},
	publisher = {arXiv},
	author = {Horat, Nina and Lerch, Sebastian},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15956},
	keywords = {Physics - Atmospheric and Oceanic Physics},
}

@misc{dziugaite_training_2015,
	title = {Training generative neural networks via {Maximum} {Mean} {Discrepancy} optimization},
	url = {http://arxiv.org/abs/1505.03906},
	abstract = {We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic---informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. (2012). We compare to the adversarial nets framework introduced by Goodfellow et al. (2014), in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M. and Ghahramani, Zoubin},
	month = may,
	year = {2015},
	note = {arXiv:1505.03906},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{duncan_generative_2022,
	title = {Generative {Modeling} of {High}-resolution {Global} {Precipitation} {Forecasts}},
	abstract = {Forecasting global precipitation patterns and, in particular, extreme precipitation events is of critical importance to preparing for and adapting to climate change. Making accurate high-resolution precipitation forecasts using traditional physical models remains a major challenge in operational weather forecasting as they incur substantial computational costs and struggle to achieve sufficient forecast skill. Recently, deep-learning-based models have shown great promise in closing the gap with numerical weather prediction (NWP) models in terms of precipitation forecast skill, opening up exciting new avenues for precipitation modeling. However, it is challenging for these deep learning models to fully resolve the fine-scale structures of precipitation phenomena and adequately characterize the extremes of the long-tailed precipitation distribution. In this work, we present several improvements to the architecture and training process of a current state-of-the art deep learning precipitation model (FourCastNet) using a novel generative adversarial network (GAN) to better capture fine scales and extremes. Our improvements achieve superior performance in capturing the extreme percentiles of global precipitation, while comparable to state-of-the-art NWP models in terms of forecast skill at 1-2 day lead times. Together, these improvements set a new state-of-the-art in global precipitation forecasting.},
	author = {Duncan, James and Subramanian, Shashank and Harrington, Peter},
	year = {2022},
	note = {arXiv:2210.12504},
}

@misc{bi_pangu-weather_2022,
	title = {Pangu-{Weather}: {A} {3D} {High}-{Resolution} {Model} for {Fast} and {Accurate} {Global} {Weather} {Forecast}},
	url = {http://arxiv.org/abs/2211.02556},
	abstract = {In this paper, we present Pangu-Weather, a deep learning based system for fast and accurate global weather forecast. For this purpose, we establish a data-driven environment by downloading \$43\$ years of hourly global weather data from the 5th generation of ECMWF reanalysis (ERA5) data and train a few deep neural networks with about \$256\$ million parameters in total. The spatial resolution of forecast is \$0.25{\textasciicircum}{\textbackslash}circ{\textbackslash}times0.25{\textasciicircum}{\textbackslash}circ\$, comparable to the ECMWF Integrated Forecast Systems (IFS). More importantly, for the first time, an AI-based method outperforms state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy (latitude-weighted RMSE and ACC) of all factors (e.g., geopotential, specific humidity, wind speed, temperature, etc.) and in all time ranges (from one hour to one week). There are two key strategies to improve the prediction accuracy: (i) designing a 3D Earth Specific Transformer (3DEST) architecture that formulates the height (pressure level) information into cubic data, and (ii) applying a hierarchical temporal aggregation algorithm to alleviate cumulative forecast errors. In deterministic forecast, Pangu-Weather shows great advantages for short to medium-range forecast (i.e., forecast time ranges from one hour to one week). Pangu-Weather supports a wide range of downstream forecast scenarios, including extreme weather forecast (e.g., tropical cyclone tracking) and large-member ensemble forecast in real-time. Pangu-Weather not only ends the debate on whether AI-based methods can surpass conventional NWP methods, but also reveals novel directions for improving deep learning weather forecast systems.},
	author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
	month = nov,
	year = {2022},
	note = {arXiv: 2211.02556},
}

@inproceedings{arjovsky_towards_2016,
	title = {Towards {Principled} {Methods} for {Training} {Generative} {Adversarial} {Networks}},
	url = {https://openreview.net/forum?id=Hk4_qw5xe},
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Learning} {Representations}},
	author = {Arjovsky, Martin and Bottou, Leon},
	month = nov,
	year = {2016},
}

@techreport{ifrc_world_2014,
	title = {World {Disasters} {Report}, {Focus} on culture and risk},
	url = {https://www.ifrc.org/document/world-disasters-report-2014},
	language = {en},
	urldate = {2023-07-27},
	institution = {International Federation of Red Cross and Red Crescent Societies},
	author = {IFRC},
	year = {2014},
}

@article{palmer_drivers_2023,
	title = {Drivers and impacts of {Eastern} {African} rainfall variability},
	volume = {4},
	copyright = {2023 Springer Nature Limited},
	issn = {2662-138X},
	url = {https://www.nature.com/articles/s43017-023-00397-x},
	doi = {10.1038/s43017-023-00397-x},
	abstract = {Eastern Africa exhibits bimodal rainfall consisting of long rains (March–May) and short rains (October–December), changes in which have profound socioeconomic and environmental impacts. In this Review, we examine the drivers and corresponding impacts of Eastern African rainfall variability. Remote teleconnections, namely the El Niño–Southern Oscillation and the Indian Ocean Dipole, exert a dominant influence on interannual variability. From the mid-1980s to 2010, the long rains have tended toward a drier state (trends of −0.65 to −2.95 mm season−1 year−1), with some recovery thereafter, while the short rains have become wetter since the mid 1980s (1.44 to 2.36 mm season−1 year−1). These trends, overlain by substantial year-to-year variations, affect the severity and frequency of extreme flooding and droughts, the stability of food and energy systems, the susceptibility to water-borne and vector-borne diseases, and ecosystem stability. Climate model projections of rainfall changes differ, but there is some consensus that the short rains will deliver more rainfall than the long rains by 2030–2040, with implications for sustaining agricultural yields and triggering climate-related public health emergencies. Mitigating the impacts of future Eastern African climate requires continued investments in agriculture, clean water, medical and emergency infrastructures, and development and adoption of adaptation strategies, as well as targeted early-warning systems driven by improved meteorological observations.},
	language = {en},
	number = {4},
	urldate = {2023-09-01},
	journal = {Nature Reviews Earth \& Environment},
	author = {Palmer, Paul I. and Wainwright, Caroline M. and Dong, Bo and Maidment, Ross I. and Wheeler, Kevin G. and Gedney, Nicola and Hickman, Jonathan E. and Madani, Nima and Folwell, Sonja S. and Abdo, Gamal and Allan, Richard P. and Black, Emily C. L. and Feng, Liang and Gudoshava, Masilin and Haines, Keith and Huntingford, Chris and Kilavi, Mary and Lunt, Mark F. and Shaaban, Ahmed and Turner, Andrew G.},
	month = apr,
	year = {2023},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Atmospheric dynamics, Carbon cycle, Climate and Earth system modelling, Climate-change impacts},
	pages = {254--270},
}

@inproceedings{lucic_are_2018,
	title = {Are {GANs} {Created} {Equal}? {A} {Large}-{Scale} {Study}},
	volume = {31},
	shorttitle = {Are {GANs} {Created} {Equal}?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html},
	abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others.  We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes.  To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed.  Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in {\textbackslash}cite\{goodfellow2014generative\}.},
	urldate = {2023-09-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier},
	year = {2018},
}

@article{hamill_interpretation_2001,
	title = {Interpretation of {Rank} {Histograms} for {Verifying} {Ensemble} {Forecasts}},
	volume = {129},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/129/3/1520-0493_2001_129_0550_iorhfv_2.0.co_2.xml},
	doi = {10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2},
	abstract = {Abstract Rank histograms are a tool for evaluating ensemble forecasts. They are useful for determining the reliability of ensemble forecasts and for diagnosing errors in its mean and spread. Rank histograms are generated by repeatedly tallying the rank of the verification (usually an observation) relative to values from an ensemble sorted from lowest to highest. However, an uncritical use of the rank histogram can lead to misinterpretations of the qualities of that ensemble. For example, a flat rank histogram, usually taken as a sign of reliability, can still be generated from unreliable ensembles. Similarly, a U-shaped rank histogram, commonly understood as indicating a lack of variability in the ensemble, can also be a sign of conditional bias. It is also shown that flat rank histograms can be generated for some model variables if the variance of the ensemble is correctly specified, yet if covariances between model grid points are improperly specified, rank histograms for combinations of model variables may not be flat. Further, if imperfect observations are used for verification, the observational errors should be accounted for, otherwise the shape of the rank histogram may mislead the user about the characteristics of the ensemble. If a statistical hypothesis test is to be performed to determine whether the differences from uniformity of rank are statistically significant, then samples used to populate the rank histogram must be located far enough away from each other in time and space to be considered independent.},
	language = {EN},
	number = {3},
	urldate = {2023-09-01},
	journal = {Monthly Weather Review},
	author = {Hamill, Thomas M.},
	month = mar,
	year = {2001},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {550--560},
}

@article{woodcock_evaluation_1976,
	title = {The {Evaluation} of {Yes}/{No} {Forecasts} for {Scientific} and {Administrative} {Purposes}},
	volume = {104},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/104/10/1520-0493_1976_104_1209_teoyff_2_0_co_2.xml},
	doi = {10.1175/1520-0493(1976)104<1209:TEOYFF>2.0.CO;2},
	abstract = {Abstract The basis upon which skill scores for evaluating yes/no categorical forecasts for scientific and administrative purposes depends is, discussed and many of the common discriminants (formulas from which skill scores are derived) are reviewed and compared. The common process of subjecting forecasts to a trial consisting of a mixture of event and non-event occasions is outlined. Those discriminants which prove to be measures of a forecasting technique's skill are shown, with the exception of Hanssen and Kuipers’ (1965) discriminant, to give skill scores which depend upon the mixture of events and non-events in the trial. All these discriminants give incompatible rankings of forecasts because they are based on different standards of skill. It is shown that this discrepancy is resolved by ensuring that the trials under which forecasts are compared have equal numbers of event and non-event occasions; under these conditions, rankings become compatible. Hanssen and Kuipers' discriminant is shown to give the best estimate on an “unequal“ trial to that expected if equalization were to be enforced. Hence, it is argued that Hanssen and Kuipers' discriminant is universally acceptable for evaluating yes/no forecasts for scientific and administrative purposes. Finally, the variance of Hanssen and Kuipers’ discriminant is given to enable the statistical significance of the difference between two scores to be assessed and thereby make comparisons between techniques more meaningful.},
	language = {EN},
	number = {10},
	urldate = {2023-08-29},
	journal = {Monthly Weather Review},
	author = {Woodcock, Frank},
	month = oct,
	year = {1976},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {1209--1214},
}

@misc{verma_deep_2023,
	title = {Deep {Learning} {Techniques} in {Extreme} {Weather} {Events}: {A} {Review}},
	shorttitle = {Deep {Learning} {Techniques} in {Extreme} {Weather} {Events}},
	url = {http://arxiv.org/abs/2308.10995},
	abstract = {Extreme weather events pose significant challenges, thereby demanding techniques for accurate analysis and precise forecasting to mitigate its impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. This review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures, across various aspects of weather prediction such as thunderstorm, lightning, precipitation, drought, heatwave, cold waves and tropical cyclones. We highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decisions and mitigate the impacts of extreme weather events.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Verma, Shikha and Srivastava, Kuldeep and Tiwari, Akhilesh and Verma, Shekhar},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10995 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@misc{harder_generating_2022,
	title = {Generating physically-consistent high-resolution climate data with hard-constrained neural networks},
	url = {http://arxiv.org/abs/2208.05424},
	abstract = {The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and therefore often predict quantities at a coarse spatial resolution. Statistical downscaling can provide an efficient method of upsampling low-resolution data. In this field, deep learning has been applied successfully, often using methods from the super-resolution domain in computer vision. Despite often achieving visually compelling results, such models often violate conservation laws when predicting physical variables. In order to conserve important physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep downscaling model while also increasing their performance according to traditional metrics. We introduce two ways of constraining the network: A renormalization layer added to the end of the neural network and a successive approach that scales with increasing upsampling factors. We show the applicability of our methods across different popular architectures and upsampling factors using ERA5 reanalysis data.},
	author = {Harder, Paula and Yang, Qidong and Ramesh, Venkatesh and Sattigeri, Prasanna and Hernandez-Garcia, Alex and Watson, Campbell and Szwarcman, Daniela and Rolnick, David},
	month = aug,
	year = {2022},
	note = {arXiv: 2208.05424},
	keywords = {★},
}

@misc{lam_graphcast_2022,
	title = {{GraphCast}: {Learning} skillful medium-range global weather forecasting},
	abstract = {We introduce a machine-learning (ML)-based weather simulator--called "GraphCast"--which outperforms the most accurate deterministic operational medium-range weather forecasting system in the world, as well as all previous ML baselines. GraphCast is an autoregressive model, based on graph neural networks and a novel high-resolution multi-scale mesh representation, which we trained on historical weather data from the European Centre for Medium-Range Weather Forecasts (ECMWF)'s ERA5 reanalysis archive. It can make 10-day forecasts, at 6-hour time intervals, of five surface variables and six atmospheric variables, each at 37 vertical pressure levels, on a 0.25-degree latitude-longitude grid, which corresponds to roughly 25 x 25 kilometer resolution at the equator. Our results show GraphCast is more accurate than ECMWF's deterministic operational forecasting system, HRES, on 90.0\% of the 2760 variable and lead time combinations we evaluated. GraphCast also outperforms the most accurate previous ML-based weather forecasting model on 99.2\% of the 252 targets it reported. GraphCast can generate a 10-day forecast (35 gigabytes of data) in under 60 seconds on Cloud TPU v4 hardware. Unlike traditional forecasting methods, ML-based forecasting scales well with data: by training on bigger, higher quality, and more recent data, the skill of the forecasts can improve. Together these results represent a key step forward in complementing and improving weather modeling with ML, open new opportunities for fast, accurate forecasting, and help realize the promise of ML-based simulation in the physical sciences.},
	author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Pritzel, Alexander and Ravuri, Suman and Ewalds, Timo and Alet, Ferran and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Stott, Jacklynn and Vinyals, Oriol and Mohamed, Shakir and Battaglia, Peter},
	month = dec,
	year = {2022},
	note = {arXiv: 2212.12794},
}

@misc{mouatadid_adaptive_2022,
	title = {Adaptive {Bias} {Correction} for {Improved} {Subseasonal} {Forecasting}},
	url = {http://arxiv.org/abs/2209.10666},
	abstract = {Subseasonal forecasting \${\textbackslash}unicode\{x2013\}\$ predicting temperature and precipitation 2 to 6 weeks \${\textbackslash}unicode\{x2013\}\$ ahead is critical for effective water allocation, wildfire management, and drought and flood mitigation. Recent international research efforts have advanced the subseasonal capabilities of operational dynamical models, yet temperature and precipitation prediction skills remains poor, partly due to stubborn errors in representing atmospheric dynamics and physics inside dynamical models. To counter these errors, we introduce an adaptive bias correction (ABC) method that combines state-of-the-art dynamical forecasts with observations using machine learning. When applied to the leading subseasonal model from the European Centre for Medium-Range Weather Forecasts (ECMWF), ABC improves temperature forecasting skill by 60-90\% and precipitation forecasting skill by 40-69\% in the contiguous U.S. We couple these performance improvements with a practical workflow, based on Cohort Shapley, for explaining ABC skill gains and identifying higher-skill windows of opportunity based on specific climate conditions.},
	author = {Mouatadid, Soukayna and Orenstein, Paulo and Flaspohler, Genevieve and Cohen, Judah and Oprescu, Miruna and Fraenkel, Ernest and Mackey, Lester},
	month = sep,
	year = {2022},
	note = {arXiv: 2209.10666},
	keywords = {★},
}

@misc{singh_urban_2022,
	title = {Urban precipitation downscaling using deep learning: a smart city application over {Austin}, {Texas}, {USA}},
	url = {http://arxiv.org/abs/2209.06848},
	abstract = {Urban downscaling is a link to transfer the knowledge from coarser climate information to city scale assessments. These high-resolution assessments need multiyear climatology of past data and future projections, which are complex and computationally expensive to generate using traditional numerical weather prediction models. The city of Austin, Texas, USA has seen tremendous growth in the past decade. Systematic planning for the future requires the availability of fine resolution city-scale datasets. In this study, we demonstrate a novel approach generating a general purpose operator using deep learning to perform urban downscaling. The algorithm employs an iterative super-resolution convolutional neural network (Iterative SRCNN) over the city of Austin, Texas, USA. We show the development of a high-resolution gridded precipitation product (300 m) from a coarse (10 km) satellite-based product (JAXA GsMAP). High resolution gridded datasets of precipitation offer insights into the spatial distribution of heavy to low precipitation events in the past. The algorithm shows improvement in the mean peak-signal-to-noise-ratio and mutual information to generate high resolution gridded product of size 300 m X 300 m relative to the cubic interpolation baseline. Our results have implications for developing high-resolution gridded-precipitation urban datasets and the future planning of smart cities for other cities and other climatic variables.},
	author = {Singh, Manmeet and Acharya, Nachiketa and Jamshidi, Sajad and Jiao, Junfeng and Yang, Zong-Liang and Coudert, Marc and Baumer, Zach and Niyogi, Dev},
	month = aug,
	year = {2022},
	note = {arXiv: 2209.06848},
}

@misc{gascon_post-processing_2023,
	title = {Post-processing output from ensembles with and without parametrised convection, to create accurate, blended, high-fidelity rainfall forecasts},
	url = {http://arxiv.org/abs/2301.04485},
	abstract = {Flash flooding is a significant societal problem, but related precipitation forecasts are often poor. To address this, one can try to use output from convection-parametrising (global) ensembles, post-processed to forecast at point-scale, or convection-resolving limited area ensembles. In this study, we combine both. First, we apply the "ecPoint-rainfall" post-processing to the ECMWF global ensemble. Then, we use 2.2km COSMO LAM ensemble output (centred on Italy), and also post-process it using a scale-selective neighbourhood approach to compensate for insufficient members. The two components then undergo lead-time-weighted blending, to create the final probabilistic 6h rainfall forecasts. Product creation for forecasters constituted the "Italy Flash Flood use case" within the EU-funded MISTRAL project and it will be a real-time open-access product. One year of verification shows that ecPoint is the most skilful ensemble product. The post-processed COSMO ensemble adds most value to summer convective events in the evening, when the global model has an underprediction bias. In two heavy rainfall case studies we observed underestimation of the largest point totals in the raw ECMWF ensemble, and overestimation in the raw COSMO ensemble. However, ecPoint increase the value and highlighted best the most affected areas, whilst post-processing of COSMO diminished extremes by eradicating unreliable detail. The final merged products looked best from a user perspective and seemed to be the most skilful of all. Although our LAM post-processing does not implicitly include bias correction (a topic for further work) our study nonetheless provides a unique blueprint for successfully combining ensemble rainfall forecasts from global and LAM systems around the world. It also has important implications for forecast products as global ensembles move ever closer to having convection-permitting resolution.},
	author = {Gascón, Estíbaliz and Montani, Andrea and Hewson, Tim D.},
	month = jan,
	year = {2023},
	note = {arXiv: 2301.04485},
}

@misc{leinonen_latent_2023,
	title = {Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification},
	url = {http://arxiv.org/abs/2304.12891},
	abstract = {Diffusion models have been widely adopted in image generation, producing higher-quality and more diverse samples than generative adversarial networks (GANs). We introduce a latent diffusion model (LDM) for precipitation nowcasting - short-term forecasting based on the latest observational data. The LDM is more stable and requires less computation to train than GANs, albeit with more computationally expensive generation. We benchmark it against the GAN-based Deep Generative Models of Rainfall (DGMR) and a statistical model, PySTEPS. The LDM produces more accurate precipitation predictions, while the comparisons are more mixed when predicting whether the precipitation exceeds predefined thresholds. The clearest advantage of the LDM is that it generates more diverse predictions than DGMR or PySTEPS. Rank distribution tests indicate that the distribution of samples from the LDM accurately reflects the uncertainty of the predictions. Thus, LDMs are promising for any applications where uncertainty quantification is important, such as weather and climate.},
	author = {Leinonen, Jussi and Hamann, Ulrich and Nerini, Daniele and Germann, Urs and Franch, Gabriele},
	month = apr,
	year = {2023},
	note = {arXiv: 2304.12891},
}

@misc{nguyen_climax_2023,
	title = {{ClimaX}: {A} foundation model for weather and climate},
	url = {http://arxiv.org/abs/2301.10343},
	abstract = {Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute while maintaining general utility. ClimaX is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6. The pre-trained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. Compared to existing data-driven baselines, we show that this generality in ClimaX results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets.},
	author = {Nguyen, Tung and Brandstetter, Johannes and Kapoor, Ashish and Gupta, Jayesh K. and Grover, Aditya},
	month = jan,
	year = {2023},
	note = {arXiv: 2301.10343},
	keywords = {★},
}

@misc{ramavajjala_verification_2023,
	title = {Verification against in-situ observations for {Data}-{Driven} {Weather} {Prediction}},
	url = {http://arxiv.org/abs/2305.00048},
	abstract = {Data-driven weather prediction models (DDWPs) have made rapid strides in recent years, demonstrating an ability to approximate Numerical Weather Prediction (NWP) models to a high degree of accuracy. The fast, accurate, and low-cost DDWP forecasts make their use in operational forecasting an attractive proposition, however, there remains work to be done in rigorously evaluating DDWPs in a true operational setting. Typically trained and evaluated using ERA5 reanalysis data, DDWPs have been tested only in a simulation, which cannot represent the real world with complete accuracy even if it is of a very high quality. The safe use of DDWPs in operational forecasting requires more thorough "real-world" verification, as well as a careful examination of how DDWPs are currently trained and evaluated. It is worth asking, for instance, how well do the reanalysis datasets, used for training, simulate the real world? With an eye towards climate justice and the uneven availability of weather data: is the simulation equally good for all regions of the world, and would DDWPs exacerbate biases present in the training data? Does a good performance in simulation correspond to good performance in operational settings? In addition to approximating the physics of NWP models, how can ML be uniquely deployed to provide more accurate weather forecasts? As a first step towards answering such questions, we present a robust dataset of in-situ observations derived from the NOAA MADIS program to serve as a benchmark to validate DDWPs in an operational setting. By providing a large corpus of quality-controlled, in-situ observations, this dataset provides a meaningful real-world task that all NWPs and DDWPs can be tested against. We hope that this data can be used not only to rigorously and fairly compare operational weather models but also to spur future research in new directions.},
	author = {Ramavajjala, Vivek and Mitra, Peetak P.},
	month = apr,
	year = {2023},
	note = {arXiv: 2305.00048},
	keywords = {★},
}

@misc{chammas_accelerating_2023,
	title = {Accelerating large-eddy simulations of clouds with {Tensor} {Processing} {Units}},
	url = {http://arxiv.org/abs/2301.04698},
	abstract = {Clouds, especially low clouds, are crucial for regulating Earth's energy balance and mediating the response of the climate system to changes in greenhouse gas concentrations. Despite their importance for climate, they remain relatively poorly understood and are inaccurately represented in climate models. A principal reason is that the high computational expense of simulating them with large-eddy simulations (LES) has inhibited broad and systematic numerical experimentation and the generation of large datasets for training parametrization schemes for climate models. Here we demonstrate LES of low clouds on Tensor Processing Units (TPUs), application-specific integrated circuits that were originally developed for machine learning applications. We show that TPUs in conjunction with tailored software implementations can be used to simulate computationally challenging stratocumulus clouds in conditions observed during the Dynamics and Chemistry of Marine Stratocumulus (DYCOMS) field study. The TPU-based LES code successfully reproduces clouds during DYCOMS and opens up the large computational resources available on TPUs to cloud simulations. The code enables unprecedented weak and strong scaling of LES, making it possible, for example, to simulate stratocumulus with \$10{\textbackslash}times\$ speedup over real-time evolution in domains with a \$34.7{\textasciitilde}{\textbackslash}mathrm\{km\} {\textbackslash}times 53.8{\textasciitilde}{\textbackslash}mathrm\{km\}\$ horizontal cross section. The results open up new avenues for computational experiments and for substantially enlarging the sample of LES available to train parameterizations of low clouds.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Chammas, Sheide and Wang, Qing and Schneider, Tapio and Ihme, Matthias and Chen, Yi-fan and Anderson, John},
	month = aug,
	year = {2023},
	note = {arXiv:2301.04698 [physics]},
	keywords = {J.2, Mathematics - Numerical Analysis, Physics - Atmospheric and Oceanic Physics, Physics - Computational Physics, Physics - Fluid Dynamics, Physics - Geophysics},
}

@misc{addison_machine_2022,
	title = {Machine learning emulation of a local-scale {UK} climate model},
	url = {http://arxiv.org/abs/2211.16116},
	abstract = {Climate change is causing the intensification of rainfall extremes. Precipitation projections with high spatial resolution are important for society to prepare for these changes, e.g. to model flooding impacts. Physics-based simulations for creating such projections are very computationally expensive. This work demonstrates the effectiveness of diffusion models, a form of deep generative models, for generating much more cheaply realistic high resolution rainfall samples for the UK conditioned on data from a low resolution simulation. We show for the first time a machine learning model that is able to produce realistic samples of high-resolution rainfall based on a physical model that resolves atmospheric convection, a key process behind extreme rainfall. By adding self-learnt, location-specific information to low resolution relative vorticity, quantiles and time-mean of the samples match well their counterparts from the high-resolution simulation.},
	author = {Addison, Henry and Kendon, Elizabeth and Ravuri, Suman and Aitchison, Laurence and Watson, Peter AG},
	month = nov,
	year = {2022},
	note = {arXiv: 2211.16116},
}

@article{macleod_are_2021,
	title = {Are {Kenya} {Meteorological} {Department} heavy rainfall advisories useful for forecast-based early action and early preparedness for flooding?},
	volume = {21},
	issn = {1561-8633},
	url = {https://nhess.copernicus.org/articles/21/261/2021/},
	doi = {10.5194/nhess-21-261-2021},
	abstract = {Preparedness saves lives. Forecasts can help improve preparedness by triggering early actions as part of pre-defined protocols under the Forecast-based Financing (FbF) approach; however it is essential to understand the skill of a forecast before using it as a trigger. In order to support the development of early-action protocols over Kenya, we evaluate the 33 heavy rainfall advisories (HRAs) issued by the Kenya Meteorological Department (KMD) during 2015–2019.

 The majority of HRAs warn counties which subsequently receive heavy rainfall within the forecast window. We also find a significant improvement in the advisory ability to anticipate flood events over time, with particularly high levels of skill in recent years. For instance actions with a 2-week lifetime based on advisories issued in 2015 and 2016 would have failed to anticipate nearly all recorded flood events in that period, whilst actions in 2019 would have anticipated over 70 \% of the instances of flooding at the county level. When compared against the most significant flood events over the period which led to significant loss of life, all three such periods during 2018 and 2019 were preceded by HRAs, and in these cases the advisories accurately warned the specific counties for which significant impacts were recorded. By contrast none of the four significant flooding events in 2015–2017 were preceded by advisories. This step change in skill may be due to developing forecaster experience with synoptic patterns associated with extremes as well as access to new dynamical prediction tools that specifically address extreme event probability; for example, KMD access to the UK Met Office Global Hazard Map was introduced at the end of 2017.

 Overall we find that KMD HRAs effectively warn of heavy rainfall and flooding and can be a vital source of information for early preparedness. However a lack of spatial detail on flood impacts and broad probability ranges limit their utility for systematic FbF approaches. We conclude with suggestions for making the HRAs more useful for FbF and outline the developing approach to flood forecasting in Kenya.},
	language = {English},
	number = {1},
	urldate = {2023-08-18},
	journal = {Natural Hazards and Earth System Sciences},
	author = {MacLeod, David and Kilavi, Mary and Mwangi, Emmah and Ambani, Maurine and Osunga, Michael and Robbins, Joanne and Graham, Richard and Rowhani, Pedram and Todd, Martin C.},
	month = jan,
	year = {2021},
	note = {Publisher: Copernicus GmbH},
	pages = {261--277},
}

@article{pendergrass_what_2018,
	title = {What precipitation is extreme?},
	volume = {360},
	url = {https://www.science.org/doi/10.1126/science.aat1871},
	doi = {10.1126/science.aat1871},
	number = {6393},
	urldate = {2023-08-15},
	journal = {Science},
	author = {Pendergrass, Angeline G.},
	month = jun,
	year = {2018},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1072--1073},
}

@misc{asperti_precipitation_2023,
	title = {Precipitation nowcasting with generative diffusion models},
	url = {http://arxiv.org/abs/2308.06733},
	doi = {10.48550/arXiv.2308.06733},
	abstract = {In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely prediction. In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data pertaining to Central Europe from the years 2016 to 2021. Within this context, we examine the efficacy of diffusion models in handling the task of precipitation nowcasting. Our work is conducted in comparison to the performance of well-established U-Net models, as documented in the existing literature. Our proposed approach of Generative Ensemble Diffusion (GED) utilizes a diffusion model to generate a set of possible weather scenarios which are then amalgamated into a probable prediction via the use of a post-processing network. This approach, in comparison to recent deep learning models, substantially outperformed them in terms of overall performance.},
	urldate = {2023-08-15},
	publisher = {arXiv},
	author = {Asperti, Andrea and Merizzi, Fabio and Paparella, Alberto and Pedrazzi, Giorgio and Angelinelli, Matteo and Colamonaco, Stefano},
	month = aug,
	year = {2023},
	note = {arXiv:2308.06733 [physics]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@article{pendergrass_what_2018-1,
	title = {What precipitation is extreme?},
	volume = {360},
	url = {https://www.science.org/doi/full/10.1126/science.aat1871},
	doi = {10.1126/science.aat1871},
	number = {6393},
	urldate = {2023-08-10},
	journal = {Science},
	author = {Pendergrass, Angeline G.},
	month = jun,
	year = {2018},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1072--1073},
}

@article{hession_spatial_2011,
	title = {A spatial regression analysis of the influence of topography on monthly rainfall in {East} {Africa}},
	volume = {31},
	copyright = {Copyright © 2010 Royal Meteorological Society},
	issn = {1097-0088},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.2174},
	doi = {10.1002/joc.2174},
	abstract = {Precipitation in Kenya is highly variable and dominated by a variety of physical processes. Statistical studies of climate patterns have historically focused on application of ordinary least squares (OLS) regression to test hypotheses related to multiple predictive variables, perhaps in an attempt to better understand the physical mechanisms that drive precipitation, or on use of spatially explicit models, typically kriging- or spline-based analyses, for the purpose of improving predictions. Each of these approaches may be individually useful; however, they all possess limitations. OLS approaches have yielded biased results in the presence of spatially autocorrelated data. Kriging- and spline-based studies often focus on providing improved predictions rather than understanding. Here we use spatial regression, a method not commonly used in analysis of climate data, to assess the role of predictive variables while explicitly incorporating spatial autocorrelation in parameter estimation and hypothesis testing. This approach can yield a better understanding of relationships between precipitation and multiple predictive variables with improved statistical rigour. Using spatial regression, we show that topographic variables such as elevation and slope strongly influence rainfall during the ‘long rains’ and ‘short rains’, which are vital for Kenyan agriculture. Outside these seasons, we find that smaller (mesoscale) variations in elevation are statistically significant. Further, we demonstrate the shortcomings of automated selection procedures such as stepwise regression through the identification of spurious results due to confounding. Copyright © 2010 Royal Meteorological Society},
	language = {en},
	number = {10},
	urldate = {2023-08-10},
	journal = {International Journal of Climatology},
	author = {Hession, S. L. and Moore, N.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.2174},
	keywords = {East Africa, Kenya, geostatistics, precipitation, spatial regression, topography},
	pages = {1440--1456},
}

@misc{yang_fourier_2023,
	title = {Fourier {Neural} {Operators} for {Arbitrary} {Resolution} {Climate} {Data} {Downscaling}},
	url = {http://arxiv.org/abs/2305.14452},
	abstract = {Climate simulations are essential in guiding our understanding of climate change and responding to its effects. However, it is computationally expensive to resolve complex climate processes at high spatial resolution. As one way to speed up climate simulations, neural networks have been used to downscale climate variables from fast-running low-resolution simulations, but high-resolution training data are often unobtainable or scarce, greatly limiting accuracy. In this work, we propose a downscaling method based on the Fourier neural operator. It trains with data of a small upsampling factor and then can zero-shot downscale its input to arbitrary unseen high resolution. Evaluated both on ERA5 climate model data and on the Navier-Stokes equation solution data, our downscaling model significantly outperforms state-of-the-art convolutional and generative adversarial downscaling models, both in standard single-resolution downscaling and in zero-shot generalization to higher upsampling factors. Furthermore, we show that our method also outperforms state-of-the-art data-driven partial differential equation solvers on Navier-Stokes equations. Overall, our work bridges the gap between simulation of a physical process and interpolation of low-resolution output, showing that it is possible to combine both approaches and significantly improve upon each other.},
	urldate = {2023-08-09},
	publisher = {arXiv},
	author = {Yang, Qidong and Hernandez-Garcia, Alex and Harder, Paula and Ramesh, Venkatesh and Sattegeri, Prasanna and Szwarcman, Daniela and Watson, Campbell D. and Rolnick, David},
	month = may,
	year = {2023},
	note = {arXiv:2305.14452 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@article{nicholson_turkana_2016,
	title = {The {Turkana} low-level jet: mean climatology and association with regional aridity},
	volume = {36},
	copyright = {© 2015 Royal Meteorological Society},
	issn = {1097-0088},
	shorttitle = {The {Turkana} low-level jet},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.4515},
	doi = {10.1002/joc.4515},
	abstract = {This article develops the first climatology of the low-level jet that prevails in the Turkana Channel of northern Kenya. ERA-Interim data are utilized, giving the analysis a resolution of 0.75° of latitude/longitude and a temporal resolution of 6 h. The jet is found to be a semi-permanent feature, its occurrence ranging from 69\% of the days in May to 90\% of the days in October. It is strongest during the June–September dry season. Typical core speeds in a 0.75 × 0.75 grid point are 10–15 m s−1. The Turkana Jet is clearly a nocturnal feature, being strongest at 0000 UTC or 0600 UTC and usually barely discernible at 1200 UTC. There are distinct patterns of divergence and vertical motion associated with the jet and these differ between the entrance, core, and exit regions. The jet appears to modulate rainfall, especially during the nocturnal hours, with strong jets suppressing rainfall. It also appears to be a factor in the prevailing aridity in the northeast Kenya, southern Somalia, and southeastern Ethiopia and the absence of a summer rainy season in this region.},
	language = {en},
	number = {6},
	urldate = {2023-08-08},
	journal = {International Journal of Climatology},
	author = {Nicholson, Sharon},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.4515},
	keywords = {Africa, East Africa, Turkana channel, aridity, low-level jets},
	pages = {2598--2614},
}

@article{hoerling_detection_2006,
	title = {Detection and {Attribution} of {Twentieth}-{Century} {Northern} and {Southern} {African} {Rainfall} {Change}},
	volume = {19},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/19/16/jcli3842.1.xml},
	doi = {10.1175/JCLI3842.1},
	abstract = {Abstract The spatial patterns, time history, and seasonality of African rainfall trends since 1950 are found to be deducible from the atmosphere’s response to the known variations of global sea surface temperatures (SSTs). The robustness of the oceanic impact is confirmed through the diagnosis of 80 separate 50-yr climate simulations across a suite of atmospheric general circulation models. Drying over the Sahel during boreal summer is shown to be a response to warming of the South Atlantic relative to North Atlantic SST, with the ensuing anomalous interhemispheric SST contrast favoring a more southern position of the Atlantic intertropical convergence zone. Southern African drying during austral summer is shown to be a response to Indian Ocean warming, with enhanced atmospheric convection over those warm waters driving subsidence drying over Africa. The ensemble of greenhouse-gas-forced experiments, conducted as part of the Fourth Assessment Report of the Intergovernmental Panel on Climate Change, fails to simulate the pattern or amplitude of the twentieth-century African drying, indicating that the drought conditions were likely of natural origin. For the period 2000–49, the ensemble mean of the forced experiments yields a wet signal over the Sahel and a dry signal over southern Africa. These rainfall changes are physically consistent with a projected warming of the North Atlantic Ocean compared with the South Atlantic Ocean, and a further warming of the Indian Ocean. However, considerable spread exists among the individual members of the multimodel ensemble.},
	language = {EN},
	number = {16},
	urldate = {2023-08-08},
	journal = {Journal of Climate},
	author = {Hoerling, Martin and Hurrell, James and Eischeid, Jon and Phillips, Adam},
	month = aug,
	year = {2006},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {3989--4008},
}

@article{yang_east_2014,
	title = {The {East} {African} {Long} {Rains} in {Observations} and {Models}},
	volume = {27},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/27/19/jcli-d-13-00447.1.xml},
	doi = {10.1175/JCLI-D-13-00447.1},
	abstract = {Abstract Decadal variability of the East African precipitation during the season of March–May (long rains) is examined and the performance of a series of models in simulating the observed features is assessed. Observational results show that the drying trend of the long rains is associated with decadal natural variability associated with sea surface temperature (SST) variations over the Pacific Ocean. Empirical orthogonal function (EOF), linear regression, and composite analyses all show the spatial pattern of the associated SST field to be La Niña like. The SST-forced International Research Institute for Climate and Society (IRI) forecast models are able to capture the East African precipitation climatology, the decadal variability of the long rains, and the associated SST anomaly pattern but are not consistent with observations from the 1970s. The multimodel mean of the SST-forced models from the Coupled Model Intercomparison Project phase 5 (CMIP5) Atmospheric Model Intercomparison Project (AMIP) experiment captures the climatology and the drying trend in recent decades. The fully coupled models from the CMIP5 historical experiment, however, have systematic errors in simulating the East African precipitation climatology by underestimating the long rains while overestimating the short rains. The multimodel mean of the historical simulations of the long rains anomalies, which is the best estimate of the radiatively forced change, shows a weak wetting trend associated with anthropogenic forcing. The SST anomaly pattern associated with the long rains has large discrepancies with the observations. The results herein suggest caution in projections of East African precipitation from CMIP5 or the relationship between the East African precipitation and the SST spatial pattern found in paleoclimate studies with coupled climate models.},
	language = {EN},
	number = {19},
	urldate = {2023-08-08},
	journal = {Journal of Climate},
	author = {Yang, Wenchang and Seager, Richard and Cane, Mark A. and Lyon, Bradfield},
	month = oct,
	year = {2014},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {7185--7202},
}

@article{rowell_reconciling_2015,
	title = {Reconciling {Past} and {Future} {Rainfall} {Trends} over {East} {Africa}},
	volume = {28},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/28/24/jcli-d-15-0140.1.xml},
	doi = {10.1175/JCLI-D-15-0140.1},
	abstract = {Abstract The “long rains” season of East Africa has recently experienced a series of devastating droughts, whereas the majority of climate models predict increasing rainfall for the coming decades. This has been termed the East African climate paradox and has implications for developing viable adaptation policies. A logical framework is adopted that leads to six key hypotheses that could explain this paradox. The first hypothesis that the recent observed trend is due to poor quality data is promptly rejected. An initial judgment on the second hypothesis that the projected trend is founded on poor modeling is beyond the scope of a single study. Analysis of a natural variability hypothesis suggests this is unlikely to have been the dominant driver of recent droughts, although it may have contributed. The next two hypotheses explore whether the balance between competing forcings could be changing. Regarding the possibility that the past trend could be due to changing anthropogenic aerosol emissions, the results of sensitivity experiments are highly model dependent, but some show a significant impact on the patterns of tropical SST trends, aspects of which likely caused the recent long rains droughts. Further experiments suggest land-use changes are unlikely to have caused the recent droughts. The last hypothesis that the response to CO2 emissions is nonlinear explains no more than 10\% of the contrast between recent and projected trends. In conclusion, it is recommended that research priorities now focus on providing a process-based expert judgment of the reliability of East Africa projections, improving the modeling of aerosol impacts on rainfall, and better understanding the relevant natural variability.},
	language = {EN},
	number = {24},
	urldate = {2023-08-08},
	journal = {Journal of Climate},
	author = {Rowell, David P. and Booth, Ben B. B. and Nicholson, Sharon E. and Good, Peter},
	month = dec,
	year = {2015},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {9768--9788},
}

@article{camberlin_east_2002,
	title = {The {East} {African} {March}–{May} {Rainy} {Season}: {Associated} {Atmospheric} {Dynamics} and {Predictability} over the 1968–97 {Period}},
	volume = {15},
	issn = {0894-8755, 1520-0442},
	shorttitle = {The {East} {African} {March}–{May} {Rainy} {Season}},
	url = {https://journals.ametsoc.org/view/journals/clim/15/9/1520-0442_2002_015_1002_teammr_2.0.co_2.xml},
	doi = {10.1175/1520-0442(2002)015<1002:TEAMMR>2.0.CO;2},
	abstract = {Abstract This paper focuses on the East African March–May “long rains.” Particularly, it investigates the atmospheric patterns associated to the March–May rainfall anomalies, then proposes a seasonal prediction model. In a preliminary step, in order to define a regional rainfall index, a new form of extended principal component analysis is performed, aimed at capturing both the spatial and intraseasonal rainfall coherence. What emerges is that although the long rains exhibit a low temporal coherence, calling for a separation between the months of March–April and May in teleconnection studies, they show a relatively strong spatial consistency over the Kenya–Uganda inland region. From composite analyses performed using NCEP–NCAR reanalyzed atmospheric data, three major signals appear for that region. Two are during March–April involving ENSO and the latitudinal location of the ITCZ, and ENSO interactions with the northern extratropical dynamics (by way of cool surges toward the Tropics and upper-ridge–trough systems). The third signal is the Asian monsoon in May. As shown using a rainfall index for Ethiopia, the interactions with the midlatitudes get stronger when considering rainfall farther to the north. The predictability study identifies four February indexes, involving several scales and several atmospheric and oceanic parameters, to serve as predictors in linear multiple regression (LMR) and linear discriminant analysis (LDA) models. The predictors, selected by a stepwise procedure, depict both regional (energy gradient and zonal wind) and remote dynamics (ENSO and 500-hPa geopotential height over the Near East): they are consistent with the signals shown in the synchronous composites. The robustness of the LMR and LDA models is demonstrated by the high linear error in probability space (LEPS) scores (44\% for continuous variables and 51\% for categorical variables) obtained on cross-validated results.},
	language = {EN},
	number = {9},
	urldate = {2023-08-08},
	journal = {Journal of Climate},
	author = {Camberlin, P. and Philippon, N.},
	month = may,
	year = {2002},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {1002--1019},
}

@article{schwarzwald_understanding_2023,
	title = {Understanding {CMIP6} biases in the representation of the {Greater} {Horn} of {Africa} long and short rains},
	volume = {61},
	issn = {1432-0894},
	url = {https://doi.org/10.1007/s00382-022-06622-5},
	doi = {10.1007/s00382-022-06622-5},
	abstract = {The societies of the Greater Horn of Africa (GHA) are vulnerable to variability in two distinct rainy seasons, the March–May ‘long’ rains and the October–December ‘short’ rains. Recent trends in both rainy seasons, possibly related to patterns of low-frequency variability, have increased interest in future climate projections from General Circulation Models (GCMs). However, previous generations of GCMs historically have poorly simulated the regional hydroclimate. This study conducts a process-based evaluation of simulations of the long and short rains in CMIP6, the latest generation of GCMs. Key biases in CMIP5 remain or are worsened, including long rains that are too short and weak and short rains that are too long and strong. Model biases are driven by a complex set of related oceanic and atmospheric factors, including simulations of the Walker Circulation. Biased wet short rains in models are connected with Indian Ocean zonal sea surface temperature (SST) gradients that are too warm in the west and convection that is too deep. Models connect equatorial African winds with the strength of the short rains, though in observations a robust connection is primarily found in the long rains. Model mean state biases in the timing of the western Indian Ocean SST seasonal cycle are associated with certain rainfall timing biases, though both biases may be due to a common source. Simulations driven by historical SSTs (AMIP runs) often have larger biases than fully coupled runs. A path towards using biases to better understand uncertainty in projections of GHA rainfall is suggested.},
	language = {en},
	number = {3},
	urldate = {2023-08-08},
	journal = {Climate Dynamics},
	author = {Schwarzwald, Kevin and Goddard, Lisa and Seager, Richard and Ting, Mingfang and Marvel, Kate},
	month = aug,
	year = {2023},
	keywords = {Climate models, East Africa, Indian Ocean Dipole, Precipitation variability, Walker Circulation},
	pages = {1229--1255},
}

@article{liebmann_understanding_2014,
	title = {Understanding {Recent} {Eastern} {Horn} of {Africa} {Rainfall} {Variability} and {Change}},
	volume = {27},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/27/23/jcli-d-13-00714.1.xml},
	doi = {10.1175/JCLI-D-13-00714.1},
	abstract = {Abstract Observations and sea surface temperature (SST)-forced ECHAM5 simulations are examined to study the seasonal cycle of eastern Africa rainfall and its SST sensitivity during 1979–2012, focusing on interannual variability and trends. The eastern Horn is drier than the rest of equatorial Africa, with two distinct wet seasons, and whereas the October–December wet season has become wetter, the March–May season has become drier. The climatological rainfall in simulations driven by observed SSTs captures this bimodal regime. The simulated trends also qualitatively reproduce the opposite-sign changes in the two rainy seasons, suggesting that SST forcing has played an important role in the observed changes. The consistency between the sign of 1979–2012 trends and interannual SST–precipitation correlations is exploited to identify the most likely locations of SST forcing of precipitation trends in the model, and conceivably also in nature. Results indicate that the observed March–May drying since 1979 is due to sensitivity to an increased zonal gradient in SST between Indonesia and the central Pacific. In contrast, the October–December precipitation increase is mostly due to western Indian Ocean warming. The recent upward trend in the October–December wet season is rather weak, however, and its statistical significance is compromised by strong year-to-year fluctuations. October–December eastern Horn rain variability is strongly associated with El Niño–Southern Oscillation and Indian Ocean dipole phenomena on interannual scales, in both model and observations. The interannual October–December correlation between the ensemble-average and observed Horn rainfall 0.87. By comparison, interannual March–May Horn precipitation is only weakly constrained by SST anomalies.},
	language = {EN},
	number = {23},
	urldate = {2023-08-08},
	journal = {Journal of Climate},
	author = {Liebmann, Brant and Hoerling, Martin P. and Funk, Chris and Bladé, Ileana and Dole, Randall M. and Allured, Dave and Quan, Xiaowei and Pegion, Philip and Eischeid, Jon K.},
	month = dec,
	year = {2014},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {8630--8645},
}

@article{ayugi_comparison_2021,
	title = {Comparison of {CMIP6} and {CMIP5} models in simulating mean and extreme precipitation over {East} {Africa}},
	volume = {41},
	copyright = {© 2021 Royal Meteorological Society},
	issn = {1097-0088},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.7207},
	doi = {10.1002/joc.7207},
	abstract = {This study examines the improvement in Coupled Model Intercomparison Project Phase Six (CMIP6) models against the predecessor CMIP5 in simulating mean and extreme precipitation over the East Africa region. The study compares the climatology of the precipitation indices simulated by the CMIP models with the CHIRPS data set using robust statistical techniques for 1981–2005. The results display the varying performance of the general circulation models (GCMs) in the simulation of annual and seasonal precipitation climatology over the study domain. CMIP6 multi-model ensemble mean (hereafter MME) shows improved performance in the local annual mean cycle simulation with a better representation of the rainfall within the two peaks, especially the MAM rainfall relative to their predecessor. Moreover, simulation of extreme indices is well captured in CMIP6 models relative to CMIP5. The CMIP6-MME performed better than the CMIP5-MME with lesser biases in simulating Simple Daily Intensity Index (SDII), consecutive dry days (CDD), and very heavy precipitation days {\textgreater}20 mm (R20mm) over East Africa. Remarkably, most CMIP6 models are unable to simulate extremely wet days (R95p). Some CMIP6 models (e.g., NorESM2-MM and CNRM-CM6-1) depict robust performance in reproducing the observed indices across all analyses. OND season shows wet biases for some indices (i.e., R95p, PRCPTOT), except for SDII, CDD, and R20mm in CMIP6 models. Consistent with other studies, the mean ensemble performance for both CMIP5/6 shows better performance as compared with individual models due to the cancellation of some systematic errors in the individual models. Generally, CMIP6 depicts improved performance in the simulation of MAM rainfall compared with CMIP5 models. However, the new model generation is still marred by uncertainty, thereby depicting unsatisfactory performance over the East Africa domain. This calls for further investigation into the sources of persistent systematic biases and a methodology for identifying individual models with robust features that can accurately simulate observed patterns for future usage.},
	language = {en},
	number = {15},
	urldate = {2023-08-08},
	journal = {International Journal of Climatology},
	author = {Ayugi, Brian and Zhihong, Jiang and Zhu, Huanhuan and Ngoma, Hamida and Babaousmail, Hassen and Rizwan, Karim and Dike, Victor},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.7207},
	keywords = {CMIP5/6, East Africa, climate extremes, evaluation, precipitation},
	pages = {6474--6496},
}

@article{noauthor_comparison_nodate,
	title = {Comparison of {CMIP6} and {CMIP5} models in simulating mean and extreme precipitation over {East} {Africa}},
	doi = {https://doi.org/10.1002/joc.7207},
}

@article{liebmann_climatology_2017,
	title = {Climatology and {Interannual} {Variability} of {Boreal} {Spring} {Wet} {Season} {Precipitation} in the {Eastern} {Horn} of {Africa} and {Implications} for {Its} {Recent} {Decline}},
	volume = {30},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/30/10/jcli-d-16-0452.1.xml},
	doi = {10.1175/JCLI-D-16-0452.1},
	abstract = {Abstract The 1981–2014 climatology and variability of the March–May eastern Horn of Africa boreal spring wet season are examined using precipitation, upper- and lower-level winds, low-level specific humidity, and convective available potential energy (CAPE), with the aim of better understanding the establishment of the wet season and the cause of the recent observed decline. At 850 mb, the development of the wet season is characterized by increasing specific humidity and winds that veer from northeasterly in February to southerly in June and advect moisture into the region, in agreement with an earlier study. Equally important, however, is a substantial weakening of the 200-mb climatological easterly winds in March. Likewise, the shutdown of the wet season coincides with the return of strong easterly winds in June. Similar changes are seen in the daily evolution of specific humidity and 200-mb wind when composited relative to the interannual wet season onset and end, with the easterlies decreasing (increasing) several days prior to the start (end) of the wet season. The 1981–2014 decrease in March–May precipitation has also coincided with an increase in 200-mb easterly winds, with no attendant change in specific humidity, leading to the conclusion that, while high values of specific humidity are an important ingredient of the wet season, the recent observed precipitation decline has resulted mostly from a strengthening of the 200-mb easterlies. This change in the easterly winds appears to be related to an increase in convection over the Indonesian region and in the associated outflow from that enhanced heat source.},
	language = {EN},
	number = {10},
	urldate = {2023-08-08},
	journal = {Journal of Climate},
	author = {Liebmann, Brant and Bladé, Ileana and Funk, Chris and Allured, Dave and Quan, Xiao-Wei and Hoerling, Martin and Hoell, Andrew and Peterson, Pete and Thiaw, Wassila M.},
	month = may,
	year = {2017},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {3867--3886},
}

@article{zadra_systematic_2018,
	title = {Systematic {Errors} in {Weather} and {Climate} {Models}: {Nature}, {Origins}, and {Ways} {Forward}},
	volume = {99},
	issn = {0003-0007, 1520-0477},
	shorttitle = {Systematic {Errors} in {Weather} and {Climate} {Models}},
	url = {https://journals.ametsoc.org/view/journals/bams/99/4/bams-d-17-0287.1.xml},
	doi = {10.1175/BAMS-D-17-0287.1},
	abstract = {"Systematic Errors in Weather and Climate Models: Nature, Origins, and Ways Forward" published on Apr 2018 by American Meteorological Society.},
	language = {EN},
	number = {4},
	urldate = {2023-08-04},
	journal = {Bulletin of the American Meteorological Society},
	author = {Zadra, Ayrton and Williams, Keith and Frassoni, Ariane and Rixen, Michel and Adames, Ángel F. and Berner, Judith and Bouyssel, François and Casati, Barbara and Christensen, Hannah and Ek, Michael B. and Flato, Greg and Huang, Yi and Judt, Falko and Lin, Hai and Maloney, Eric and Merryfield, William and Niekerk, Annelize Van and Rackow, Thomas and Saito, Kazuo and Wedi, Nils and Yadav, Priyanka},
	month = apr,
	year = {2018},
	note = {Publisher: American Meteorological Society
Section: Bulletin of the American Meteorological Society},
	pages = {ES67--ES70},
}

@article{watson_machine_2022,
	title = {Machine learning applications for weather and climate need greater focus on extremes},
	volume = {17},
	issn = {1748-9326},
	url = {https://dx.doi.org/10.1088/1748-9326/ac9d4e},
	doi = {10.1088/1748-9326/ac9d4e},
	language = {en},
	number = {11},
	urldate = {2023-08-04},
	journal = {Environmental Research Letters},
	author = {Watson, Peter A. G.},
	month = nov,
	year = {2022},
	note = {Publisher: IOP Publishing},
	pages = {111004},
}

@techreport{watkiss_socio-economic_2020,
	title = {The {Socio}-{Economic} {Benefits} of the {HIGHWAY} project},
	language = {en},
	institution = {Weather and Climate Information Services for Africa (WISER)},
	author = {Watkiss, Paul and Powell, Robert and Hunt, Alistair and Cimato, Federica},
	month = sep,
	year = {2020},
}

@article{harris_multiscale_2001,
	title = {Multiscale {Statistical} {Properties} of a {High}-{Resolution} {Precipitation} {Forecast}},
	volume = {2},
	issn = {1525-7541, 1525-755X},
	url = {https://journals.ametsoc.org/view/journals/hydr/2/4/1525-7541_2001_002_0406_mspoah_2_0_co_2.xml},
	doi = {10.1175/1525-7541(2001)002<0406:MSPOAH>2.0.CO;2},
	abstract = {Abstract Small-scale (less than ∼15 km) precipitation variability significantly affects the hydrologic response of a basin and the accurate estimation of water and energy fluxes through coupled land–atmosphere modeling schemes. It also affects the radiative transfer through precipitating clouds and thus rainfall estimation from microwave sensors. Because both land–atmosphere and cloud–radiation interactions are nonlinear and occur over a broad range of scales (from a few centimeters to several kilometers), it is important that, over these scales, cloud-resolving numerical models realistically reproduce the observed precipitation variability. This issue is examined herein by using a suite of multiscale statistical methods to compare the scale dependence of precipitation variability of a numerically simulated convective storm with that observed by radar. In particular, Fourier spectrum, structure function, and moment-scale analyses are used to show that, although the variability of modeled precipitation agrees with that observed for scales larger than approximately 5 times the model resolution, the model shows a falloff in variability at smaller scales. Thus, depending upon the smallest scale at which variability is considered to be important for a specific application, one has to resort either to very high resolution model runs (resolutions 5 times higher than the scale of interest) or to stochastic methods that can introduce the missing small-scale variability. The latter involve upscaling the model output to a scale approximately 5 times the model resolution and then stochastically downscaling it to smaller scales. The results of multiscale analyses, such as those presented herein, are key to the implementation of such stochastic downscaling methodologies.},
	language = {EN},
	number = {4},
	urldate = {2023-08-03},
	journal = {Journal of Hydrometeorology},
	author = {Harris, Daniel and Foufoula-Georgiou, Efi and Droegemeier, Kelvin K. and Levit, Jason J.},
	month = aug,
	year = {2001},
	note = {Publisher: American Meteorological Society
Section: Journal of Hydrometeorology},
	pages = {406--418},
}

@article{sinclair_empirical_2005,
	title = {Empirical {Mode} {Decomposition} in 2-{D} space and time: a tool for space-time rainfall analysis and nowcasting},
	volume = {9},
	issn = {1027-5606},
	shorttitle = {Empirical {Mode} {Decomposition} in 2-{D} space and time},
	url = {https://hess.copernicus.org/articles/9/127/2005/},
	doi = {10.5194/hess-9-127-2005},
	abstract = {A data-driven method for extracting temporally persistent information, at different spatial scales, from rainfall data (as measured by radar/satellite) is described, which extends the Empirical Mode Decomposition (EMD) algorithm into two dimensions. The EMD technique is used here to decompose spatial rainfall data into a sequence of high through to low frequency components. This process is equivalent to the application of successive low-pass spatial filters, but based on the observed properties of the data rather than the predetermined basis functions used in traditional Fourier or Wavelet decompositions. It has been suggested in the literature that the lower frequency components (those with large spatial extent) of spatial rainfall data exhibit greater temporal persistence than the higher frequency ones. This idea is explored here in the context of Empirical Mode Decomposition. The paper focuses on the implementation and development of the two-dimensional extension to the EMD algorithm and it's application to radar rainfall data, as well as examining temporal persistence in the data at different spatial scales.},
	language = {English},
	number = {3},
	urldate = {2023-08-03},
	journal = {Hydrology and Earth System Sciences},
	author = {Sinclair, S. and Pegram, G. G. S.},
	month = jul,
	year = {2005},
	note = {Publisher: Copernicus GmbH},
	pages = {127--137},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{gebremeskel_haile_droughts_2019,
	title = {Droughts in {East} {Africa}: {Causes}, impacts and resilience},
	volume = {193},
	issn = {0012-8252},
	shorttitle = {Droughts in {East} {Africa}},
	url = {https://www.sciencedirect.com/science/article/pii/S0012825218303519},
	doi = {10.1016/j.earscirev.2019.04.015},
	abstract = {East Africa (EA) has been the primary focus for various drought studies in recent years. However, a comprehensive analysis of droughts, including their evolution, complexity, social implications and people's vulnerability is currently lacking. Hence, there is a pressing need for an overview of drought studies in EA. Here, we present a state-of-the-art review of the causes and impacts of, and resilience to droughts in EA. Studies reveal that droughts tend to be more frequent, longer and more severe in the boreal spring and summer in EA, as the overall precipitation and water storage abruptly decline. A decrease in drought frequency is observed during the boreal autumn season (October–November). As these studies have only been analysed within the context of sparse and short-term regional climate data with very complex spatial and seasonal climate patterns, they are subject to uncertainties. The main causes for the changing pattern of droughts include climate variabilities and anthropogenic effects. Droughts have extensive impacts on human beings, environment, water resources and agriculture. Environmental rehabilitation involving the development of ecosystem services, biodiversity enhancement and soil and water conservation is found to be a suitable strategy to adapt to drought conditions. A better understanding of the causes and impacts of droughts, participatory management and community level actions are essential for building resilience to drought. Strong citizens–government–stakeholder cooperation is also valuable in monitoring and managing drought. The knowledge and insights gained from this review will help the countries in EA to build a drought-resilient society and will form a basis of information for other regions outside of EA.},
	language = {en},
	urldate = {2023-08-03},
	journal = {Earth-Science Reviews},
	author = {Gebremeskel Haile, Gebremedhin and Tang, Qiuhong and Sun, Siao and Huang, Zhongwei and Zhang, Xuejun and Liu, Xingcai},
	month = jun,
	year = {2019},
	keywords = {Anthropogenic activities, Climate variability, Drought, East Africa, Horn of Africa, Rainfall},
	pages = {146--161},
}

@article{luhunga_analysis_2020,
	title = {Analysis of {Climate} {Change} and {Extreme} {Climatic} {Events} in the {Lake} {Victoria} {Region} of {Tanzania}},
	volume = {2},
	issn = {2624-9553},
	url = {https://www.frontiersin.org/articles/10.3389/fclim.2020.559584},
	abstract = {The understanding of climate change impacts and the associated climate extreme events at regional and local scales is of critical importance for planning and development of feasible adaptation strategies. In this paper, we present an analysis of climate change and extreme climate events in the Lake Victoria region of Tanzania, focusing on the Kagera and Geita regions. We use daily simulated climate variables (rainfall and minimum and maximum temperatures) from the Coordinated Regional Climate Downscaling Experiment Program Regional Climate Models (CORDEX\_RCMs) for the analysis. Extreme climate event, rainfall, and minimum and maximum temperatures time series during historical (1971–2000) climate condition are compared to future climate projection (2011–2100) under two Representative Concentration Pathway (RCP): RCP 4.5 and RCP 8.5 emission scenarios. The existence, magnitude, and statistical significance of potential trends in climate data time series are estimated using the Mann–Kendall (MK) non-parametric test and Theil-SEN slope estimator methods. Results show that during historical (1971–2000) climate, the Lake Victoria region of Tanzania experienced a statistically significant increasing trend in temperature. The annual minimum and maximum temperatures in the Kagera and Geita regions have increased by 0.54–0.69°C, and 0.51–0.69°C, respectively. The numbers of warm days (TX90p) and warm nights (TN90p) during the historical climate have increased, while the numbers of cold days (TX10p) and cold nights (TN10p) have decreased significantly. However, in future climate condition (2011–2100) under both RCP 4.5 and RCP 8.5 emission scenarios, the Lake Victoria region is likely to experience increased temperatures and rainfall. The frequency of cold events (cold days and cold nights) is likely to decrease, while the frequency of warm events (warm days and warm nights) is likely to increase significantly. The number of consecutive wet days, the intensity of very wet days, and the number of extreme wet days are likely to increase. These results indicate that in future climate condition, socioeconomic livelihoods of people in the Kagera and Geita regions are likely to experience significant challenges from climate-related stresses. It is, therefore, recommended that appropriate planning and effective adaptation policies are in place for disaster risk prevention.},
	urldate = {2023-08-02},
	journal = {Frontiers in Climate},
	author = {Luhunga, Philbert Modest and Songoro, Alexander Elias},
	year = {2020},
}

@misc{wang_esrgan_2018,
	title = {{ESRGAN}: {Enhanced} {Super}-{Resolution} {Generative} {Adversarial} {Networks}},
	shorttitle = {{ESRGAN}},
	url = {http://arxiv.org/abs/1809.00219},
	abstract = {The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at https://github.com/xinntao/ESRGAN .},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Loy, Chen Change and Qiao, Yu and Tang, Xiaoou},
	month = sep,
	year = {2018},
	note = {arXiv:1809.00219 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kashinath_physics-informed_2021,
	title = {Physics-informed machine learning: case studies for weather and climate modelling},
	volume = {379},
	shorttitle = {Physics-informed machine learning},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0093},
	doi = {10.1098/rsta.2020.0093},
	abstract = {Machine learning (ML) provides novel and powerful ways of accurately and efficiently recognizing complex patterns, emulating nonlinear dynamics, and predicting the spatio-temporal evolution of weather and climate processes. Off-the-shelf ML models, however, do not necessarily obey the fundamental governing laws of physical systems, nor do they generalize well to scenarios on which they have not been trained. We survey systematic approaches to incorporating physics and domain knowledge into ML models and distill these approaches into broad categories. Through 10 case studies, we show how these approaches have been used successfully for emulating, downscaling, and forecasting weather and climate processes. The accomplishments of these studies include greater physical consistency, reduced training time, improved data efficiency, and better generalization. Finally, we synthesize the lessons learned and identify scientific, diagnostic, computational, and resource challenges for developing truly robust and reliable physics-informed ML models for weather and climate processes.

This article is part of the theme issue ‘Machine learning for weather and climate modelling’.},
	number = {2194},
	urldate = {2023-08-02},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Kashinath, K. and Mustafa, M. and Albert, A. and Wu, J-L. and Jiang, C. and Esmaeilzadeh, S. and Azizzadenesheli, K. and Wang, R. and Chattopadhyay, A. and Singh, A. and Manepalli, A. and Chirila, D. and Yu, R. and Walters, R. and White, B. and Xiao, H. and Tchelepi, H. A. and Marcus, P. and Anandkumar, A. and Hassanzadeh, P. and Prabhat, null},
	month = feb,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {neural networks, physical constraints, physics-informed machine learning, turbulent flows, weather and climate modeling},
	pages = {20200093},
}

@article{giorgi_regional_2015,
	title = {Regional {Dynamical} {Downscaling} and the {CORDEX} {Initiative}},
	volume = {40},
	url = {https://doi.org/10.1146/annurev-environ-102014-021217},
	doi = {10.1146/annurev-environ-102014-021217},
	abstract = {We review the challenges and future perspectives of regional climate model (RCM), or dynamical downscaling, activities. Among the main technical issues in need of better understanding are those of selection and sensitivity to the model domain and resolution, techniques for providing lateral boundary conditions, and RCM internal variability. The added value (AV) obtained with the use of RCMs remains a central issue, which needs more rigorous and comprehensive analysis strategies. Within the context of regional climate projections, large ensembles of simulations are needed to better understand the models and characterize uncertainties. This has provided an impetus for the development of the Coordinated Regional Downscaling Experiment (CORDEX), the first international program offering a common protocol for downscaling experiments, and we discuss how CORDEX can address the key scientific challenges in downscaling research. Among the main future developments in RCM research, we highlight the development of coupled regional Earth system models and the transition to very high-resolution, cloud-resolving models.},
	number = {1},
	urldate = {2023-07-31},
	journal = {Annual Review of Environment and Resources},
	author = {Giorgi, Filippo and Gutowski, William J.},
	year = {2015},
	note = {\_eprint: https://doi.org/10.1146/annurev-environ-102014-021217},
	keywords = {CORDEX, RCM, climate change, downscaling added value, dynamical downscaling, high-resolution modeling, model evaluation, regional Earth system modeling, regional climate, regional climate modeling},
	pages = {467--490},
}

@article{eyring_overview_2016,
	title = {Overview of the {Coupled} {Model} {Intercomparison} {Project} {Phase} 6 ({CMIP6}) experimental design and organization},
	volume = {9},
	issn = {1991-959X},
	url = {https://gmd.copernicus.org/articles/9/1937/2016/},
	doi = {10.5194/gmd-9-1937-2016},
	abstract = {By coordinating the design and distribution of global climate model simulations of the past, current, and future climate, the Coupled Model Intercomparison Project (CMIP) has become one of the foundational elements of climate science. However, the need to address an ever-expanding range of scientific questions arising from more and more research communities has made it necessary to revise the organization of CMIP. After a long and wide community consultation, a new and more federated structure has been put in place. It consists of three major elements: (1) a handful of common experiments, the DECK (Diagnostic, Evaluation and Characterization of Klima) and CMIP historical simulations (1850–near present) that will maintain continuity and help document basic characteristics of models across different phases of CMIP; (2) common standards, coordination, infrastructure, and documentation that will facilitate the distribution of model outputs and the characterization of the model ensemble; and (3) an ensemble of CMIP-Endorsed Model Intercomparison Projects (MIPs) that will be specific to a particular phase of CMIP (now CMIP6) and that will build on the DECK and CMIP historical simulations to address a large range of specific questions and fill the scientific gaps of the previous CMIP phases. The DECK and CMIP historical simulations, together with the use of CMIP data standards, will be the entry cards for models participating in CMIP. Participation in CMIP6-Endorsed MIPs by individual modelling groups will be at their own discretion and will depend on their scientific interests and priorities. With the Grand Science Challenges of the World Climate Research Programme (WCRP) as its scientific backdrop, CMIP6 will address three broad questions: 

 \&ndash; How does the Earth system respond to forcing?

 \&ndash; What are the origins and consequences of systematic model biases? 

 \&ndash; How can we assess future climate changes given internal climate variability, predictability, and uncertainties in scenarios?

 This CMIP6 overview paper presents the background and rationale for the new structure of CMIP, provides a detailed description of the DECK and CMIP6 historical simulations, and includes a brief introduction to the 21 CMIP6-Endorsed MIPs.},
	language = {English},
	number = {5},
	urldate = {2023-07-31},
	journal = {Geoscientific Model Development},
	author = {Eyring, Veronika and Bony, Sandrine and Meehl, Gerald A. and Senior, Catherine A. and Stevens, Bjorn and Stouffer, Ronald J. and Taylor, Karl E.},
	month = may,
	year = {2016},
	note = {Publisher: Copernicus GmbH},
	pages = {1937--1958},
}

@article{luhunga_analysis_2020-1,
	title = {Analysis of {Climate} {Change} and {Extreme} {Climatic} {Events} in the {Lake} {Victoria} {Region} of {Tanzania}},
	volume = {2},
	issn = {2624-9553},
	url = {https://www.frontiersin.org/articles/10.3389/fclim.2020.559584},
	abstract = {The understanding of climate change impacts and the associated climate extreme events at regional and local scales is of critical importance for planning and development of feasible adaptation strategies. In this paper, we present an analysis of climate change and extreme climate events in the Lake Victoria region of Tanzania, focusing on the Kagera and Geita regions. We use daily simulated climate variables (rainfall and minimum and maximum temperatures) from the Coordinated Regional Climate Downscaling Experiment Program Regional Climate Models (CORDEX\_RCMs) for the analysis. Extreme climate event, rainfall, and minimum and maximum temperatures time series during historical (1971–2000) climate condition are compared to future climate projection (2011–2100) under two Representative Concentration Pathway (RCP): RCP 4.5 and RCP 8.5 emission scenarios. The existence, magnitude, and statistical significance of potential trends in climate data time series are estimated using the Mann–Kendall (MK) non-parametric test and Theil-SEN slope estimator methods. Results show that during historical (1971–2000) climate, the Lake Victoria region of Tanzania experienced a statistically significant increasing trend in temperature. The annual minimum and maximum temperatures in the Kagera and Geita regions have increased by 0.54–0.69°C, and 0.51–0.69°C, respectively. The numbers of warm days (TX90p) and warm nights (TN90p) during the historical climate have increased, while the numbers of cold days (TX10p) and cold nights (TN10p) have decreased significantly. However, in future climate condition (2011–2100) under both RCP 4.5 and RCP 8.5 emission scenarios, the Lake Victoria region is likely to experience increased temperatures and rainfall. The frequency of cold events (cold days and cold nights) is likely to decrease, while the frequency of warm events (warm days and warm nights) is likely to increase significantly. The number of consecutive wet days, the intensity of very wet days, and the number of extreme wet days are likely to increase. These results indicate that in future climate condition, socioeconomic livelihoods of people in the Kagera and Geita regions are likely to experience significant challenges from climate-related stresses. It is, therefore, recommended that appropriate planning and effective adaptation policies are in place for disaster risk prevention.},
	urldate = {2023-07-31},
	journal = {Frontiers in Climate},
	author = {Luhunga, Philbert Modest and Songoro, Alexander Elias},
	year = {2020},
}

@article{dosio_projected_2021,
	title = {Projected future daily characteristics of {African} precipitation based on global ({CMIP5}, {CMIP6}) and regional ({CORDEX}, {CORDEX}-{CORE}) climate models},
	volume = {57},
	issn = {1432-0894},
	url = {https://doi.org/10.1007/s00382-021-05859-w},
	doi = {10.1007/s00382-021-05859-w},
	abstract = {We provide an assessment of future daily characteristics of African precipitation by explicitly comparing the results of large ensembles of global (CMIP5, CMIP6) and regional (CORDEX, CORE) climate models, specifically highlighting the similarities and inconsistencies between them. Results for seasonal mean precipitation are not always consistent amongst ensembles: in particular, global models tend to project a wetter future compared to regional models, especially over the Eastern Sahel, Central and East Africa. However, results for other precipitation characteristics are more consistent. In general, all ensembles project an increase in maximum precipitation intensity during the wet season over all regions and emission scenarios (except the West Sahel for CORE) and a decrease in precipitation frequency (under the Representative Concentration Pathways RCP8.5) especially over the West Sahel, the Atlas region, southern central Africa, East Africa and southern Africa. Depending on the season, the length of dry spells is projected to increase consistently by all ensembles and for most (if not all) models over southern Africa, the Ethiopian highlands and the Atlas region. Discrepancies exist between global and regional models on the projected change in precipitation characteristics over specific regions and seasons. For instance, over the Eastern Sahel in July–August most global models show an increase in precipitation frequency but regional models project a robust decrease. Global and regional models also project an opposite sign in the change of the length of dry spells. CORE results show a marked drying over the regions affected by the West Africa monsoon throughout the year, accompanied by a decrease in mean precipitation intensity between May and July that is not present in the other ensembles. This enhanced drying may be related to specific physical mechanisms that are better resolved by the higher resolution models and highlights the importance of a process-based evaluation of the mechanisms controlling precipitation over the region.},
	language = {en},
	number = {11},
	urldate = {2023-07-31},
	journal = {Climate Dynamics},
	author = {Dosio, Alessandro and Jury, Martin W. and Almazroui, Mansour and Ashfaq, Moetasim and Diallo, Ismaila and Engelbrecht, Francois A. and Klutse, Nana A. B. and Lennard, Christopher and Pinto, Izidine and Sylla, Mouhamadou B. and Tamoffo, Alain T.},
	month = dec,
	year = {2021},
	pages = {3135--3158},
}

@article{mbigi_coupled_2022,
	title = {Coupled {Model} {Intercomparison} {Project} {Phase} 6 simulations of the spatial structure of rainfall variability over {East} {Africa}: {Evaluation} and projection},
	volume = {42},
	copyright = {© 2022 Royal Meteorological Society},
	issn = {1097-0088},
	shorttitle = {Coupled {Model} {Intercomparison} {Project} {Phase} 6 simulations of the spatial structure of rainfall variability over {East} {Africa}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.7868},
	doi = {10.1002/joc.7868},
	abstract = {Modifications in rainfall patterns may have significant effects on a variety of natural and human systems. This study evaluates the ability of 20 Coupled Model Intercomparison Project Phase 6 (CMIP6) to simulate the interannual variability of rainfall over East Africa (EA) using a method based on the empirical orthogonal function (EOF) analysis. The future changes in rainfall variability during the near (2021–2040), middle (2041–2060) and late (2080–2099) future are analysed under two different shared socioeconomic pathways (SSP), SSP2-4.5 and SSP5-8.5. Results reveal that most models captured better spatial climatological rainfall pattern than simulated amplitude in the EA region receiving bimodal rainfall pattern (EABM) compared to that with unimodal rainfall regime (EAUM) in the historical period. An ensemble mean of all models (AMME) and a set of 13 models that best simulated the rainfall variability in the base period were selected using a robust method based on the EOF analysis for further analysis. Most of the selected models and their ensemble mean (BMME) displayed good capability in representing the annual standard deviation (SD) in recent decades, whereas BMME corroborates AMME, particularly over the EABM and EAUM regions. Based on these findings, the AMME and BMME were used to evaluate the future changes in rainfall variability. The models project a significant increase in rainfall variability during March by the mid and late 21st century over the EAUM region under SSP5-8.5, whereas the increase appears much earlier in the near-future over the EABM region. In all future periods and SSPs, SD demonstrates a considerable increase over most of the EABM region, and the magnitude gradually increases from the AMME to BMME projections. Moreover, a relatively stronger increase is anticipated to actualize by the mid of 21st century.},
	language = {en},
	number = {16},
	urldate = {2023-07-31},
	journal = {International Journal of Climatology},
	author = {Mbigi, Dickson and Onyango, Augustine Omondi and Mtewele, Zacharia Florence and Kiprotich, Paul and Xiao, Ziniu},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.7868},
	keywords = {CMIP6, East Africa, empirical orthogonal function, rainfall variability},
	pages = {9865--9885},
}

@article{ngoma_projected_2022,
	title = {Projected changes in rainfall over {Uganda} based on {CMIP6} models},
	volume = {149},
	issn = {1434-4483},
	url = {https://doi.org/10.1007/s00704-022-04106-4},
	doi = {10.1007/s00704-022-04106-4},
	abstract = {Information about likely future patterns of climate variables is important in climate change mitigation and adaptation efforts. This study investigates future (2021–2100) changes in rainfall based on CMIP6 datasets over Uganda. The projection period is divided into two sub-periods: 2021–2060 (near future) and 2061–2100 (far future), relative to the baseline period (1985–2014). Two emission scenarios: SSP2-4.5 and SSP5-8.5, are considered. The results reveal a larger decrease (increase) in rainfall during March–April (November–December) under both SSPs. Moreover, an enhanced decline (increase) is projected under SSP2-4.5 (SSP5-8.5). The spatial distribution of future changes in seasonal rainfall reveals a decrease in MAM rainfall in the near future over most parts of the country under both emission scenarios. However, a recovery is exhibited towards the end of the century with more increase in the south-western parts of the country, and a higher magnitude under SSP5-8.5. In contrast, SON rainfall reveals wetter conditions during both timelines and emission scenarios. Maximum (minimum) wet conditions are expected in the north-western parts of the country (around the Lake Victoria basin). The linear trend analysis shows a non-significant (z =  − 0.714) decreasing trend for MAM rainfall during the historical period. This pattern is reflected in the near future with z-scores of − 0.757 and − 1.281 under SSP2-4.5 and SSP5-8.5, respectively. However, a significant increase for MAM and annual rainfall (z-scores of 2.785 and 3.46, respectively) is projected towards the end of the century under SSP5-8.5. These findings provide guidance to policy makers in devising appropriate adaptation measures to cope with expected changes in the local climate. Given the increase in intensity and frequency of extreme rainfall over the study region, future work should focus on examining projected changes in rainfall extremes under different global warming scenarios with consideration of model performance and independence.},
	language = {en},
	number = {3},
	urldate = {2023-07-31},
	journal = {Theoretical and Applied Climatology},
	author = {Ngoma, Hamida and Ayugi, Brian and Onyutha, Charles and Babaousmail, Hassen and Lim Kam Sian, Kenny T. C. and Iyakaremye, Vedaste and Mumo, Richard and Ongoma, Victor},
	month = aug,
	year = {2022},
	pages = {1117--1134},
}

@article{ayugi_east_2022,
	title = {East {African} population exposure to precipitation extremes under 1.5 °{C} and 2.0 °{C} warming levels based on {CMIP6} models},
	volume = {17},
	issn = {1748-9326},
	url = {https://dx.doi.org/10.1088/1748-9326/ac5d9d},
	doi = {10.1088/1748-9326/ac5d9d},
	abstract = {Understanding population exposure to precipitation-related extreme events is important for effective climate change adaptation and mitigation measures. We analyze extreme precipitation using indices (EPIs), including consecutive dry days (CDD), annual total precipitation, simple daily intensity, and the number of extremely wet days, under the past and future climatic conditions over East Africa. The exposure of the East African population to these extreme events at 1.5 °C and 2.0 °C global warming levels (GWLs) is analyzed based on Climate Model Intercomparison Project phase 6 models. Exposure is computed from extremely wet and dry days (R95p and CDD, respectively). Under both GWLs, EPIs (except CDD) averaged over East Africa are projected to increase under the Shared Socio-economic Pathways (SSP)2-4.5 and SSP5-8.5 scenarios. The largest increase in wet events will likely occur in eastern and northern Kenya. The results also reveal an intensification of precipitation extremes over Burundi, Rwanda, and some parts of Uganda. However, small changes are expected over most parts of Kenya and Tanzania. Examination of population exposure to EPIs shows that the most prominent and net intense occurrence is over Burundi, Rwanda, and some parts of Uganda. In contrast, less change is noted to occur over vast parts of Kenya and Tanzania. Meanwhile, limiting the warming target to less than 1.5 °C but not more than 2.0 °C has 37\% (44.2\%) and 92\% (4\%) less impact on the occurrence of EPIs for R95p (CDD) under SSP2-4.5 (SSP5-8.5) scenarios, respectively. The study establishes that future exposure is predominantly driven by changes in population compared to other factors such as climate or concurrent changes in climate and population (the nonlinear interaction effect). For instance, climate effects are anticipated to contribute ∼10.6\% (12.6\%) of the total change in population exposure under 1.5 °C (2.0 °C) warming levels, while population and interaction effects are expected to contribute ∼77.4\% (71.9\%) and 12\% (15.5\%), respectively, under 1.5 °C (2.0 °C) scenarios. Interestingly, the projected changes in regional exposure due to the interaction effects under SSP2-4.5 are greater than the climate effect, while the reverse pattern is observed under SSP5-8.5. For example, under SSP5-8.5, climate effects for 1.5 °C and 2.0 °C are larger (after population effect) with ∼3.8 × 105 (15.7\%) and ∼6.1 × 105 (17.5\%) billion person-mm, respectively. The high exposure noted over East Africa calls for a shift in policies to instate suitable adaptation measures to cushion the already vulnerable population.},
	language = {en},
	number = {4},
	urldate = {2023-07-31},
	journal = {Environmental Research Letters},
	author = {Ayugi, Brian and Jiang, Zhihong and Iyakaremye, Vedaste and Ngoma, Hamida and Babaousmail, Hassen and Onyutha, Charles and Dike, Victor Nnamdi and Mumo, Richard and Ongoma, Victor},
	month = mar,
	year = {2022},
	note = {Publisher: IOP Publishing},
	pages = {044051},
}

@article{ayugi_future_2021,
	title = {Future {Changes} in {Precipitation} {Extremes} over {East} {Africa} {Based} on {CMIP6} {Models}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4441},
	url = {https://www.mdpi.com/2073-4441/13/17/2358},
	doi = {10.3390/w13172358},
	abstract = {This paper presents an analysis of projected precipitation extremes over the East African region. The study employs six indices defined by the Expert Team on Climate Change Detection Indices to evaluate extreme precipitation. Observed datasets and Coupled Model Intercomparison Project Phase six (CMIP6) simulations are employed to assess the changes during the two main rainfall seasons: March to May (MAM) and October to December (OND). The results show an increase in consecutive dry days (CDD) and decrease in consecutive wet days (CWD) towards the end of the 21st century (2081–2100) relative to the baseline period (1995–2014) in both seasons. Moreover, simple daily intensity (SDII), very wet days (R95 p), very heavy precipitation {\textgreater}20 mm (R20 mm), and total wet-day precipitation (PRCPTOT) demonstrate significant changes during OND compared to the MAM season. The spatial variation for extreme incidences shows likely intensification over Uganda and most parts of Kenya, while a reduction is observed over the Tanzania region. The increase in projected extremes may pose a serious threat to the sustainability of societal infrastructure and ecosystem wellbeing. The results from these analyses present an opportunity to understand the emergence of extreme events and the capability of model outputs from CMIP6 in estimating the projected changes. More studies are recommended to examine the underlying physical features modulating the occurrence of extreme incidences projected for relevant policies.},
	language = {en},
	number = {17},
	urldate = {2023-07-31},
	journal = {Water},
	author = {Ayugi, Brian and Dike, Victor and Ngoma, Hamida and Babaousmail, Hassen and Mumo, Richard and Ongoma, Victor},
	month = jan,
	year = {2021},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {CMIP6, East Africa, extreme precipitation, projections, scenarios},
	pages = {2358},
}

@article{thiery_impact_2015,
	title = {The {Impact} of the {African} {Great} {Lakes} on the {Regional} {Climate}},
	volume = {28},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/28/10/jcli-d-14-00565.1.xml},
	doi = {10.1175/JCLI-D-14-00565.1},
	abstract = {Abstract Although the African Great Lakes are important regulators for the East African climate, their influence on atmospheric dynamics and the regional hydrological cycle remains poorly understood. This study aims to assess this impact by comparing a regional climate model simulation that resolves individual lakes and explicitly computes lake temperatures to a simulation without lakes. The Consortium for Small-Scale Modelling model in climate mode (COSMO-CLM) coupled to the Freshwater Lake model (FLake) and Community Land Model (CLM) is used to dynamically downscale a simulation from the African Coordinated Regional Downscaling Experiment (CORDEX-Africa) to 7-km grid spacing for the period of 1999–2008. Evaluation of the model reveals good performance compared to both in situ and satellite observations, especially for spatiotemporal variability of lake surface temperatures (0.68-K bias), and precipitation (−116 mm yr−1 or 8\% bias). Model integrations indicate that the four major African Great Lakes almost double the annual precipitation amounts over their surface but hardly exert any influence on precipitation beyond their shores. Except for Lake Kivu, the largest lakes also cool the annual near-surface air by −0.6 to −0.9 K on average, this time with pronounced downwind influence. The lake-induced cooling happens during daytime, when the lakes absorb incoming solar radiation and inhibit upward turbulent heat transport. At night, when this heat is released, the lakes warm the near-surface air. Furthermore, Lake Victoria has a profound influence on atmospheric dynamics and stability, as it induces circular airflow with over-lake convective inhibition during daytime and the reversed pattern at night. Overall, this study shows the added value of resolving individual lakes and realistically representing lake surface temperatures for climate studies in this region.},
	language = {EN},
	number = {10},
	urldate = {2023-07-31},
	journal = {Journal of Climate},
	author = {Thiery, Wim and Davin, Edouard L. and Panitz, Hans-Jürgen and Demuzere, Matthias and Lhermitte, Stef and Lipzig, Nicole van},
	month = may,
	year = {2015},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {4061--4085},
}

@misc{annau_algorithmic_2023,
	title = {Algorithmic {Hallucinations} of {Near}-{Surface} {Winds}: {Statistical} {Downscaling} with {Generative} {Adversarial} {Networks} to {Convection}-{Permitting} {Scales}},
	shorttitle = {Algorithmic {Hallucinations} of {Near}-{Surface} {Winds}},
	url = {http://arxiv.org/abs/2302.08720},
	abstract = {This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations influence spatial structures in the generated fields, particularly biases in spatial variability spectra. Using power spectra to evaluate the FS experiments reveals that successful applications of FS in computer vision do not translate to climate fields. However, the FS experiments demonstrate the sensitivity of power spectra to a commonly used GAN-based SR objective function, which helps interpret and understand its role in determining spatial structures. This result motivates the development of a novel partial frequency-separation scheme as a promising configuration option. We also quantify the influence on GAN performance of non-idealized LR fields resulting from internal variability. Furthermore, we conduct a spectra-based feature-importance experiment allowing us to explore the dependence of the spatial structure of generated fields on different physically relevant LR covariates.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Annau, Nicolaas J. and Cannon, Alex J. and Monahan, Adam H.},
	month = jul,
	year = {2023},
	note = {arXiv:2302.08720 [physics]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@article{zipser_where_2006,
	title = {Where are the most intense thunderstorms on earth?},
	volume = {87},
	issn = {0003-0007, 1520-0477},
	url = {https://journals.ametsoc.org/view/journals/bams/87/8/bams-87-8-1057.xml},
	doi = {10.1175/BAMS-87-8-1057},
	abstract = {The instruments on the Tropical Rainfall Measuring Mission (TRMM) satellite have been observing storms as well as rainfall since December 1997. This paper shows the results of a systematic search through seven full years of the TRMM database to find indicators of uncommonly intense storms. These include strong ({\textgreater} 40 dBZ) radar echoes extending to great heights, high lightning flash rates, and very low brightness temperatures at 37 and 85 GHz. These are used as proxy variables, indicating powerful convective updrafts. The main physical principles supporting this assertion involve the effects of such updrafts in producing and lofting large ice particles high into the storm, where TRMM's radar easily detects them near storm top. TRMM's passive microwave radiometer detects the large integrated ice water path as very low brightness temperatures, while high lightning flash rates are a physically related but instrumentally independent indicator. The geographical locations of these very intense convective storms demonstrate strong regional preferences for certain land areas while they are extremely rare over tropical oceans. Favored locations include the south-central United States, southeast South America, and equatorial Africa. Other regions have extreme storms mainly in specific seasons, such as the Sahel, the Indian subcontinent, and northern Australia. Because intense storms are distributed quite differently from rainfall, these maps provide some new metrics for global models, if they are to simulate the type of convection as a component of our climate system.},
	language = {en},
	number = {8},
	urldate = {2023-07-27},
	journal = {Bulletin of the American Meteorological Society},
	author = {Zipser, E. J. and Cecil, Daniel J. and Liu, Chuntao and Nesbitt, Stephen W. and Yorty, David P.},
	month = aug,
	year = {2006},
	note = {Publisher: American Meteorological Society
Section: Bulletin of the American Meteorological Society},
	pages = {1057--1072},
}

@article{thiery_early_2017,
	title = {Early warnings of hazardous thunderstorms over {Lake} {Victoria}},
	volume = {12},
	issn = {1748-9326},
	url = {https://dx.doi.org/10.1088/1748-9326/aa7521},
	doi = {10.1088/1748-9326/aa7521},
	abstract = {Weather extremes have harmful impacts on communities around Lake Victoria in East Africa. Every year, intense nighttime thunderstorms cause numerous boating accidents on the lake, resulting in thousands of deaths among fishermen. Operational storm warning systems are therefore crucial. Here we complement ongoing early warning efforts based on numerical weather prediction, by presenting a new satellite data-driven storm prediction system, the prototype Lake Victoria Intense storm Early Warning System (VIEWS). VIEWS derives predictability from the correlation between afternoon land storm activity and nighttime storm intensity on Lake Victoria, and relies on logistic regression techniques to forecast extreme thunderstorms from satellite observations. Evaluation of the statistical model reveals that predictive power is high and independent of the type of input dataset. We then optimise the configuration and show that false alarms also contain valuable information. Our results suggest that regression-based models that are motivated through process understanding have the potential to reduce the vulnerability of local fishing communities around Lake Victoria. The experimental prediction system is publicly available under the MIT licence at http://github.com/wthiery/VIEWS.},
	language = {en},
	number = {7},
	urldate = {2023-07-27},
	journal = {Environmental Research Letters},
	author = {Thiery, Wim and Gudmundsson, Lukas and Bedka, Kristopher and Semazzi, Fredrick H. M. and Lhermitte, Stef and Willems, Patrick and Lipzig, Nicole P. M. van and Seneviratne, Sonia I.},
	month = jul,
	year = {2017},
	note = {Publisher: IOP Publishing},
	pages = {074012},
}

@article{thiery_hazardous_2016,
	title = {Hazardous thunderstorm intensification over {Lake} {Victoria}},
	volume = {7},
	copyright = {2016 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms12786},
	doi = {10.1038/ncomms12786},
	abstract = {Weather extremes have harmful impacts on communities around Lake Victoria, where thousands of fishermen die every year because of intense night-time thunderstorms. Yet how these thunderstorms will evolve in a future warmer climate is still unknown. Here we show that Lake Victoria is projected to be a hotspot of future extreme precipitation intensification by using new satellite-based observations, a high-resolution climate projection for the African Great Lakes and coarser-scale ensemble projections. Land precipitation on the previous day exerts a control on night-time occurrence of extremes on the lake by enhancing atmospheric convergence (74\%) and moisture availability (26\%). The future increase in extremes over Lake Victoria is about twice as large relative to surrounding land under a high-emission scenario, as only over-lake moisture advection is high enough to sustain Clausius–Clapeyron scaling. Our results highlight a major hazard associated with climate change over East Africa and underline the need for high-resolution projections to assess local climate change.},
	language = {en},
	number = {1},
	urldate = {2023-07-27},
	journal = {Nature Communications},
	author = {Thiery, Wim and Davin, Edouard L. and Seneviratne, Sonia I. and Bedka, Kristopher and Lhermitte, Stef and van Lipzig, Nicole P. M.},
	month = sep,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Atmospheric science, Climate and Earth system modelling, Projection and prediction},
	pages = {12786},
}

@article{finney_implications_2019,
	title = {Implications of {Improved} {Representation} of {Convection} for the {East} {Africa} {Water} {Budget} {Using} a {Convection}-{Permitting} {Model}},
	volume = {32},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/32/7/jcli-d-18-0387.1.xml},
	doi = {10.1175/JCLI-D-18-0387.1},
	abstract = {Abstract The precipitation and diabatic heating resulting from moist convection make it a key component of the atmospheric water budget in the tropics. With convective parameterization being a known source of uncertainty in global models, convection-permitting (CP) models are increasingly being used to improve understanding of regional climate. Here, a new 10-yr CP simulation is used to study the characteristics of rainfall and atmospheric water budget for East Africa and the Lake Victoria basin. The explicit representation of convection leads to a widespread improvement in the intensities and diurnal cycle of rainfall when compared with a parameterized simulation. Differences in large-scale moisture fluxes lead to a shift in the mean rainfall pattern from the Congo to Lake Victoria basin in the CP simulation—highlighting the important connection between local changes in the representation of convection and larger-scale dynamics and rainfall. Stronger lake–land contrasts in buoyancy in the CP model lead to a stronger nocturnal land breeze over Lake Victoria, increasing evaporation and moisture flux convergence (MFC), and likely unrealistically high rainfall. However, for the mountains east of the lake, the CP model produces a diurnal rainfall cycle much more similar to satellite estimates, which is related to differences in the timing of MFC. Results here demonstrate that, while care is needed regarding lake forcings, a CP approach offers a more realistic representation of several rainfall characteristics through a more physically based realization of the atmospheric dynamics around the complex topography of East Africa.},
	language = {EN},
	number = {7},
	urldate = {2023-07-27},
	journal = {Journal of Climate},
	author = {Finney, Declan L. and Marsham, John H. and Jackson, Lawrence S. and Kendon, Elizabeth J. and Rowell, David P. and Boorman, Penelope M. and Keane, Richard J. and Stratton, Rachel A. and Senior, Catherine A.},
	month = apr,
	year = {2019},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {2109--2129},
}

@article{noauthor_dynamics_nodate,
	title = {Dynamics of the {Lorenz}-96 model},
	language = {en},
}

@article{christensen_simulating_2015,
	title = {Simulating weather regimes: impact of stochastic and perturbed parameter schemes in a simple atmospheric model},
	volume = {44},
	issn = {1432-0894},
	shorttitle = {Simulating weather regimes},
	url = {https://doi.org/10.1007/s00382-014-2239-9},
	doi = {10.1007/s00382-014-2239-9},
	abstract = {Representing model uncertainty is important for both numerical weather and climate prediction. Stochastic parametrisation schemes are commonly used for this purpose in weather prediction, while perturbed parameter approaches are widely used in the climate community. The performance of these two representations of model uncertainty is considered in the context of the idealised Lorenz ’96 system, in terms of their ability to capture the observed regime behaviour of the system. These results are applicable to the atmosphere, where evidence points to the existence of persistent weather regimes, and where it is desirable that climate models capture this regime behaviour. The stochastic parametrisation schemes considerably improve the representation of regimes when compared to a deterministic model: both the structure and persistence of the regimes are found to improve. The stochastic parametrisation scheme represents the small scale variability present in the full system, which enables the system to explore a larger portion of the system’s attractor, improving the simulated regime behaviour. It is important that temporally correlated noise is used in the stochastic parametrisation—white noise schemes performed similarly to the deterministic model. In contrast, the perturbed parameter ensemble was unable to capture the regime structure of the attractor, with many individual members exploring only one regime. This poor performance was not evident in other climate diagnostics. Finally, a ‘climate change’ experiment was performed, where a change in external forcing resulted in changes to the regime structure of the attractor. The temporally correlated stochastic schemes captured these changes well.},
	language = {en},
	number = {7},
	urldate = {2023-07-26},
	journal = {Climate Dynamics},
	author = {Christensen, H. M. and Moroz, I. M. and Palmer, T. N.},
	month = apr,
	year = {2015},
	keywords = {Climate change, Lorenz ’96 system, Model uncertainty, Perturbed parameter schemes, Stochastic physics, Weather regimes},
	pages = {2195--2214},
}

@article{palmer_towards_2012,
	title = {Towards the probabilistic {Earth}-system simulator: a vision for the future of climate and weather prediction},
	volume = {138},
	issn = {1477-870X},
	shorttitle = {Towards the probabilistic {Earth}-system simulator},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.1923},
	doi = {10.1002/qj.1923},
	abstract = {There is no more challenging problem in computational science than that of estimating, as accurately as science and technology allows, the future evolution of Earth's climate; nor indeed is there a problem whose solution has such importance and urgency. Historically, the simulation tools needed to predict climate have been developed, somewhat independently, at a number of weather and climate institutes around the world. While these simulators are individually deterministic, it is often assumed that the resulting diversity provides a useful quantification of uncertainty in global or regional predictions. However, this notion is not well founded theoretically and corresponding ‘multi-simulator’ estimates of uncertainty can be prone to systemic failure. Separate to this, individual institutes are now facing considerable challenges in finding the human and computational resources needed to develop more accurate weather and climate simulators with higher resolution and full Earth-system complexity. A new approach, originally designed to improve reliability in ensemble-based numerical weather prediction, is introduced to help solve these two rather different problems. Using stochastic mathematics, this approach recognizes uncertainty explicitly in the parametrized representation of unresolved climatic processes. Stochastic parametrization is shown to be more consistent with the underlying equations of motion and, moreover, provides more skilful estimates of uncertainty when compared with estimates from traditional multi-simulator ensembles, on time-scales where verification data exist. Stochastic parametrization can also help reduce long-term biases which have bedevilled numerical simulations of climate from the earliest days to the present. As a result, it is suggested that the need to maintain a large ‘gene pool’ of quasi-independent deterministic simulators may be obviated by the development of probabilistic Earth-system simulators. Consistent with the conclusions of the World Summit on Climate Modelling, this in turn implies that individual institutes will be able to pool human and computational resources in developing future-generation simulators, thus benefitting from economies of scale; the establishment of the Airbus consortium provides a useful analogy here. As a further stimulus for such evolution, discussion is given to a potential new synergy between the development of dynamical cores, and stochastic processing hardware. However, it is concluded that the traditional challenge in numerical weather prediction, of reducing deterministic measures of forecast error, may increasingly become an obstacle to the seamless development of probabilistic weather and climate simulators, paradoxical as that may appear at first sight. Indeed, going further, it is argued that it may be time to consider focusing operational weather forecast development entirely on high-resolution ensemble prediction systems. Finally, by considering the exceptionally challenging problem of quantifying cloud feedback in climate change, it is argued that the development of the probabilistic Earth-system simulator may actually provide a route to reducing uncertainty in climate prediction. Copyright © 2012 Royal Meteorological Society},
	language = {en},
	number = {665},
	urldate = {2023-07-26},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Palmer, T. N.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.1923},
	keywords = {Earth-system simulation, ensemble prediction, stochastic parametrization},
	pages = {841--861},
}

@article{arnold_stochastic_2013,
	title = {Stochastic parametrizations and model uncertainty in the {Lorenz} ’96 system},
	volume = {371},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2011.0479},
	doi = {10.1098/rsta.2011.0479},
	abstract = {Simple chaotic systems are useful tools for testing methods for use in numerical weather simulations owing to their transparency and computational cheapness. The Lorenz system was used here; the full system was defined as ‘truth’, whereas a truncated version was used as a testbed for parametrization schemes. Several stochastic parametrization schemes were investigated, including additive and multiplicative noise. The forecasts were started from perfect initial conditions, eliminating initial condition uncertainty. The stochastically generated ensembles were compared with perturbed parameter ensembles and deterministic schemes. The stochastic parametrizations showed an improvement in weather and climate forecasting skill over deterministic parametrizations. Including a temporal autocorrelation resulted in a significant improvement over white noise, challenging the standard idea that a parametrization should only represent sub-gridscale variability. The skill of the ensemble at representing model uncertainty was tested; the stochastic ensembles gave better estimates of model uncertainty than the perturbed parameter ensembles. The forecasting skill of the parametrizations was found to be linked to their ability to reproduce the climatology of the full model. This is important in a seamless prediction system, allowing the reliability of short-term forecasts to provide a quantitative constraint on the accuracy of climate predictions from the same system.},
	number = {1991},
	urldate = {2023-07-26},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Arnold, H. M. and Moroz, I. M. and Palmer, T. N.},
	month = may,
	year = {2013},
	note = {Publisher: Royal Society},
	keywords = {ensemble prediction, model uncertainty, reliability, seamless prediction, stochastic parametrizations},
	pages = {20110479},
}

@article{christensen_stochastic_2015,
	title = {Stochastic and {Perturbed} {Parameter} {Representations} of {Model} {Uncertainty} in {Convection} {Parameterization}},
	volume = {72},
	issn = {0022-4928, 1520-0469},
	url = {https://journals.ametsoc.org/view/journals/atsc/72/6/jas-d-14-0250.1.xml},
	doi = {10.1175/JAS-D-14-0250.1},
	abstract = {Abstract It is now acknowledged that representing model uncertainty in atmospheric simulators is essential for the production of reliable probabilistic forecasts, and a number of different techniques have been proposed for this purpose. This paper presents new perturbed parameter schemes for use in the European Centre for Medium-Range Weather Forecasts (ECMWF) convection scheme. Two types of scheme are developed and implemented. Both schemes represent the joint uncertainty in four of the parameters in the convection parameterization scheme, which was estimated using the Ensemble Prediction and Parameter Estimation System (EPPES). The first scheme developed is a fixed perturbed parameter scheme, where the values of uncertain parameters are varied between ensemble members, but held constant over the duration of the forecast. The second is a stochastically varying perturbed parameter scheme. The performance of these schemes was compared to the ECMWF operational stochastic scheme, stochastically perturbed parameterization tendencies (SPPT), and to a model that does not represent uncertainty in convection. The skill of probabilistic forecasts made using the different models was evaluated. While the perturbed parameter schemes improve on the stochastic parameterization in some regards, the SPPT scheme outperforms the perturbed parameter approaches when considering forecast variables that are particularly sensitive to convection. Overall, SPPT schemes are the most skillful representations of model uncertainty owing to convection parameterization.},
	language = {EN},
	number = {6},
	urldate = {2023-07-26},
	journal = {Journal of the Atmospheric Sciences},
	author = {Christensen, H. M. and Moroz, I. M. and Palmer, T. N.},
	month = jun,
	year = {2015},
	note = {Publisher: American Meteorological Society
Section: Journal of the Atmospheric Sciences},
	pages = {2525--2544},
}

@inproceedings{singh_numerical_2019,
	address = {Vancouver, Canada},
	title = {Numerical {Weather} {Model} {Super}-{Resolution}},
	abstract = {Numerical simulation of weather is constrained due to the high computational cost of integrating the coupled PDEs that govern atmospheric motion. Even the ﬁnest-scale numerical weather prediction models cannot model the scales that dictate weather in urban areas and regions with high topographic complexity, like mountains. Thus, several statistical methods have been developed in the climate community to upsample numerical model output to ﬁner resolutions. This is conceptually similar to image super-resolution (SR) [1] and in this work we report the results of applying SR methods to this problem. We compare several methods and ﬁnd ESRGAN [2] to give high-ﬁdelity qualitative recovery but poorer performance on metrics such as MSE. However, the high frequency power spectrum is captured remarkably well by ESRGAN, virtually identical to the real data, while other method’s ﬁdelity drops signiﬁcantly at high frequency. We use this observation to modify our approach to optimize the power spectrum directly in our loss function, and call this technique PSD-Net. We achieve better performance across all metrics, along with increased stability and faster training time.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Singh, Alok and White, Brian and Albert, Adrian},
	year = {2019},
}

@misc{parthipan_using_2022,
	title = {Using {Probabilistic} {Machine} {Learning} to {Better} {Model} {Temporal} {Patterns} in {Parameterizations}: a case study with the {Lorenz} 96 model},
	shorttitle = {Using {Probabilistic} {Machine} {Learning} to {Better} {Model} {Temporal} {Patterns} in {Parameterizations}},
	url = {http://arxiv.org/abs/2203.14814},
	abstract = {The modelling of small-scale processes is a major source of error in climate models, hindering the accuracy of low-cost models which must approximate such processes through parameterization. Red noise is essential to many operational parameterization schemes, helping model temporal correlations. We show how to build on the successes of red noise by combining the known benefits of stochasticity with machine learning. This is done using a physically-informed recurrent neural network within a probabilistic framework. Our model is competitive and often superior to both a bespoke baseline and an existing probabilistic machine learning approach (GAN) when applied to the Lorenz 96 atmospheric simulation. This is due to its superior ability to model temporal patterns compared to standard first-order autoregressive schemes. It also generalises to unseen scenarios. We evaluate across a number of metrics from the literature, and also discuss the benefits of using the probabilistic metric of hold-out likelihood.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Parthipan, Raghul and Christensen, Hannah M. and Hosking, J. Scott and Wischik, Damon J.},
	month = sep,
	year = {2022},
	note = {arXiv:2203.14814 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@article{gagne_ii_machine_2020,
	title = {Machine {Learning} for {Stochastic} {Parameterization}: {Generative} {Adversarial} {Networks} in the {Lorenz} '96 {Model}},
	volume = {12},
	issn = {1942-2466},
	shorttitle = {Machine {Learning} for {Stochastic} {Parameterization}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001896},
	doi = {10.1029/2019MS001896},
	abstract = {Stochastic parameterizations account for uncertainty in the representation of unresolved subgrid processes by sampling from the distribution of possible subgrid forcings. Some existing stochastic parameterizations utilize data-driven approaches to characterize uncertainty, but these approaches require significant structural assumptions that can limit their scalability. Machine learning models, including neural networks, are able to represent a wide range of distributions and build optimized mappings between a large number of inputs and subgrid forcings. Recent research on machine learning parameterizations has focused only on deterministic parameterizations. In this study, we develop a stochastic parameterization using the generative adversarial network (GAN) machine learning framework. The GAN stochastic parameterization is trained and evaluated on output from the Lorenz '96 model, which is a common baseline model for evaluating both parameterization and data assimilation techniques. We evaluate different ways of characterizing the input noise for the model and perform model runs with the GAN parameterization at weather and climate time scales. Some of the GAN configurations perform better than a baseline bespoke parameterization at both time scales, and the networks closely reproduce the spatiotemporal correlations and regimes of the Lorenz '96 system. We also find that, in general, those models which produce skillful forecasts are also associated with the best climate simulations.},
	language = {en},
	number = {3},
	urldate = {2023-07-26},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Gagne II, David John and Christensen, Hannah M. and Subramanian, Aneesh C. and Monahan, Adam H.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001896},
	keywords = {climate, generative adversarial networks, lorenz, machine learning, stochastic parameterization, weather},
	pages = {e2019MS001896},
}

@article{delaunay_interpretable_2022,
	title = {Interpretable {Deep} {Learning} for {Probabilistic} {MJO} {Prediction}},
	volume = {49},
	issn = {1944-8007},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022GL098566},
	doi = {10.1029/2022GL098566},
	abstract = {The Madden-Julian oscillation (MJO) is the dominant source of sub-seasonal variability in the tropics. It consists of an Eastward moving region of enhanced convection coupled to changes in zonal winds. It is not possible to predict the precise evolution of the MJO, so sub-seasonal forecasts are generally probabilistic. We present a deep convolutional neural network (CNN) that produces skilful state-dependent probabilistic MJO forecasts. Importantly, the CNN's forecast uncertainty varies depending on the instantaneous predictability of the MJO. The CNN accounts for intrinsic chaotic uncertainty by predicting the standard deviation about the mean, and model uncertainty using Monte-Carlo dropout. Interpretation of the CNN mean forecasts highlights known MJO mechanisms, providing confidence in the model. Interpretation of forecast uncertainty indicates mechanisms governing MJO predictability. In particular, we find an initially stronger MJO signal is associated with more uncertainty, and that MJO predictability is affected by the state of the Walker Circulation.},
	language = {en},
	number = {16},
	urldate = {2023-07-26},
	journal = {Geophysical Research Letters},
	author = {Delaunay, Antoine and Christensen, Hannah M.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2022GL098566},
	keywords = {MJO, Madden-Julian oscillation, XAI, deep learning, predictability},
	pages = {e2022GL098566},
}

@book{murphy_probabilistic_2022,
	title = {Probabilistic {Machine} {Learning}},
	publisher = {MIT Press},
	author = {Murphy, Kevin},
	year = {2022},
}

@inproceedings{li_generative_2015,
	title = {Generative {Moment} {Matching} {Networks}},
	url = {https://proceedings.mlr.press/v37/li15.html},
	abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
	language = {en},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Yujia and Swersky, Kevin and Zemel, Rich},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {1718--1727},
}

@article{li_generative_nodate,
	title = {Generative {Moment} {Matching} {Networks}},
	abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difﬁcult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
	language = {en},
	author = {Li, Yujia and Swersky, Kevin and Zemel, Richard},
}

@article{brecht_computing_2023,
	title = {Computing the {Ensemble} {Spread} {From} {Deterministic} {Weather} {Predictions} {Using} {Conditional} {Generative} {Adversarial} {Networks}},
	volume = {50},
	copyright = {© 2023 The Authors.},
	issn = {1944-8007},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022GL101452},
	doi = {10.1029/2022GL101452},
	abstract = {Ensemble prediction systems are an invaluable tool for weather forecasting. Practically, ensemble predictions are obtained by running several perturbations of the deterministic control forecast. However, ensemble prediction is associated with a high computational cost and often involves statistical post-processing steps to improve its quality. Here we propose to use deep-learning-based algorithms to learn the statistical properties of an ensemble prediction system, the ensemble spread, given only the deterministic control forecast. Thus, once trained, the costly ensemble prediction system will not be needed anymore to obtain future ensemble forecasts, and the statistical properties of the ensemble can be derived from a single deterministic forecast. We adapt the classical pix2pix architecture to a three-dimensional model and train them against several years of operational (ensemble) weather forecasts for the 500 hPa geopotential height. The results demonstrate that the trained models indeed allow obtaining a highly accurate ensemble spread from the control forecast only.},
	language = {en},
	number = {2},
	urldate = {2023-07-20},
	journal = {Geophysical Research Letters},
	author = {Brecht, Rüdiger and Bihlo, Alex},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2022GL101452},
	pages = {e2022GL101452},
}

@misc{pacchiardi_probabilistic_2022,
	title = {Probabilistic {Forecasting} with {Generative} {Networks} via {Scoring} {Rule} {Minimization}},
	url = {http://arxiv.org/abs/2112.08217},
	abstract = {Generative networks are often trained to minimize a statistical divergence between the reference distribution and the generative one in an adversarial setting. Some works trained instead generative networks to minimize Scoring Rules, functions assessing how well the generative distribution matches each training sample individually. We show how the Scoring Rule formulation easily extends to the so-called prequential (predictive-sequential) score, whose minimization allows performing probabilistic forecasting with generative networks. This objective leads to adversarial-free training, therefore easily avoiding uncertainty underestimation due to mode collapse, which is a common issue in the adversarial setting and undesirable for probabilistic forecasting. We provide consistency guarantees for the minimizer of the prequential score and employ that to perform probabilistic forecasting for two chaotic dynamical models and a benchmark dataset of global weather observations. For this last example, we define scoring rules for spatial data by drawing from the relevant literature, with which we obtain better uncertainty quantification with little hyperparameter tuning compared to adversarial training.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Pacchiardi, Lorenzo and Adewoyin, Rilwan and Dueben, Peter and Dutta, Ritabrata},
	month = may,
	year = {2022},
	note = {arXiv:2112.08217 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_generative_2022,
	title = {Generative machine learning methods for multivariate ensemble post-processing},
	url = {http://arxiv.org/abs/2211.01345},
	doi = {10.48550/arXiv.2211.01345},
	abstract = {Ensemble weather forecasts based on multiple runs of numerical weather prediction models typically show systematic errors and require post-processing to obtain reliable forecasts. Accurately modeling multivariate dependencies is crucial in many practical applications, and various approaches to multivariate post-processing have been proposed where ensemble predictions are first post-processed separately in each margin and multivariate dependencies are then restored via copulas. These two-step methods share common key limitations, in particular the difficulty to include additional predictors in modeling the dependencies. We propose a novel multivariate post-processing method based on generative machine learning to address these challenges. In this new class of nonparametric data-driven distributional regression models, samples from the multivariate forecast distribution are directly obtained as output of a generative neural network. The generative model is trained by optimizing a proper scoring rule which measures the discrepancy between the generated and observed data, conditional on exogenous input variables. Our method does not require parametric assumptions on univariate distributions or multivariate dependencies and allows for incorporating arbitrary predictors. In two case studies on multivariate temperature and wind speed forecasting at weather stations over Germany, our generative model shows significant improvements over state-of-the-art methods and particularly improves the representation of spatial dependencies.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Chen, Jieyu and Janke, Tim and Steinke, Florian and Lerch, Sebastian},
	month = sep,
	year = {2022},
	note = {arXiv:2211.01345 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics, Statistics - Methodology},
}

@article{creswell_generative_2018,
	title = {Generative {Adversarial} {Networks}: {An} {Overview}},
	volume = {35},
	issn = {1558-0792},
	shorttitle = {Generative {Adversarial} {Networks}},
	doi = {10.1109/MSP.2017.2765202},
	abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
	month = jan,
	year = {2018},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Convolutional codes, Data models, Generators, Image resolution, Machine learning, Semantics, Signal resolution, Training data},
	pages = {53--65},
}

@article{zhang_skilful_2023,
	title = {Skilful nowcasting of extreme precipitation with {NowcastNet}},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06184-4},
	doi = {10.1038/s41586-023-06184-4},
	abstract = {Extreme precipitation is a considerable contributor to meteorological disasters and there is a great need to mitigate its socioeconomic effects through skilful nowcasting that has high resolution, long lead times and local details1–3. Current methods are subject to blur, dissipation, intensity or location errors, with physics-based numerical methods struggling to capture pivotal chaotic dynamics such as convective initiation4 and data-driven learning methods failing to obey intrinsic physical laws such as advective conservation5. We present NowcastNet, a nonlinear nowcasting model for extreme precipitation that unifies physical-evolution schemes and conditional-learning methods into a neural-network framework with end-to-end forecast error optimization. On the basis of radar observations from the USA and China, our model produces physically plausible precipitation nowcasts with sharp multiscale patterns over regions of 2,048 km × 2,048 km and with lead times of up to 3 h. In a systematic evaluation by 62 professional meteorologists from across China, our model ranks first in 71\% of cases against the leading methods. NowcastNet provides skilful forecasts at light-to-heavy rain rates, particularly for extreme-precipitation events accompanied by advective or convective processes that were previously considered intractable.},
	language = {en},
	urldate = {2023-07-14},
	journal = {Nature},
	author = {Zhang, Yuchen and Long, Mingsheng and Chen, Kaiyuan and Xing, Lanxiang and Jin, Ronghua and Jordan, Michael I. and Wang, Jianmin},
	month = jul,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Atmospheric science, Computational science, Computer science},
	pages = {1--7},
}

@article{harris_generative_2022,
	title = {A {Generative} {Deep} {Learning} {Approach} to {Stochastic} {Downscaling} of {Precipitation} {Forecasts}},
	volume = {14},
	abstract = {Despite continuous improvements, precipitation forecasts are still not as accurate and reliable as those of other meteorological variables. A major contributing factor to this is that several key processes affecting precipitation distribution and intensity occur below the resolved scale of global weather models. Generative adversarial networks (GANs) have been demonstrated by the computer vision community to be successful at super-resolution problems, i.e., learning to add fine-scale structure to coarse images. Leinonen et al. (2020) previously applied a GAN to produce ensembles of reconstructed high-resolution atmospheric fields, given coarsened input data. In this paper, we demonstrate this approach can be extended to the more challenging problem of increasing the accuracy and resolution of comparatively low-resolution input from a weather forecasting model, using high-resolution radar measurements as a "ground truth". The neural network must learn to add resolution and structure whilst accounting for non-negligible forecast error. We show that GANs and VAE-GANs can match the statistical properties of state-of-the-art pointwise post-processing methods whilst creating high-resolution, spatially coherent precipitation maps. Our model compares favourably to the best existing downscaling methods in both pixel-wise and pooled CRPS scores, power spectrum information and rank histograms (used to assess calibration). We test our models and show that they perform in a range of scenarios, including heavy rainfall.},
	number = {10},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Harris, Lucy and McRae, Andrew T. T. and Chantry, Matthew and Dueben, Peter D. and Palmer, Tim N.},
	month = apr,
	year = {2022},
}

@article{ravuri_skilful_2021,
	title = {Skilful precipitation nowcasting using deep generative models of radar: {Supplementary} {Information}},
	volume = {597},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03854-z},
	doi = {10.1038/s41586-021-03854-z},
	abstract = {Abstract
            
              Precipitation nowcasting, the high-resolution forecasting of precipitation up to two hours ahead, supports the real-world socioeconomic needs of many sectors reliant on weather-dependent decision-making
              1,2
              . State-of-the-art operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture important non-linear events such as convective initiations
              3,4
              . Recently introduced deep learning methods use radar to directly predict future rain rates, free of physical constraints
              5,6
              . While they accurately predict low-intensity rainfall, their operational utility is limited because their lack of constraints produces blurry nowcasts at longer lead times, yielding poor performance on rarer medium-to-heavy rain events. Here we present a deep generative model for the probabilistic nowcasting of precipitation from radar that addresses these challenges. Using statistical, economic and cognitive measures, we show that our method provides improved forecast quality, forecast consistency and forecast value. Our model produces realistic and spatiotemporally consistent predictions over regions up to 1,536 km × 1,280 km and with lead times from 5–90 min ahead. Using a systematic evaluation by more than 50 expert meteorologists, we show that our generative model ranked first for its accuracy and usefulness in 89\% of cases against two competitive methods. When verified quantitatively, these nowcasts are skillful without resorting to blurring. We show that generative nowcasting can provide probabilistic predictions that improve forecast value and support operational utility, and at resolutions and lead times where alternative methods struggle.},
	language = {en},
	number = {7878},
	urldate = {2023-07-06},
	journal = {Nature},
	author = {Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and Prudden, Rachel and Mandhane, Amol and Clark, Aidan and Brock, Andrew and Simonyan, Karen and Hadsell, Raia and Robinson, Niall and Clancy, Ellen and Arribas, Alberto and Mohamed, Shakir},
	month = sep,
	year = {2021},
	pages = {672--677},
}

@inproceedings{arjovsky_wasserstein_2017,
	title = {Wasserstein {Generative} {Adversarial} {Networks}},
	url = {https://proceedings.mlr.press/v70/arjovsky17a.html},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {214--223},
}

@article{goodfellow_generative_2020,
	title = {Generative adversarial networks},
	volume = {63},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3422622},
	doi = {10.1145/3422622},
	abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
	number = {11},
	urldate = {2023-07-05},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = oct,
	year = {2020},
	pages = {139--144},
}

@inproceedings{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
	abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
	urldate = {2023-07-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
}

@inproceedings{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2023-07-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
	year = {2017},
}

@article{schwartz_comparison_2017,
	title = {A {Comparison} of {Methods} {Used} to {Populate} {Neighborhood}-{Based} {Contingency} {Tables} for {High}-{Resolution} {Forecast} {Verification}},
	volume = {32},
	issn = {1520-0434, 0882-8156},
	url = {https://journals.ametsoc.org/view/journals/wefo/32/2/waf-d-16-0187_1.xml},
	doi = {10.1175/WAF-D-16-0187.1},
	abstract = {Abstract As high-resolution numerical weather prediction models are now commonplace, “neighborhood” verification metrics are regularly employed to evaluate forecast quality. These neighborhood approaches relax the requirement that perfect forecasts must match observations at the grid scale, contrasting traditional point-by-point verification methods. One recently proposed metric, the neighborhood equitable threat score, is calculated from 2 × 2 contingency tables that are populated within a neighborhood framework. However, the literature suggests three subtly different methods of populating neighborhood-based contingency tables. Thus, this work compares and contrasts these three variants and shows they yield statistically significantly different conclusions regarding forecast performance, illustrating that neighborhood-based contingency tables should be constructed carefully and transparently. Furthermore, this paper shows how two of the methods use inconsistent event definitions and suggests a “neighborhood maximum” approach be used to fill neighborhood-based contingency tables.},
	language = {EN},
	number = {2},
	urldate = {2023-07-04},
	journal = {Weather and Forecasting},
	author = {Schwartz, Craig S.},
	month = apr,
	year = {2017},
	note = {Publisher: American Meteorological Society
Section: Weather and Forecasting},
	pages = {733--741},
}

@article{stein_neighborhood-based_2019,
	title = {Neighborhood-{Based} {Contingency} {Tables} {Including} {Errors} {Compensation}},
	volume = {147},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/147/1/mwr-d-17-0288.1.xml},
	doi = {10.1175/MWR-D-17-0288.1},
	abstract = {Abstract Some specific scores use a neighborhood strategy in order to reduce double penalty effects, which penalize high-resolution models, compared to large-scale models. Contingency tables based on this strategy have already been proposed, but can sometimes display undesirable behavior. A new method of populating contingency tables is proposed: pairs of missed events and false alarms located in the same local neighborhood compensate in order to give pairs of hits and correct rejections. Local tables are summed up so as to provide the final table for the whole verification domain. It keeps track of the bias of the forecast when neighborhoods are taken into account. Moreover, the scores computed from this table depend on the distance between forecast and observed patterns. This method is applied to binary and multicategorical events in a simplified framework so as to present the method and to compare the new tables with previous neighborhood-based contingency tables. The new tables are then used for the verification of two models operational at Météo-France: AROME, a high-resolution model, and ARPEGE, a large-scale global model. The comparison of several contingency scores shows that the importance of the double penalty decreases more for AROME than for ARPEGE when the neighboring size increases. Scores designed for rare events are also applied to these neighborhood-based contingency tables.},
	language = {EN},
	number = {1},
	urldate = {2023-07-04},
	journal = {Monthly Weather Review},
	author = {Stein, Joël and Stoop, Fabien},
	month = jan,
	year = {2019},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {329--344},
}

@misc{youds_gcrf_2021,
	type = {Monograph},
	title = {{GCRF} {African} {SWIFT} and {ForPAc} {SHEAR} {White} {Paper} on the {Potential} of {Operational} {Weather} {Prediction} to {Save} {Lives} and {Improve} {Livelihoods} and {Economies} in {Sub}-{Saharan} {Africa}},
	copyright = {cc\_by\_4},
	url = {https://eprints.whiterose.ac.uk/181045/},
	abstract = {The ‘silent revolution’ of numerical weather prediction (NWP) has led to significant social benefits and billions of dollars in economic benefits to mid-latitude countries, however the level of benefit in sub-Saharan Africa has been very limited, despite the potential to save lives, improve livelihoods, protect property and infrastructure and boost economies. Ongoing climate change in Africa, and the associated projected intensification of weather impacts in coming decades, makes the realisation of effective and more reliable weather forecasts and climate services even more urgent. It is widely recognised that to achieve this potential, investment is required in strengthening decision makers’ understanding of weather predictions and confidence in interpreting and appropriately applying forecasts, alongside transparent communication of the levels of skill and probability or certainty in forecast products. However, on all time scales of prediction, it is generally unrecognised that many forecasts that produce user-relevant metrics have such low skill that they are only marginally valuable to stakeholders, creating significant practical and ethical barriers to increasing uptake and generating benefits. Here, we present substantial evidence that even a modest investment in science for weather information and forecast techniques, to provide new technology and tools for Africa, can significantly increase the skill of user-relevant forecast products on all time scales. This will be a necessary enabler for building trust in and uptake of decision-relevant forecasts with the potential to deliver significant social and economic benefits. We present here an argument that incremental improvements in the skill of weather forecasting across all timescales in the African tropics, alongside strengthening communication and understanding of these forecasts, is fundamental to saving lives and enhancing livelihoods. Investing in the capacity and capability of National Meteorological Services and research institutions is essential to ensure lifesaving and life-enhancing services continue to be developed with and designed to serve the populations of sub-Saharan countries.},
	language = {en},
	urldate = {2023-07-04},
	author = {Youds, L. H. and Parker, D. J. and Adefisan, E. A. and Antwi-Agyei, P. and Bain, C. L. and Black, E. C. L. and Blyth, A. M. and Dougill, A. J. and Hirons, L. C. and Indasi, V. S. and Lamptey, B. L. and Marshall, F. and Marsham, J. H. and Stein, T. H. M. and Taylor, C. M. and Todd, M. C. and Visman, E. L. and Woolnough, S. J.},
	month = nov,
	year = {2021},
	note = {Publisher: University of Leeds},
}

@article{schaefer_critical_1990,
	title = {The {Critical} {Success} {Index} as an {Indicator} of {Warning} {Skill}},
	volume = {5},
	issn = {1520-0434, 0882-8156},
	url = {https://journals.ametsoc.org/view/journals/wefo/5/4/1520-0434_1990_005_0570_tcsiaa_2_0_co_2.xml},
	doi = {10.1175/1520-0434(1990)005<0570:TCSIAA>2.0.CO;2},
	abstract = {Abstract A form of the critical success index (CSI) is used by the National Weather Service to indicate the value of warnings. This verification statistic assumes that the times when an event was neither expected nor observed are of no consequence. It can be shown that the CSI is not an unbiased indicator of forecast skill but is proportional to the frequency of the event being forecast. This innate bias is demonstrated theoretically and via example. An unbiased verification statistic appropriate for forecast of rare events is presented and applied to severe convective weather warnings. Comparisons of this score to the CSI show the extent of the penalty the CSI extracts from forecasters who work in areas that are not climatically prone to given events.},
	language = {EN},
	number = {4},
	urldate = {2023-07-03},
	journal = {Weather and Forecasting},
	author = {Schaefer, Joseph T.},
	month = dec,
	year = {1990},
	note = {Publisher: American Meteorological Society
Section: Weather and Forecasting},
	pages = {570--575},
}

@book{molnar_interpretable_nodate,
	title = {Interpretable {Machine} {Learning}},
	url = {https://christophm.github.io/interpretable-ml-book/},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	urldate = {2023-07-03},
	author = {Molnar, Christoph},
}

@article{bhatia_exgan_2021,
	title = {{ExGAN}: {Adversarial} {Generation} of {Extreme} {Samples}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{ExGAN}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16834},
	doi = {10.1609/aaai.v35i8.16834},
	abstract = {Mitigating the risk arising from extreme events is a fundamental goal with many applications, such as the modelling of natural disasters, financial crashes, epidemics, and many others. To manage this risk, a vital step is to be able to understand or generate a wide range of extreme scenarios. Existing approaches based on Generative Adversarial Networks (GANs) excel at generating realistic samples, but seek to generate typical samples, rather than extreme samples. Hence, in this work, we propose ExGAN, a GAN-based approach to generate realistic and extreme samples. To model the extremes of the training distribution in a principled way, our work draws from Extreme Value Theory (EVT), a probabilistic approach for modelling the extreme tails of distributions. For practical utility, our framework allows the user to specify both the desired extremeness measure, as well as the desired extremeness probability they wish to sample at. Experiments on real US Precipitation data show that our method generates realistic samples, based on visual inspection and quantitative measures, in an efficient manner. Moreover, generating increasingly extreme examples using ExGAN can be done in constant time (with respect to the extremeness probability τ), as opposed to the O(1/τ) time required by the baseline approach.},
	language = {en},
	number = {8},
	urldate = {2023-07-03},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bhatia, Siddharth and Jain, Arjit and Hooi, Bryan},
	month = may,
	year = {2021},
	note = {Number: 8},
	keywords = {Adversarial Learning \& Robustness},
	pages = {6750--6758},
}

@misc{noauthor_ethiopia_nodate,
	title = {Ethiopia – 240,000 {Impacted} by {Heavy} {Rains} and {Floods}, 29 {Dead}, {Says} {UN} – {FloodList}},
	url = {https://floodlist.com/africa/ethiopia-floods-march-april-2023},
	urldate = {2023-06-30},
}

@article{moise_new_2011,
	title = {New climate model metrics based on object-orientated pattern matching of rainfall},
	volume = {116},
	copyright = {Copyright 2011 by the American Geophysical Union.},
	issn = {2156-2202},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2010JD015318},
	doi = {10.1029/2010JD015318},
	abstract = {Climate metrics are becoming a more widespread tool used in global circulation model (GCM) evaluation as well as climate change projections. In their more simple form they provide a quick overview of the performance of a large ensemble of GCM simulations such as the CMIP3 archive of coupled ocean-atmosphere GCMs. Most existing metrics focus on the comparison of fields at each grid point. We present here a complementary metric which targets structures as a whole (patterns). The methodology is based on a pattern matching technique used previously in numerical weather prediction and has been modified for the analysis of mean climate fields. The resulting error decomposition allows for a more detailed assessment of the field structure with regard to errors in placement, rotation, volume, and pattern. The technique is applied to two observational rainfall data sets and GCM simulations from the CMIP3 archive for seasonal rainfall structures over the South Pacific Convergence Zone.},
	language = {en},
	number = {D12},
	urldate = {2023-06-30},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Moise, Aurel F. and Delage, Francois P.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2010JD015318},
	keywords = {South Pacific Convergence Zone, climate change, global climate models, model evaluation, precipitation},
}

@article{yu_benchmark_2020,
	title = {Benchmark rainfall verification of landfall tropical cyclone forecasts by operational {ACCESS}-{TC} over {China}},
	volume = {27},
	issn = {1469-8080},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/met.1842},
	doi = {10.1002/met.1842},
	abstract = {Results from object-based verification of rainfall forecasts for landfalling tropical cyclones (TCs) over China during the period 2012–2015 are presented. The sample consists of 25 landfall events and 133 operational numerical forecasts from the TC version of the Australian Community Climate and Earth System Simulator. Mean equitable threat scores, probabilities of detection and false alarm ratios for the 30 mm isohyet for the unadjusted forecasts at 0–6 hr (essentially the initialization) are (0.23, 0.55, 0.65), while the performance measures of 24 hr forecast accumulations are the best for the 0–24 hr forecast (0.37, 0.67, 0.40) and then worsen to (0.16, 0.38, 0.66) for the 48–72 hr forecast. Forecast ability also decreases with the increase in rainfall amount. The contiguous rain area (CRA) verification method is used to diagnose the source of systematic errors from the displacement, rotation, volume and pattern of the forecasted rain fields. Results show that the errors are mostly from rainfall patterns, followed by displacement errors, particularly for very heavy rain. After application of the displacement and rotation adjustments of the CRA method, averaged errors improve by about 15\%. Results suggest that rainfall prediction will continue to improve with improved track prediction, but more work is needed on model initialization and the prediction of TC structure. The study has uncertainty related to the limited sample size, which could cause large variability, particularly for heavy rainfall at 6 and 72 hr. However, the results still represent a useful benchmark for future verification of landfalling TCs.},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {Meteorological Applications},
	author = {Yu, Zifeng and Chen, Ying Jun and Ebert, Beth and Davidson, Noel E. and Xiao, Yi and Yu, Hui and Duan, Yihong},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/met.1842},
	keywords = {precipitation forecast, rainfall evaluation, tropical cyclone, typhoon},
	pages = {e1842},
}

@article{dorninger_editorial_2020,
	title = {Editorial: {Recent} developments and application examples on forecast verification},
	volume = {27},
	copyright = {© 2020 The Authors. Meteorological Applications published by John Wiley \& Sons Ltd on behalf of the Royal Meteorological Society.},
	issn = {1469-8080},
	shorttitle = {Editorial},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/met.1934},
	doi = {10.1002/met.1934},
	language = {en},
	number = {4},
	urldate = {2023-06-30},
	journal = {Meteorological Applications},
	author = {Dorninger, Manfred and Ghelli, Anna and Lerch, Sebastian},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/met.1934},
	pages = {e1934},
}

@article{bofinger_qualification_2002,
	title = {Qualification of wind power forecasts},
	abstract = {In order to manage the remarkable share of wind power as present in several utilities forecast information is needed. In the last years several respective procedures have been developed and put into operation. Up to now the outcome of the forecast tools is mainly restricted to the forecasted value itself and general information of the overall error of the procedure as e.g. the standard deviation. However, for the handling of these forecast information in the framework of e.g. power station dispatch schemes, more detailed information of the structure of the expected errors - beyond the expected standard deviation - seem to be desirable. Due to the fact that the output of wind turbine systems is limited between zero and the maximum power, the error statistics cannot follow a normal distribution. Thus, for the assessment of the probability of occurrence of a certain forecast error a model for the distribution function of the errors has to be set up. We have analysed the errors of the forecast model PREVIENTO as applied for an ensemble of installations representing the lumped power output of the turbines within a given region. Given bias free forecasts, the applicability of various models for the distribution function of the set of errors has been tested. It turned out, that the use of a beta function is justifiable for this task with respect to chi -squared tests. Together with an empirically derived parametric model for the expected standard deviation of the ensemble forecast, the knowledge of the respective distribution function allows for the assignment of risk figures to any decision taken based on the wind power forecast.},
	journal = {2002 Global Windpower Conference, Vol. 2; Paris},
	author = {Bofinger, Stefan and Luig, A. and Beyer, Hans Georg},
	month = jan,
	year = {2002},
}

@article{bremnes_probabilistic_2004,
	title = {Probabilistic wind power forecasts using local quantile regression},
	volume = {7},
	issn = {1099-1824},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/we.107},
	doi = {10.1002/we.107},
	abstract = {Wind power forecasts are in various ways valuable for users in decision-making processes. However, most forecasts are deterministic, and hence possibly important information about uncertainty is not available. Complete information about future production can be obtained by using probabilistic forecasts, and this article demonstrates how such forecasts can be created by means of local quantile regression. The approach has several advantages, such as no distributional assumptions and flexible inclusion of predictive information. In addition, it can be shown that, for some purposes, forecasts in terms of quantiles provide the type of information required to make optimal economic decisions. The methodology is applied to data from a wind farm in Norway. Copyright © 2004 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {Wind Energy},
	author = {Bremnes, John Bjørnar},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/we.107},
	keywords = {economic value, probabilistic forecasts, quantile regression, wind power},
	pages = {47--54},
}

@article{meinshausen_quantile_2006,
	title = {Quantile {Regression} {Forests}},
	volume = {7},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v7/meinshausen06a.html},
	abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.},
	number = {35},
	urldate = {2023-06-30},
	journal = {Journal of Machine Learning Research},
	author = {Meinshausen, Nicolai},
	year = {2006},
	pages = {983--999},
}

@article{hayatbini_conditional_2019,
	title = {Conditional {Generative} {Adversarial} {Networks} ({cGANs}) for {Near} {Real}-{Time} {Precipitation} {Estimation} from {Multispectral} {GOES}-16 {Satellite} {Imageries}—{PERSIANN}-{cGAN}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/19/2193},
	doi = {10.3390/rs11192193},
	abstract = {In this paper, we present a state-of-the-art precipitation estimation framework which leverages advances in satellite remote sensing as well as Deep Learning (DL). The framework takes advantage of the improvements in spatial, spectral and temporal resolutions of the Advanced Baseline Imager (ABI) onboard the GOES-16 platform along with elevation information to improve the precipitation estimates. The procedure begins by first deriving a Rain/No Rain (R/NR) binary mask through classification of the pixels and then applying regression to estimate the amount of rainfall for rainy pixels. A Fully Convolutional Network is used as a regressor to predict precipitation estimates. The network is trained using the non-saturating conditional Generative Adversarial Network (cGAN) and Mean Squared Error (MSE) loss terms to generate results that better learn the complex distribution of precipitation in the observed data. Common verification metrics such as Probability Of Detection (POD), False Alarm Ratio (FAR), Critical Success Index (CSI), Bias, Correlation and MSE are used to evaluate the accuracy of both R/NR classification and real-valued precipitation estimates. Statistics and visualizations of the evaluation measures show improvements in the precipitation retrieval accuracy in the proposed framework compared to the baseline models trained using conventional MSE loss terms. This framework is proposed as an augmentation for PERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network- Cloud Classification System) algorithm for estimating global precipitation.},
	language = {en},
	number = {19},
	urldate = {2023-06-30},
	journal = {Remote Sensing},
	author = {Hayatbini, Negin and Kong, Bailey and Hsu, Kuo-lin and Nguyen, Phu and Sorooshian, Soroosh and Stephens, Graeme and Fowlkes, Charless and Nemani, Ramakrishna and Ganguly, Sangram},
	month = jan,
	year = {2019},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {convolutional neural networks (CNNs), generative adversarial networks (GANs), machine learning, multispectral satellite imagery, precipitation},
	pages = {2193},
}

@article{widmann_validation_2019,
	title = {Validation of spatial variability in downscaling results from the {VALUE} perfect predictor experiment},
	volume = {39},
	issn = {1097-0088},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.6024},
	doi = {10.1002/joc.6024},
	abstract = {The spatial dependence of meteorological variables is crucial for many impacts, for example, droughts, floods, river flows, energy demand, and crop yield. There is thus a need to understand how well it is represented in downscaling (DS) products. Within the COST Action VALUE, we have conducted a comprehensive analysis of spatial variability in the output of over 40 different DS methods in a perfect predictor setup. The DS output is evaluated against daily precipitation and temperature observations for the period 1979–2008 at 86 sites across Europe and 53 sites across Germany. We have analysed the dependency of correlations of daily temperature and precipitation series at station pairs on the distance between the stations. For the European data set, we have also investigated the complexity of the downscaled data by calculating the number of independent spatial degrees of freedom. For daily precipitation at the German network, we have additionally evaluated the dependency of the joint exceedance of the wet day threshold and of the local 90th percentile on the distance between the stations. Finally, we have investigated regional patterns of European monthly precipitation obtained from rotated principal component analysis. We analysed Perfect Prog (PP) methods, which are based on statistical relationships derived from observations, as well as Model Output Statistics (MOS) approaches, which attempt to correct simulated variables. In summary, we found that most PP DS methods, with the exception of multisite analog methods and a method that explicitly models spatial dependence yield unrealistic spatial characteristics. Regional climate model-based MOS methods showed good performance with respect to correlation lengths and the joint occurrence of wet days, but a substantial overestimation of the joint occurrence of heavy precipitation events. These findings apply to the spatial scales that are resolved by our observation network, and similar studies with higher resolutions, which are relevant for small hydrological catchment, are desirable.},
	language = {en},
	number = {9},
	urldate = {2023-06-29},
	journal = {International Journal of Climatology},
	author = {Widmann, Martin and Bedia, Joaquin and Gutiérrez, José M. and Bosshard, Thomas and Hertig, Elke and Maraun, Douglas and Casado, María J. and Ramos, Petra and Cardoso, Rita M. and Soares, Pedro M. M. and Ribalaygua, Jamie and Pagé, Christian and Fischer, Andreas M. and Herrera, Sixto and Huth, Radan},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.6024},
	keywords = {bias adjustment, downscaling, model output statistics, perfect prognosis, regional climate, spatial variability, validation},
	pages = {3819--3845},
}

@article{trentini_novel_2023,
	title = {A {Novel} {Bias} {Correction} {Method} for {Extreme} {Events}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2225-1154},
	url = {https://www.mdpi.com/2225-1154/11/1/3},
	doi = {10.3390/cli11010003},
	abstract = {When one is using climate simulation outputs, one critical issue to consider is the systematic bias affecting the modelled data. The bias correction of modelled data is often used when one is using impact models to assess the effect of climate events on human activities. However, the efficacy of most of the currently available methods is reduced in the case of extreme events because of the limited number of data for these low probability and high impact events. In this study, a novel bias correction methodology is proposed, which corrects the bias of extreme events. To do so, we extended one of the most popular bias correction techniques, i.e., quantile mapping (QM), by improving the description of extremes through a generalised extreme value distribution (GEV) fitting. The technique was applied to the daily mean temperature and total precipitation data from three seasonal forecasting systems: SEAS5, System7 and GCFS2.1. The bias correction efficiency was tested over the Southern African Development Community (SADC) region, which includes 15 Southern African countries. The performance was verified by comparing each of the three models with a reference dataset, the ECMWF reanalysis ERA5. The results reveal that this novel technique significantly reduces the systematic biases in the forecasting models, yielding further improvements over the classic QM. For both the mean temperature and total precipitation, the bias correction produces a decrease in the Root Mean Squared Error (RMSE) and in the bias between the simulated and the reference data. After bias correcting the data, the ensemble forecasts members that correctly predict the temperature extreme increases. On the other hand, the number of members identifying precipitation extremes decreases after the bias correction.},
	language = {en},
	number = {1},
	urldate = {2023-06-29},
	journal = {Climate},
	author = {Trentini, Laura and Dal Gesso, Sara and Venturini, Marco and Guerrini, Federica and Calmanti, Sandro and Petitta, Marcello},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Africa, bias correction, climate services, extreme events, seasonal forecast},
	pages = {3},
}

@article{gronquist_deep_2021,
	title = {Deep learning for post-processing ensemble weather forecasts},
	volume = {379},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2020.0092},
	doi = {10.1098/rsta.2020.0092},
	abstract = {Quantifying uncertainty in weather forecasts is critical, especially for predicting extreme weather events. This is typically accomplished with ensemble prediction systems, which consist of many perturbed numerical weather simulations, or trajectories, run in parallel. These systems are associated with a high computational cost and often involve statistical post-processing steps to inexpensively improve their raw prediction qualities. We propose a mixed model that uses only a subset of the original weather trajectories combined with a post-processing step using deep neural networks. These enable the model to account for non-linear relationships that are not captured by current numerical models or post-processing methods. Applied to the global data, our mixed models achieve a relative improvement in ensemble forecast skill (CRPS) of over 14\%. Furthermore, we demonstrate that the improvement is larger for extreme weather events on select case studies. We also show that our post-processing can use fewer trajectories to achieve comparable results to the full ensemble. By using fewer trajectories, the computational costs of an ensemble prediction system can be reduced, allowing it to run at higher resolution and produce more accurate forecasts.

This article is part of the theme issue ‘Machine learning for weather and climate modelling’.},
	number = {2194},
	urldate = {2023-06-29},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Grönquist, Peter and Yao, Chengyuan and Ben-Nun, Tal and Dryden, Nikoli and Dueben, Peter and Li, Shigang and Hoefler, Torsten},
	month = feb,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {deep learning, ensemble post-processing, extreme weather events, weather uncertainty quantification},
	pages = {20200092},
}

@article{han_deep_2021,
	title = {A {Deep} {Learning} {Method} for {Bias} {Correction} of {ECMWF} 24–240 h {Forecasts}},
	volume = {38},
	issn = {1861-9533},
	url = {https://doi.org/10.1007/s00376-021-0215-y},
	doi = {10.1007/s00376-021-0215-y},
	abstract = {Correcting the forecast bias of numerical weather prediction models is important for severe weather warnings. The refined grid forecast requires direct correction on gridded forecast products, as opposed to correcting forecast data only at individual weather stations. In this study, a deep learning method called CU-net is proposed to correct the gridded forecasts of four weather variables from the European Centre for Medium-Range Weather Forecast Integrated Forecasting System global model (ECMWF-IFS): 2-m temperature, 2-m relative humidity, 10-m wind speed, and 10-m wind direction, with a forecast lead time of 24 h to 240 h in North China. First, the forecast correction problem is transformed into an image-to-image translation problem in deep learning under the CU-net architecture, which is based on convolutional neural networks. Second, the ECMWF-IFS forecasts and ECMWF reanalysis data (ERA5) from 2005 to 2018 are used as training, validation, and testing datasets. The predictors and labels (ground truth) of the model are created using the ECMWF-IFS and ERA5, respectively. Finally, the correction performance of CU-net is compared with a conventional method, anomaly numerical correction with observations (ANO). Results show that forecasts from CU-net have lower root mean square error, bias, mean absolute error, and higher correlation coefficient than those from ANO for all forecast lead times from 24 h to 240 h. CU-net improves upon the ECMWF-IFS forecast for all four weather variables in terms of the above evaluation metrics, whereas ANO improves upon ECMWF-IFS performance only for 2-m temperature and relative humidity. For the correction of the 10-m wind direction forecast, which is often difficult to achieve, CU-net also improves the correction performance.},
	language = {en},
	number = {9},
	urldate = {2023-06-29},
	journal = {Advances in Atmospheric Sciences},
	author = {Han, Lei and Chen, Mingxuan and Chen, Kangkai and Chen, Haonan and Zhang, Yanbiao and Lu, Bing and Song, Linye and Qin, Rui},
	month = sep,
	year = {2021},
	keywords = {ECMWF, Numerical weather prediction, bias correction, deep learning, 偏差订正, 数值天气预报, 深度学习},
	pages = {1444--1459},
}

@inproceedings{graubner_calibration_2022,
	title = {Calibration of {Large} {Neural} {Weather} {Models}},
	url = {https://www.climatechange.ai/papers/neurips2022/87},
	abstract = {Climate Change AI - NeurIPS 2022 Accepted Work},
	language = {en-US},
	urldate = {2023-06-29},
	booktitle = {Climate {Change} {AI}},
	publisher = {Climate Change AI},
	author = {Graubner, Andre and Kamyar Azizzadenesheli, Kamyar and Pathak, Jaideep and Mardani, Morteza and Pritchard, Mike and Kashinath, Karthik and Anandkumar, Anima},
	month = dec,
	year = {2022},
}

@misc{horat_deep_2023,
	title = {Deep learning for post-processing global probabilistic forecasts on sub-seasonal time scales},
	url = {http://arxiv.org/abs/2306.15956},
	abstract = {Sub-seasonal weather forecasts are becoming increasingly important for a range of socio-economic activities. However, the predictive ability of physical weather models is very limited on these time scales. We propose several post-processing methods based on convolutional neural networks to improve sub-seasonal forecasts by correcting systematic errors of numerical weather prediction models. Our post-processing models operate directly on spatial input fields and are therefore able to retain spatial relationships and to generate spatially homogeneous predictions. They produce global probabilistic tercile forecasts for biweekly aggregates of temperature and precipitation for weeks 3-4 and 5-6. In a case study based on a public forecasting challenge organized by the World Meteorological Organization, our post-processing models outperform recalibrated forecasts from the European Centre for Medium-Range Weather Forecasts (ECMWF), and achieve improvements over climatological forecasts for all considered variables and lead times. We compare several model architectures and training modes and demonstrate that all approaches lead to skillful and well-calibrated probabilistic forecasts. The good calibration of the post-processed forecasts emphasizes that our post-processing models reliably quantify the forecast uncertainty based on deterministic input information in form of the ECMWF ensemble mean forecast fields only.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Horat, Nina and Lerch, Sebastian},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15956 [physics]},
	keywords = {Physics - Atmospheric and Oceanic Physics},
}

@inproceedings{yang_improving_2023,
	title = {Improving {Seasonal} {Prediction} of {Summer} {Precipitation} in the {Middle}–{Lower} {Reaches} of the {Yangtze} {River} {Using} a {TU}-{Net} {Deep} {Learning} {Approach}},
	url = {https://www.semanticscholar.org/paper/Improving-Seasonal-Prediction-of-Summer-in-the-of-a-Yang-Ling/47b70cad0936bb6ff0cd24e47db34ea7f29d6201?utm_source=alert_email&utm_content=LibraryFolder&utm_campaign=AlertEmails_WEEKLY&utm_term=LibraryFolder&email_index=3-0-5&utm_medium=17770581},
	abstract = {The two-step U-Net model (TU-Net) contains a western North Pacific subtropical high (WNPSH) prediction model and a precipitation prediction model fed by the WNPSH predictions, oceanic heat content, and surface temperature. The data-driven forecast model provides improved 4-month lead predictions of the WNPSH and precipitation in the middle and lower reaches of the Yangtze River (MLYR), which has important implications for water resources management and precipitation-related disaster prevention in China. When compared with five state-of-the-art dynamical climate models including the Climate Forecast System of Nanjing University of Information Science and Technology (NUIST-CFS1.0) and four models participating in the North American Multi-Model Ensemble (NMME) project, the TU-Net produces comparable skills in forecasting 4-month lead geopotential height and winds at the 500and 850-hPa levels. For the 4-month lead prediction of precipitation over the MLYR region, the TU-Net has the best correlation scores and mean latitude-weighted RMSE in each summer month and in boreal summer [June–August (JJA)], and pattern correlation coefficient scores are slightly lower than the dynamical models only in June and JJA. In addition, the results show that the constructed TU-Net is also superior to most of the dynamical models in predicting 2-m air temperature in the MLYR region at a 4-month lead. Thus, the deep learning-based TU-Net model can provide a rapid and inexpensive way to improve the seasonal prediction of summer precipitation and 2-m air temperature over the MLYR region. SIGNIFICANCE STATEMENT: The purpose of this study is to examine the seasonal predictive skill of the western North Pacific subtropical high anomalies and summer rainfall anomalies over the middle and lower reaches of the Yangtze River region by means of deep learning methods. Our deep learning model provides a rapid and inexpensive way to improve the seasonal prediction of summer precipitation as well as 2-m air temperature. The work has important implications for water resources management and precipitation-related disaster prevention in China and can be extended in the future to predict other climate variables as well.},
	urldate = {2023-06-26},
	author = {Yang, Shu-Chih and Ling, Fenghua and Yue, L. and LUOa, JING-JIA},
	year = {2023},
}

@article{ravuri_skilful_2021,
	title = {Skilful precipitation nowcasting using deep generative models of radar: {Supplementary} {Information}},
	volume = {597},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03854-z},
	doi = {10.1038/s41586-021-03854-z},
	abstract = {Abstract
            
              Precipitation nowcasting, the high-resolution forecasting of precipitation up to two hours ahead, supports the real-world socioeconomic needs of many sectors reliant on weather-dependent decision-making
              1,2
              . State-of-the-art operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture important non-linear events such as convective initiations
              3,4
              . Recently introduced deep learning methods use radar to directly predict future rain rates, free of physical constraints
              5,6
              . While they accurately predict low-intensity rainfall, their operational utility is limited because their lack of constraints produces blurry nowcasts at longer lead times, yielding poor performance on rarer medium-to-heavy rain events. Here we present a deep generative model for the probabilistic nowcasting of precipitation from radar that addresses these challenges. Using statistical, economic and cognitive measures, we show that our method provides improved forecast quality, forecast consistency and forecast value. Our model produces realistic and spatiotemporally consistent predictions over regions up to 1,536 km × 1,280 km and with lead times from 5–90 min ahead. Using a systematic evaluation by more than 50 expert meteorologists, we show that our generative model ranked first for its accuracy and usefulness in 89\% of cases against two competitive methods. When verified quantitatively, these nowcasts are skillful without resorting to blurring. We show that generative nowcasting can provide probabilistic predictions that improve forecast value and support operational utility, and at resolutions and lead times where alternative methods struggle.},
	language = {en},
	number = {7878},
	urldate = {2023-06-21},
	journal = {Nature},
	author = {Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and Prudden, Rachel and Mandhane, Amol and Clark, Aidan and Brock, Andrew and Simonyan, Karen and Hadsell, Raia and Robinson, Niall and Clancy, Ellen and Arribas, Alberto and Mohamed, Shakir},
	month = sep,
	year = {2021},
	pages = {672--677},
}

@article{ravuri_skilful_2021-1,
	title = {Skilful precipitation nowcasting using deep generative models of radar},
	volume = {597},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03854-z},
	doi = {10.1038/s41586-021-03854-z},
	abstract = {Precipitation nowcasting, the high-resolution forecasting of precipitation up to two hours ahead, supports the real-world socioeconomic needs of many sectors reliant on weather-dependent decision-making1,2. State-of-the-art operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture important non-linear events such as convective initiations3,4. Recently introduced deep learning methods use radar to directly predict future rain rates, free of physical constraints5,6. While they accurately predict low-intensity rainfall, their operational utility is limited because their lack of constraints produces blurry nowcasts at longer lead times, yielding poor performance on rarer medium-to-heavy rain events. Here we present a deep generative model for the probabilistic nowcasting of precipitation from radar that addresses these challenges. Using statistical, economic and cognitive measures, we show that our method provides improved forecast quality, forecast consistency and forecast value. Our model produces realistic and spatiotemporally consistent predictions over regions up to 1,536 km × 1,280 km and with lead times from 5–90 min ahead. Using a systematic evaluation by more than 50 expert meteorologists, we show that our generative model ranked first for its accuracy and usefulness in 89\% of cases against two competitive methods. When verified quantitatively, these nowcasts are skillful without resorting to blurring. We show that generative nowcasting can provide probabilistic predictions that improve forecast value and support operational utility, and at resolutions and lead times where alternative methods struggle.},
	language = {en},
	number = {7878},
	urldate = {2023-06-23},
	journal = {Nature},
	author = {Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and Prudden, Rachel and Mandhane, Amol and Clark, Aidan and Brock, Andrew and Simonyan, Karen and Hadsell, Raia and Robinson, Niall and Clancy, Ellen and Arribas, Alberto and Mohamed, Shakir},
	month = sep,
	year = {2021},
	note = {Number: 7878
Publisher: Nature Publishing Group},
	keywords = {Computer science, Environmental sciences},
	pages = {672--677},
}

@article{mcgovern_review_2023,
	title = {A {Review} of {Machine} {Learning} for {Convective} {Weather}},
	volume = {-1},
	issn = {2769-7525},
	url = {https://journals.ametsoc.org/view/journals/aies/aop/AIES-D-22-0077.1/AIES-D-22-0077.1.xml},
	doi = {10.1175/AIES-D-22-0077.1},
	abstract = {Abstract We present an overviewof recentwork on using artificial intelligence/machine learning techniques for forecasting convective weather and its associated hazards, including tornadoes, hail, wind, and lightning. These high-impact phenomena globally cause both massive property damage and loss of life yet they are quite challenging to forecast. Given the recent explosion in developing machine learning techniques across the weather spectrum and the fact that the skillful prediction of convective weather has immediate societal benefits, we present a thorough review of the current state of the art in artificial intelligence and machine learning techniques for convective hazards. Our review includes both traditional approaches, including support vector machines and decision trees as well as deep learning approaches. We highlight the challenges in developing machine learning approaches to forecast these phenomena across a variety of spatial and temporal scales. We end with a discussion of promising areas of future work for ML for convective weather, including a discussion of the need to create trustworthy AI forecasts that can be used for forecasters in real-time and the need for active cross-sector collaboration on testbeds to validate machine learning methods in operational situations.},
	language = {EN},
	number = {aop},
	urldate = {2023-06-22},
	journal = {Artificial Intelligence for the Earth Systems},
	author = {McGovern, Amy and Chase, Randy J. and Flora, Montgomery and Gagne, David J. and Lagerquist, Ryan and Potvin, Corey K. and Snook, Nathan and Loken, Eric},
	month = may,
	year = {2023},
	note = {Publisher: American Meteorological Society
Section: Artificial Intelligence for the Earth Systems},
	pages = {1--61},
}

@article{holloway_precipitation_2012,
	title = {Precipitation distributions for explicit versus parametrized convection in a large-domain high-resolution tropical case study},
	volume = {138},
	copyright = {Copyright © 2012 Royal Meteorological Society},
	issn = {1477-870X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.1903},
	doi = {10.1002/qj.1903},
	abstract = {Global climate and weather models tend to produce rainfall that is too light and too regular over the tropical ocean. This is likely because of convective parametrizations, but the problem is not well understood. Here, distributions of precipitation rates are analyzed for high-resolution UK Met Office Unified Model simulations of a 10 day case study over a large tropical domain (∼20°S–20°N and 42°E–180°E). Simulations with 12 km grid length and parametrized convection have too many occurrences of light rain and too few of heavier rain when interpolated onto a 1° grid and compared with Tropical Rainfall Measuring Mission (TRMM) data. In fact, this version of the model appears to have a preferred scale of rainfall around 0.4 mm h−1 (10 mm day−1), unlike observations of tropical rainfall. On the other hand, 4 km grid length simulations with explicit convection produce distributions much more similar to TRMM observations. The apparent preferred scale at lighter rain rates seems to be a feature of the convective parametrization rather than the coarse resolution, as demonstrated by results from 12 km simulations with explicit convection and 40 km simulations with parametrized convection. In fact, coarser resolution models with explicit convection tend to have even more heavy rain than observed. Implications for models using convective parametrizations, including interactions of heating and moistening profiles with larger scales, are discussed. One important implication is that the explicit convection 4 km model has temperature and moisture tendencies that favour transitions in the convective regime. Also, the 12 km parametrized convection model produces a more stable temperature profile at its extreme high-precipitation range, which may reduce the chance of very heavy rainfall. Further study is needed to determine whether unrealistic precipitation distributions are due to some fundamental limitation of convective parametrizations or whether parametrizations can be improved, in order to better simulate these distributions. Copyright © 2012 Royal Meteorological Society},
	language = {en},
	number = {668},
	urldate = {2023-06-21},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Holloway, C. E. and Woolnough, S. J. and Lister, G. M. S.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.1903},
	keywords = {CRM, CSRM, GCM, TRMM, cascade, rainfall},
	pages = {1692--1708},
}

@article{gebremeskel_haile_droughts_2019,
	title = {Droughts in {East} {Africa}: {Causes}, impacts and resilience},
	volume = {193},
	issn = {0012-8252},
	shorttitle = {Droughts in {East} {Africa}},
	url = {https://www.sciencedirect.com/science/article/pii/S0012825218303519},
	doi = {10.1016/j.earscirev.2019.04.015},
	abstract = {East Africa (EA) has been the primary focus for various drought studies in recent years. However, a comprehensive analysis of droughts, including their evolution, complexity, social implications and people's vulnerability is currently lacking. Hence, there is a pressing need for an overview of drought studies in EA. Here, we present a state-of-the-art review of the causes and impacts of, and resilience to droughts in EA. Studies reveal that droughts tend to be more frequent, longer and more severe in the boreal spring and summer in EA, as the overall precipitation and water storage abruptly decline. A decrease in drought frequency is observed during the boreal autumn season (October–November). As these studies have only been analysed within the context of sparse and short-term regional climate data with very complex spatial and seasonal climate patterns, they are subject to uncertainties. The main causes for the changing pattern of droughts include climate variabilities and anthropogenic effects. Droughts have extensive impacts on human beings, environment, water resources and agriculture. Environmental rehabilitation involving the development of ecosystem services, biodiversity enhancement and soil and water conservation is found to be a suitable strategy to adapt to drought conditions. A better understanding of the causes and impacts of droughts, participatory management and community level actions are essential for building resilience to drought. Strong citizens–government–stakeholder cooperation is also valuable in monitoring and managing drought. The knowledge and insights gained from this review will help the countries in EA to build a drought-resilient society and will form a basis of information for other regions outside of EA.},
	language = {en},
	urldate = {2023-06-21},
	journal = {Earth-Science Reviews},
	author = {Gebremeskel Haile, Gebremedhin and Tang, Qiuhong and Sun, Siao and Huang, Zhongwei and Zhang, Xuejun and Liu, Xingcai},
	month = jun,
	year = {2019},
	keywords = {Anthropogenic activities, Climate variability, Drought, East Africa, Horn of Africa, Rainfall},
	pages = {146--161},
}

@techreport{watkiss_socio-economic_2021,
	title = {Socio-{Economic} {Benefits} of the {WISER} {Programme}. {Synthesis} of {Results}.},
	url = {https://www.metoffice.gov.uk/binaries/content/assets/metofficegovuk/pdf/business/international/wiser/wiser-seb-results_final-web.pdf},
	institution = {Met Office UK},
	author = {Watkiss, P and Cimato, F},
	month = sep,
	year = {2021},
}

@incollection{wilcox_chapter_2012,
	address = {Boston},
	series = {Statistical {Modeling} and {Decision} {Science}},
	title = {Chapter 3 - {Estimating} {Measures} of {Location} and {Scale}},
	isbn = {978-0-12-386983-8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123869838000032},
	language = {en},
	urldate = {2023-06-21},
	booktitle = {Introduction to {Robust} {Estimation} and {Hypothesis} {Testing} ({Third} {Edition})},
	publisher = {Academic Press},
	author = {Wilcox, Rand},
	editor = {Wilcox, Rand},
	month = jan,
	year = {2012},
	doi = {10.1016/B978-0-12-386983-8.00003-2},
	pages = {43--101},
}

@incollection{efron_6_1982,
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {6. {The} {Infinitesimal} {Jackknife}, the {Delta} {Method} and the {Influence} {Function}},
	isbn = {978-0-89871-179-0},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611970319.ch6},
	abstract = {In this section we show the connection between the jackknife and the bootstrap, using a simple picture. The picture suggests another version of the jackknife, Jaeckel's (1972) “infinitesimal jackknife”. The infinitesimal jackknife turns out to be exactly the same as the ordinary delta method, when the latter applies, and also the same as methods based on the influence function (Hampel (1974)). We begin with a brief discussion of resampling procedures, a generic name for all methods which evaluate 
θ
ˆ
𝜃
{\textasciicircum}
 at reweighted versions of the empirical probability distribution 
F
ˆ
𝐹
{\textasciicircum}
.},
	urldate = {2023-06-21},
	booktitle = {The {Jackknife}, the {Bootstrap} and {Other} {Resampling} {Plans}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Efron, Bradley},
	month = jan,
	year = {1982},
	doi = {10.1137/1.9781611970319.ch6},
	pages = {37--47},
}

@article{goodfellow_generative_2020,
	title = {Generative adversarial networks},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3422622},
	doi = {10.1145/3422622},
	abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the
              generative modeling
              problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
	language = {en},
	number = {11},
	urldate = {2023-06-20},
	journal = {Communications of the ACM},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = oct,
	year = {2020},
	pages = {139--144},
}

@inproceedings{arjovsky_wasserstein_2017,
	title = {Wasserstein {Generative} {Adversarial} {Networks}},
	url = {https://proceedings.mlr.press/v70/arjovsky17a.html},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {214--223},
}

@article{cavanaugh_probability_2015,
	title = {The probability distribution of intense daily precipitation},
	volume = {42},
	issn = {1944-8007},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/2015GL063238},
	doi = {10.1002/2015GL063238},
	abstract = {The probability tail structure of over 22,000 weather stations globally is examined in order to identify the physically and mathematically consistent distribution type for modeling the probability of intense daily precipitation and extremes. Results indicate that when aggregating data annually, most locations are to be considered heavy tailed with statistical significance. When aggregating data by season, it becomes evident that the thickness of the probability tail is related to the variability in precipitation causing events and thus that the fundamental cause of precipitation volatility is weather diversity. These results have both theoretical and practical implications for the modeling of high-frequency climate variability worldwide.},
	language = {en},
	number = {5},
	urldate = {2023-06-16},
	journal = {Geophysical Research Letters},
	author = {Cavanaugh, Nicholas R. and Gershunov, Alexander and Panorska, Anna K. and Kozubowski, Tomasz J.},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/2015GL063238},
	keywords = {Pareto, extreme, precipitation, probability, weather station},
	pages = {1560--1567},
}

@article{kallache_nonstationary_2011,
	title = {Nonstationary probabilistic downscaling of extreme precipitation},
	volume = {116},
	copyright = {Copyright 2011 by the American Geophysical Union.},
	issn = {2156-2202},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2010JD014892},
	doi = {10.1029/2010JD014892},
	abstract = {Reanalysis data and general circulation model outputs typically provide information at a coarse spatial resolution, which cannot directly be used for local impact studies. Downscaling methods have been developed to overcome this problem, and to obtain local-scale information from large-scale atmospheric variables. The deduction of local-scale extremes still is a challenge. Here a probabilistic downscaling approach is presented where the cumulative distribution functions (CDFs) of large- and local-scale extremes are linked by means of a transfer function. In this way, the CDF of the local-scale extremes is obtained for a projection period, and statistical characteristics, like return levels, are inferred. The input series are assumed to be distributed according to an extreme value distribution, the Generalized Pareto distribution (GPD). The GPD parameters are linked to further explanatory variables, hence defining a nonstationary model. The methodology (XCDF-t) results in a parametric CDF, which is as well a GPD. Realizations generated from this CDF provide confidence bands. The approach is applied to downscale National Centers for Environmental Prediction reanalysis precipitation in winter. Daily local precipitation at five stations in southern France is obtained. The calibration period 1951–1985 is used to infer precipitation over the validation period 1986–1999. The applicability of the approach is verified by using observations, quantile-quantile plots, and the continuous ranked probability score. The stationary XCDF-t approach shows good results and outperforms the nonparametric CDF-t approach or quantile mapping for some stations. The inclusion of covariate information improves results only sometimes; therefore, covariates have to be chosen with care.},
	language = {en},
	number = {D5},
	urldate = {2023-06-16},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Kallache, M. and Vrac, M. and Naveau, P. and Michelangeli, P.-A.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2010JD014892},
	keywords = {CDF, CDF-t, GPD, extremes, probabilistic downscaling, quantile mapping},
}

@article{deque_frequency_2007,
	series = {Extreme {Climatic} {Events}},
	title = {Frequency of precipitation and temperature extremes over {France} in an anthropogenic scenario: {Model} results and statistical correction according to observed values},
	volume = {57},
	issn = {0921-8181},
	shorttitle = {Frequency of precipitation and temperature extremes over {France} in an anthropogenic scenario},
	url = {https://www.sciencedirect.com/science/article/pii/S0921818106002748},
	doi = {10.1016/j.gloplacha.2006.11.030},
	abstract = {Météo-France atmospheric model ARPEGE/Climate has been used to simulate present climate (1961–1990) and a possible future climate (2071–2100) through two ensembles of three 30-year numerical experiments. In the scenario experiment, the greenhouse gas and aerosol concentrations are prescribed by the so-called SRES-A2 hypotheses, whereas the sea surface temperature and sea ice extent come from an earlier ocean–atmosphere coupled simulation. The model covers the whole globe, with a variable resolution reaching 50 to 60 km over France. Model responses on daily minimum and maximum temperature and precipitation are analyzed over France. The distribution of daily values is compared with observed data from the French climatological network. The extreme cold temperatures and summer heavy precipitations are underestimated by the model. A correction technique is proposed in order to adjust the simulated values according to the observed ones. This process is applied to both reference and scenario simulation. Synthetic indices of extreme events are calculated with corrected simulations. The number of heavy rain ({\textgreater}10 mm) days increases by one quarter in winter. The maximum length of summer dry episodes increases by one half in summer. The number of heat wave days is multiplied by 10. The response in precipitation is less when only the change in the mean is considered. Such a corrected simulation is useful to feed impact models which are sensitive to threshold values, but the correction does not reduce, and may enhance in some cases, the uncertainty about the climate projections. Using several models and scenarios is the appropriate technique to deal with uncertainty.},
	language = {en},
	number = {1},
	urldate = {2023-06-16},
	journal = {Global and Planetary Change},
	author = {Déqué, Michel},
	month = may,
	year = {2007},
	keywords = {extreme values, numerical simulation, regional climate, scenario},
	pages = {16--26},
}

@article{jakob_themesl_empirical-statistical_2011,
	title = {Empirical-statistical downscaling and error correction of daily precipitation from regional climate models},
	volume = {31},
	issn = {1097-0088},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.2168},
	doi = {10.1002/joc.2168},
	abstract = {Although regional climate models (RCMs) are powerful tools for describing regional and even smaller scale climate conditions, they still feature severe systematic errors. In order to provide optimized climate scenarios for climate change impact research, this study merges linear and nonlinear empirical-statistical downscaling techniques with bias correction methods and investigates their ability for reducing RCM error characteristics. An ensemble of seven empirical-statistical downscaling and error correction methods (DECMs) is applied to post-process daily precipitation sums of a high-resolution regional climate hindcast simulation over the Alpine region, their error characteristics are analysed and compared to the raw RCM results. Drastic reductions in error characteristics due to application of DECMs are demonstrated. Direct point-wise methods like quantile mapping and local intensity scaling as well as indirect spatial methods as nonlinear analogue methods yield systematic improvements in median, variance, frequency, intensity and extremes of daily precipitation. Multiple linear regression methods, even if optimized by predictor selection, transformation and randomization, exhibit significant shortcomings for modelling daily precipitation due to their linear framework. Comparing the well-performing methods to each other, quantile mapping shows the best performance, particularly at high quantiles, which is advantageous for applications related to extreme precipitation events. The improvements are obtained regardless of season and region, which indicates the potential transferability of these methods to other regions. Copyright © 2010 Royal Meteorological Society},
	language = {en},
	number = {10},
	urldate = {2023-06-16},
	journal = {International Journal of Climatology},
	author = {Jakob Themeßl, Matthias and Gobiet, Andreas and Leuprecht, Armin},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.2168},
	keywords = {Alpine region, bias correction, empirical statistical downscaling, error correction, precipitation modelling, regional climate modelling},
	pages = {1530--1544},
}

@incollection{maraun_references_2018,
	address = {Cambridge},
	title = {References},
	isbn = {978-1-107-06605-2},
	url = {https://www.cambridge.org/core/books/statistical-downscaling-and-bias-correction-for-climate-research/references/3F1AA4E0E7D019B0DF8A770FBFDFCDF4},
	urldate = {2023-06-16},
	booktitle = {Statistical {Downscaling} and {Bias} {Correction} for {Climate} {Research}},
	publisher = {Cambridge University Press},
	editor = {Maraun, Douglas and Widmann, Martin},
	year = {2018},
	doi = {10.1017/9781107588783.023},
	pages = {303--341},
}

@article{macleod_are_2021,
	title = {Are {Kenya} {Meteorological} {Department} heavy rainfall advisories useful for forecast-based early action and early preparedness for flooding?},
	volume = {21},
	issn = {1561-8633},
	url = {https://nhess.copernicus.org/articles/21/261/2021/},
	doi = {10.5194/nhess-21-261-2021},
	abstract = {Preparedness saves lives. Forecasts can help improve preparedness by triggering early actions as part of pre-defined protocols under the Forecast-based Financing (FbF) approach; however it is essential to understand the skill of a forecast before using it as a trigger. In order to support the development of early-action protocols over Kenya, we evaluate the 33 heavy rainfall advisories (HRAs) issued by the Kenya Meteorological Department (KMD) during 2015–2019.

 The majority of HRAs warn counties which subsequently receive heavy rainfall within the forecast window. We also find a significant improvement in the advisory ability to anticipate flood events over time, with particularly high levels of skill in recent years. For instance actions with a 2-week lifetime based on advisories issued in 2015 and 2016 would have failed to anticipate nearly all recorded flood events in that period, whilst actions in 2019 would have anticipated over 70 \% of the instances of flooding at the county level. When compared against the most significant flood events over the period which led to significant loss of life, all three such periods during 2018 and 2019 were preceded by HRAs, and in these cases the advisories accurately warned the specific counties for which significant impacts were recorded. By contrast none of the four significant flooding events in 2015–2017 were preceded by advisories. This step change in skill may be due to developing forecaster experience with synoptic patterns associated with extremes as well as access to new dynamical prediction tools that specifically address extreme event probability; for example, KMD access to the UK Met Office Global Hazard Map was introduced at the end of 2017.

 Overall we find that KMD HRAs effectively warn of heavy rainfall and flooding and can be a vital source of information for early preparedness. However a lack of spatial detail on flood impacts and broad probability ranges limit their utility for systematic FbF approaches. We conclude with suggestions for making the HRAs more useful for FbF and outline the developing approach to flood forecasting in Kenya.},
	language = {English},
	number = {1},
	urldate = {2023-06-15},
	journal = {Natural Hazards and Earth System Sciences},
	author = {MacLeod, David and Kilavi, Mary and Mwangi, Emmah and Ambani, Maurine and Osunga, Michael and Robbins, Joanne and Graham, Richard and Rowhani, Pedram and Todd, Martin C.},
	month = jan,
	year = {2021},
	note = {Publisher: Copernicus GmbH},
	pages = {261--277},
}

@article{camberlin_major_2018,
	title = {Major role of water bodies on diurnal precipitation regimes in {Eastern} {Africa}},
	volume = {38},
	issn = {1097-0088},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.5197},
	doi = {10.1002/joc.5197},
	abstract = {Mean diurnal rainfall regimes over Eastern Africa (also referred to as the Greater Horn of Africa) are studied based on 3-hourly data from the TRMM 3B42 data set, averaged over a 17-year period (1998–2014). The consistency with long-term mean raingauge data, available for partly independent periods, varies from good (Sudan, Ethiopia, Eritrea and Somalia) to very good (Kenya, Tanzania and Uganda). Over sea (Indian Ocean and Red Sea), the diurnal rainfall distribution is quite uniform; however, a morning peak dominates and there is evidence of offshore phase propagation south of the equator. Over land, both rainfall frequency and rainfall amounts show a dominant afternoon maximum (1500–1800 East African Time, i.e. GMT + 3). However, many inland regions show a delayed rainfall maximum (evening, night-time or morning). The evening to night-time maximum found over some land areas is associated with a phase propagation from areas showing an afternoon peak. This occurs west of high ground areas (Sudan and parts of the Great Lakes region) and in belts parallel to the seashores (Eritrea, northeastern Ethiopia, Somalia and eastern Kenya). The latter provide indirect evidence that sea breeze effects can be detected at unexpectedly great distances from the coast (up to 300–400 km) in parts of Eastern Africa. A remarkable ring of early afternoon (1500) maxima is found around most lakes, although some east–west asymmetries occur. Over the lakes, a morning or late night maximum is mostly found. It is generally inversely related to the distance to the shorelines for the larger lakes, but over the mid-size lakes it is replaced by or competes with a late afternoon to evening maximum.},
	language = {en},
	number = {2},
	urldate = {2023-06-15},
	journal = {International Journal of Climatology},
	author = {Camberlin, Pierre and Gitau, Wilson and Planchon, Olivier and Dubreuil, Vincent and Funatsu, Beatriz M. and Philippon, Nathalie},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.5197},
	keywords = {East Africa, Horn of Africa, Indian Ocean, Lake Victoria, diurnal cycle, lakes, rainfall, sea breeze},
	pages = {613--629},
}

@article{kimani_assessment_2017,
	title = {An {Assessment} of {Satellite}-{Derived} {Rainfall} {Products} {Relative} to {Ground} {Observations} over {East} {Africa}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/9/5/430},
	doi = {10.3390/rs9050430},
	abstract = {Accurate and consistent rainfall observations are vital for climatological studies in support of better agricultural and water management decision-making and planning. In East Africa, accurate rainfall estimation with an adequate spatial distribution is limited due to sparse rain gauge networks. Satellite rainfall products can potentially play a role in increasing the spatial coverage of rainfall estimates; however, their performance needs to be understood across space–time scales and factors relating to their errors. This study assesses the performance of seven satellite products: Tropical Applications of Meteorology using Satellite and ground-based observations (TAMSAT), African Rainfall Climatology And Time series (TARCAT), Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS), Tropical Rainfall Measuring Mission (TRMM-3B43), Climate Prediction Centre (CPC) Morphing technique (CMORPH), Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks Climate Data Record (PERSIANN-CDR), CPC Merged Analysis of Precipitation (CMAP), and Global Precipitation Climatology Project (GPCP), using locally developed gridded (0.05°) rainfall data for 15 years (1998–2012) over East Africa. The products’ assessments were done at monthly and yearly timescales and were remapped to the gridded rain gauge data spatial scale during the March to May (MAM) and October to December (OND) rainy seasons. A grid-based statistical comparison between the two datasets was used, but only pixel values located at the rainfall stations were considered for validation. Additionally, the impact of topography on the performance of the products was assessed by analyzing the pixels in areas of highest negative bias. All the products could substantially replicate rainfall patterns, but their differences are mainly based on retrieving high rainfall amounts, especially of localized orographic types. The products exhibited systematic errors, which decreased with an increase in temporal resolution from a monthly to yearly scale. Challenges in retrieving orographic rainfall, especially during the OND season, were identified as the main cause of high underestimations. Underestimation was observed when elevation was {\textless}2500 m and above this threshold; overestimation was evident in mountainous areas. CMORPH, CHIRPS, and TRMM showed consistently high performance during both seasons, and this was attributed to their ability to retrieve rainfall of different rainfall regimes.},
	language = {en},
	number = {5},
	urldate = {2023-06-13},
	journal = {Remote Sensing},
	author = {Kimani, Margaret Wambui and Hoedjes, Joost C. B. and Su, Zhongbo},
	month = may,
	year = {2017},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {East Africa, rain gauge, satellite-derived rainfall estimates, seasonality, topography},
	pages = {430},
}

@article{camberlin_intraseasonal_2019,
	title = {Intraseasonal to {Interannual} {Modulation} of {Diurnal} {Precipitation} {Distribution} {Over} {Eastern} {Africa}},
	volume = {124},
	copyright = {©2019. American Geophysical Union. All Rights Reserved.},
	issn = {2169-8996},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019JD031167},
	doi = {10.1029/2019JD031167},
	abstract = {The modulation of the rainfall diurnal cycle at intraseasonal and interannual time scales is examined using gridded rainfall data over Eastern Africa and the nearby Indian Ocean for the period 1998–2014. Our focus is on the October-December season which is strongly impacted by both the Madden-Julian Oscillation (MJO) and the Indian Ocean Dipole Mode (IODM). The effect of the MJO, which is not synchronized across the whole region, is mainly through changes in the amplitude of the diurnal cycle, especially an enhancement of the late afternoon peak in active phases of the MJO. The rainfall phase only shows minor variations between the active and quiescent phases of the MJO, except over the drier regions and the Indian Ocean where the overall diurnal rainfall signal is more uneven. The effect of the positive phase of the IODM (warm western Indian Ocean) on Eastern Africa rainfall is found at any time of the day, but the enhancement is more prominent during the wettest part of the day (generally the afternoon). Substantial changes in the diurnal phase of the rains are found in some land areas away from the coast or from mountain ranges, where the dominant afternoon peak during the negative IODM phase is replaced by a nighttime maximum during the positive phase. Cross-sections and a cluster analysis of diurnal rainfall patterns over Kenya and southern Somalia suggest that this feature may be associated with the development of longer-lived rainfall systems propagating inland from the coast during the IODM positive phase.},
	language = {en},
	number = {22},
	urldate = {2023-06-13},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Camberlin, Pierre and Gitau, Wilson and Kiladis, George and Bosire, Emily and Pohl, Benjamin},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2019JD031167},
	keywords = {Eastern Africa, Indian Ocean Dipole Mode, Madden-Julian Oscillation, diurnal cycle, precipitation, sea-surface temperature},
	pages = {11863--11886},
}

@article{kovachki_neural_nodate,
	title = {Neural {Operator}: {Learning} {Maps} {Between} {Function} {Spaces} {With} {Applications} to {PDEs}},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, lowrank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
	language = {en},
	author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew},
}

@article{dey_new_2016,
	title = {A new method for the characterization and verification of local spatial predictability for convective-scale ensembles},
	volume = {142},
	issn = {1477-870X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.2792},
	doi = {10.1002/qj.2792},
	abstract = {The use of kilometre-scale ensembles in operational weather forecasting provides new challenges for forecast interpretation and evaluation to account for uncertainty on the convective scale. A new neighbourhood-based method is presented for evaluating and characterizing the local predictability variations from convective-scale ensembles. Spatial scales over which ensemble forecasts agree (agreement scales, SA) are calculated at each grid point ij, providing a map of the spatial agreement between forecasts. By comparing the average agreement scale obtained from ensemble member pairs () with that between members and radar observations (), this approach allows the location-dependent spatial spread–skill relationship of the ensemble to be assessed. The properties of the agreement scales are demonstrated using an idealized experiment. To demonstrate the methods in an operational context, and are calculated for six convective cases run with the Met Office UK Ensemble Prediction System (MOGREPS-UK). highlights predictability differences between cases, which can be linked to physical processes. Maps of are found to summarize the spatial predictability in a compact and physically meaningful manner that is useful for forecasting and model interpretation. Comparison of and demonstrates the case-by-case and temporal variability of the spatial spread–skill, which can again be linked to physical processes.},
	language = {en},
	number = {698},
	urldate = {2023-06-07},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Dey, Seonaid R. A. and Roberts, Nigel M. and Plant, Robert S. and Migliorini, Stefano},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.2792},
	keywords = {convective-scale, ensemble, forecasting, neighbourhood, spatial, spread–skill, verification},
	pages = {1982--1996},
}

@book{wolter_introduction_2007,
	title = {Introduction to variance estimation},
	volume = {53},
	publisher = {Springer},
	author = {Wolter, Kirk M},
	year = {2007},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@article{finney_implications_2019,
	title = {Implications of {Improved} {Representation} of {Convection} for the {East} {Africa} {Water} {Budget} {Using} a {Convection}-{Permitting} {Model}},
	volume = {32},
	issn = {0894-8755, 1520-0442},
	url = {https://journals.ametsoc.org/view/journals/clim/32/7/jcli-d-18-0387.1.xml},
	doi = {10.1175/JCLI-D-18-0387.1},
	abstract = {Abstract The precipitation and diabatic heating resulting from moist convection make it a key component of the atmospheric water budget in the tropics. With convective parameterization being a known source of uncertainty in global models, convection-permitting (CP) models are increasingly being used to improve understanding of regional climate. Here, a new 10-yr CP simulation is used to study the characteristics of rainfall and atmospheric water budget for East Africa and the Lake Victoria basin. The explicit representation of convection leads to a widespread improvement in the intensities and diurnal cycle of rainfall when compared with a parameterized simulation. Differences in large-scale moisture fluxes lead to a shift in the mean rainfall pattern from the Congo to Lake Victoria basin in the CP simulation—highlighting the important connection between local changes in the representation of convection and larger-scale dynamics and rainfall. Stronger lake–land contrasts in buoyancy in the CP model lead to a stronger nocturnal land breeze over Lake Victoria, increasing evaporation and moisture flux convergence (MFC), and likely unrealistically high rainfall. However, for the mountains east of the lake, the CP model produces a diurnal rainfall cycle much more similar to satellite estimates, which is related to differences in the timing of MFC. Results here demonstrate that, while care is needed regarding lake forcings, a CP approach offers a more realistic representation of several rainfall characteristics through a more physically based realization of the atmospheric dynamics around the complex topography of East Africa.},
	language = {EN},
	number = {7},
	urldate = {2023-06-02},
	journal = {Journal of Climate},
	author = {Finney, Declan L. and Marsham, John H. and Jackson, Lawrence S. and Kendon, Elizabeth J. and Rowell, David P. and Boorman, Penelope M. and Keane, Richard J. and Stratton, Rachel A. and Senior, Catherine A.},
	month = apr,
	year = {2019},
	note = {Publisher: American Meteorological Society
Section: Journal of Climate},
	pages = {2109--2129},
}

@misc{yang_fourier_2023,
	title = {Fourier {Neural} {Operators} for {Arbitrary} {Resolution} {Climate} {Data} {Downscaling}},
	url = {http://arxiv.org/abs/2305.14452},
	abstract = {Climate simulations are essential in guiding our understanding of climate change and responding to its effects. However, it is computationally expensive to resolve complex climate processes at high spatial resolution. As one way to speed up climate simulations, neural networks have been used to downscale climate variables from fast-running low-resolution simulations, but high-resolution training data are often unobtainable or scarce, greatly limiting accuracy. In this work, we propose a downscaling method based on the Fourier neural operator. It trains with data of a small upsampling factor and then can zero-shot downscale its input to arbitrary unseen high resolution. Evaluated both on ERA5 climate model data and on the Navier-Stokes equation solution data, our downscaling model significantly outperforms state-of-the-art convolutional and generative adversarial downscaling models, both in standard single-resolution downscaling and in zero-shot generalization to higher upsampling factors. Furthermore, we show that our method also outperforms state-of-the-art data-driven partial differential equation solvers on Navier-Stokes equations. Overall, our work bridges the gap between simulation of a physical process and interpolation of low-resolution output, showing that it is possible to combine both approaches and significantly improve upon each other.},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Yang, Qidong and Hernandez-Garcia, Alex and Harder, Paula and Ramesh, Venkatesh and Sattegeri, Prasanna and Szwarcman, Daniela and Watson, Campbell D. and Rolnick, David},
	month = may,
	year = {2023},
	note = {arXiv:2305.14452 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
}

@article{patakchi_yousefi_deep_2023,
	title = {Deep learning of model- and reanalysis-based precipitation and pressure mismatches over {Europe}},
	volume = {5},
	issn = {2624-9375},
	url = {https://www.frontiersin.org/articles/10.3389/frwa.2023.1178114},
	abstract = {Physically based numerical weather prediction and climate models provide useful information for a large number of end users, such as flood forecasters, water resource managers, and farmers. However, due to model uncertainties arising from, e.g., initial value and model errors, the simulation results do not match the in situ or remotely sensed observations to arbitrary accuracy. Merging model-based data with observations yield promising results benefiting simultaneously from the information content of the model results and observations. Machine learning (ML) and/or deep learning (DL) methods have been shown to be useful tools in closing the gap between models and observations due to the capacity in the representation of the non-linear space–time correlation structure. This study focused on using UNet encoder–decoder convolutional neural networks (CNNs) for extracting spatiotemporal features from model simulations for predicting the actual mismatches (errors) between the simulation results and a reference data set. Here, the climate simulations over Europe from the Terrestrial Systems Modeling Platform (TSMP) were used as input to the CNN. The COSMO-REA6 reanalysis data were used as a reference. The proposed merging framework was applied to mismatches in precipitation and surface pressure representing more and less chaotic variables, respectively. The merged data show a strong average improvement in mean error ({\textasciitilde} 47\%), correlation coefficient ({\textasciitilde} 37\%), and root mean square error ({\textasciitilde}22\%). To highlight the performance of the DL-based method, the results were compared with the results obtained by a baseline method, quantile mapping. The proposed DL-based merging methodology can be used either during the simulation to correct model forecast output online or in a post-processing step, for downstream impact applications, such as flood forecasting, water resources management, and agriculture.},
	urldate = {2023-05-31},
	journal = {Frontiers in Water},
	author = {Patakchi Yousefi, Kaveh and Kollet, Stefan},
	year = {2023},
}

@misc{zanetta_physics-constrained_2023,
	title = {Physics-constrained deep learning postprocessing of temperature and humidity},
	url = {http://arxiv.org/abs/2212.04487},
	abstract = {Weather forecasting centers currently rely on statistical postprocessing methods to minimize forecast error. This improves skill but can lead to predictions that violate physical principles or disregard dependencies between variables, which can be problematic for downstream applications and for the trustworthiness of postprocessing models, especially when they are based on new machine learning approaches. Building on recent advances in physics-informed machine learning, we propose to achieve physical consistency in deep learning-based postprocessing models by integrating meteorological expertise in the form of analytic equations. Applied to the post-processing of surface weather in Switzerland, we find that constraining a neural network to enforce thermodynamic state equations yields physically-consistent predictions of temperature and humidity without compromising performance. Our approach is especially advantageous when data is scarce, and our findings suggest that incorporating domain expertise into postprocessing models allows to optimize weather forecast information while satisfying application-specific requirements.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Zanetta, Francesco and Nerini, Daniele and Beucler, Tom and Liniger, Mark A.},
	month = may,
	year = {2023},
	note = {arXiv:2212.04487 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics, Statistics - Applications},
}

@article{chantry_opportunities_2021,
	title = {Opportunities and challenges for machine learning in weather and climate modelling: {Hard}, medium and soft {AI}},
	volume = {379},
	issn = {1364503X},
	doi = {10.1098/rsta.2020.0083},
	abstract = {In September 2019, a workshop was held to highlight the growing area of applying machine learning techniques to improve weather and climate prediction. In this introductory piece, we outline the motivations, opportunities and challenges ahead in this exciting avenue of research. This article is part of the theme issue 'Machine learning for weather and climate modelling'.},
	number = {2194},
	journal = {Phil. Trans. T. Soc. A},
	author = {Chantry, Matthew and Christensen, Hannah and Dueben, Peter and Palmer, Tim},
	month = apr,
	year = {2021},
	pmid = {33583261},
	note = {Publisher: Royal Society Publishing},
	keywords = {climate modelling, machine learning, weather prediction},
}

@article{chantry_opportunities_2021-1,
	title = {Opportunities and challenges for machine learning in weather and climate modelling: {Hard}, medium and soft {AI}},
	volume = {379},
	issn = {1364503X},
	doi = {10.1098/rsta.2020.0083},
	abstract = {In September 2019, a workshop was held to highlight the growing area of applying machine learning techniques to improve weather and climate prediction. In this introductory piece, we outline the motivations, opportunities and challenges ahead in this exciting avenue of research. This article is part of the theme issue 'Machine learning for weather and climate modelling'.},
	number = {2194},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Chantry, Matthew and Christensen, Hannah and Dueben, Peter and Palmer, Tim},
	month = apr,
	year = {2021},
	pmid = {33583261},
	note = {Publisher: Royal Society Publishing},
	keywords = {climate modelling, machine learning, weather prediction},
}

@article{price_increasing_2022,
	title = {Increasing the accuracy and resolution of precipitation forecasts using deep generative models},
	volume = {151},
	doi = {https://doi.org/10.48550/arXiv.2203.12297},
	abstract = {Accurately forecasting extreme rainfall is notoriously difficult, but is also ever more crucial for society as climate change increases the frequency of such extremes. Global numerical weather prediction models often fail to capture extremes, and are produced at too low a resolution to be actionable, while regional, high-resolution models are hugely expensive both in computation and labour. In this paper we explore the use of deep generative models to simultaneously correct and downscale (super-resolve) global ensemble forecasts over the Continental US. Specifically, using fine-grained radar observations as our ground truth, we train a conditional Generative Adversarial Network-coined CorrectorGAN-via a custom training procedure and augmented loss function, to produce ensembles of high-resolution, bias-corrected forecasts based on coarse, global precipitation forecasts in addition to other relevant meteorological fields. Our model outperforms an interpolation baseline, as well as super-resolution-only and CNN-based uni-variate methods, and approaches the performance of an operational regional high-resolution model across an array of established probabilistic metrics. Crucially, Cor-rectorGAN, once trained, produces predictions in seconds on a single machine. These results raise exciting questions about the necessity of regional models, and whether data-driven downscaling and correction methods can be transferred to data-poor regions that},
	journal = {Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)},
	author = {Price, Ilan and Rasp, Stephan},
	year = {2022},
}

@article{schwartz_medium-range_2019,
	title = {Medium-range convection-allowing ensemble forecasts with a variable-resolution global model},
	volume = {147},
	issn = {15200493},
	doi = {10.1175/MWR-D-18-0452.1},
	abstract = {Two sets of global, 132-h (5.5-day), 10-member ensemble forecasts were produced with the Model for Prediction Across Scales (MPAS) for 35 cases in April and May 2017. One MPAS ensemble had a quasi-uniform 15-km mesh while the other employed a variable-resolution mesh with 3-km cell spacing over the conterminous United States (CONUS) that smoothly relaxed to 15 km over the rest of the globe. Precipitation forecasts from both MPAS ensembles were objectively verified over the central and eastern CONUS to assess the potential benefits of configuring MPAS with a 3-km mesh refinement region for medium-range forecasts. In addition, forecasts from NCEP’s operational Global Ensemble Forecast System were evaluated and served as a baseline against which to compare the experimental MPAS ensembles. The 3-km MPAS ensemble most faithfully reproduced the observed diurnal cycle of precipitation throughout the 132-h forecasts and had superior precipitation skill and reliability over the first 48 h. However, after 48 h, the three ensembles had more similar spread, reliability, and skill, and differences between probabilistic precipitation forecasts derived from the 3- and 15-km MPAS ensembles were typically statistically insignificant. Nonetheless, despite fewer benefits of increased resolution for spatial placement after 48 h, 3-km ensemble members explicitly provided potentially valuable guidance regarding convective mode throughout the 132-h forecasts while the other ensembles did not. Collectively, these results suggest both strengths and limitations of medium-range high-resolution ensemble forecasts and reveal pathways for future investigations to improve understanding of high-resolution global ensembles with variable-resolution meshes.},
	number = {8},
	journal = {Monthly Weather Review},
	author = {Schwartz, Craig S.},
	year = {2019},
	note = {Publisher: American Meteorological Society},
	pages = {2997--3023},
}

@article{cafaro_convection-permitting_2021,
	title = {Do convection-permitting ensembles lead to more skillful short-range probabilistic rainfall forecasts over tropical east africa?},
	volume = {36},
	issn = {15200434},
	doi = {10.1175/WAF-D-20-0172.1},
	abstract = {Convection-permitting ensemble prediction systems (CP-ENS) have been implemented in the midlatitudes for weather forecasting time scales over the past decade, enabled by the increase in computational resources. Recently, efforts are being made to study the benefits of CP-ENS for tropical regions. This study examines CP-ENS forecasts produced by the Met Office over tropical East Africa, for 24 cases in the period April–May 2019. The CP-ENS, an ensemble with parameterized convection (Glob-ENS), and their deterministic counterparts are evaluated against rainfall estimates derived from satellite observations (GPM-IMERG). The CP configurations have the best representation of the diurnal cycle, although heavy rainfall amounts are overestimated compared to observations. Pairwise comparisons between the different configurations reveal that the CP-ENS is generally the most skillful forecast for both 3-and 24-h accumulations of heavy rainfall (97th percentile), followed by the CP deterministic forecast. More precisely, probabilistic forecasts of heavy rainfall, verified using a neighborhood approach, show that the CP-ENS is skillful at scales greater than 100 km, significantly better than the Glob-ENS, although not as good as found in the midlatitudes. Skill decreases with lead time and varies diurnally, especially for CP forecasts. The CP-ENS is underspread both in terms of forecasting the locations of heavy rainfall and in terms of domain-averaged rainfall. This study demonstrates potential benefits in using CP-ENS for operational forecasting of heavy rainfall over tropical Africa and gives specific suggestions for further research and development, including probabilistic forecast guidance.},
	number = {2},
	journal = {Weather and Forecasting},
	author = {Cafaro, Carlo and Woodhams, Beth J. and Stein, Thorwald H.M. and Birch, Cathryn E. and Webster, Stuart and Bain, Caroline L. and Hartley, Andrew and Clarke, Samantha and Ferrett, Samantha and Hill, Peter},
	month = apr,
	year = {2021},
	note = {Publisher: American Meteorological Society},
	keywords = {Ensembles, Forecast verification/skill, Model comparison, Probabilistic Quantitative Precipitation Forecasting (PQPF)},
	pages = {697--716},
}

@article{slater_hybrid_2023,
	title = {Hybrid forecasting: blending climate predictions with {AI} models},
	volume = {27},
	issn = {1607-7938},
	url = {https://hess.copernicus.org/articles/27/1865/2023/},
	doi = {10.5194/hess-27-1865-2023},
	abstract = {{\textless}p{\textgreater}{\textless}![CDATA[Abstract. Hybrid hydroclimatic forecasting systems employ data-driven (statistical or machine learning) methods to harness and integrate a broad variety of predictions from dynamical, physics-based models – such as numerical weather prediction, climate, land, hydrology, and Earth system models – into a final prediction product. They are recognized as a promising way of enhancing the prediction skill of meteorological and hydroclimatic variables and events, including rainfall, temperature, streamflow, floods, droughts, tropical cyclones, or atmospheric rivers. Hybrid forecasting methods are now receiving growing attention due to advances in weather and climate prediction systems at subseasonal to decadal scales, a better appreciation of the strengths of AI, and expanding access to computational resources and methods. Such systems are attractive because they may avoid the need to run a computationally expensive offline land model, can minimize the effect of biases that exist within dynamical outputs, benefit from the strengths of machine learning, and can learn from large datasets, while combining different sources of predictability with varying time horizons. Here we review recent developments in hybrid hydroclimatic forecasting and outline key challenges and opportunities for further research. These include obtaining physically explainable results, assimilating human influences from novel data sources, integrating new ensemble techniques to improve predictive skill, creating seamless prediction schemes that merge short to long lead times, incorporating initial land surface and ocean/ice conditions, acknowledging spatial variability in landscape and atmospheric forcing, and increasing the operational uptake of hybrid prediction schemes.]]{\textgreater}{\textless}/p{\textgreater}},
	number = {9},
	journal = {Hydrology and Earth System Sciences},
	author = {Slater, Louise J. and Arnal, Louise and Boucher, Marie-Amélie and Chang, Annie Y.-Y. and Moulds, Simon and Murphy, Conor and Nearing, Grey and Shalev, Guy and Shen, Chaopeng and Speight, Linda and Villarini, Gabriele and Wilby, Robert L. and Wood, Andrew and Zappa, Massimiliano},
	month = may,
	year = {2023},
	pages = {1865--1889},
}

@article{pulkkinen_pysteps_2019,
	title = {Pysteps: an open-source {Python} library for probabilistic precipitation nowcasting (v1.0)},
	volume = {12},
	issn = {1991-9603},
	doi = {10.5194/gmd-12-4185-2019},
	abstract = {{\textless}p{\textgreater}Abstract. Pysteps is an open-source and community-driven Python library for probabilistic precipitation nowcasting, that is, very-short-range forecasting (0–6 h). The aim of pysteps is to serve two different needs. The first is to provide a modular and well-documented framework for researchers interested in developing new methods for nowcasting and stochastic space–time simulation of precipitation. The second aim is to offer a highly configurable and easily accessible platform for practitioners ranging from weather forecasters to hydrologists. In this sense, pysteps has the potential to become an important component for integrated early warning systems for severe weather. The pysteps library supports various input/output file formats and implements several optical flow methods as well as advanced stochastic generators to produce ensemble nowcasts. In addition, it includes tools for visualizing and post-processing the nowcasts and methods for deterministic, probabilistic and neighborhood forecast verification. The pysteps library is described and its potential is demonstrated using radar composite images from Finland, Switzerland, the United States and Australia. Finally, scientific experiments are carried out to help the reader to understand the pysteps framework and sensitivity to model parameters.{\textless}/p{\textgreater}},
	number = {10},
	journal = {Geoscientific Model Development},
	author = {Pulkkinen, Seppo and Nerini, Daniele and Pérez Hortal, Andrés A. and Velasco-Forero, Carlos and Seed, Alan and Germann, Urs and Foresti, Loris},
	month = oct,
	year = {2019},
	pages = {4185--4219},
}

@article{cannon_bias_2015,
	title = {Bias correction of {GCM} precipitation by quantile mapping: {How} well do methods preserve changes in quantiles and extremes?},
	volume = {28},
	issn = {08948755},
	doi = {10.1175/JCLI-D-14-00754.1},
	abstract = {Quantile mapping bias correction algorithms are commonly used to correct systematic distributional biases in precipitation outputs from climate models. Although they are effective at removing historical biases relative to observations, it has been found that quantile mapping can artificially corrupt future model-projected trends. Previous studies on the modification of precipitation trends by quantile mapping have focused on mean quantities, with less attention paid to extremes. This article investigates the extent to which quantile mapping algorithms modify global climate model (GCM) trends in mean precipitation and precipitation extremes indices. First, a bias correction algorithm, quantile delta mapping (QDM), that explicitly preserves relative changes in precipitation quantiles is presented. QDM is compared on synthetic data with detrended quantile mapping (DQM), which is designed to preserve trends in the mean, and with standard quantile mapping (QM). Next, methods are applied to phase 5 of the Coupled Model Intercomparison Project (CMIP5) daily precipitation projections over Canada. Performance is assessed based on precipitation extremes indices and results from a generalized extreme value analysis applied to annual precipitation maxima. QM can inflate the magnitude of relative trends in precipitation extremes with respect to the raw GCM, often substantially, as compared to DQM and especially QDM. The degree of corruption in the GCM trends by QM is particularly large for changes in long period return values. By the 2080s, relative changes in excess of +500\% with respect to historical conditions are noted at some locations for 20-yr return values, with maximum changes by DQM and QDM nearing +240\% and +140\%, respectively, whereas raw GCM changes are never projected to exceed +120\%.},
	number = {17},
	journal = {Journal of Climate},
	author = {Cannon, Alex J. and Sobie, Stephen R. and Murdock, Trevor Q.},
	year = {2015},
	note = {Publisher: American Meteorological Society},
	keywords = {Bias, Climate models, Extreme events, Precipitation, Statistical techniques, Trends},
	pages = {6938--6959},
}

@article{wilks_comparison_2006,
	title = {Comparison of ensemble-{MOS} methods in the {Lorenz} '96 setting},
	volume = {13},
	issn = {14698080},
	doi = {10.1017/S1350482706002192},
	abstract = {A suite of methods that have been proposed for statistical post-processing of ensemble forecasts based on historical verification data (i.e. ensemble-MOS methods) are compared with each other, and with direct probability estimates using ensemble relative frequencies, in the idealised Lorenz '96 setting. The three most promising methods are logistic regressions predicting probabilities associated with selected quantiles, ensemble dressing (a kernel density estimation approach), and linear regressions with non-constant prediction errors that depend on the ensemble variance.},
	number = {3},
	journal = {Meteorological Applications},
	author = {Wilks, D. S.},
	year = {2006},
	note = {Publisher: Cambridge University Press},
	keywords = {Ensemble dressing, Ensemble forecasting, Logistic regression},
	pages = {243--256},
}

@techreport{mudelsee_estimating_2003,
	title = {Estimating {Pearson}'s {Correlation} {Coefficient} {With} {Bootstrap} {Confidence} {Interval} {From} {Serially} {Dependent} {Time} {Series} 1},
	abstract = {Pearson's correlation coefficient, r xy , is often used when measuring the influence of one time-dependent variable on another in bivariate climate time series. Thereby, positive serial dependence (persis-tence) and unknown data distributions impose a challenge for obtaining accurate confidence intervals for r xy. This is met by the presented approach, employing the nonparametric stationary bootstrap with an average block length proportional to the maximum estimated persistence time of the data. A Monte Carlo experiment reveals that this method can produce accurate (in terms of coverage) confidence intervals (of type bias-corrected and accelerated). However, since persistence reduces the number of independent observations, substantially more data points are required for achieving an accuracy comparable to a situation without persistence. The experiment further shows that neglecting serial dependence may lead to serious coverage errors. The presented method proves robust with respect to distributional shape (lognormal/normal) and time spacing (uneven/even). The method is used to confirm that a previous finding of a correlation between solar activity and Indian Ocean monsoon strength in early Holocene is valid. A further result is that the correlation between sunspot number and cosmogenic 10 Be concentration vanishes after approximately 1870.},
	author = {Mudelsee, Manfred},
	year = {2003},
	note = {Publication Title: Mathematical Geology
Volume: 35
Issue: 6},
	keywords = {BCa method, Monte Carlo simulation, irregular sampling interval, persistence, sun-climate relationship},
}

@article{bechtold_simulation_2004,
	title = {The simulation of the diurnal cycle of convective precipitation over land in a global model},
	volume = {130 C},
	issn = {00359009},
	doi = {10.1256/qj.03.103},
	abstract = {In the context of the European Cloud Systems project, the problem of the simulation of the diurnal cycle of convective precipitation over land is addressed with the aid of cloud-resolving (CRM) and single-column (SCM) model simulations of an idealized midlatitude case for which observations of large-scale and surface forcing are available. The CRM results are compared to different versions of the European Centre for Medium-Range Weather Forecasts (ECMWF) convection schemes using different convective trigger procedures and convective closures. In the CRM, maximum rainfall intensity occurs at 15 h (local time). In this idealized midlatitude case, most schemes do not reproduce the afternoon precipitation peak, as (i) they cannot reproduce the gradual growth (typically over 3 hours) of the deep convective cloud layer and (ii) they produce a diurnal cycle of precipitation that is in phase with the diurnal cycle of the convective available potential energy (CAPE) and the convective inhibition (CIN), consistent with the parcel theory and CAPE closure used in the bulk mass-flux scheme. The scheme that links the triggering to the large-scale vertical velocity gets the maximum precipitation at the right time, but this may be artificial as the vertical velocity is enforced in the single-column context. The study is then extended to the global scale using ensembles of 72-hour global forecasts at resolution T511 (40 km), and long-range single 40-day forecasts at resolution T159 (125 km) with the ECMWF general-circulation model. The focus is on tropical South America and Africa where the diurnal cycle is most pronounced. The forecasts are evaluated against analyses and observed radiosonde data, as well as observed surface and satellite-derived rainfall rates. The ECMWF model version with improved convective trigger produces the smallest biases overall. It also shifts the rainfall maximum to 12 h compared to 9.5 h in the original version. In contrast to the SCM, the vertical-velocity-dependent trigger does not further improve the phase of the diurnal cycle. However, further work is necessary to match the observed 15 h precipitation peak. © Royal Meteorological Society, 2004.},
	number = {604},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Bechtold, Peter and Chaboureau, J. P. and Beljaars, A. and Betts, A. K. and Köhler, M. and Miller, M. and Redelsperger, J. L.},
	month = oct,
	year = {2004},
	keywords = {General-circulation models},
	pages = {3119--3137},
}

@article{maraun_statistical_2019,
	title = {Statistical downscaling skill under present climate conditions: {A} synthesis of the {VALUE} perfect predictor experiment},
	volume = {39},
	issn = {10970088},
	doi = {10.1002/joc.5877},
	abstract = {VALUE is a network that developed a framework to evaluate statistical downscaling methods including model output statistics such as simple bias correction and quantile mapping; perfect prognosis methods such as regression models and analog methods; and weather generators. The first experiment addresses the downscaling performance in present climate with perfect predictors. This paper presents a synthesis of the VALUE special issue, with a focus on the results of this first experiment. This paper presents a synthesis of the results. Model output statistics performs mostly well, but requires predictors at a resolution close to the target one. Perfect prog performance depends crucially on model structure and predictor choice. Weather generators perform in principle well for all aspects that can be expressed by the available model structure. Inter-annual variability is underrepresented by both perfect prog and weather generator approaches. Spatial variability is poorly represented by almost all participating methods (inherited by model output statistics from the driving model, not represented by the perfect prog and weather generator methods). Further studies are required to systematically assess (a) the role of predictor choice for perfect prog; (b) the performance of spatial weather generators, to study the performance based on GCM predictors; (c) downscaling skill in simulated future climates; and (d) the credibility of simulated predictors in a future climate.},
	number = {9},
	journal = {International Journal of Climatology},
	author = {Maraun, Douglas and Widmann, Martin and Gutiérrez, José M.},
	month = jul,
	year = {2019},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {bias correction, evaluation, regional climate, statistical downscaling, validation, ★},
	pages = {3692--3703},
}

@article{feng_global_2021,
	title = {A {Global} {High}-{Resolution} {Mesoscale} {Convective} {System} {Database} {Using} {Satellite}-{Derived} {Cloud} {Tops}, {Surface} {Precipitation}, and {Tracking}},
	volume = {126},
	issn = {21698996},
	doi = {10.1029/2020JD034202},
	abstract = {A new methodology is developed to construct a global (60°S–60°N) long-term (2000–2019) high-resolution (∼10-km h) mesoscale convective system (MCS) database by tracking MCS jointly using geostationary satellite infrared brightness temperature (Tb) and precipitation feature (PF) characteristics from the Integrated Multi-satellitE Retrievals for GPM precipitation data sets. Independent validation shows that the satellite-based MCS data set is able to reproduce important MCS statistics derived from ground-based radar network observations in the United States and China. We show that by carefully considering key PF characteristics in addition to Tb signatures, the new method significantly improves upon previous Tb-only methods in detecting MCSs in the midlatitudes for all seasons. Results show that MCSs account for over 50\% of annual total rainfall across most of the tropical belt and in selected regions of the midlatitudes, with a strong seasonality over many regions of the globe. The tracking database allows Lagrangian aspects such as MCS lifetime and translational speed and direction to be analyzed. The longest-lived MCSs preferentially occur over the subtropical oceans. The land MCSs have higher cloud-tops associated with more intense convection, and oceanic MCSs have much higher rainfall production. While MCSs are observed in many regions of the globe, there are fundamental differences in their dynamic and thermodynamic structures that warrant a better understanding of processes that control their evolution. This global database provides significant opportunities for observational and modeling studies of MCSs, their characteristics, and roles in regional and global water and energy cycles, as well as their hydrologic and other impacts.},
	number = {8},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Feng, Zhe and Leung, L. Ruby and Liu, Nana and Wang, Jingyu and Houze, Robert A. and Li, Jianfeng and Hardin, Joseph C. and Chen, Dandan and Guo, Jianping},
	month = apr,
	year = {2021},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {convective clouds, global climatology, mesoscale convection, precipitation, satellite observations, storm tracking},
}

@article{glawion_spategan_nodate,
	title = {{spateGAN}: {Spatio}-{Temporal} {Downscaling} of {Rainfall} {Fields} using {acGAN} {Approach}},
	doi = {https://doi.org/10.22541/essoar.167690003.33629126/v1},
	author = {Glawion, Luca},
}

@article{ben_bouallegue_spatial_2014,
	title = {Spatial techniques applied to precipitation ensemble forecasts: {From} verification results to probabilistic products},
	volume = {21},
	issn = {14698080},
	doi = {10.1002/met.1435},
	abstract = {Spatial techniques have been developed to quantify the performance of a system beyond the classical point-to-point comparison with observations. Including spatial neighbourhood information in the verification process, the quality of a forecast can be better characterized. Guidance for the interpretation of deterministic forecasts can also be delivered. This paper investigates the application of spatial techniques to ensemble forecasts. The aim is to assess ensemble forecast skills better and to provide improved guidance to the forecasters in the form of refined probabilistic products. Two spatial techniques are applied to precipitation forecasts derived from an ensemble system at the convective scale (COSMO-DE-EPS). The first technique is a smoothing method which enlarges the ensemble sample size by neighbouring forecasts. The resulting forecasts are called fuzzy probabilistic forecasts. The second method is an upscaling procedure which modifies the reference area of the probabilities. Fuzzy and upscaled probabilistic forecasts are assessed over a 3month period covering summer2011. The impact of smoothing and upscaling is investigated for a range of neighbourhood sizes and spatial scales respectively. Based on the verification results, recommendations are drawn how to use these techniques in optimally presenting COSMO-DE-EPS probabilistic products to forecasters who issue weather warnings.},
	number = {4},
	journal = {Meteorological Applications},
	author = {Ben Bouallègue, Zied and Theis, Susanne E.},
	month = oct,
	year = {2014},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Convection-permitting, Guidance, Neighbourhood, Predictability},
	pages = {922--929},
}

@article{wang_image_2004,
	title = {Image quality assessment: {From} error visibility to structural similarity},
	volume = {13},
	issn = {10577149},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
	month = apr,
	year = {2004},
	pmid = {15376593},
	keywords = {Error sensitivity, Human visual system (HVS), Image coding, Image quality assessment, JPEG, JPEG2000, Perceptual quality, Structural information, Structural similarity (SSIM)},
	pages = {600--612},
}

@inproceedings{ebert_fuzzy_2008,
	title = {Fuzzy verification of high-resolution gridded forecasts: {A} review and proposed framework},
	volume = {15},
	doi = {10.1002/met.25},
	abstract = {High-resolution forecasts from numerical models can look quite realistic and provide the forecaster with very useful guidance. However, when verified using traditional metrics they often score quite poorly because of the difficulty of predicting an exact match to the observations at high resolution. 'Fuzzy' verification rewards closeness by relaxing the requirement for exact matches between forecasts and observations. The key to the fuzzy approach is the use of a spatial window or neighbourhood surrounding the forecast and/or observed points. The treatment of the data within the window may include averaging (upscaling), thresholding, or generation of a PDF, depending on the particular fuzzy method used and its implicit decision model concerning what makes a good forecast. The size of the neighbourhood can be varied to provide verification results at multiple scales, thus allowing the user to determine at which scales the forecast has useful skill. This article describes a framework for fuzzy verification that incorporates several fuzzy verification methods. It is demonstrated on a high-resolution precipitation forecast from the United Kingdom (UK) and the results interpreted to show the additional information that can be gleaned from this approach. Copyright © 2008 Royal Meteorological Society.},
	booktitle = {Meteorological {Applications}},
	publisher = {John Wiley and Sons Ltd},
	author = {Ebert, Elizabeth E.},
	year = {2008},
	note = {Issue: 1
ISSN: 14698080},
	keywords = {Fuzzy, High-resolution forecast, Verification},
	pages = {51--64},
}

@article{wang_mean_2009,
	title = {Mean squared error: {Lot} it or leave it? {A} new look at signal fidelity measures},
	volume = {26},
	issn = {10535888},
	doi = {10.1109/MSP.2008.930649},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Wang, Zhou and Bovik, Alan C.},
	year = {2009},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Distortion, Distortion measurement, Noise, PSNR, Pixel, Pollution measurement, Visualization},
	pages = {98--117},
}

@article{dosselmann_comprehensive_2011,
	title = {A comprehensive assessment of the structural similarity index},
	volume = {5},
	issn = {18631711},
	doi = {10.1007/s11760-009-0144-1},
	abstract = {In recent years the structural similarity index has become an accepted standard among image quality metrics. Made up of three components, this technique assesses the visual impact of changes in image luminance, contrast, and structure. Applications of the index include image enhancement, video quality monitoring, and image encoding. As its status continues to rise, however, so do questions about its performance. In this paper, it is shown, both empirically and analytically, that the index is directly related to the conventional, and often unreliable, mean squared error. In the first evaluation, the two metrics are statistically compared with one another. Then, in the second, a pair of functions that algebraically connects the two is derived. These results suggest a much closer relationship between the structural similarity index and mean squared error. © 2009 Springer-Verlag London Limited.},
	number = {1},
	journal = {Signal, Image and Video Processing},
	author = {Dosselmann, Richard and Yang, Xue Dong},
	month = mar,
	year = {2011},
	note = {Publisher: Springer London},
	keywords = {Image quality metric, MSE, Mean squared error, SSIM, Structural similarity index},
	pages = {81--91},
}

@article{brunet_mathematical_2012,
	title = {On the mathematical properties of the structural similarity index},
	volume = {21},
	issn = {10577149},
	doi = {10.1109/TIP.2011.2173206},
	abstract = {Since its introduction in 2004, the structural similarity (SSIM) index has gained widespread popularity as a tool to assess the quality of images and to evaluate the performance of image processing algorithms and systems. There has been also a growing interest of using SSIM as an objective function in optimization problems in a variety of image processing applications. One major issue that could strongly impede the progress of such efforts is the lack of understanding of the mathematical properties of the SSIM measure. For example, some highly desirable properties such as convexity and triangular inequality that are possessed by the mean squared error may not hold. In this paper, we first construct a series of normalized and generalized (vector-valued) metrics based on the important ingredients of SSIM. We then show that such modified measures are valid distance metrics and have many useful properties, among which the most significant ones include quasi-convexity, a region of convexity around the minimizer, and distance preservation under orthogonal or unitary transformations. The groundwork laid here extends the potentials of SSIM in both theoretical development and practical applications. © 2011 IEEE.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Brunet, Dominique and Vrscay, Edward R. and Wang, Zhou},
	month = apr,
	year = {2012},
	pmid = {22042163},
	keywords = {Cone metrics, normalized metrics, perceptually optimized algorithms and methods, quality metrics and assessment tools, quasi-convexity and convexity, structural similarity (SSIM) index},
	pages = {1488--1495},
}

@article{taillardat_calibrated_2016,
	title = {Calibrated ensemble forecasts using quantile regression forests and ensemble model output statistics},
	volume = {144},
	issn = {15200493},
	doi = {10.1175/MWR-D-15-0260.1},
	abstract = {Ensembles used for probabilistic weather forecasting tend to be biased and underdispersive. This paper proposes a statistical method for postprocessing ensembles based on quantile regression forests (QRF), a generalization of random forests for quantile regression. This method does not fit a parametric probability density function (PDF) like in ensemble model output statistics (EMOS) but provides an estimation of desired quantiles. This is a nonparametric approach that eliminates any assumption on the variable subject to calibration. This method can estimate quantiles using not only members of the ensemble but any predictor available including statistics on other variables. The method is applied to the Météo-France 35-member ensemble forecast (PEARP) for surface temperature and wind speed for available lead times from 3 up to 54 h and compared to EMOS. All postprocessed ensembles are much better calibrated than the PEARP raw ensemble and experiments on real data also show that QRF performs better than EMOS, and can bring a real gain for human forecasters compared to EMOS. QRF provides sharp and reliable probabilistic forecasts. At last, classical scoring rules to verify predictive forecasts are completed by the introduction of entropy as a general measure of reliability.},
	number = {6},
	journal = {Monthly Weather Review},
	author = {Taillardat, Maxime and Mestre, Olivier and Zamo, Michaël and Naveau, Philippe},
	month = jun,
	year = {2016},
	note = {Publisher: American Meteorological Society},
	pages = {2375--2393},
}

@article{feng_global_2021-1,
	title = {A {Global} {High}-{Resolution} {Mesoscale} {Convective} {System} {Database} {Using} {Satellite}-{Derived} {Cloud} {Tops}, {Surface} {Precipitation}, and {Tracking}},
	volume = {126},
	issn = {21698996},
	doi = {10.1029/2020JD034202},
	abstract = {A new methodology is developed to construct a global (60°S–60°N) long-term (2000–2019) high-resolution (∼10-km h) mesoscale convective system (MCS) database by tracking MCS jointly using geostationary satellite infrared brightness temperature (Tb) and precipitation feature (PF) characteristics from the Integrated Multi-satellitE Retrievals for GPM precipitation data sets. Independent validation shows that the satellite-based MCS data set is able to reproduce important MCS statistics derived from ground-based radar network observations in the United States and China. We show that by carefully considering key PF characteristics in addition to Tb signatures, the new method significantly improves upon previous Tb-only methods in detecting MCSs in the midlatitudes for all seasons. Results show that MCSs account for over 50\% of annual total rainfall across most of the tropical belt and in selected regions of the midlatitudes, with a strong seasonality over many regions of the globe. The tracking database allows Lagrangian aspects such as MCS lifetime and translational speed and direction to be analyzed. The longest-lived MCSs preferentially occur over the subtropical oceans. The land MCSs have higher cloud-tops associated with more intense convection, and oceanic MCSs have much higher rainfall production. While MCSs are observed in many regions of the globe, there are fundamental differences in their dynamic and thermodynamic structures that warrant a better understanding of processes that control their evolution. This global database provides significant opportunities for observational and modeling studies of MCSs, their characteristics, and roles in regional and global water and energy cycles, as well as their hydrologic and other impacts.},
	number = {8},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Feng, Zhe and Leung, L. Ruby and Liu, Nana and Wang, Jingyu and Houze, Robert A. and Li, Jianfeng and Hardin, Joseph C. and Chen, Dandan and Guo, Jianping},
	month = apr,
	year = {2021},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {convective clouds, global climatology, mesoscale convection, precipitation, satellite observations, storm tracking},
}

@incollection{christensen_parametrization_2022,
	title = {Parametrization in {Weather} and {Climate} {Models}},
	booktitle = {Oxford {Research} {Encyclopedia} of {Climate} {Science}},
	publisher = {Oxford University Press},
	author = {Christensen, Hannah and Zanna, Laure},
	month = dec,
	year = {2022},
	doi = {10.1093/acrefore/9780190228620.013.826},
}

@techreport{bechtold_convection_2008,
	title = {Convection {Parametrization}},
	author = {Bechtold, Peter},
	year = {2008},
	note = {Publication Title: ECMWF Seminar on Parametrization of Subgrid Physical Processes},
	pages = {1--4},
}

@techreport{bechtold_convection_2008-1,
	title = {Convection {Parametrization}},
	author = {Bechtold, Peter},
	year = {2008},
	note = {Publication Title: ECMWF Seminar on Parametrization of Subgrid Physical Processes},
	pages = {1--4},
}

@incollection{christensen_parametrization_2022-1,
	title = {Parametrization in {Weather} and {Climate} {Models}},
	booktitle = {Oxford {Research} {Encyclopedia} of {Climate} {Science}},
	publisher = {Oxford University Press},
	author = {Christensen, Hannah and Zanna, Laure},
	month = dec,
	year = {2022},
	doi = {10.1093/acrefore/9780190228620.013.826},
}

@article{casati_scale-separation_2022,
	title = {Scale-separation diagnostics and the {Symmetric} {Bounded} {Efficiency} for the inter-comparison of precipitation reanalyses},
	issn = {10970088},
	doi = {10.1002/joc.7975},
	abstract = {The ERA5 global reanalysis has been compared against a high-resolution regional reanalysis (COSMO-REA6) by means of scale-separation diagnostics based on 2d Haar discrete wavelet transforms. The presented method builds upon existing methods and enables the assessment of bias, error and skill for individual spatial scales, separately. A new skill score (evaluated against random chance) and the Symmetric Bounded Efficiency are introduced. These are compared to the Nash-Sutcliffe and the Kling-Gupta Efficiencies, evaluated on different scales, and the benefits of symmetric statistics are illustrated. As expected, the wavelet statistics show that the coarser resolution ERA5 products underestimate small-to-medium scale precipitation compared to COSMO-REA6. The newly introduced skill score shows that the ERA5 control member (EA-HRES), despite its higher variability, exhibits better skill in representing small-to-medium scales with respect to the smoother ensemble members. The Symmetric Bounded Efficiency is suitable for the inter-comparison of reanalyses, since it is invariant with respect to the order of comparison.},
	journal = {International Journal of Climatology},
	author = {Casati, Barbara and Lussana, Cristian and Crespi, Alice},
	month = apr,
	year = {2022},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {COSMO-REA6, ERA5, precipitation, scale separation, spatial verification, wavelet},
}

@article{duc_spatial-temporal_2013,
	title = {Spatial-temporal fractions verification for high-resolution ensemble forecasts},
	volume = {65},
	issn = {16000870},
	doi = {10.3402/tellusa.v65i0.18171},
	abstract = {Experiments with two ensemble systems of resolutions 10 km (MF10km) and 2 km (MF2km) were designed to examine the value of cloud-resolving ensemble forecast in predicting precipitation on small spatio-temporal scales. Since the verification was performed on short-term precipitation at high resolution, uncertainties from small-scale processes caused the traditional verification methods to be inconsistent with the subjective evaluation. An extended verification method based on the Fractions Skill Score (FSS) was introduced to account for these uncertainties. The main idea is to extend the concept of spatial neighbourhood in FSS to the time and ensemble dimension. The extension was carried out by recognising that even if ensemble forecast is used, small-scale variability still exists in forecasts and influences verification results. In addition to FSS, the neighbourhood concept was also incorporated into reliability diagrams and relative operating characteristics to verify the reliability and resolution of two systems. The extension of FSS in time dimension demonstrates the important role of temporal scales in short-term precipitation verification at small spatial scales. The extension of FSS in ensemble space is called the ensemble FSS, which is a good representative of FSS for ensemble forecast in comparison with the FSS of ensemble mean. The verification results show that MF2km outperforms MF10km in heavy rain forecasts. In contrast, MF10km was slightly better than MF2km in predicting light rains, suggesting that the horizontal resolution of 2 km is not necessarily enough to completely resolve convective cells. © 2013 Le Duc et al.},
	journal = {Tellus, Series A: Dynamic Meteorology and Oceanography},
	author = {Duc, Le and Saito, Kazuo and Seko, Hiromu},
	year = {2013},
	note = {Publisher: Co-Action Publishing},
	keywords = {Fractions skill score, Intensity-scale diagram, Reliability, Resolution, Small-scale variability},
}

@article{jiang_tsit_2020,
	title = {{TSIT}: {A} {Simple} and {Versatile} {Framework} for {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/2007.12072},
	abstract = {We introduce a simple and versatile framework for image-to-image translation. We unearth the importance of normalization layers, and provide a carefully designed two-stream generative model with newly proposed feature transformations in a coarse-to-fine fashion. This allows multi-scale semantic structure information and style representation to be effectively captured and fused by the network, permitting our method to scale to various tasks in both unsupervised and supervised settings. No additional constraints (e.g., cycle consistency) are needed, contributing to a very clean and simple method. Multi-modal image synthesis with arbitrary style control is made possible. A systematic study compares the proposed method with several state-of-the-art task-specific baselines, verifying its effectiveness in both perceptual quality and quantitative evaluations.},
	author = {Jiang, Liming and Zhang, Changxu and Huang, Mingyang and Liu, Chunxiao and Shi, Jianping and Loy, Chen Change},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.12072},
}

@article{ebert-uphoff_cira_2021,
	title = {{CIRA} {Guide} to {Custom} {Loss} {Functions} for {Neural} {Networks} in {Environmental} {Sciences} -- {Version} 1},
	url = {http://arxiv.org/abs/2106.09757},
	abstract = {Neural networks are increasingly used in environmental science applications. Furthermore, neural network models are trained by minimizing a loss function, and it is crucial to choose the loss function very carefully for environmental science applications, as it determines what exactly is being optimized. Standard loss functions do not cover all the needs of the environmental sciences, which makes it important for scientists to be able to develop their own custom loss functions so that they can implement many of the classic performance measures already developed in environmental science, including measures developed for spatial model verification. However, there are very few resources available that cover the basics of custom loss function development comprehensively, and to the best of our knowledge none that focus on the needs of environmental scientists. This document seeks to fill this gap by providing a guide on how to write custom loss functions targeted toward environmental science applications. Topics include the basics of writing custom loss functions, common pitfalls, functions to use in loss functions, examples such as fractions skill score as loss function, how to incorporate physical constraints, discrete and soft discretization, and concepts such as focal, robust, and adaptive loss. While examples are currently provided in this guide for Python with Keras and the TensorFlow backend, the basic concepts also apply to other environments, such as Python with PyTorch. Similarly, while the sample loss functions provided here are from meteorology, these are just examples of how to create custom loss functions. Other fields in the environmental sciences have very similar needs for custom loss functions, e.g., for evaluating spatial forecasts effectively, and the concepts discussed here can be applied there as well. All code samples are provided in a GitHub repository.},
	author = {Ebert-Uphoff, Imme and Lagerquist, Ryan and Hilburn, Kyle and Lee, Yoonjin and Haynes, Katherine and Stock, Jason and Kumler, Christina and Stewart, Jebb Q.},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09757},
	keywords = {★},
}

@article{authors_gcrf_nodate,
	title = {{GCRF} {African} {SWIFT} and {ForPAc} {SHEAR} {White} {Paper} on the {Potential} of {Operational} {Weather} {Prediction} to {Save} {Lives} and {Improve} {Livelihoods} and {Economies} in {Sub}-{Saharan} {Africa}},
	url = {https://doi.org/10.5518/100/79},
	doi = {10.5518/100/79},
	abstract = {White Rose Research Online URL for this paper: https://eprints.whiterose.ac.uk/181045/ Version: Published Version Monograph: Youds, LH orcid.org/0000-0003-0302-3307, Parker, DJ orcid.org/0000-0003-2335-8198, Adefisan, EA et al. (15 more authors) (2021) GCRF African SWIFT and ForPAc SHEAR White Paper on the Potential of Operational Weather Prediction to Save Lives and Improve Livelihoods and Economies in Sub-Saharan Africa. Report. University of Leeds https://doi.org/10.5518/100/79 eprints@whiterose.ac.uk https://eprints.whiterose.ac.uk/ Reuse This article is distributed under the terms of the Creative Commons Attribution (CC BY) licence. This licence allows you to distribute, remix, tweak, and build upon the work, even commercially, as long as you credit the authors for the original work. More information and the full terms of the licence here: https://creativecommons.org/licenses/ Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request.},
	author = {Authors, Lead and Youds, Lorraine H and Parker Co-authors, Douglas J and Adefisan, Elijah A and Antwi-Agyei, Philip and Bain, Caroline L and L Black, Emily C and Blyth, Alan M and Dougill, Andrew J and Hirons, Linda C and Indasi, Victor S and Lamptey, Benjamin L and Marshall, Fionne and Marsham, John H and M Stein, Thorwald H and Taylor, Christopher M and Todd, Martin C and Visman, Emma L and Woolnough, Steven J},
	keywords = {★},
}

@inproceedings{kim_precipitation_2020,
	title = {Precipitation {Nowcasting} {Using} {Grid}-based {Data} in {South} {Korea} {Region}},
	volume = {2020-November},
	isbn = {978-1-72819-012-9},
	doi = {10.1109/ICDMW51313.2020.00099},
	abstract = {Recently, precipitation nowcasting has gained significant attention. For instance, the demand for precise precipitation nowcasting is significantly increasing in South Korea since the economic damage has been severe in recent days because of frequent and unexpected heavy rainfall. In this paper, we propose a U-Net based deep learning model that predicts from a numerical model and then corrects the data using the U-Net based deep learning model so that it can improve the accuracy of the final prediction. We use two data sets: reanalysis data and LDAPS(Local Data Assimilation and Prediction System) prediction data. Both data sets are grid-based data that covers the whole South Korea region. We first experiment with reanalysis data to identify that our U-Net model can find atmospheric dynamics patterns, even if it is not image data. Next, we use LDAPS prediction data and apply it to the U-Net model. Because LDAPS prediction data is also a prediction, we essentially conduct correcting task for this data. To this aim, a learnable layer is added at the front of the U-Net model and concatenated with the input batch to learn location-specific information. The experiment shows that the U-Net based model can find patterns using reanalysis data. Further, it has the potential to improve the accuracy of LDAPS prediction data. We also find that the learnable layer enhances test accuracy.},
	booktitle = {{IEEE} {International} {Conference} on {Data} {Mining} {Workshops}, {ICDMW}},
	publisher = {IEEE Computer Society},
	author = {Kim, Chang Hwan and Yun, Se Young},
	month = nov,
	year = {2020},
	note = {ISSN: 23759259},
	keywords = {Deep Learning, LDAPS, Numerical Model Correction, Precipitation Nowcasting, U-Net},
	pages = {701--706},
}

@article{jeong_correcting_2023,
	title = {Correcting rainfall forecasts of a numerical weather prediction model using generative adversarial networks},
	volume = {79},
	issn = {15730484},
	doi = {10.1007/s11227-022-04686-y},
	abstract = {In recent years, the use of deep learning techniques to forecast the weather has increased significantly; however, existing machine learning methods based on observed data are only suitable for very short-term forecasting. Numerical models are more stable for short- and medium-term forecasting, but the results may deviate from the observed data. This study proposes a deep learning method to improve the performance of numerical weather prediction models. In this method, the transformation relationship between the output of the numerical model and the observed data is learned by a generative adversarial network, which is then used to correct the forecasts of the numerical model. Experiments on 9 months of paired numerical model data and observed radar data demonstrate that correction of the forecast data using this method improves prediction performance, especially of heavy rainfall events. The proposed method provides a practical approach to combining conventional numerical weather prediction with data-driven deep learning models.},
	number = {2},
	journal = {Journal of Supercomputing},
	author = {Jeong, Chang Hoo and Yi, Mun Yong},
	month = feb,
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Forecasts correction, Generative adversarial network, Numerical weather prediction, Weather research and forecasting model, ★},
	pages = {1289--1317},
}

@article{jankov_partition_2021,
	title = {Partition of {Forecast} {Error} into {Positional} and {Structural} {Components}},
	volume = {38},
	issn = {18619533},
	doi = {10.1007/s00376-021-0251-7},
	abstract = {Weather manifests in spatiotemporally coherent structures. Weather forecasts hence are affected by both positional and structural or amplitude errors. This has been long recognized by practicing forecasters (cf., e.g., Tropical Cyclone track and intensity errors). Despite the emergence in recent decades of various objective methods for the diagnosis of positional forecast errors, most routine verification or statistical post-processing methods implicitly assume that forecasts have no positional error. The Forecast Error Decomposition (FED) method proposed in this study uses the Field Alignment technique which aligns a gridded forecast with its verifying analysis field. The total error is then partitioned into three orthogonal components: (a) large scale positional, (b) large scale structural, and (c) small scale error variance. The use of FED is demonstrated over a month-long MSLP data set. As expected, positional errors are often characterized by dipole patterns related to the displacement of features, while structural errors appear with single extrema, indicative of magnitude problems. The most important result of this study is that over the test period, more than 50\% of the total mean sea level pressure forecast error variance is associated with large scale positional error. The importance of positional error in forecasts of other variables and over different time periods remain to be explored.},
	number = {6},
	journal = {Advances in Atmospheric Sciences},
	author = {Jankov, Isidora and Gregory, Scott and Ravela, Sai and Toth, Zoltan and Peña, Malaquías},
	month = jun,
	year = {2021},
	note = {Publisher: Science Press},
	keywords = {forecast error, orthogonal decomposition, positional, structural},
	pages = {1012--1019},
}

@techreport{goos_lncs_nodate,
	title = {{LNCS} 3723 - {Analysis} and {Modelling} of {Faces} and {Gestures}},
	author = {Goos, Gerhard and Hartmanis, Juris and Van, Jan and Board, Leeuwen Editorial and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Zurich, Eth and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard},
	note = {Publication Title: Lecture Notes in},
}

@article{zick_quantifying_2020,
	title = {Quantifying extreme precipitation forecasting skill in high-resolution models using spatial patterns: {A} case study of the 2016 and 2018 ellicott city floods},
	volume = {11},
	issn = {20734433},
	doi = {10.3390/atmos11020136},
	abstract = {Recent historic floods in Ellicott City, MD, on 30 July 2016 and 27 May 2018 provide stark examples of the types of floods that are expected to become more frequent due to urbanization and climate change. Given the profound impacts associated with flood disasters, it is crucial to evaluate the capability of state-of-the-art weather models in predicting these hydrometeorological events. This study utilizes an object-based approach to evaluate short range ({\textless}12 h) hourly forecast precipitation from the High-Resolution Rapid Refresh (HRRR) versus observations from the National Centers for Environmental Prediction (NCEP) Stage IV precipitation analysis. For both datasets, a binary precipitation field is delineated using thresholds that span trace to extreme precipitation rates. Next, spatial metrics of area, perimeter, solidity, elongation, and fragmentation, as well as centroid positions for the forecast and observed fields are calculated. A Mann-Whitney fi-test reveals biases (using a confidence level of 90\%) related to the spatial attributes and locations of model forecast precipitation. Results indicate that traditional pixel-based precipitation verification metrics are limited in their ability to quantify and characterize model skill. In contrast, an object-based methodology offers encouraging results in that the HRRR can skillfully predict the extreme precipitation rates that are anticipated with anthropogenic climate change. Yet, there is still room for improvement, since model forecasts of extreme convective rainfall tend to be slightly too numerous and fragmented compared with observations. Lastly, results are sensitive to the HRRR model's representation of synoptic-scale and mesoscale processes. Therefore, detailed surface analyses and an "ingredients-based" approach should remain central to the process of forecasting excessive rainfall.},
	number = {2},
	journal = {Atmosphere},
	author = {Zick, Stephanie E.},
	month = feb,
	year = {2020},
	note = {Publisher: MDPI AG},
	keywords = {Extreme flood, Hydromereorology, Precipitation verification, Spatial analysis},
}

@article{chen_application_2018,
	title = {Application of {Contiguous} {Rain} {Area} ({CRA}) {Methods} to {Tropical} {Cyclone} {Rainfall} {Forecast} {Verification}},
	volume = {5},
	issn = {23335084},
	doi = {10.1029/2018EA000412},
	abstract = {This study demonstrates the useful information that can be derived from contiguous rain area (CRA) evaluation, such as systematic errors in tropical cyclone (TC) rainfall location and components of rainfall error due to incorrect predictions of location, rain volume, and rain pattern. CRA verification uses pattern matching techniques to determine the location error, as well as errors in area, mean and maximum intensity, and spatial pattern. In this study, CRA verification was applied to evaluate Australian Community Climate and Earth System Simulator (ACCESS)-TC, the TC version of ACCESS, daily rainfall forecasts over 15 TCs in the north west Pacific ocean during 2012–2013, by comparing with Tropical Rainfall Measuring Mission (TRMM) 3B42 satellite estimates. The results showed that pattern error was the major contributor to the total TC rainfall forecast error, followed by volume and displacement. ACCESS-TC forecasts tended to predict more rainfall closer to the TC center compared to Tropical Rainfall Measuring Mission (TRMM) 3B42 estimates. This bias occurred for different CRA rainfall thresholds, verification grid resolutions and forecast lead times. Furthermore, rain event verification showed that for short lead time (24 hr) forecasts, overestimation of rain volume was a major problem for ACCESS-TC forecasts, while displacement error was more significant in longer lead time (72 hr) forecasts. Finally, we compared empirical probability distribution functions and radial probability distributions of rainfall in the forecasts and observations to further characterise the rain volume error. This confirmed that ACCESS-TC tended to produce more extreme rain in the locations closer to the TC center (eyewall).},
	number = {11},
	journal = {Earth and Space Science},
	author = {Chen, Yingjun and Ebert, Elizabeth E. and Davidson, Noel E. and Walsh, Kevin J.E.},
	month = nov,
	year = {2018},
	note = {Publisher: Wiley-Blackwell Publishing Ltd},
	keywords = {evaluation, hurricane, model forecast, precipitation, spatial verification, typhoon},
	pages = {736--752},
}

@article{zschenderlein_application_2019,
	title = {Application of an object-based verification method to ensemble forecasts of 10-m wind gusts during winter storms},
	volume = {28},
	issn = {16101227},
	doi = {10.1127/metz/2019/0880},
	abstract = {The object-based method SAL (Structure, Amplitude and Location) was adapted for investigating the errors of forecasts of extreme 10-m wind gusts associated with winter storms in Germany. It has been applied to a statistically downscaled version of the 51 member ECMWF (European Centre for Medium Range Weather Forecasts) operational ensemble forecast. The horizontal resolution of both downscaled data and of the German weather service's operational analysis data used for verification is 7 km. Forecast errors are subdivided in terms of storm intensity, location and extent. After identifying a set of storm events, objects of moderate and intense 10-m wind gusts were identified with a local percentile-based threshold (90th percentile for moderate and 98th percentile for intense gust objects). Depending on the intensity of the storm, the gust objects differ in terms of size, shape and intensity. The characteristics of the ensemble forecasts of 10-m wind gusts can basically be assessed in two different ways. Individual forecast members can be evaluated with respect to the location, intensity and extent of the gust field, and then address the ensemble characteristics by the score distributions. Alternatively, the gust fields' location, intensity and extent can be evaluated by directly using the ensemble mean forecast instead of the individual members. The results of the identified set of storms clearly indicate a high case-to-case variability in the predictability of 10-m wind gusts objects, particularly when focusing on the structure of intense wind gust objects. It is found, that the gust fields' location and overall intensity can be better estimated from the ensemble mean forecast, compared to the individual forecast members. From a forecaster's perspective this means, that a storms' location and intensity can be well estimated by considering the ensemble mean wind forecasts. Considering the structure of the gust objects, results are different. While for longer lead times, there also seems to be a benefit from applying ensemble averaging, at short lead times the ensemble mean forecast performs equally or worse than most of the individual forecast members. The amplitude error is often the smallest component of the three error types. The findings are particularly relevant when deriving warning information, by giving guidance to forecasters when interpreting ensemble forecasts for severe storms.},
	number = {3},
	journal = {Meteorologische Zeitschrift},
	author = {Zschenderlein, Philipp and Pardowitz, Tobias and Ulbrich, Uwe},
	year = {2019},
	note = {Publisher: Gebruder Borntraeger Verlagsbuchhandlung},
	keywords = {Ensemble forecasts, Object-based verification, Spatial verification, Winter storms},
	pages = {203--213},
}

@techreport{davis_object-based_2006,
	title = {Object-{Based} {Verification} of {Precipitation} {Forecasts}. {Part} {I}: {Methodology} and {Application} to {Mesoscale} {Rain} {Areas}},
	abstract = {A recently developed method of defining rain areas for the purpose of verifying precipitation produced by numerical weather prediction models is described. Precipitation objects are defined in both forecasts and observations based on a convolution (smoothing) and thresholding procedure. In an application of the new verification approach, the forecasts produced by the Weather Research and Forecasting (WRF) model are evaluated on a 22-km grid covering the continental United States during July-August 2001. Observed rainfall is derived from the stage-IV product from NCEP on a 4-km grid (averaged to a 22-km grid). It is found that the WRF produces too many large rain areas, and the spatial and temporal distribution of the rain areas reveals regional underestimates of the diurnal cycle in rain-area occurrence frequency. Objects in the two datasets are then matched according to the separation distance of their centroids. Overall, WRF rain errors exhibit no large biases in location, but do suffer from a positive size bias that maximizes during the later afternoon. This coincides with an excessive narrowing of the rainfall intensity range, consistent with the dominance of parameterized convection. Finally, matching ability has a strong dependence on object size and is interpreted as the influence of relatively predictable synoptic-scale systems on the larger areas.},
	author = {Davis, Christopher and Brown, Barbara and Bullock, Randy},
	year = {2006},
	keywords = {★},
}

@techreport{davis_object-based_2006-1,
	title = {Object-{Based} {Verification} of {Precipitation} {Forecasts}. {Part} {II}: {Application} to {Convective} {Rain} {Systems}},
	abstract = {The authors develop and apply an algorithm to define coherent areas of precipitation, emphasizing mesoscale convection, and compare properties of these areas with observations obtained from NCEP stage-IV precipitation analyses (gauge and radar combined). In Part II, fully explicit 12-36-h forecasts of rainfall from the Weather Research and Forecasting model (WRF) are evaluated. These forecasts are integrated on a 4-km mesh without a cumulus parameterization. Rain areas are defined similarly to Part I, but emphasize more intense, smaller areas. Furthermore, a time-matching algorithm is devised to group spatially and temporally coherent areas into rain systems that approximate mesoscale convective systems. In general, the WRF model produces too many rain areas with length scales of 80 km or greater. Rain systems typically last too long, and are forecast to occur 1-2 h later than observed. The intensity distribution among rain systems in the 4-km forecasts is generally too broad, especially in the late afternoon, in sharp contrast to the intensity distribution obtained on a coarser grid with parameterized convection in Part I. The model exhibits the largest positive size and intensity bias associated with systems over the Midwest and Mississippi Valley regions, but little size bias over the High Plains, Ohio Valley, and the southeast United States. For rain systems lasting 6 h or more, the critical success index for matching forecast and observed rain systems agrees closely with that obtained in a related study using manually determined rain systems.},
	author = {Davis, Christopher and Brown, Barbara and Bullock, Randy},
	year = {2006},
	keywords = {★},
}

@article{marzban_three_2009,
	title = {Three spatial verification techniques: {Cluster} analysis, variogram, and optical flow},
	volume = {24},
	issn = {08828156},
	doi = {10.1175/2009WAF2222261.1},
	abstract = {Three spatial verification techniques are applied to three datasets. The datasets consist of a mixture of real and artificial forecasts, and corresponding observations, designed to aid in better understanding the effects of global (i.e., across the entire field) displacement and intensity errors. The three verification techniques, each based on well-known statistical methods, have little in common and, so, present different facets of forecast quality. It is shown that a verification method based on cluster analysis can identify "objects" in a forecast and an observation field, thereby allowing for object-oriented verification in the sense that it considers displacement, missed forecasts, and false alarms. A second method compares the observed and forecast fields, not in terms of the objects within them, but in terms of the covariance structure of the fields, as summarized by their variogram. The last method addresses the agreement between the two fields by inferring the function that maps one to the other. The map-generally called optical flow-provides a (visual) summary of the "difference" between the two fields. A further summary measure of that map is found to yield useful information on the distortion error in the forecasts. © 2009 American Meteorological Society.},
	number = {6},
	journal = {Weather and Forecasting},
	author = {Marzban, Caren and Sandgathe, Scott and Lyons, Hilary and Lederer, Nicholas},
	month = dec,
	year = {2009},
	keywords = {★},
	pages = {1457--1471},
}

@article{wernli_sal_2008,
	title = {{SAL} - {A} novel quality measure for the verification of quantitative precipitation forecasts},
	volume = {136},
	issn = {00270644},
	doi = {10.1175/2008MWR2415.1},
	abstract = {A novel object-based quality measure, which contains three distinct components that consider aspects of the structure (S), amplitude (A), and location (L) of the precipitation field in a prespecified domain (e.g., a river catchment) is introduced for the verification of quantitative precipitation forecasts (QPF). This quality measure is referred to as SAL. The amplitude component A measures the relative deviation of the domain-averaged QPF from observations. Positive values of A indicate an overestimation of total precipitation; negative values indicate an underestimation. For the components S and L, coherent precipitation objects are separately identified in the forecast and observations; however, no matching is performed of the objects in the two datasets. The location component L combines information about the displacement of the predicted (compared to the observed) precipitation field's center of mass and about the error in the weighted-average distance of the precipitation objects from the total field's center of mass. The structure component S is constructed in such a way that positive values occur if precipitation objects are too large and/or too flat, and negative values if the objects are too small and/or too peaked. Perfect QPFs are characterized by zero values for all components of SAL. Examples with both synthetic precipitation fields and real data are shown to illustrate the concept and characteristics of SAL. SAL is applied to 4 yr of daily accumulated QPFs from a global and finer-scale regional model for a German river catchment, and the SAL diagram is introduced as a compact means of visualizing the results. SAL reveals meaningful information about the systematic differences in the performance of the two models. While the median of the S component is close to zero for the regional model, it is strongly positive for the coarser-scale global model. Consideration is given to the strengths and limitations of the novel quality measure and to possible future applications, in particular, for the verification of QPFs from convection-resolving weather prediction models on short time scales. © 2008 American Meteorological Society.},
	number = {11},
	journal = {Monthly Weather Review},
	author = {Wernli, Heini and Paulat, Marcus and Hagen, Martin and Frei, Christoph},
	year = {2008},
	pages = {4470--4487},
}

@article{zick_anatomy_2022,
	title = {Anatomy of a storm: {A} review of shape analysis research that fuses form and function in weather forecasting and analysis},
	issn = {14770296},
	doi = {10.1177/03091333221133098},
	abstract = {Features in the natural and built environment can be viewed as objects, and an object’s shape provides valuable information about the physical processes that generate those features. Formally, shape is defined as an object’s characteristics independent of rotation, translation, and scale. Shape analysis involves quantification of an object’s form. Shape metrics, or indices, are mathematical quantities that characterize the object’s size and shape. Shape analysis has a rich history in geography and, more specifically, in meteorology and climatology research, with early examples in the identification of comma-shaped clouds in satellite imagery and “hook echoes” in radar reflectivity imagery. At its basis, shape analysis can be characterized as image analysis, which involves processing an image to extract meaningful information. Shape analysis usually involves image segmentation to isolate objects of interest and region analysis to calculate statistical data about these object(s). Current shape analysis research in meteorology and climatology can be split into two broad themes: (1) verification studies to compare model forecasts with observations and (2) process studies that provide information about the dynamical structure of a particular weather or climate phenomenon. In this report, I provide examples of emerging research that uses shape analysis to study tropical cyclones, mesoscale weather phenomena, and atmospheric rivers. Thus far, most of the process studies have been related to TC structure. Future research should also consider innovative approaches to image segmentation, new spatial verification methods for ensemble forecasting products, and more shaped-based process studies in mesoscale meteorology.},
	journal = {Progress in Physical Geography},
	author = {Zick, Stephanie E.},
	month = feb,
	year = {2022},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Climate, image analysis, object-based, shape analysis, weather},
}

@article{gilleland_analyzing_2010,
	title = {Analyzing the image warp forecast verification method on precipitation fields from the {ICP}},
	volume = {25},
	issn = {08828156},
	doi = {10.1175/2010WAF2222365.1},
	abstract = {Image warping for spatial forecast verification is applied to the test cases employed by the Spatial Forecast Verification Intercomparison Project (ICP), which includes both real and contrived cases.A larger set of cases is also used to investigate aggregating results for summarizing forecast performance over a long record of forecasts. The technique handles the geometric and perturbed cases with nearly exact precision, as would be expected. A statistic, dubbed here the IWS for image warp statistic, is proposed for ranking multiple forecasts and tested on the perturbed cases. IWS rankings for perturbed and real test cases are found to be sensible and physically interpretable. A powerful result of this study is that the image warp can be employed using a relatively sparse, preset regular grid without having to first identify features. © 2010 American Meteorological Society.},
	number = {4},
	journal = {Weather and Forecasting},
	author = {Gilleland, Eric and Lindstrom, Johan and Finn, Lindgren},
	month = aug,
	year = {2010},
	keywords = {Forecast verification, Precipitation, Statistical forecasting, Stochastic models, ★},
	pages = {1249--1262},
}

@article{keil_displacement-based_2007,
	title = {A displacement-based error measure applied in a regional ensemble forecasting system},
	volume = {135},
	issn = {00270644},
	doi = {10.1175/MWR3457.1},
	abstract = {Errors in regional forecasts often take the form of phase errors, where a forecasted weather system is displaced in space or time. For such errors, a direct measure of the displacement is likely to be more valuable than traditional measures. A novel forecast quality measure is proposed that is based on a comparison of observed and forecast satellite imagery from the Meteosat-7 geostationary satellite. The measure combines the magnitude of a displacement vector calculated with a pyramid matching algorithm and the local squared difference of observed and morphed forecast brightness temperature fields. Following the description of the method and its application for a simplified case, the measure is applied to regional ensemble forecasts for an episode of prefrontal summertime convection in Bavaria. It is shown that this new method provides a plausible measure of forecast error, which is consistent with a subjective ranking of ensemble members for a sample forecast. The measure is then applied to hourly images over a 36-h forecast period and compared with the bias and equitable threat score. The two conventional measures fail to provide any systematic distinction between different ensemble members, while the new measure identifies ensemble members of differing skill levels with a strong degree of temporal consistency. Using the displacement-based error measure, individual ensemble members are found to compare better with observations than either a short-term deterministic forecast or the ensemble mean throughout the convective period. © 2007 American Meteorological Society.},
	number = {9},
	journal = {Monthly Weather Review},
	author = {Keil, Christian and Craig, George C.},
	month = sep,
	year = {2007},
	pages = {3248--3259},
}

@article{gagne_machine_2020,
	title = {Machine {Learning} for {Stochastic} {Parameterization}: {Generative} {Adversarial} {Networks} in the {Lorenz} '96 {Model}},
	volume = {12},
	issn = {19422466},
	doi = {10.1029/2019MS001896},
	abstract = {Stochastic parameterizations account for uncertainty in the representation of unresolved subgrid processes by sampling from the distribution of possible subgrid forcings. Some existing stochastic parameterizations utilize data-driven approaches to characterize uncertainty, but these approaches require significant structural assumptions that can limit their scalability. Machine learning models, including neural networks, are able to represent a wide range of distributions and build optimized mappings between a large number of inputs and subgrid forcings. Recent research on machine learning parameterizations has focused only on deterministic parameterizations. In this study, we develop a stochastic parameterization using the generative adversarial network (GAN) machine learning framework. The GAN stochastic parameterization is trained and evaluated on output from the Lorenz '96 model, which is a common baseline model for evaluating both parameterization and data assimilation techniques. We evaluate different ways of characterizing the input noise for the model and perform model runs with the GAN parameterization at weather and climate time scales. Some of the GAN configurations perform better than a baseline bespoke parameterization at both time scales, and the networks closely reproduce the spatiotemporal correlations and regimes of the Lorenz '96 system. We also find that, in general, those models which produce skillful forecasts are also associated with the best climate simulations.},
	number = {3},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Gagne, David John and Christensen, Hannah M. and Subramanian, Aneesh C. and Monahan, Adam H.},
	month = mar,
	year = {2020},
	note = {arXiv: 1909.04711
Publisher: Blackwell Publishing Ltd},
	keywords = {climate, generative adversarial networks, lorenz, machine learning, stochastic parameterization, weather},
}

@article{amemiya_application_2023,
	title = {Application of recurrent neural networks to model bias correction: {Idealized} experiments with the {Lorenz}‐96 model},
	issn = {1942-2466},
	doi = {10.1029/2022ms003164},
	abstract = {Systematic biases in numerical weather prediction models cause forecast deviation from reality. While model biases also affect data assimilation and degrade the analysis accuracy, observation information incorporated through data assimilation can provide information for detecting and alleviating such biases. In this study, the application of machine learning to model bias correction is demonstrated, emphasizing the effectiveness of recurrent neural networks. Idealized experiments are performed using the two‐scale coupled Lorenz‐96 model as the true system and single Lorenz‐96 model as the imperfect forecast model, to compare the effectiveness of bias correction methods based on various architectures of neural networks and simple linear regression. The neural networks generally outperformed linear regression, and recurrent neural networks showed the best ability in finding the systematic bias component from the analysis increment data. Bias correction using the recurrent neural networks also gives the most significant improvement in reducing the error growth rate in extended range forecasts. The results suggest that including past time series of the forecast variables improve model bias correction when limited information of the observation is incorporated through data assimilation.},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Amemiya, A. and Shlok, M. and Miyoshi, T.},
	month = feb,
	year = {2023},
	note = {Publisher: American Geophysical Union (AGU)},
}

@article{harrison_advancing_2022,
	title = {Advancing early warning capabilities with {CHIRPS}-compatible {NCEP} {GEFS} precipitation forecasts},
	volume = {9},
	issn = {20524463},
	doi = {10.1038/s41597-022-01468-2},
	abstract = {CHIRPS-GEFS is an operational data set that provides daily bias-corrected forecasts for next 1-day to {\textasciitilde}15-day precipitation totals and anomalies at a quasi-global 50-deg N to 50-deg S extent and 0.05-degree resolution. These are based on National Centers for Environmental Prediction (NCEP) Global Ensemble Forecast System version 12 (GEFS v12) precipitation forecasts. CHIRPS-GEFS forecasts are compatible with Climate Hazards center InfraRed Precipitation with Stations (CHIRPS) data, which is actively used for drought monitoring, early warning, and near real-time impact assessments. A rank-based quantile matching procedure is used to transform GEFS v12 “reforecast” and “real-time” forecast ensemble means to CHIRPS spatial-temporal characteristics. Matching distributions to CHIRPS makes forecasts better reflect local climatology at finer spatial resolution and reduces moderate-to-large forecast errors. As shown in this study, having a CHIRPS-compatible version of the latest generation of NCEP GEFS forecasts enables rapid assessment of current forecasts and local historical context. CHIRPS-GEFS effectively bridges the gap between observations and weather predictions, increasing the value of both by connecting monitoring resources (CHIRPS) with interoperable forecasts.},
	number = {1},
	journal = {Scientific Data},
	author = {Harrison, Laura and Landsfeld, Martin and Husak, Greg and Davenport, Frank and Shukla, Shraddhanand and Turner, William and Peterson, Pete and Funk, Chris},
	month = dec,
	year = {2022},
	pmid = {35773449},
	note = {Publisher: Nature Research},
}

@techreport{katz_weather_nodate,
	title = {Do {Weather} or {Climate} {Variables} and {Their} {Impacts} {Have} {Heavy}-{Tailed} {Distributions}?},
	url = {http://ulysses.atmos.colostate.edu},
	author = {Katz, Richard W},
}

@article{clauset_power-law_2007,
	title = {Power-law distributions in empirical data},
	url = {http://arxiv.org/abs/0706.1062},
	doi = {10.1137/070710111},
	abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.},
	author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, M. E. J.},
	month = jun,
	year = {2007},
	note = {arXiv: 0706.1062},
}

@article{newman_power_2005,
	title = {Power laws, {Pareto} distributions and {Zipf}'s law},
	volume = {46},
	issn = {00107514},
	doi = {10.1080/00107510500052444},
	abstract = {When the probability of measuring a particular value of some quantity varies inversely as a power of that value, the quantity is said to follow a power law, also known variously as Zipf's law or the Pareto distribution. Power laws appear widely in physics, biology, earth and planetary sciences, economics and finance, computer science, demography and the social sciences. For instance, the distributions of the sizes of cities, earthquakes, forest fires, solar flares, moon craters and people's personal fortunes all appear to follow power laws. The origin of power-law behaviour has been a topic of debate in the scientific community for more than a century. Here we review some of the empirical evidence for the existence of power-law forms and the theories proposed to explain them. © 2005 Taylor \& Francis Group Ltd.},
	number = {5},
	journal = {Contemporary Physics},
	author = {Newman, M. E.J.},
	month = sep,
	year = {2005},
	note = {arXiv: cond-mat/0412004},
	pages = {323--351},
}

@article{gabaix_zipfs_1999,
	title = {Zipf's {Law} for {Cities}: {An} {Explanation}},
	volume = {114},
	issn = {0033-5533},
	url = {https://academic.oup.com/qje/article-lookup/doi/10.1162/003355399556133},
	doi = {10.1162/003355399556133},
	number = {3},
	journal = {The Quarterly Journal of Economics},
	author = {Gabaix, X.},
	month = aug,
	year = {1999},
	pages = {739--767},
}

@article{aitchison_zipfs_2016,
	title = {Zipf’s {Law} {Arises} {Naturally} {When} {There} {Are} {Underlying}, {Unobserved} {Variables}},
	volume = {12},
	issn = {15537358},
	doi = {10.1371/journal.pcbi.1005110},
	abstract = {Zipf’s law, which states that the probability of an observation is inversely proportional to its rank, has been observed in many domains. While there are models that explain Zipf’s law in each of them, those explanations are typically domain specific. Recently, methods from statistical physics were used to show that a fairly broad class of models does provide a general explanation of Zipf’s law. This explanation rests on the observation that real world data is often generated from underlying causes, known as latent variables. Those latent variables mix together multiple models that do not obey Zipf’s law, giving a model that does. Here we extend that work both theoretically and empirically. Theoretically, we provide a far simpler and more intuitive explanation of Zipf’s law, which at the same time considerably extends the class of models to which this explanation can apply. Furthermore, we also give methods for verifying whether this explanation applies to a particular dataset. Empirically, these advances allowed us extend this explanation to important classes of data, including word frequencies (the first domain in which Zipf’s law was discovered), data with variable sequence length, and multi-neuron spiking activity.},
	number = {12},
	journal = {PLoS Computational Biology},
	author = {Aitchison, Laurence and Corradi, Nicola and Latham, Peter E.},
	month = dec,
	year = {2016},
	pmid = {27997544},
	note = {Publisher: Public Library of Science},
}

@article{gabaix_power_2016,
	title = {Power laws in economics: {An} introduction},
	volume = {30},
	issn = {08953309},
	doi = {10.1257/jep.30.1.185},
	abstract = {Many of the insights of economics seem to be qualitative, with many fewer reliable quantitative laws. However a series of power laws in economics do count as true and nontrivial quantitative laws - and they are not only established empirically, but also understood theoretically. I will start by providing several illustrations of empirical power laws having to do with patterns involving cities, firms, and the stock market. I summarize some of the theoretical explanations that have been proposed. I suggest that power laws help us explain many economic phenomena, including aggregate economic fluctuations. I hope to clarify why power laws are so special, and to demonstrate their utility. In conclusion, I list some power-law-related economic enigmas that demand further exploration.},
	number = {1},
	journal = {Journal of Economic Perspectives},
	author = {Gabaix, Xavier},
	month = dec,
	year = {2016},
	note = {Publisher: American Economic Association},
	pages = {185--206},
}

@article{noauthor_csu_todorovic_1970_nodate,
	title = {{CSU}\_Todorovic\_1970},
}

@article{hansen_three_2020,
	title = {The {Three} {Extreme} {Value} {Distributions}: {An} {Introductory} {Review}},
	volume = {8},
	issn = {2296424X},
	doi = {10.3389/fphy.2020.604053},
	abstract = {The statistical distribution of the largest value drawn from a sample of a given size has only three possible shapes: it is either a Weibull, a Fréchet or a Gumbel extreme value distributions. I describe in this short review how to relate the statistical distribution followed by the numbers in the sample to the associate extreme value distribution followed by the largest value within the sample. Nothing I present here is new. However, from experience, I have found that a simple, short and compact guide on this matter written for the physics community is missing.},
	journal = {Frontiers in Physics},
	author = {Hansen, Alex},
	month = dec,
	year = {2020},
	note = {arXiv: 2009.03711
Publisher: Frontiers Media S.A.},
	keywords = {Frechet distribution, Weibull distribution, Gumbel distribution, Weibull analysis, extreme value statistics, statistical analysis},
}

@article{davison_models_1990,
	title = {Models for {Exceedances} {Over} {High} {Thresholds}},
	volume = {52},
	doi = {10.1111/j.2517-6161.1990.tb01796.x},
	abstract = {We discuss the analysis of the extremes of data by modelling the sizes and occurrence of exceedances over high thresholds. The natural distribution for such exceedances, the generalized Pareto distribution, is described and its properties elucidated. Estimation and model-checking procedures for univariate and regression data are developed, and the influence of and information contained in the most extreme observations in a sample are studied. Models for seasonality and serial dependence in the point process of exceedances are described. Sets of data on river flows and wave heights are discussed, and an application to the siting of nuclear installations is described.},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Davison, A. C. and Smith, R. L.},
	month = jul,
	year = {1990},
	note = {Publisher: Wiley},
	pages = {393--425},
}

@article{gabaix_power_2009,
	title = {Power {Laws} in {Economics} and {Finance}},
	volume = {1},
	issn = {1941-1383},
	doi = {10.1146/annurev.economics.050708.142940},
	abstract = {A power law (PL) is the form taken by a large number of surprising empirical regularities in economics and finance. This review surveys well-documented empirical PLs regarding income and wealth, the size of cities and firms, stock market returns, trading volume, international trade, and executive pay. It reviews detail-independent theoretical motivations that make sharp predictions concerning the existence and coefficients of PLs, without requiring delicate tuning of model parameters. These theoretical mechanisms include random growth, optimization, and the economics of superstars, coupled with extreme value theory. Some empirical regularities currently lack an appropriate explanation. This article highlights these open areas for future research.},
	number = {1},
	journal = {Annual Review of Economics},
	author = {Gabaix, Xavier},
	month = sep,
	year = {2009},
	note = {Publisher: Annual Reviews},
	pages = {255--294},
}

@techreport{sutton_gibrats_1997,
	title = {Gibrat's {Legacy}},
	author = {Sutton, John},
	year = {1997},
	note = {Publication Title: Source: Journal of Economic Literature
Volume: 35
Issue: 1},
	pages = {40--59},
}

@inproceedings{adrian_twenty_2005,
	title = {Twenty years of particle image velocimetry},
	volume = {39},
	doi = {10.1007/s00348-005-0991-7},
	abstract = {The development of the method of particle image velocimetry (PIV) is traced by describing some of the milestones that have enabled new and/or better measurements to be made. The current status of PIV is summarized, and some goals for future advances are addressed. © Springer-Verlag 2005.},
	booktitle = {Experiments in {Fluids}},
	author = {Adrian, R. J.},
	month = aug,
	year = {2005},
	note = {Issue: 2
ISSN: 07234864},
	pages = {159--169},
}

@article{pan_two-dimensional_2009,
	title = {Two-dimensional digital image correlation for in-plane displacement and strain measurement: {A} review},
	volume = {20},
	issn = {13616501},
	doi = {10.1088/0957-0233/20/6/062001},
	abstract = {As a practical and effective tool for quantitative in-plane deformation measurement of a planar object surface, two-dimensional digital image correlation (2D DIC) is now widely accepted and commonly used in the field of experimental mechanics. It directly provides full-field displacements to sub-pixel accuracy and full-field strains by comparing the digital images of a test object surface acquired before and after deformation. In this review, methodologies of the 2D DIC technique for displacement field measurement and strain field estimation are systematically reviewed and discussed. Detailed analyses of the measurement accuracy considering the influences of both experimental conditions and algorithm details are provided. Measures for achieving high accuracy deformation measurement using the 2D DIC technique are also recommended. Since microscale and nanoscale deformation measurement can easily be realized by combining the 2D DIC technique with high-spatial- resolution microscopes, the 2D DIC technique should find more applications in broad areas. © 2009 IOP Publishing Ltd.},
	number = {6},
	journal = {Measurement Science and Technology},
	author = {Pan, Bing and Qian, Kemao and Xie, Huimin and Asundi, Anand},
	year = {2009},
	note = {Publisher: Institute of Physics Publishing},
	keywords = {Digital image correlation, Displacement/deformation measurement},
}

@article{palmer_economic_2002,
	title = {The economic value of ensemble forecasts as a tool for risk assessment: {From} days to decades},
	volume = {128},
	issn = {00359009},
	doi = {10.1256/0035900021643593},
	abstract = {Despite the revolutionary development of numerical weather and climate prediction (NWCP) in the second half of the last century, quantitative interaction between model developers and forecast customers has been rather limited. This is apparent in the diverse ways in which weather forecasts are assessed by these two groups: rootmean-square error of 500 hPa height on the one hand; pounds, euros or dollars saved on the other. These differences of approach are changing with the development of ensemble forecasting. Ensemble forecasts provide a qualitative tool for the assessment of weather and climate risk for a range of user applications, and on a range of time-scales, from days to decades. Examples of the commercial application of ensemble forecasting, from electricity generation, ship routeing, pollution modelling, weather-risk finance, disease prediction and crop yield modelling, are shown from all these time-scales. A generic user decision model is described that allows one to assess the potential economic value of numerical weather and climate forecasts for a range of customers. Using this, it is possible to relate analytically, potential economic value to conventional meteorological skill scores. A generalized meteorological measure of forecast skill is proposed which takes the distribution of customers into account. It is suggested that when customers' exposure to weather or climate risk can be quantified, such more generalized measures of skill should be used in assessing the performance of an operational NWCP system.},
	number = {581},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Palmer, T. N.},
	month = apr,
	year = {2002},
	keywords = {Climate change, Cost/loss ratio, Probability forecasting, Seasonal forecasts, User application models},
	pages = {747--774},
}

@article{vannitsem_statistical_2021,
	title = {Statistical postprocessing for weather forecasts review, challenges, and avenues in a big data world},
	volume = {102},
	issn = {00030007},
	doi = {10.1175/BAMS-D-19-0308.1},
	abstract = {Statistical postprocessing techniques are nowadays key components of the forecasting suites in many national meteorological services (NMS), with, for most of them, the objective of correcting the impact of different types of errors on the forecasts. The final aim is to provide optimal, automated, seamless forecasts for end users. Many techniques are now flourishing in the statistical, meteorological, climatological, hydrological, and engineering communities. The methods range in complexity from simple bias corrections to very sophisticated distribution-adjusting techniques that incorporate correlations among the prognostic variables. The paper is an attempt to summarize the main activities going on in this area from theoretical developments to operational applications, with a focus on the current challenges and potential avenues in the field. Among these challenges is the shift in NMS toward running ensemble numerical weather prediction (NWP) systems at the kilometer scale that produce very large datasets and require high-density high-quality observations, the necessity to preserve space-time correlation of high-dimensional corrected fields, the need to reduce the impact of model changes affecting the parameters of the corrections, the necessity for techniques to merge different types of forecasts and ensembles with different behaviors, and finally the ability to transfer research on statistical postprocessing to operations. Potential new avenues are also discussed.},
	number = {3},
	journal = {Bulletin of the American Meteorological Society},
	author = {Vannitsem, Stéphane and Bremnes, John Bjørnar and Demaeyer, Jonathan and Evans, Gavin R. and Flowerdew, Jonathan and Hemri, Stephan and Lerch, Sebastian and Roberts, Nigel and Theis, Susanne and Atencia, Aitor and Bouallègue, Zied Ben and Bhend, Jonas and Dabernig, Markus and de Cruz, Lesley and Hieta, Leila and Mestre, Olivier and Moret, Lionel and Plenković, Iris Odak and Schmeits, Maurice and Taillardat, Maxime and van den Bergh, Joris and van Schaeybroeck, Bert and Whan, Kirien and Ylhaisi, Jussi},
	year = {2021},
	note = {arXiv: 2004.06582
Publisher: American Meteorological Society},
	keywords = {Bias, Data science, Model output statistics, Operational forecasting, Probability forecasts/models/distribution, Regression},
	pages = {E681--E699},
}

@misc{unicef_more_2022,
	title = {More than twenty million children suffering in the {Horn} of {Africa} as drought intensifies},
	url = {https://www.unicef.org/press-releases/more-twenty-million-children-suffering-horn-africa-drought-intensifies-unicef},
	urldate = {2023-03-06},
	author = {{UNICEF}},
	month = dec,
	year = {2022},
}

@techreport{katz_statistics_nodate,
	title = {Statistics of extremes in hydrology},
	url = {www.elsevier.com/locate/advwatres},
	abstract = {The statistics of extremes have played an important role in engineering practice for water resources design and management. How recent developments in the statistical theory of extreme values can be applied to improve the rigor of hydrologic applications and to make such analyses more physically meaningful is the central theme of this paper. Such methodological developments primarily relate to maximum likelihood estimation in the presence of covariates, in combination with either the block maxima or peaks over threshold approaches. Topics that are treated include trends in hydrologic extremes, with the anticipated intensification of the hydrologic cycle as part of global climate change. In an attempt to link downscaling (i.e., relating large-scale atmosphere-ocean circulation to smaller-scale hydrologic variables) with the statistics of extremes, statistical downscaling of hydrologic extremes is considered. Future challenges are reviewed, such as the development of more rigorous statistical methodology for regional analysis of extremes, as well as the extension of Bayesian methods to more fully quantify uncertainty in extremal estimation. Examples include precipitation and streamflow extremes, as well as economic damage associated with such extreme events, with consideration of trends and dependence on patterns in atmosphere-ocean circulation (e.g., El Ni{\textasciitilde} n no phenomenon).},
	author = {Katz, Richard W and Parlange, Marc B and Naveau, Philippe},
	keywords = {Climate change, Covariates, Maximum likelihood, Statistical downscaling},
}

@article{demirdjian_statistical_2018,
	title = {Statistical modeling of extreme precipitation with {TRMM} data},
	volume = {57},
	issn = {15588432},
	doi = {10.1175/JAMC-D-17-0023.1},
	abstract = {This paper improves upon an existing extreme precipitation monitoring system that is based on the Tropical Rainfall Measuring Mission (TRMM) daily product (3B42) using new statistical models. The proposed system utilizes a regional modeling approach in which data from similar locations are pooled to increase the quality of the resulting model parameter estimates to compensate for the short data record. The regional analysis is divided into two stages. First, the region defined by the TRMM measurements is partitioned into approximately 28 000 nonoverlapping clusters using a recursive k-means clustering scheme. Next, a statistical model is used characterize the extreme precipitation events occurring in each cluster. Instead of applying the block maxima approach used in the existing system, in which the generalized extreme value probability distribution is fit to the annual precipitation maxima at each site separately, the present work adopts the peak-over-threshold method of classifying points as extreme if they exceed a prespecified threshold. Theoretical considerations motivate using the point process framework for modeling extremes. The fitted parameters are used to estimate trends and to construct simple and intuitive average recurrence interval (ARI) maps that reveal how rare a particular precipitation event is. This information could be used by policy makers for disaster monitoring and prevention. The new method eliminates much of the noise that was produced by the existing models because of a short data record, producing more reasonable ARI maps when compared with NOAA's long-term Climate Prediction Center ground-based observations. Furthermore, the proposed method can be applied to other extreme climate records.},
	number = {1},
	journal = {Journal of Applied Meteorology and Climatology},
	author = {Demirdjian, Levon and Zhou, Yaping and Huffman, George J.},
	month = jan,
	year = {2018},
	note = {Publisher: American Meteorological Society},
	keywords = {Extreme events, Hydrology, Precipitation, Statistical techniques},
	pages = {15--30},
}

@article{piani_statistical_2010,
	title = {Statistical bias correction for daily precipitation in regional climate models over {Europe}},
	volume = {99},
	issn = {14344483},
	doi = {10.1007/s00704-009-0134-9},
	abstract = {We design, apply, and validate a methodology for correcting climate model output to produce internally consistent fields that have the same statistical intensity distribution as the observations. We refer to this as a statistical bias correction. Validation of the methodology is carried out using daily precipitation fields, defined over Europe, from the ENSEMBLES climate model dataset. The bias correction is calculated using data from 1961 to 1970, without distinguishing between seasons, and applied to seasonal data from 1991 to 2000. This choice of time periods is made to maximize the lag between calibration and validation within the ERA40 reanalysis period. Results show that the method performs unexpectedly well. Not only are the mean and other moments of the intensity distribution improved, as expected, but so are a drought and a heavy precipitation index, which depend on the autocorrelation spectra. Given that the corrections were derived without seasonal distinction and are based solely on intensity distributions, a statistical quantity oblivious of temporal correlations, it is encouraging to find that the improvements are present even when seasons and temporal statistics are considered. This encourages the application of this method to multi-decadal climate projections. © 2009 Springer-Verlag.},
	number = {1-2},
	journal = {Theoretical and Applied Climatology},
	author = {Piani, C. and Haerter, J. O. and Coppola, E.},
	year = {2010},
	note = {Publisher: Springer Wien},
	pages = {187--192},
}

@article{wood_long-range_2002,
	title = {Long-range experimental hydrologic forecasting for the eastern {United} {States}},
	volume = {107},
	issn = {01480227},
	doi = {10.1029/2001JD000659},
	abstract = {We explore a strategy for long-range hydrologic forecasting that uses ensemble climate model forecasts as input to a macroscale hydrologic model to produce runoff and streamflow forecasts at spatial and temporal scales appropriate for water management. Monthly ensemble climate model forecasts produced by the National Centers for Environmental Prediction/Climate Prediction Center global spectral model (GSM) are bias corrected, downscaled to 1/8° horizontal resolution, and disaggregated to a daily time step for input to the Variable Infiltration Capacity hydrologic model. Bias correction is effected by evaluating the GSM ensemble forecast variables as percentiles relative to the GSM model climatology and then extracting the percentiles' associated variable values instead from the observed climatology. The monthly meteorological forecasts are then interpolated to the finer hydrologic model scale, at which a daily signal that preserves the forecast anomaly is imposed through resampling of the historic record. With the resulting monthly runoff and streamflow forecasts for the East Coast and Ohio River basin, we evaluate the bias correction and resampling approaches during the southeastern United States drought from May to August 2000 and also for the El Niño conditions of December 1997 to February 1998. For the summer 2000 study period, persistence in anomalous initial hydrologic states predominates in determining the hydrologic forecasts. In contrast, the El Niño-condition hydrologic forecasts derive direction both from the climate model forecast signal and the antecedent land surface state. From a qualitative standpoint the hydrologic forecasting strategy appears successful in translating climate forecast signals to hydrologic variables of interest for water management. Copyright 2002 by the American Geophysical Union.},
	number = {20},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Wood, Andrew W. and Maurer, Edwin P. and Kumar, Arun and Lettenmaier, Dennis P.},
	year = {2002},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {1833 Hydrology: Hydroclimatology, 1836 Hydrology: Hydrologic budget (1655), 1860 Hydrology: Runoff and streamflow, 1863 Hydrology: Snow and ice (1827), Climate downscaling, Eastern United States, Hydrologic forecast, Seasonal forecast, Streamflow forecast},
	pages = {ACL 6--1--ACL 6--15},
}

@article{amengual_statistical_2012,
	title = {A statistical adjustment of regional climate model outputs to local scales: {Application} to {Platja} de {Palma}, {Spain}},
	volume = {25},
	issn = {08948755},
	doi = {10.1175/JCLI-D-10-05024.1},
	abstract = {Projections of climate change effects for the System of Platja de Palma (SPdP) are derived using a novel statistical technique. Socioeconomic activities developed in this settlement are very closely linked to its climate. Any planning for socioeconomic opportunities in the mid- and long term must take into account the possible effects of climate change. To this aim, daily observed series of minimum and maximum temperatures, precipitation, relative humidity, cloud cover, and wind speed have been analyzed. For the climate projections, daily data generated by an ensemble of regional climate models (RCMs) have been used. To properly use RCM data at local scale, a quantile-quantile adjustment has been applied to the simulated regional projections. The method is based on detecting changes in the cumulative distribution functions between the recent past and successive time slices of the simulated climate and applying these, after calibration, to the recent past (observed) series. Results show an overall improvement in reproducing the present climate baseline when using calibrated series instead of raw RCM outputs, although the correction does not result in such clear improvement when dealing with very extreme rainfalls. Next, the corrected series are analyzed to quantify the climate change signal. Anincrease of the annual means for temperatures together with a decrease for the remaining variables is projected throughout the twenty-first century. Increases in weak and intense daily rainfalls and in high extremes for daily maximum temperature can also be expected. With this information at hand, the experts planning the future of SPdP can respond more effectively to the problem of local adaptation to climate change. © 2012 American Meteorological Society.},
	number = {3},
	journal = {Journal of Climate},
	author = {Amengual, A. and Homar, V. and Romero, R. and Alonso, S. and Ramis, C.},
	month = feb,
	year = {2012},
	keywords = {Climate change, Climatology, Europe, Mediterranean Sea, Societal impacts},
	pages = {939--957},
}

@inproceedings{casati_2020_2022,
	title = {The 2020 {International} {Verification} {Methods} {Workshop} {Online}: {Major} {Outcomes} and {Way} {Forward}},
	volume = {103},
	doi = {10.1175/BAMS-D-21-0126.1},
	abstract = {The International Verification Methods Workshop was held online in November 2020 and included sessions on physical error characterization using process diagnostics and error tracking techniques; exploitation of data assimilation techniques in verification practices, e.g., to address representativeness issues and observation uncertainty; spatial verification methods and the Model Evaluation Tools, as unified reference verification software; and meta-verification and best practices for scores computation. The workshop reached out to diverse research communities working in the areas of high-impact weather, subseasonal to seasonal prediction, polar prediction, and sea ice and ocean prediction. This article summarizes the major outcomes of the workshop and outlines future strategic directions for verification research.},
	booktitle = {Bulletin of the {American} {Meteorological} {Society}},
	publisher = {American Meteorological Society},
	author = {Casati, Barbara and Dorninger, Manfred and Coelho, Caio A.S. and Ebert, Elizabeth E. and Marsigli, Chiara and Mittermaier, Marion P. and Gilleland, Eric},
	month = mar,
	year = {2022},
	note = {Issue: 3
ISSN: 15200477},
	keywords = {Forecast verification/skill},
	pages = {E899--E910},
}

@article{noauthor_fundamentals_of_numerical_weather_prediction_----_2_weather_prediction_equations_nodate,
	title = {Fundamentals\_of\_Numerical\_Weather\_Prediction\_----\_(2\_Weather\_prediction\_equations)},
}

@article{woodhams_identifying_2019,
	title = {Identifying key controls on storm formation over the lake {Victoria} basin},
	volume = {147},
	issn = {15200493},
	doi = {10.1175/MWR-D-19-0069.1},
	abstract = {The Lake Victoria region in East Africa is a hot spot for intense convective storms that are responsible forthe deaths of thousands of fishermen each year. The processes responsible for the initiation, development, andpropagation of the storms are poorly understood and forecast skill is limited. Key processes for the life cycle oftwo storms are investigated using Met Office Unified Model convection-permitting simulations with 1.5 kmhorizontal grid spacing. The two cases are analyzed alongside a simulation of a period with no storms to assessthe roles of the lake-land breeze, downslope mountain winds, prevailing large-scale winds, and moistureavailability. While seasonal changes in large-scale moisture availability play a key role in storm development,the lake-land-breeze circulation is a major control on the initiation location, timing, and propagation ofconvection. In the dry season, opposing offshore winds form a bulge of moist air above the lake surfaceovernight that extends from the surface to;1.5 km and may trigger storms in high CAPE/low CIN environments. Such a feature has not been explicitly observed or modeled in previous literature. Storms over landon the preceding day are shown to alter the local atmospheric moisture and circulation to promote stormformation over the lake. The variety of initiation processes and differing characteristics of just two stormsanalyzed here show that the mean diurnal cycle over Lake Victoria alone is inadequate to fully understandstorm formation. Knowledge of daily changes in local-scale moisture variability and circulations are keys forskillful forecasts over the lake.},
	number = {9},
	journal = {Monthly Weather Review},
	author = {Woodhams, Beth J. and Birch, Cathryn E. and Marsham, John H. and Lane, Todd P. and Bain, Caroline L. and Webster, Stuart},
	year = {2019},
	note = {Publisher: American Meteorological Society},
	keywords = {★},
	pages = {3365--3390},
}

@article{kastner_intercomparison_2006,
	title = {Intercomparison of satellite-based and model-based rainfall analyses},
	volume = {13},
	issn = {14698080},
	doi = {10.1017/S1350482706002246},
	abstract = {Four satellite rain estimations based on microwave (MW), infrared (IR) or combined MW-IR techniques are compared with the BOlogna Limited Area Model (BOLAM) rain forecast for a severe weather event (8-13 November 2001) over the western Mediterranean Sea. Two of the investigated multi-channel MW rainfall algorithms use data from the Tropical Rainfall Measuring Mission (TRMM). The Frequency Difference Algorithm relies on data from the TRMM Microwave Imager (TMI) and the other one combines data from the Precipitation Radar (PR) with those from the nine-channel radiometer TMI, called PR Adjusted TMI Estimations of Rainfall (PATER) algorithm. The pure IR Rain Estimator uses geostationary IR METEOSAT data and the combined Naval Research Laboratory algorithm uses both MW data from low orbiting satellites and IR data from the geostationary orbit. Validation results, computed over a common grid, which is independent of the different field of view sizes of the applied data sets, indicate that there is generally a better performance for heavy rain ({\textgreater} 6 mm h-1) than for light rain ({\textless}1 mm h-1). Both MW algorithms perform rather similarly, although PATER shows some rain detection problems due to thick aerosol loads originating from the desert. The BOLAM model presents a good agreement with the MW and only a minor location error of a heavy rain area was detected. Both IR-based algorithms have problems in identifying the correct rainy areas compared to MW. Overall, the results suggest that there are advantages in combining both techniques - the well-known rain physics of the MW channels with the high temporal resolution of IR algorithms - to retrieve precipitation from satellite data.},
	number = {3},
	journal = {Meteorological Applications},
	author = {Kästner, Martina and Torricella, Francesca and Davolio, Silvio},
	year = {2006},
	note = {Publisher: Cambridge University Press},
	keywords = {BOLAM, METEOSAT, Mesoscale model, Rain retrieval, Satellite},
	pages = {213--223},
}

@article{watson_does_2015,
	title = {Does the {ECMWF} {IFS} convection parameterization with stochastic physics correctly reproduce relationships between convection and the large-scale state?},
	volume = {72},
	issn = {15200469},
	doi = {10.1175/JAS-D-14-0252.1},
	abstract = {Important questions concerning parameterization of tropical convection are how should subgrid-scale variability be represented and which large-scale variables should be used in the parameterizations? Here the statistics of observational data in Darwin, Australia, are compared with those of short-term forecasts of convection made by the European Centre for Medium-Range Weather Forecasts Integrated Forecast System. The forecasts use multiplicative-noise stochastic physics (MNSP) that has led to many improvements in weather forecast skill. However, doubts have recently been raised about whether MNSP is consistent with observations of tropical convection. It is shown that the model can reproduce the variability of convection intensity for a given large-scale state, both with and without MNSP. Therefore MNSP is not inconsistent with observations, and much of the modeled variability arises from nonlinearity of the deterministic part of the convection scheme. It is also shown that the model can reproduce the lack of correlation between convection intensity and large-scale CAPE and an entraining CAPE, even though the convection parameterization assumes that deep convection is more intense when the vertical temperature profile is more unstable, with entrainment taken into account. Relationships between convection and large-scale convective inhibition and vertical velocity are also correctly captured.},
	number = {1},
	journal = {Journal of the Atmospheric Sciences},
	author = {Watson, Peter A.G. and Christensen, H. M. and Palmer, T. N.},
	year = {2015},
	note = {Publisher: American Meteorological Society},
	keywords = {Convective clouds, Convective parameterization, Model evaluation/performance, Numerical weather prediction/forecasting, Stochastic models},
	pages = {236--242},
}

@article{wainwright_eastern_2019,
	title = {‘{Eastern} {African} {Paradox}’ rainfall decline due to shorter not less intense {Long} {Rains}},
	volume = {2},
	issn = {2397-3722},
	url = {https://www.nature.com/articles/s41612-019-0091-7},
	doi = {10.1038/s41612-019-0091-7},
	abstract = {{\textless}p{\textgreater}An observed decline in the Eastern African Long Rains from the 1980s to late 2000s appears contrary to the projected increase under future climate change. This “Eastern African climate paradox” confounds use of climate projections for adaptation planning across Eastern Africa. Here we show the decline corresponds to a later onset and earlier cessation of the long rains, with a similar seasonal maximum in area-averaged daily rainfall. Previous studies have explored the role of remote teleconnections, but those mechanisms do not sufficiently explain the decline or the newly identified change in seasonality. Using a large ensemble of observations, reanalyses and atmospheric simulations, we propose a regional mechanism that explains both the observed decline and the recent partial recovery. A decrease in surface pressure over Arabia and warmer north Arabian Sea is associated with enhanced southerlies and an earlier cessation of the long rains. This is supported by a similar signal in surface pressure in many atmosphere-only models giving lower May rainfall and an earlier cessation. Anomalously warm seas south of Eastern Africa delay the northward movement of the tropical rain-band, giving a later onset. These results are key in understanding the paradox. It is now a priority to establish the balance of mechanisms that have led to these trends, which are partially captured in atmosphere-only simulations.{\textless}/p{\textgreater}},
	number = {1},
	journal = {npj Climate and Atmospheric Science},
	author = {Wainwright, Caroline M. and Marsham, John H. and Keane, Richard J. and Rowell, David P. and Finney, Declan L. and Black, Emily and Allan, Richard P.},
	month = sep,
	year = {2019},
	note = {Publisher: Nature Research},
	pages = {34},
}

@inproceedings{karras_style-based_2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	month = jun,
	year = {2019},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {https://commoncrawl.org/the-data/},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and Mccandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@article{ebert_toward_2009,
	title = {Toward better understanding of the contiguous rain area ({CRA}) method for spatial forecast verification},
	volume = {24},
	issn = {08828156},
	doi = {10.1175/2009WAF2222252.1},
	abstract = {The contiguous rain area (CRA) method for spatial forecast verification is a features-based approach that evaluates the properties of forecast rain systems, namely, their location, size, intensity, and finescale pattern. It is one of many recently developed spatial verification approaches that are being evaluated as part of a Spatial Forecast Verification Methods Intercomparison Project. To better understand the strengths and weaknesses of the CRA method, it has been tested here on a set of idealized geometric and perturbed forecasts with known errors, as well as nine precipitation forecasts from three high-resolution numerical weather prediction models. The CRA method was able to identify the known errors for the geometric forecasts, but only after a modification was introduced to allow nonoverlapping forecast and observed features to be matched. For the perturbed cases in which a radar rain field was spatially translated and amplified to simulate forecast errors, the CRA method also reproduced the known errors except when a high-intensity threshold was used to define the CRA (≫10 mm h21) and a large translation error was imposed ({\textgreater}200 km). The decomposition of total error into displacement, volume, and pattern components reflected the source of the error almost all of the time when a mean squared error formulation was used, but not necessarily when a correlation-based formulation was used. When applied to real forecasts, the CRA method gave similar results when either best-fit criteria, minimization of the mean squared error, or maximization of the correlation coefficient, was chosen for matching forecast and observed features. The diagnosed displacement error was somewhat sensitive to the choice of search distance. Of the many diagnostics produced by this method, the errors in the mean and peak rain rate between the forecast and observed features showed the best correspondence with subjective evaluations of the forecasts, while the spatial correlation coefficient (after matching) did not reflect the subjective judgments. © 2009 American Meteorological Society.},
	number = {5},
	journal = {Weather and Forecasting},
	author = {Ebert, Elizabeth E. and Gallus, William A.},
	month = oct,
	year = {2009},
	pages = {1401--1415},
}

@article{ebert_verification_2000,
	title = {Verification of precipitation in weather systems: determination of systematic errors},
	volume = {239},
	issn = {00221694},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022169400003437},
	doi = {10.1016/S0022-1694(00)00343-7},
	abstract = {An object-oriented verification procedure is presented for gridded quantitative precipitation forecasts (QPFs). It is carried out within the framework of "contiguous rain areas" (CRAs), whereby a weather system is defined as a region bounded by a user-specified isopleth of precipitation in the union of the forecast and observed rain fields. The horizontal displacement of the forecast is determined by translating the forecast rain field until the total squared difference between the observed and forecast fields is minimized. This allows a decomposition of total error into components due to: (a) location; (b) rain volume and (c) pattern. Results are first presented for a Monte Carlo simulation of 40,000 synthetic CRAs in order to determine the accuracy of the verification procedure when the rain systems are only partially observed due to the presence of domain boundaries. Verification is then carried out for operational 24-h forecasts from the Australian Bureau of Meteorology LAPS numerical weather prediction model over a four-year period. Forty-five percent of all rain events were well forecast by the model, with small location and intensity errors. Location error was generally the dominant source of QPF error, with the directions of most frequent displacement varying by region. Forty-five percent of extreme rainfall events (100 mm d 1) were well forecast, but in this case the model's underestimation of rain intensity was the most frequent source of error.},
	number = {1-4},
	journal = {Journal of Hydrology},
	author = {Ebert, E.E and McBride, J.L},
	month = dec,
	year = {2000},
	keywords = {Australia, Precipitation, Rainfall, Storms},
	pages = {179--202},
}

@article{chen_application_2018,
	title = {Application of {Contiguous} {Rain} {Area} ({CRA}) {Methods} to {Tropical} {Cyclone} {Rainfall} {Forecast} {Verification}},
	volume = {5},
	issn = {23335084},
	doi = {10.1029/2018EA000412},
	abstract = {This study demonstrates the useful information that can be derived from contiguous rain area (CRA) evaluation, such as systematic errors in tropical cyclone (TC) rainfall location and components of rainfall error due to incorrect predictions of location, rain volume, and rain pattern. CRA verification uses pattern matching techniques to determine the location error, as well as errors in area, mean and maximum intensity, and spatial pattern. In this study, CRA verification was applied to evaluate Australian Community Climate and Earth System Simulator (ACCESS)-TC, the TC version of ACCESS, daily rainfall forecasts over 15 TCs in the north west Pacific ocean during 2012–2013, by comparing with Tropical Rainfall Measuring Mission (TRMM) 3B42 satellite estimates. The results showed that pattern error was the major contributor to the total TC rainfall forecast error, followed by volume and displacement. ACCESS-TC forecasts tended to predict more rainfall closer to the TC center compared to Tropical Rainfall Measuring Mission (TRMM) 3B42 estimates. This bias occurred for different CRA rainfall thresholds, verification grid resolutions and forecast lead times. Furthermore, rain event verification showed that for short lead time (24 hr) forecasts, overestimation of rain volume was a major problem for ACCESS-TC forecasts, while displacement error was more significant in longer lead time (72 hr) forecasts. Finally, we compared empirical probability distribution functions and radial probability distributions of rainfall in the forecasts and observations to further characterise the rain volume error. This confirmed that ACCESS-TC tended to produce more extreme rain in the locations closer to the TC center (eyewall).},
	number = {11},
	journal = {Earth and Space Science},
	author = {Chen, Yingjun and Ebert, Elizabeth E. and Davidson, Noel E. and Walsh, Kevin J.E.},
	month = nov,
	year = {2018},
	note = {Publisher: Wiley-Blackwell Publishing Ltd},
	keywords = {evaluation, hurricane, model forecast, precipitation, spatial verification, typhoon},
	pages = {736--752},
}

@article{kelly_aspects_2007,
	title = {Some {Aspects} of {Measurement} {Error} in {Linear} {Regression} of {Astronomical} {Data}},
	url = {http://arxiv.org/abs/0705.2774},
	doi = {10.1086/519947},
	abstract = {I describe a Bayesian method to account for measurement errors in linear regression of astronomical data. The method allows for heteroscedastic and possibly correlated measurement errors, and intrinsic scatter in the regression relationship. The method is based on deriving a likelihood function for the measured data, and I focus on the case when the intrinsic distribution of the independent variables can be approximated using a mixture of Gaussians. I generalize the method to incorporate multiple independent variables, non-detections, and selection effects (e.g., Malmquist bias). A Gibbs sampler is described for simulating random draws from the probability distribution of the parameters, given the observed data. I use simulation to compare the method with other common estimators. The simulations illustrate that the Gaussian mixture model outperforms other common estimators and can effectively give constraints on the regression parameters, even when the measurement errors dominate the observed scatter, source detection fraction is low, or the intrinsic distribution of the independent variables is not a mixture of Gaussians. I conclude by using this method to fit the X-ray spectral slope as a function of Eddington ratio using a sample of 39 z {\textless} 0.8 radio-quiet quasars. I confirm the correlation seen by other authors between the radio-quiet quasar X-ray spectral slope and the Eddington ratio, where the X-ray spectral slope softens as the Eddington ratio increases.},
	author = {Kelly, Brandon C.},
	month = may,
	year = {2007},
	note = {arXiv: 0705.2774},
}

@techreport{grams_use_2006,
	title = {The {Use} of a {Modified} {Ebert}-{McBride} {Technique} to {Evaluate} {Mesoscale} {Model} {QPF} as a {Function} of {Convective} {System} {Morphology} during {IHOP} 2002},
	abstract = {The Ebert-McBride technique (EMT) is an entity-oriented method useful for quantitative precipitation verification. The EMT was modified to optimize its ability to identify contiguous rain areas (CRAs) during the 2002 International H 2 O Project (IHOP). This technique was then used to identify systematic sources of error as a function of observed convective system morphology in three 12-km model simulations run over the IHOP domain: Eta, the fifth-generation Pennsylvania State University-NCAR Mesoscale Model (MM5), and the Weather Research and Forecasting (WRF). The EMT was fine-tuned to optimize the pattern matching of forecasts to observations for the scales of precipitation systems observed during IHOP. To investigate several error measures provided by the EMT, a detailed morphological analysis of observed systems was performed using radar data for all CRAs identified in the IHOP domain. The modified EMT suggests that the Eta Model produced average rain rates, peak rainfall amounts, and total rain volumes that were lower than observed for almost all types of convective systems, likely because of its production of overly smoothed and low-variability quantitative precipitation forecasts. The MM5 and WRF typically produced average rain rates and peak rainfall amounts that were larger than observed in most linear convective systems. However, the rain volume for these models was too low for almost all types of con-vective systems, implying a sizeable underestimate in areal coverage. All three models forecast rainfall too far northwest for linear systems. The results for the WRF and MM5 are consistent with previous observations of mesoscale models run with explicit microphysics and no convective parameterization scheme, suggesting systematic problems with the prediction of mesoscale convective system cold pool dynamics.},
	author = {Grams, Jeremy S and Gallus, Willam A and Koch, Steven E and Wharton, Linda S and Loughe, Andrew and Ebert, Elizabeth E},
	year = {2006},
}

@article{sharma_assessment_2019,
	title = {Assessment of {Met} {Office} {Unified} {Model} ({UM}) quantitative precipitation forecasts during the {Indian} summer monsoon: {Contiguous} {Rain} {Area} ({CRA}) approach},
	volume = {128},
	issn = {0973774X},
	doi = {10.1007/s12040-018-1023-3},
	abstract = {The operational medium range rainfall forecasts of the Met Office Unified Model (UM) are evaluated over India using the Contiguous Rainfall Area (CRA) verification technique. In the CRA method, forecast and observed weather systems (defined by a user-specified rain threshold) are objectively matched to estimate location, volume, and pattern errors. In this study, UM rainfall forecasts from nine (2007–2015) Indian monsoon seasons are evaluated against 0. 5 ∘× 0. 5 ∘ IMD–NCMRWF gridded observed rainfall over India (6. 5 ∘- 38. 5 ∘N , 66. 5 ∘- 100. 5 ∘E). The model forecasts show a wet bias due to excessive number of rainy days particularly of low amounts ({\textless}1mmd-1). Verification scores consistently suggest good skill the forecasts at threshold of 10mmd-1, while moderate (poor) skill at thresholds of {\textless}20mmd-1({\textless}40mmd-1). Spatial verification of rainfall forecasts is carried out for 10, 20, 40 and 80mmd-1 CRA thresholds for four sub-regions namely (i) northwest (NW), (ii) southwest (SW), (iii) eastern (E), and (iv) northeast (NE) sub-region. Over the SW sub-region, the forecasts tend to underestimate rain intensity. In the SW region, the forecast events tended to be displaced to the west and southwest of the observed position on an average by about 1 ∘ distance. Over eastern India (E) forecasts of light (heavy) rainfall events, like 10mmd-1 (20 and 40mmd-1) tend to be displaced to the south on an average by about 1 ∘ (southeast by 1 - 2 ∘). In all four regions, the relative contribution to total error due to displacement increases with increasing CRA threshold. These findings can be useful for forecasters and for model developers with regard to the model systematic errors associated with the monsoon rainfall over different parts of India.},
	number = {1},
	journal = {Journal of Earth System Science},
	author = {Sharma, Kuldeep and Ashrit, Raghavendra and Ebert, Elizabeth and Mitra, Ashis and Bhatla, R. and Iyengar, Gopal and Rajagopal, E. N.},
	month = feb,
	year = {2019},
	note = {Publisher: Springer},
	keywords = {CRA, NWP, Unified model, forecast verification},
}

@article{tian_performance_2016,
	title = {Performance metrics, error modeling, and uncertainty quantification},
	volume = {144},
	issn = {15200493},
	doi = {10.1175/MWR-D-15-0087.1},
	abstract = {A common set of statisticalmetrics has been used to summarize the performance of models or measurements- the most widely used ones being bias, mean square error, and linear correlation coefficient. They assume linear, additive, Gaussian errors, and they are interdependent, incomplete, and incapable of directly quantifying uncertainty. The authors demonstrate that these metrics can be directly derived from the parameters of the simple linear errormodel. Since a correct errormodel captures the full error information, it is argued that the specification of a parametric error model should be an alternative to the metrics-based approach. The error-modeling methodology is applicable to both linear and nonlinear errors, while themetrics are onlymeaningful for linear errors. In addition, the error model expresses the error structure more naturally, and directly quantifies uncertainty. This argument is further explained by highlighting the intrinsic connections between the performance metrics, the error model, and the joint distribution between the data and the reference.},
	number = {2},
	journal = {Monthly Weather Review},
	author = {Tian, Yudong and Nearing, Grey S. and Peters-Lidard, Christa D. and Harrison, Kenneth W. and Tang, Ling},
	year = {2016},
	note = {Publisher: American Meteorological Society},
	pages = {607--613},
}

@article{livadiotis_fitting_2013,
	title = {Fitting method based on correlation maximization: {Applications} in space physics},
	volume = {118},
	issn = {21699402},
	doi = {10.1002/jgra.50304},
	abstract = {We develop a new fitting method based on the maximization of the correlation between two curves or sets of discreet observations. We show that this correlation maximization fitting method is mathematically well defined under certain conditions. The key element is the sensitivity of the method - a measure of how localized the correlation maximum is. The most important advantage of the method is that it can be applied to disparate data sets that are expected to be correlated but not fitted to each other. The method is valuable in the analysis of space data sets from (1) physically remote sources that may have complicated and hidden causal linkages or (2) physically distinguished quantities that are reasonably connected. The derived possible relations can be examined by testing the correlation between their observational signals or other measurements. Finally, we examine data of density and temperature in the inner heliosheath, inferred from Interstellar Boundary Explorer observations, and show that the globally distributed flux of energetic neutral atoms represents a source plasma under isobaric thermodynamic processes. Key Points Development of the maximum correlation method of 2 sets of discreet observations A systematic way for applying the new method. We provide some examples. We explain why the method can be useful in space physics and geophysics. ©2013. American Geophysical Union. All Rights Reserved.},
	number = {6},
	journal = {Journal of Geophysical Research: Space Physics},
	author = {Livadiotis, G. and McComas, D. J.},
	month = jun,
	year = {2013},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {Correlation, Space data analysis},
	pages = {2863--2875},
}

@article{jankov_partition_2021,
	title = {Partition of {Forecast} {Error} into {Positional} and {Structural} {Components}},
	volume = {38},
	issn = {18619533},
	doi = {10.1007/s00376-021-0251-7},
	abstract = {Weather manifests in spatiotemporally coherent structures. Weather forecasts hence are affected by both positional and structural or amplitude errors. This has been long recognized by practicing forecasters (cf., e.g., Tropical Cyclone track and intensity errors). Despite the emergence in recent decades of various objective methods for the diagnosis of positional forecast errors, most routine verification or statistical post-processing methods implicitly assume that forecasts have no positional error. The Forecast Error Decomposition (FED) method proposed in this study uses the Field Alignment technique which aligns a gridded forecast with its verifying analysis field. The total error is then partitioned into three orthogonal components: (a) large scale positional, (b) large scale structural, and (c) small scale error variance. The use of FED is demonstrated over a month-long MSLP data set. As expected, positional errors are often characterized by dipole patterns related to the displacement of features, while structural errors appear with single extrema, indicative of magnitude problems. The most important result of this study is that over the test period, more than 50\% of the total mean sea level pressure forecast error variance is associated with large scale positional error. The importance of positional error in forecasts of other variables and over different time periods remain to be explored.},
	number = {6},
	journal = {Advances in Atmospheric Sciences},
	author = {Jankov, Isidora and Gregory, Scott and Ravela, Sai and Toth, Zoltan and Peña, Malaquías},
	month = jun,
	year = {2021},
	note = {Publisher: Science Press},
	keywords = {forecast error, orthogonal decomposition, positional, structural},
	pages = {1012--1019},
}

@article{keil_displacement-based_2007,
	title = {A displacement-based error measure applied in a regional ensemble forecasting system},
	volume = {135},
	issn = {00270644},
	doi = {10.1175/MWR3457.1},
	abstract = {Errors in regional forecasts often take the form of phase errors, where a forecasted weather system is displaced in space or time. For such errors, a direct measure of the displacement is likely to be more valuable than traditional measures. A novel forecast quality measure is proposed that is based on a comparison of observed and forecast satellite imagery from the Meteosat-7 geostationary satellite. The measure combines the magnitude of a displacement vector calculated with a pyramid matching algorithm and the local squared difference of observed and morphed forecast brightness temperature fields. Following the description of the method and its application for a simplified case, the measure is applied to regional ensemble forecasts for an episode of prefrontal summertime convection in Bavaria. It is shown that this new method provides a plausible measure of forecast error, which is consistent with a subjective ranking of ensemble members for a sample forecast. The measure is then applied to hourly images over a 36-h forecast period and compared with the bias and equitable threat score. The two conventional measures fail to provide any systematic distinction between different ensemble members, while the new measure identifies ensemble members of differing skill levels with a strong degree of temporal consistency. Using the displacement-based error measure, individual ensemble members are found to compare better with observations than either a short-term deterministic forecast or the ensemble mean throughout the convective period. © 2007 American Meteorological Society.},
	number = {9},
	journal = {Monthly Weather Review},
	author = {Keil, Christian and Craig, George C.},
	month = sep,
	year = {2007},
	pages = {3248--3259},
}

@article{gilleland_intercomparison_2009,
	title = {Intercomparison of spatial forecast verification methods},
	volume = {24},
	issn = {08828156},
	doi = {10.1175/2009WAF2222269.1},
	abstract = {Advancements in weather forecast models and their enhanced resolution have led to substantially improved and more realistic-appearing forecasts for some variables. However, traditional verification scores often indicate poor performance because of the increased small-scale variability so that the true quality of the forecasts is not always characterized well. As a result, numerous new methods for verifying these forecasts have been proposed. These new methods can mostly be classified into two overall categories: filtering methods and displacement methods. The filtering methods can be further delineated into neighborhood and scale separation, and the displacement methods can be divided into features based and field deformation. Each method gives considerably more information than the traditional scores, but it is not clear which method(s) should be used for which purpose. Averificationmethods intercomparison project has been established in order to glean a better understanding of the proposed methods in terms of their various characteristics and to determine what verification questions each method addresses. The study is ongoing, and preliminary qualitative results for the different approaches applied to different situations are described here. In particular, the various methods and their basic characteristics, similarities, and differences are described. In addition, several questions are addressed regarding the application of the methods and the information that they provide. These questions include (i) how the method(s) informperformance at different scales; (ii) how the methods provide information on location errors; (iii) whether the methods provide information on intensity errors and distributions; (iv) whether the methods provide information on structure errors; (v) whether the approaches have the ability to provide information about hits, misses, and false alarms; (vi) whether the methods do anything that is counterintuitive; (vii) whether the methods have selectable parameters and how sensitive the results are to parameter selection; (viii) whether the results can be easily aggregated across multiple cases; (ix) whether the methods can identify timing errors; and (x) whether confidence intervals and hypothesis tests can be readily computed. © 2009 American Meteorological Society.},
	number = {5},
	journal = {Weather and Forecasting},
	author = {Gilleland, Eric and Ahijevych, David and Brown, Barbara G. and Casati, Barbara and Ebert, Elizabeth E.},
	month = oct,
	year = {2009},
	pages = {1416--1430},
}

@techreport{gladney_computer-assisted_1969,
	title = {Computer-{Assisted} {Gas}-{Liquid} {Chromatography}},
	url = {https://pubs.acs.org/sharingguidelines},
	abstract = {RESULTS The reciprocal time computer was tested by applying voltage signals of known input slopes to the voltage interval sensor (Figure 1) to simulate outputs of typical reaction monitor-signal modifier systems. T able I shows the results of measurements of input rates from 5 mV/sec to 20 V/sec. The instrument was calibrated with a 50 mV/sec input rate by adjusting Pi (Figure 3) to give the desired readout. This adjustment is necessary because of component tolerances in the linear sweep generator. The entire range of input rates shown in Table I was measured with only one instrument change. For rates {\textgreater} 1 V/sec, the scaling factor F and integrator input voltage Ein were changed by factors of ten. At the highest rate measured, the actual time interval (At) was 5 msec. With this short At, the time interval to voltage converter output was only 5 mV. Faster rates could be measured by increasing Ein. However, the analog gates used to switch Ein (Figure 3) are only capable of switching up to 5 V. Table II shows results for the determination of phosphate. For this procedure, the input voltage to the reciprocal time computer had a negative slope, and the NAND gates shown in Figure 2 were used. With the 100\% transmittance set at 1.000 V, recorded curves enabled the selection of V{\textbackslash} and AV. Vi was set at 0.975 volt and AV was adjusted to be 25 mV. Because of the slowness of the reaction, Etn was reduced to 50 mV to avoid limiting OAi. Potentiometer Pi (Figure 3) was adjusted to give a direct concentration readout using a phosphate standard of 5 ppm P. At the lowest concentration used, the measurement time At was about 120 sec. The results shown in Tables I and II are typical of many which have been obtained. The reciprocal time computer was constructed using the same digital modules as used in the fixed time readout system described earlier (2). By a simple change of circuit cards and intercard connections, the same basic modules can be used for both variable time and fixed time methods. This allows the user of reaction rate methods to have both automated readout systems readily available for use with different reactions. The use of computers for data acquisition and analysis in gas-liquid chromatography is becoming increasingly widespread. We describe an implementation of computer-assisted chromatography within a general-purpose time-shared laboratory automation system. Particular attention is paid to a method of avoiding timing conflicts at the computer, to an inexpensive method of providing a computer-to-instrument interface , and to an economical method of curve-fitting to resolve overlapping skewed peaks. It is anticipated that many of the methods and some of the computer programs will be directly applicable to various spec-troscopic experiments. The recent implementation in this laboratory of a time-shared laboratory automation system for spectroscopic-type instruments (I) has made possible on-line data collection and computation for a high-sensitivity gas chromatograph. We wish to describe our system which includes two novel features: a simple and inexpensive interface for time-shared communications between an instrument and a remote computer, and a method for the resolution of overlapping skewed peaks, involving a least-squares curve fitting procedure. In Figure 1 is shown a simple calculated curve compared with the observed chromatogram. In general, the methods used to schedule the communications between a process-control computer and several instruments are largely selected by considerations of economy and of the relative requirements of the instruments being connected. The choice of method becomes particularly difficult if the full requirements of the laboratory cannot be anticipated which is usually the case. It is also conceded that an extremely desirable, if not absolutely necessary, feature be that each experiment can be programmed and run inde-(1). M. Gladney, J. Comp. Physics, 2, 255 (1968). Figure 1. A poorly resolved chromatogram of 30-60 °C petroleum ether showing computer fitted peaks pendently. From the point of view of the experiment and experimenter, it is perhaps conceptually simpler if the computer is enslaved to the laboratory instrument. However, at the computer, this arrangement implies not one master, but several, usually with conflicting requirements. There are at least three possible resolutions of the problem: To dedicate a computer or a sub-computer (commonly called a data channel) either permanently or temporarily to each experiment (2). To employ a computer which is so fast relative to the time-sensitive experiments that the unavoidable inter-(2) T.},
	author = {Gladney, M and Dowden, B F and Swalen, J D and Lusebrink, R and Sederholm, C H},
	year = {1969},
	note = {Volume: 13
Issue: 7},
	pages = {65},
}

@techreport{grushka_characterization_1970,
	title = {Characterization of {Exponentially} {Modified} {Gaussian} {Peaks} in {Chromatography} {MOMENT} {ANALYSIS} {The} moments, {Mn}, of {Equation} 1 can be found by employ-ing the {Laplace} (or the {Fourier}) transformation. {Use} is being},
	url = {https://pubs.acs.org/sharingguidelines},
	abstract = {The exponentially modified Gaussian peak has attracted attention recently in computer deconvolution of chromatographic peaks. This peak shape model has some theoretical justification. In this paper, we investigate the behavior of the skew and excess and of the second derivative of that shape in order to examine their utility in the analysis of strongly overlapped chromatographic peaks. The study showed that, indeed, moment as well as slope analysis can be beneficial in characterizing double peaks. In addition, moment analysis of a single peak can indicate the magnitude of extra-column effects. If the time constant of these effects is known, slope analysis might be preferred because of its simplicity, to moment analysis. The problem of strongly overlapped chromatographic peaks can be dealt with by employing statistical moments or slope analysis (1, 2). In the former (/), deviations in the peak shape, as obtained from the skew and excess, between a single peak and a double peak in the form of a single band are used as the indicator of a strongly overlapped system. In the latter (2), deviation in the second derivative behavior, when manipulated appropriately, ascertains the existence of double peaks. In discussing these two methods, the mathematical models which were described were developed mainly because of their computational ease. Although the models were realistic, they were chosen arbitrarily (except for the "kinetic tailing" model). A better theoretical model for the chro-matographic peak shape is thus needed. Exponentially modified Gaussian peaks, that is to say, a Gaussian convoluted with an exponential decay function, seem to fill that need. More than a decade ago, Schmauch (J) as well as Johnson and Stross (4) recognized that instrumental contribution, such as detector dead volume, will modify exponentially the chromatographic peak. Esser (5), in investigating spectrophotometric detectors, reached the same conclusion. Sternberg, in his comprehensive review on extra-column effects (6), shows that various dead volume contributions alter the peak exponentially. He indicates that each particular extra-column effect has a certain time constant associated with it. The magnitude of this time constant determines the extent to which the peak is distorted. More recently, Gladney et al. (7) used the exponentially modified Gaussian in numerical deconvolution techniques. They have shown that experimental peaks can be described reasonably well by this model. The same shape was used by Littlewood and coworkers (8) in their deconvolution method. In addition they have briefly investigated the effect of mixing chambers and have concluded that these affect the skewness of the peak. Me William and Bolton (9) derived the exponentially convoluted Gaussian in their discussion on instrumental peak distortion, and its effect on the resolution. More recently (10) they used this peak model in further discussion on the area recovery of two partially overlapped peaks. With this interest in exponentially modified Gaussians, it seems useful to investigate the theoretical behavior of two strongly overlapped (i.e., resolution {\textgreater}0.5) such peaks. This is particularly so since this peak model has some theoretical and experimental (7,8) justification. The exponentially modified Gaussian peak is defined by the following convolute integral. m = ray/2ir J0 Or, alternatively /(0-{\textasciicircum}exp (t-tB-t'f-' exp 2 1 2 exp T () {\textbackslash}-(2) where Z = [(/-tB)¡a-/ ]1/ /2 in Equation 2, A is the peak amplitude, is the variance of the Gaussian, tR is the center of gravity of the Gaussian, r is the time constant of the exponential modifier (which can be attributed, among others, to extra column contribution) and t' and x are dummy variables of integration. Some of the properties of these expressions were discussed by Gladney et al. (7) and by McWil-liam and Bolton (9). It should be noted that the area of this convolution expression is equal to that of a pure Gaussian and that the maximum of the peak always falls on the Gaussian which is being modified (9). Moreover, the asymmetry of the peak depends on the ratio /. Representative examples of peaks with various / values can be found elsewhere (7-9). Before proceeding to investigate, theoretically, the behavior of the skew and excess, as well as that of the second derivative, of a single and double convoluted peaks, some comments should be made concerning some practical aspects of these analyses. The experimental methodology of obtaining the moments or the derivatives was already discussed by us (1, 2). The experimental accuracies reported are well within the reach of many laboratories, especially when calibration curves are used as standards.},
	author = {Grushka, E and Monecelli, G M and Schmauch, Ibid ; ) L J and Esser, . ; R J E and Sternberg, ; ) J C},
	year = {1970},
	note = {Publication Title: Advances in Chromatography
Volume: 42
Issue: 2},
	pages = {883},
}

@techreport{grushka_characterization_1970-1,
	title = {Characterization of {Exponentially} {Modified} {Gaussian} {Peaks} in {Chromatography} {MOMENT} {ANALYSIS} {The} moments, {Mn}, of {Equation} 1 can be found by employ-ing the {Laplace} (or the {Fourier}) transformation. {Use} is being},
	url = {https://pubs.acs.org/sharingguidelines},
	abstract = {The exponentially modified Gaussian peak has attracted attention recently in computer deconvolution of chromatographic peaks. This peak shape model has some theoretical justification. In this paper, we investigate the behavior of the skew and excess and of the second derivative of that shape in order to examine their utility in the analysis of strongly overlapped chromatographic peaks. The study showed that, indeed, moment as well as slope analysis can be beneficial in characterizing double peaks. In addition, moment analysis of a single peak can indicate the magnitude of extra-column effects. If the time constant of these effects is known, slope analysis might be preferred because of its simplicity, to moment analysis. The problem of strongly overlapped chromatographic peaks can be dealt with by employing statistical moments or slope analysis (1, 2). In the former (/), deviations in the peak shape, as obtained from the skew and excess, between a single peak and a double peak in the form of a single band are used as the indicator of a strongly overlapped system. In the latter (2), deviation in the second derivative behavior, when manipulated appropriately, ascertains the existence of double peaks. In discussing these two methods, the mathematical models which were described were developed mainly because of their computational ease. Although the models were realistic, they were chosen arbitrarily (except for the "kinetic tailing" model). A better theoretical model for the chro-matographic peak shape is thus needed. Exponentially modified Gaussian peaks, that is to say, a Gaussian convoluted with an exponential decay function, seem to fill that need. More than a decade ago, Schmauch (J) as well as Johnson and Stross (4) recognized that instrumental contribution, such as detector dead volume, will modify exponentially the chromatographic peak. Esser (5), in investigating spectrophotometric detectors, reached the same conclusion. Sternberg, in his comprehensive review on extra-column effects (6), shows that various dead volume contributions alter the peak exponentially. He indicates that each particular extra-column effect has a certain time constant associated with it. The magnitude of this time constant determines the extent to which the peak is distorted. More recently, Gladney et al. (7) used the exponentially modified Gaussian in numerical deconvolution techniques. They have shown that experimental peaks can be described reasonably well by this model. The same shape was used by Littlewood and coworkers (8) in their deconvolution method. In addition they have briefly investigated the effect of mixing chambers and have concluded that these affect the skewness of the peak. Me William and Bolton (9) derived the exponentially convoluted Gaussian in their discussion on instrumental peak distortion, and its effect on the resolution. More recently (10) they used this peak model in further discussion on the area recovery of two partially overlapped peaks. With this interest in exponentially modified Gaussians, it seems useful to investigate the theoretical behavior of two strongly overlapped (i.e., resolution {\textgreater}0.5) such peaks. This is particularly so since this peak model has some theoretical and experimental (7,8) justification. The exponentially modified Gaussian peak is defined by the following convolute integral. m = ray/2ir J0 Or, alternatively /(0-{\textasciicircum}exp (t-tB-t'f-' exp 2 1 2 exp T () {\textbackslash}-(2) where Z = [(/-tB)¡a-/ ]1/ /2 in Equation 2, A is the peak amplitude, is the variance of the Gaussian, tR is the center of gravity of the Gaussian, r is the time constant of the exponential modifier (which can be attributed, among others, to extra column contribution) and t' and x are dummy variables of integration. Some of the properties of these expressions were discussed by Gladney et al. (7) and by McWil-liam and Bolton (9). It should be noted that the area of this convolution expression is equal to that of a pure Gaussian and that the maximum of the peak always falls on the Gaussian which is being modified (9). Moreover, the asymmetry of the peak depends on the ratio /. Representative examples of peaks with various / values can be found elsewhere (7-9). Before proceeding to investigate, theoretically, the behavior of the skew and excess, as well as that of the second derivative, of a single and double convoluted peaks, some comments should be made concerning some practical aspects of these analyses. The experimental methodology of obtaining the moments or the derivatives was already discussed by us (1, 2). The experimental accuracies reported are well within the reach of many laboratories, especially when calibration curves are used as standards.},
	author = {Grushka, E and Monecelli, G M and Schmauch, Ibid ; ) L J and Esser, . ; R J E and Sternberg, ; ) J C},
	year = {1970},
	note = {Publication Title: Advances in Chromatography
Volume: 42
Issue: 2},
	pages = {883},
}

@article{hao_exponential_2016,
	title = {Exponential decay of spatial correlation in driven diffusive system: {A} universal feature of macroscopic homogeneous state},
	volume = {6},
	issn = {20452322},
	doi = {10.1038/srep19652},
	abstract = {Driven diffusive systems have been a paradigm for modelling many physical, chemical, and biological transport processes. In the systems, spatial correlation plays an important role in the emergence of a variety of nonequilibrium phenomena and exhibits rich features such as pronounced oscillations. However, the lack of analytical results of spatial correlation precludes us from fully understanding the effect of spatial correlation on the dynamics of the system. Here we offer precise analytical predictions of the spatial correlation in a typical driven diffusive system, namely facilitated asymmetric exclusion process. We find theoretically that the correlation between two sites decays exponentially as their distance increases, which is in good agreement with numerical simulations. Furthermore, we find the exponential decay is a universal property of macroscopic homogeneous state in a broad class of 1D driven diffusive systems. Our findings deepen the understanding of many nonequilibrium phenomena resulting from spatial correlation in driven diffusive systems.},
	journal = {Scientific Reports},
	author = {Hao, Qing Yi and Jiang, Rui and Hu, Mao Bin and Jia, Bin and Wang, Wen Xu},
	month = jan,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
}

@article{hoffman_distortion_1995,
	title = {Distortion {Representation} of {Forecast} {Errors}},
	volume = {123},
	issn = {0027-0644},
	url = {http://journals.ametsoc.org/doi/10.1175/1520-0493(1995)123<2758:DROFE>2.0.CO;2},
	doi = {10.1175/1520-0493(1995)123<2758:DROFE>2.0.CO;2},
	number = {9},
	journal = {Monthly Weather Review},
	author = {Hoffman, Ross N. and Liu, Zheng and Louis, Jean-Francois and Grassoti, Christopher},
	month = sep,
	year = {1995},
	pages = {2758--2770},
}

@article{nachamkin_mesoscale_2004,
	title = {Mesoscale {Verification} {Using} {Meteorological} {Composites}},
	volume = {132},
	issn = {0027-0644},
	url = {http://journals.ametsoc.org/doi/10.1175/1520-0493(2004)132<0941:MVUMC>2.0.CO;2},
	doi = {10.1175/1520-0493(2004)132<0941:MVUMC>2.0.CO;2},
	abstract = {Mesoscale models are often used to explicitly predict discrete, highly structured phenomena. Information regarding the ability of the model to predict events as coherent entities is thus a useful statement of performance. Observational constraints are a significant problem, though, as the shape, size, and intensity of any given event are often only partially known. Composite techniques offer an attractive approach because the full deterministic information about any one event need not be known. If enough quasi-random observations of a distribution of events exist, bulk properties of the distributions of forecasts and observations can be estimated. Composites are also useful in that the verification measures are based on conditional samples of events. Sample distributions contingent on event existence in either the forecasts or the observations can be compared to one another. A verification technique in which meteorological events are located and composited on a relative grid centered on each event is described herein. This technique is described and demonstrated by comparing the 27-km Naval Research Laboratory's Coupled Ocean/Atmosphere Mesoscale Prediction System (COAMPS) mistral wind forecasts to the Special Sensor Microwave Imager (SSM/I) observations for a 1-yr period. Diagnostic information regarding the forecast reliability, error type, and error spatial characteristics are derived. Also, statistics from the conditional distributions of both the observed and predicted events are compared. The difference between the two conditional biases (CBD) is found to reveal valuable information regarding the contribution of false alarms and missed forecasts to the forecast errors. The results indicate the mistral is remarkably predictable with high pattern correlations out to 66 h.},
	number = {4},
	journal = {Monthly Weather Review},
	author = {Nachamkin, Jason E.},
	month = apr,
	year = {2004},
	pages = {941--955},
}

@article{skok_new_2022,
	title = {A {New} {Spatial} {Distance} {Metric} for {Verification} of {Precipitation}},
	volume = {12},
	issn = {20763417},
	doi = {10.3390/app12084048},
	abstract = {Precipitation is an essential meteorological variable affecting the biosphere and human societies. At the same time, precipitation is notoriously difficult to predict and verify. A new spatial distance metric for verification of precipitation is presented. It is called the Precipitation Smoothing Distance (PSD). The aim was to develop a measure that would provide a good and meaningful approximation of the displacement of precipitation events in the two fields. An estimate of spatial displacement is very appealing for forecast interpretation because it is easy to understand and mimics how humans tend to judge fields by eye. Contrary to most other distance metrics, the new metric does not require thresholding and can thus be used to analyze binary and non-binary fields (e.g., continuous or multi-level). The analysis of idealized situations showed that the new metric provides a meaningful approximation of the displacement. Typically the estimate of displacement provided by PSD was better than the results provided by most other metrics. The measure is also not overly sensitive to noise, its results are directly related to the actual displacements of precipitation events, and the events with a larger magnitude have a bigger influence on the resulting value. The analysis of ECMWF precipitation forecasts over Europe and North Africa confirmed that the new metric provides a meaningful approximation of the.},
	number = {8},
	journal = {Applied Sciences (Switzerland)},
	author = {Skok, Gregor},
	month = apr,
	year = {2022},
	note = {Publisher: MDPI},
	keywords = {PSD metric, precipitation, spatial displacement, verification},
}

@incollection{wilks_forecast_2019,
	title = {Forecast {Verification}},
	booktitle = {Statistical {Methods} in the {Atmospheric} {Sciences}},
	publisher = {Elsevier},
	author = {Wilks, Daniel S.},
	year = {2019},
	doi = {10.1016/b978-0-12-815823-4.00009-2},
	pages = {369--483},
}

@article{casati_new_2004,
	title = {A new intensity-scale approach for the verification of spatial precipitation forecasts},
	volume = {11},
	issn = {13504827},
	doi = {10.1017/S1350482704001239},
	abstract = {A new intensity-scale method for verifying spatial precipitation forecasts is introduced. The technique provides a way of evaluating the forecast skill as a function of precipitation rate intensity and spatial scale of the error. Six selected case studies of the UK Met Office now-casting system NIMROD are used to illustrate the method. The forecasts are assessed using the Mean Squared Error (MSE) skill score of binary images, obtained from the forecasts and analyses by thresholding at different precipitation rate intensities. The skill score is decomposed on different spatial scales using a two-dimensional discrete Haar wavelet decomposition of binary error images. The forecast skill can then be evaluated in terms of precipitation rate intensity and spatial scale. The technique reveals that loss of forecast skill in NIMROD is predominantly due to small spatial scale ({\textless} 40 km) errors of more intense events. The technique is capable of isolating specific intensity-scale errors for individual cases. As an example, in one of the case studies the displacement error of an incorrectly advected storm is well detected by a minimum negative skill score occurring at the 160 km spatial scale for thresholds between 1/2 and 4 mm/h.},
	number = {2},
	journal = {Meteorological Applications},
	author = {Casati, B. and Ross, G. and Stephenson, D. B.},
	year = {2004},
	note = {Publisher: Cambridge University Press},
	pages = {141--154},
}

@article{mishra_sharma_resdeepd_2022,
	title = {{ResDeepD}: {A} residual super-resolution network for deep downscaling of daily precipitation over {India}},
	volume = {1},
	issn = {2634-4602},
	url = {https://www.cambridge.org/core/product/identifier/S2634460222000231/type/journal_article},
	doi = {10.1017/eds.2022.23},
	abstract = {{\textless}p{\textgreater} In the twenty-first century, machine learning and deep learning have been successfully used to find hidden information from coarse-grained data in various domains. In Computer Vision, scientists have used neural networks to identify hidden pixel-level information from low-resolution (LR) image data. This approach of estimating high-resolution (HR) information from LR data is called the super-resolution (SR) approach. This approach has been borrowed by climate scientists to downscale coarse-level measurements of climate variables to obtain their local-scale projections. Climate variables are spatial in nature and can be represented as images where each pixel denotes a grid point where the variables can be measured. We can apply the deep learning-based SR techniques on such “images” for statistical downscaling of such variables. This approach of downscaling can be termed as deep downscaling. In this work, we have tried to make HR projection of the Indian summer monsoon rainfall by using a novel deep residual network called ResDeepD. The aim is to downscale the 1 $^{\textrm{0}}$ × 1 $^{\textrm{0}}$ low LR precipitation data to get the values at 0.25 $^{\textrm{0}}$ × 0.25 $^{\textrm{0}}$ resolution. The proposed model uses a series of skip connections across residual blocks to give better results as compared to the existing models like super-resolution convolutional neural network, DeepSD, and Nest-UNet that have been used previously for this task. We have also examined the model’s performance for downscaling rainfall during some extreme climatic events like cyclonic storms and deep depression and found that the model performs better than the existing models. {\textless}/p{\textgreater}},
	journal = {Environmental Data Science},
	author = {Mishra Sharma, Sumanta Chandra and Mitra, Adway},
	month = nov,
	year = {2022},
	keywords = {★},
	pages = {e19},
}

@article{sha_deep-learning-based_2020,
	title = {Deep-learning-based gridded downscaling of surface meteorological variables in complex terrain. {Part} {II}: {Daily} precipitation},
	volume = {59},
	issn = {15588432},
	doi = {10.1175/JAMC-D-20-0058.1},
	abstract = {Statistical downscaling (SD) derives localized information from larger-scale numerical models. Convolutional neural networks (CNNs) have learning and generalization abilities that can enhance the downscaling of gridded data (Part I of this study experimented with 2-m temperature). In this research, we adapt a semantic-segmentation CNN, called UNet, to the downscaling of daily precipitation in western North America, from the low resolution (LR) of 0.25° to the high resolution (HR) of 4-km grid spacings. We select LR precipitation, HR precipitation climatology, and elevation as inputs; train UNet over the subset of the south-and central-western United States using Parameter–Elevation Regressions on Independent Slopes Model (PRISM) data from 2015 to 2018, and test it independently in all available domains from 2018 to 2019. We proposed an improved version of UNet, which we call Nest-UNet, by adding deep-layer aggregation and nested skip connections. Both the original UNet and Nest-UNet show generalization ability across different regions and outperform the SD baseline (bias-correction spatial disag-gregation), with lower downscaling error and more accurate fine-grained textures. Nest-UNet also shares the highest amount of information with station observations and PRISM, indicating good ability to reduce the uncertainty of HR downscaling targets.},
	number = {12},
	journal = {Journal of Applied Meteorology and Climatology},
	author = {Sha, Yingkai and Gagne, David John and West, Gregory and Stull, Roland},
	month = dec,
	year = {2020},
	note = {Publisher: American Meteorological Society},
	keywords = {Deep learning, Error analysis, Interpolation schemes, Model evaluation/performance, Model output statistics, Neural networks},
	pages = {2075--2092},
}

@incollection{maraun_model_2017,
	title = {Model {Output} {Statistics}},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	booktitle = {Statistical {Downscaling} and {Bias} {Correction} for {Climate} {Research}},
	publisher = {Cambridge University Press},
	author = {Maraun, Douglas and Widmann, Martin},
	month = dec,
	year = {2017},
	doi = {10.1017/9781107588783.013},
	pages = {170--200},
}

@article{maraun_value_2015,
	title = {{VALUE}: {A} framework to validate downscaling approaches for climate change studies},
	volume = {3},
	issn = {23284277},
	doi = {10.1002/2014EF000259},
	abstract = {VALUE is an open European network to validate and compare downscaling methods for climate change research. VALUE aims to foster collaboration and knowledge exchange between climatologists, impact modellers, statisticians, and stakeholders to establish an interdisciplinary downscaling community. A key deliverable of VALUE is the development of a systematic validation framework to enable the assessment and comparison of both dynamical and statistical downscaling methods. In this paper, we present the key ingredients of this framework. VALUE's main approach to validation is user- focused: starting from a specific user problem, a validation tree guides the selection of relevant validation indices and performance measures. Several experiments have been designed to isolate specific points in the downscaling procedure where problems may occur: what is the isolated downscaling skill? How do statistical and dynamical methods compare? How do methods perform at different spatial scales? Do methods fail in representing regional climate change? How is the overall representation of regional climate, including errors inherited from global climate models? The framework will be the basis for a comprehensive community-open downscaling intercomparison study, but is intended also to provide general guidance for other validation studies.},
	number = {1},
	journal = {Earth's Future},
	author = {Maraun, Douglas and Widmann, Martin and Gutiérrez, José M. and Kotlarski, Sven and Chandler, Richard E. and Hertig, Elke and Wibig, Joanna and Huth, Radan and Wilcke, Renate A.I.},
	month = jan,
	year = {2015},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Bias Correction, Downscaling, Dynamical Downscaling, Regional Climate Modelling, Statistical Downscaling, Validation},
	pages = {1--14},
}

@inproceedings{tartaglione_searching_2008,
	title = {Searching for systematic location errors of quantitative precipitation forecasts over the {Calabria} region},
	volume = {15},
	doi = {10.1002/met.55},
	abstract = {This article statistically analyses the location errors of the precipitation patterns forecast by three limited area models, namely the Fifth-Generation NCAR/Penn State Mesoscale Model (MM5), the QUADRICS BOlogna Limited Area Model (QBOLAM) and the Regional Atmospheric Modelling System (RAMS), over the Calabria region (Italy) for the period October 2000-May 2002. Contiguous rain area (CRA) analysis is the diagnostic tool used to assess and quantify the position errors of the precipitation forecasts with respect to the observed precipitation patterns. Observation gridded analyses were obtained by means of the Barnes algorithm on the available rain gauge observations. Moreover, an approach to measure the quality of precipitation forecasts routinely by means of a global indicator called CRA Mean Shift (CMS) that summarizes the CRA verification outcomes is proposed. The CMS index would represent a statistical indicator of model quality in forecasting the correct positions of precipitation patterns. The model's tendency to misplace, the forecast precipitation patterns towards a particular direction was tested by using a bootstrap procedure. All models seem to show statistically poor abilities in forecasting the correct precipitation pattern position over the verification domain considered. As far as the tendency towards a particular direction is concerned, only the RAMS model seems to show a systematic horizontal misplacement of precipitation patterns towards a particular direction. Copyright © 2008 Royal Meteorological Society.},
	booktitle = {Meteorological {Applications}},
	publisher = {John Wiley and Sons Ltd},
	author = {Tartaglione, Nazario and Mariani, Stefano and Casaioli, Marco and Accadia, Christophe and Federico, Stefano and Michaelides, Silas Chr},
	year = {2008},
	note = {Issue: 1
ISSN: 14698080},
	keywords = {Forecast, Precipitation, Verification},
	pages = {85--95},
}

@article{venugopal_new_2005,
	title = {A new metric for comparing precipitation patterns with an application to ensemble forecasts},
	volume = {110},
	issn = {01480227},
	doi = {10.1029/2004JD005395},
	abstract = {Ensemble forecasting can be seen as serving two purposes: (1) by comparison of the control and ensemble members to the observed precipitation field, one can assess the forecast performance probabilistically; and (2) by comparison of ensemble members to the control forecast, one can assess the "diversity" of an ensemble and quantify the uncertainty of the forecast. Both problems are grounded to the basic requirement of being able to compare spatially nonhomogeneous, intermittent fields and come up with low-dimensional metrics that can summarize this comparison. Several standard metrics exist (e.g., root mean square error (RMSE), Brier score, and equitable threat score (EqTh)) and are adopted in many operational studies. We studied (1) a fine-scale ensemble precipitation forecast produced from the Advanced Regional Prediction System (ARPS) and (2) forecasts from multiple models (e.g., the 1998 Storm and Mesoscale Ensemble Experiment (SAMEX '98)) for the purpose of exploring how the selection of the performance metric can affect inferences about the quality and uncertainty of a forecast. We propose a new measure called forecast quality index, which combines image analysis and nonlinear shape comparison features, and we show that it is a more robust and informative metric compared to traditional metrics such as RMSE and EqTh. Copyright 2005 by the American Geophysical Union.},
	number = {8},
	journal = {Journal of Geophysical Research D: Atmospheres},
	author = {Venugopal, V. and Basu, S. and Foufoula-Georgiou, E.},
	month = apr,
	year = {2005},
	pages = {1--11},
}

@article{brocker_ensemble_2008,
	title = {From ensemble forecasts to predictive distribution functions},
	volume = {60 A},
	issn = {02806495},
	doi = {10.1111/j.1600-0870.2008.00333.x},
	abstract = {The translation of an ensemble of model runs into a probability distribution is a common task in model-based prediction. Common methods for such ensemble interpretations proceed as if verification and ensemble were draws from the same underlying distribution, an assumption not viable for most, if any, real world ensembles. An alternative is to consider an ensemble as merely a source of information rather than the possible scenarios of reality. This approach, which looks for maps between ensembles and probabilistic distributions, is investigated and extended. Common methods are revisited, and an improvement to standard kernel dressing, called 'affine kernel dressing' (AKD), is introduced. AKD assumes an affine mapping between ensemble and verification, typically not acting on individual ensemble members but on the entire ensemble as a whole, the parameters of this mapping are determined in parallel with the other dressing parameters, including a weight assigned to the unconditioned (climatological) distribution. These amendments to standard kernel dressing, albeit simple, can improve performance significantly and are shown to be appropriate for both overdispersive and underdispersive ensembles, unlike standard kernel dressing which exacerbates over dispersion. Studies are presented using operational numerical weather predictions for two locations and data from the Lorenz63 system, demonstrating both effectiveness given operational constraints and statistical significance given a large sample. © Journal compilation © 2008 Blackwell Munksgaard.},
	number = {4},
	journal = {Tellus, Series A: Dynamic Meteorology and Oceanography},
	author = {Bröcker, Jochen and Smith, Leonard A.},
	month = aug,
	year = {2008},
	pages = {663--678},
}

@article{brocker_scoring_2007,
	title = {Scoring probabilistic forecasts: {The} importance of being proper},
	volume = {22},
	issn = {08828156},
	doi = {10.1175/WAF966.1},
	abstract = {Questions remain regarding how the skill of operational probabilistic forecasts is most usefully evaluated or compared, even though probability forecasts have been a long-standing aim in meteorological forecasting. This paper explains the importance of employing proper scores when selecting between the various measures of forecast skill. It is demonstrated that only proper scores provide internally consistent evaluations of probability forecasts, justifying the focus on proper scores independent of any attempt to influence the behavior of a forecaster. Another property of scores (i.e., locality) is discussed. Several scores are examined in this light. There is, effectively, only one proper, local score for probability forecasts of a continuous variable. It is also noted that operational needs of weather forecasts suggest that the current concept of a score may be too narrow; a possible generalization is motivated and discussed in the context of propriety and locality. © 2007 American Meteorological Society.},
	number = {2},
	journal = {Weather and Forecasting},
	author = {Bröcker, Jochen and Smith, Leonard A.},
	month = apr,
	year = {2007},
	pages = {382--388},
}

@article{peirolo_information_2011,
	title = {Information gain as a score for probabilistic forecasts},
	volume = {18},
	issn = {14698080},
	doi = {10.1002/met.188},
	abstract = {A measure of the information added by a probabilistic forecast to that contained in the climatological distribution is presented in this paper. This measure, called information gain, is mathematically closely related to the traditional ignorance score, but is more intuitive. Its advantages over other scores for probabilistic forecasts are also shown. The information gain score is tested on ECMWF ensemble forecasts of 500 hPa geopotential and 850 hPa temperature. The trends observed are in good agreement with those seen in other verification measures applied to the same data. In particular, the information gain decays with increasing lead time and increases over the years, in agreement with the improvement of the model. © 2010 Royal Meteorological Society.},
	number = {1},
	journal = {Meteorological Applications},
	author = {Peirolo, Riccardo},
	year = {2011},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Forecast verification, Information theory, Intuitive accuracy measure, ★},
	pages = {9--17},
}

@article{boe_statistical_2007,
	title = {Statistical and dynamical downscaling of the {Seine} basin climate for hydro-meteorological studies},
	volume = {27},
	issn = {08998418},
	doi = {10.1002/joc.1602},
	abstract = {Two downscaling methods designed for the study of the hydrological impact of climate change on the Seine basin in France are tested for present climate. First, a multivariate statistical downscaling (SD) methodology based on weather typing and conditional resampling is described. Then, a bias correction technique for dynamical downscaling based on quantile-quantile mapping is introduced. To evaluate the end-to-end SD methodology, the atmospheric forcing derived from the large-scale circulation (LSC) of the ERA40 reanalysis by SD is used to force a hydrological model. Simulated discharges reproduce historical values reasonably well. Next, the dynamical and statistical approaches are compared using the Météo-France ARPEGE general circulation model in a variable resolution configuration (resolution around 60 km over France). The ARPEGE simulation is downscaled using the two methodologies, and hydrological simulations are performed. Regarding downscaled temperature and precipitation, the statistical approach is more efficient in reproducing the temporal and spatial autocorrelation properties. The simulated river discharges from the two approaches are nevertheless very similar: the two methods reproduce well the seasonal cycle and the daily distribution of streamflows. Finally, the results of the study are discussed from a practical impact study perspective. Copyright © 2007 Royal Meteorological Society.},
	number = {12},
	journal = {International Journal of Climatology},
	author = {Boé, J. and Terray, L. and Habets, F. and Martin, E.},
	month = oct,
	year = {2007},
	keywords = {Bias correction, Climate change, Dynamical downscaling, Hydrological impacts, Regional climate, Statistical downscaling},
	pages = {1643--1655},
}

@techreport{gneiting_calibration_2014,
	address = {Reading, UK},
	title = {Calibration of {Medium}-{Range} {Weather} {Forecasts}},
	url = {http://www.ecmwf.int/publications/},
	abstract = {Statistical postprocessing techniques serve to improve the quality of numerical weather forecasts, as they seek to generate calibrated and sharp predictive distributions of future weather quantities. This document reviews the state of the art in statistical postprocessing, with focus on potential applications to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s Integrated Forecasting System (IFS). At present, a recommended way to proceed is to apply well established, state of the art postprocessing techniques, such as nonhomogeneous regression or Bayesian model averaging , to each univariate weather quantity separately, with training data usefully augmented by reforecast datasets. Areas requiring further research are identified, in particular the suitable size and efficient use of reforecast datasets, and the generation and evaluation of probabilistic forecasts of combined events and spatio-temporal weather trajectories, thereby addressing spatial, temporal and cross-variable dependence structures.},
	institution = {Gneiting, Tilmann. Calibration of medium-range weather forecasts. Reading, UK: European Centre for Medium-Range Weather Forecasts},
	author = {Gneiting, Tilmann},
	year = {2014},
}

@incollection{wilks_statistical_2019,
	title = {Statistical {Forecasting}},
	booktitle = {Statistical {Methods} in the {Atmospheric} {Sciences}},
	publisher = {Elsevier},
	author = {Wilks, Daniel S.},
	year = {2019},
	doi = {10.1016/b978-0-12-815823-4.00007-9},
	pages = {235--312},
}

@incollection{doswell_severe_2001,
	title = {Severe {Convective} {Storms}—{An} {Overview}},
	abstract = {In general, convection, refers to the transport of some property by fluid movement, most often with reference to heat transport. As such, it is one of the three main processes by which heat is transported: radiation, conduction, and convection. Meteorologists...},
	booktitle = {Severe {Convective} {Storms}},
	publisher = {American Meteorological Society},
	author = {Doswell, Charles A.},
	year = {2001},
	doi = {10.1007/978-1-935704-06-5_1},
	pages = {1--26},
}

@techreport{woodhams_d-r74-storm_2022,
	title = {D-{R7}.4-{Storm} predictability in the context of {CP} {Ensembles}},
	url = {https://projects.},
	author = {Woodhams, Beth J and Birch, Cathryn E},
	year = {2022},
}

@article{brier_verification_1950,
	title = {{VERIFICATION} {OF} {FORECASTS} {EXPRESSED} {IN} {TERMS} {OF} {PROBABILITY}},
	volume = {78},
	issn = {0027-0644},
	url = {http://journals.ametsoc.org/doi/10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
	doi = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
	number = {1},
	journal = {Monthly Weather Review},
	author = {BRIER, GLENN W.},
	month = jan,
	year = {1950},
	pages = {1--3},
}

@techreport{waggoner_lecture_2017,
	title = {Lecture 23 {Proper} {Scoring} {Rules} and {Prediction} {Markets}},
	abstract = {Today we look at the question: How can we incentivize an agent or group of agents to make an accurate prediction? This is actually a simpler question than most of the mechanism-design settings considered in this class so far: our agents won't be getting items, only reporting predictions and receiving payments based on their accuracy.},
	author = {Waggoner, Bo},
	year = {2017},
}

@article{carvalho_overview_2016,
	title = {An overview of applications of proper scoring rules},
	volume = {13},
	issn = {15458504},
	doi = {10.1287/deca.2016.0337},
	abstract = {We present a study on the evolution of publications about applications of proper scoring rules. Specifically, we consider articles reporting the use of proper scoring rules when either measuring the accuracy of forecasts or for inducing honest reporting of private information within a certain context. Our analysis of a data set containing 201 articles published between 1950 and 2015 suggests that there has been a tremendous increase in the number of published articles about proper scoring rules over the years. Moreover, the weather/climate, prediction markets, psychology, and energy domains are the four most popular application areas. After providing some insights on how proper scoring rules are applied in different domains, we analyze the publication outlets where the articles in our data set were published. In this regard, we find that an increasing number of articles are now being published in conference proceedings related to artificial intelligence, as opposed to traditional academic journals. We conclude this review by suggesting that the wisdom-of-crowds phenomenon might be a driving force behind the recent popularity of proper scoring rules.},
	number = {4},
	journal = {Decision Analysis},
	author = {Carvalho, Arthur},
	month = dec,
	year = {2016},
	note = {Publisher: INFORMS Inst.for Operations Res.and the Management Sciences},
	keywords = {Forecast evaluation, Forecasting, Incentive engineering, Proper scoring rules},
	pages = {223--242},
}

@article{gneiting_strictly_2007,
	title = {Strictly proper scoring rules, prediction, and estimation},
	volume = {102},
	issn = {01621459},
	doi = {10.1198/016214506000001437},
	abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distribution F if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage. © 2007 American Statistical Association.},
	number = {477},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E.},
	month = mar,
	year = {2007},
	keywords = {Bayes factor, Bregman divergence, Brier score, Coherent, Continuous ranked probability score, Cross-validation, Entropy, Kernel score, Loss function, Minimum contrast estimation, Negative definite function},
	pages = {359--378},
}

@techreport{hernandez-orallo_unified_2012,
	title = {A {Unified} {View} of {Performance} {Metrics}: {Translating} {Threshold} {Choice} into {Expected} {Classification} {Loss} {C}` esar {Ferri}},
	abstract = {Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method.},
	author = {Hernández-Orallo, José and Flach PETERFLACH, Peter},
	year = {2012},
	note = {Publication Title: Journal of Machine Learning Research
Volume: 13},
	keywords = {Brier score, area under the ROC curve (AUC), calibration loss, classification performance metrics, cost-sensitive evaluation, operating condition, refinement loss},
	pages = {2813--2869},
}

@article{villani_celso_nodate,
	title = {Celso {Augusto} {Guimarães} {Santos} {STATISTICAL} {APPROACHES} {VERSUS} {WEATHER} {GENERATOR} {TO} {DOWNSCALE} {RCM} {OUTPUTS} {TO} {POINT} {SCALE}: {A} {COMPARISON} {OF} {PERFORMANCES}},
	volume = {8},
	url = {https://www.jstor.org/stable/10.2307/26203419},
	doi = {10.2307/26203419},
	number = {2},
	journal = {Journal of Urban and Environmental Engineering},
	author = {Villani, Veronica and Rianna, Guido and Mercogliano, Paola and Zollo, Alessandra Lucia and Schiano, Pasquale},
	pages = {142--154},
}

@book{barry_atmosphere_2009,
	title = {Atmosphere, {Weather} and {Climate}},
	isbn = {978-0-203-87102-7},
	publisher = {Routledge},
	author = {Barry, Roger G. and Chorley, Richard J},
	month = oct,
	year = {2009},
	doi = {10.4324/9780203871027},
}

@article{roca_comparing_2010,
	title = {Comparing satellite and surface rainfall products over {West} {Africa} at meteorologically relevant scales during the {AMMA} campaign using error estimates},
	volume = {49},
	issn = {15588424},
	doi = {10.1175/2009JAMC2318.1},
	abstract = {Monsoon rainfall is central to the climate of West Africa, and understanding its variability is a challenge for which satellite rainfall products could be well suited to contribute to. Their quality in this region has received less attention than elsewhere. The focus is set on the scales associated with atmospheric variability, and a meteorological benchmark is set up with ground-based observations from the African Monsoon Multidisciplinary Analysis (AMMA) program. The investigation is performed at various scales of accumulation using four gauge networks. The seasonal cycle is analyzed using 10-day-averaged products, the synoptic-scale variability is analyzed using daily means, and the diurnal cycle of rainfall is analyzed at the seasonal scale using a composite and at the diurnal scale using 3-hourly accumulations. A novel methodology is introduced that accounts for the errors associated with the areal-time rainfall averages. The errors from both satellite and ground rainfall data are computed using dedicated techniques that come down to an estimation of the sampling errors associated to these measurements. The results show that the new generation of combined infrared-microwave (IR-MW) satellite products is describing the rain variability similarly to ground measurements. At the 10-day scale, all products reveal high regional and seasonal skills. The day-to-day comparison indicates that some products perform better than others, whereas all of them exhibit high skills when the spectral band of African easterly waves is considered. The seasonal variability of the diurnal scale as well as its relative daily importance is only captured by some products. Plans for future extensive intercomparison exercises are briefly discussed. © 2010 American Meteorological Society.},
	number = {4},
	journal = {Journal of Applied Meteorology and Climatology},
	author = {Roca, Rémy and Chambon, Philippe and Jobard, Isabelle and Kirstetter, Pierre Emmanuel and Gosset, Marielle and Bergés, Jean Claude},
	year = {2010},
	note = {Publisher: American Meteorological Society},
	pages = {715--731},
}

@article{wainwright_extreme_2021,
	title = {Extreme rainfall in {East} {Africa}, {October} 2019–{January} 2020 and context under future climate change},
	volume = {76},
	issn = {0043-1656},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/wea.3824},
	doi = {10.1002/wea.3824},
	number = {1},
	journal = {Weather},
	author = {Wainwright, Caroline M. and Finney, Declan L. and Kilavi, Mary and Black, Emily and Marsham, John H.},
	month = jan,
	year = {2021},
	keywords = {★},
	pages = {26--31},
}

@article{hendon_diurnal_1993,
	title = {The diurnal cycle of tropical convection},
	volume = {98},
	issn = {01480227},
	doi = {10.1029/93jd00525},
	abstract = {The diurnal cycle of tropical convection is investigated with global cloud imagery constructed from 11μm radiance measurements taken aboard six satellites. To isolate deep convective activity from other processes which cause diurnal fluctuations in longwave radiance, an index of deep convective activity is constructed by thresholding to brightness temperatures less than 230 K. Significant diurnal amplitude of deep convection is found only over tropical landmasses. Over the tropical oceans the diurnal cycle is weak and is barely discernible from the background red spectrum of convective variance. Oceanic convection exhibits a systematic diurnal fluctuation with maximum intensity in the early morning. Nocturnal subsidence along the cloud-free equator is postulated to play a role in forcing diurnal variation in the intertropical convergence zones. Other mechanisms are also implied to contribute as a similar early morning maximum in deep convection is seen even where no adjoining cloud-free regions occur. -from Authors},
	number = {D9},
	journal = {Journal of Geophysical Research},
	author = {Hendon, H. H. and Woodberry, K.},
	year = {1993},
}

@incollection{wilks_parametric_2019,
	title = {Parametric {Probability} {Distributions}},
	booktitle = {Statistical {Methods} in the {Atmospheric} {Sciences}},
	publisher = {Elsevier},
	author = {Wilks, Daniel S.},
	year = {2019},
	doi = {10.1016/b978-0-12-815823-4.00004-3},
	pages = {77--141},
}

@article{holthuijzen_robust_2022,
	title = {Robust bias-correction of precipitation extremes using a novel hybrid empirical quantile-mapping method: {Advantages} of a linear correction for extremes},
	volume = {149},
	issn = {14344483},
	doi = {10.1007/s00704-022-04035-2},
	abstract = {High-resolution, daily precipitation climate products that realistically represent extremes are critical for evaluating local-scale climate impacts. A popular bias-correction method, empirical quantile mapping (EQM), can generally correct distributional discrepancies between simulated climate variables and observed data but can be highly sensitive to the choice of calibration period and is prone to overfitting. In this study, we propose a hybrid bias-correction method for precipitation, EQM-LIN, which combines the efficacy of EQM for correcting lower quantiles, with a robust linear correction for upper quantiles. We apply both EQM and EQM-LIN to historical daily precipitation data simulated by a regional climate model over a region in the northeastern USA. We validate our results using a five-fold cross-validation and quantify performance of EQM and EQM-LIN using skill score metrics and several climatological indices. As part of a high-resolution downscaling and bias-correction workflow, EQM-LIN significantly outperforms EQM in reducing mean, and especially extreme, daily distributional biases present in raw model output. EQM-LIN performed as good or better than EQM in terms of bias-correcting standard climatological indices (e.g., total annual rainfall, frequency of wet days, total annual extreme rainfall). In addition, our study shows that EQM-LIN is particularly resistant to overfitting at extreme tails and is much less sensitive to calibration data, both of which can reduce the uncertainty of bias-correction at extremes.},
	number = {1-2},
	journal = {Theoretical and Applied Climatology},
	author = {Holthuijzen, Maike and Beckage, Brian and Clemins, Patrick J. and Higdon, Dave and Winter, Jonathan M.},
	month = jul,
	year = {2022},
	note = {Publisher: Springer},
	pages = {863--882},
}

@article{qian_projecting_2021,
	title = {Projecting health impacts of future temperature: {A} comparison of quantile‐mapping bias‐correction methods},
	volume = {18},
	issn = {16604601},
	doi = {10.3390/ijerph18041992},
	abstract = {Health impact assessments of future environmental exposures are routinely conducted to quantify population burdens associated with the changing climate. It is well‐recognized that simulations from climate models need to be bias‐corrected against observations to estimate future expo-sures. Quantile mapping (QM) is a technique that has gained popularity in climate science because of its focus on bias‐correcting the entire exposure distribution. Even though improved bias‐correc-tion at the extreme tails of exposure may be particularly important for estimating health burdens, the application of QM in health impact projection has been limited. In this paper we describe and apply five QM methods to estimate excess emergency department (ED) visits due to projected changes in warm‐season minimum temperature in Atlanta, USA. We utilized temperature projections from an ensemble of regional climate models in the North American‐Coordinated Regional Climate Downscaling Experiment (NA‐CORDEX). Across QM methods, we estimated consistent increase in ED visits across climate model ensemble under RCP 8.5 during the period 2050 to 2099. We found that QM methods can significantly reduce between‐model variation in health impact projections (50–70\% decreases in between‐model standard deviation). Particularly, the quantile delta mapping approach had the largest reduction and is recommended also because of its ability to preserve model‐projected absolute temporal changes in quantiles.},
	number = {4},
	journal = {International Journal of Environmental Research and Public Health},
	author = {Qian, Weijia and Chang, Howard H.},
	month = feb,
	year = {2021},
	pmid = {33670819},
	note = {Publisher: MDPI AG},
	keywords = {Bias‐correction, Climate change, Emergency department visits, Health impact, Quantile mapping, Temperature},
	pages = {1--12},
}

@article{gudmundsson_quantile_2012,
	title = {Quantile mapping {Hydrology} and {Earth} {System} {Sciences} {Discussions} {Technical} {Note}: {Downscaling} {RCM} precipitation to the station scale using quantile mapping-a comparison of methods {Quantile} mapping},
	volume = {9},
	url = {www.hydrol-earth-syst-sci-discuss.net/9/6185/2012/},
	doi = {10.5194/hessd-9-6185-2012},
	abstract = {The impact of climate change on water resources is usually assessed at the local scale. However, regional climate models (RCM) are known to exhibit systematic biases in precipitation. Hence, RCM simulations need to be post-processed in order to produce reliable estimators of local scale climate. A popular post-processing approach is 5 quantile mapping (QM), which is designed to adjust the distribution of modeled data, such that it matches observed climatologies. However, the diversity of suggested QM methods renders the selection of optimal techniques difficult and hence there is a need for clarification. In this paper, QM methods are reviewed and classified into: (1) distribution derived transformations, (2) parametric transformations and (3) nonparametric 10 transformations; each differing with respect to their underlying assumptions. A real world application, using observations of 82 precipitation stations in Norway, showed that nonparametric transformations have the highest skill in systematically reducing biases in RCM precipitation.},
	journal = {Hydrol. Earth Syst. Sci. Discuss},
	author = {Gudmundsson, L and Bremnes, J B and Haugen, J E and Skaugen, T Engen},
	year = {2012},
	pages = {6185--6201},
}

@article{chamberlain_forecasting_2014,
	title = {Forecasting storms over {Lake} {Victoria} using a high resolution model},
	volume = {21},
	issn = {14698080},
	doi = {10.1002/met.1403},
	abstract = {Lake Victoria in East Africa is one of the world's largest freshwater lakes and is used on a daily basis by thousands of fishermen. Each year, severe storms on the lake cause multiple boating accidents which often result in fatalities. Recent initiatives have seen an effort to reduce accidents by issuing storm warnings when severe weather is expected. Here the Met Office global Unified Model is evaluated along with a 4km limited-area model which has been set up to assist forecasters in the region to issue these warnings. Findings indicate the 4km model is capable of producing more realistic strong wind speeds and rain rates than the global model. Case studies relating to fatal boating accidents on 1March and 4March2012, showed improved warning signals of severe storms in the 4km model compared to the global model. Objective comparisons between model and observations were conducted on 2months of data. An objective method was used to determine 'storm'/'no storm' in the model forecasts. These were then compared against cloud top temperature from IR satellite and lightning data from the arrival time difference (ATD) radio ground network to determine whether each model was successful at forecasting storm/calm events. The 4km model was able to capture more storm hits (thus had fewer storm misses), but also gained more false alarm events. Overall, the objective analysis showed that both models had some predictive skill and both were an improvement on a persistence forecast. © 2013 Royal Meteorological Society.},
	number = {2},
	journal = {Meteorological Applications},
	author = {Chamberlain, J. M. and Bain, C. L. and Boyd, D. F.A. and Mccourt, K. and Butcher, T. and Palmer, S.},
	year = {2014},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Forecasting, Lake Victoria, Limited area model, Met Office, Tropical thunderstorms, Uganda},
	pages = {419--430},
}

@article{wheeler_convectively_1999,
	title = {Convectively {Coupled} {Equatorial} {Waves}: {Analysis} of {Clouds} and {Temperature} in the {Wavenumber}–{Frequency} {Domain}},
	volume = {56},
	issn = {0022-4928},
	url = {http://journals.ametsoc.org/doi/10.1175/1520-0469(1999)056<0374:CCEWAO>2.0.CO;2},
	doi = {10.1175/1520-0469(1999)056<0374:CCEWAO>2.0.CO;2},
	abstract = {A wavenumber-frequency spectrum analysis is performed for all longitudes in the domain 15S-15N using a long (18 years) twice-daily record of satellite-observed outgoing longwave radiation (OLR), a good proxy for deep tropical convection. The broad nature of the spectrum is red in both zonal wavenumber and frequency. By removing an estimated background spectrum, numerous statistically significant spectral peaks are isolated. Some of the peaks correspond quite well to the dispersion relations of the equatorially trapped wave modes of shallow water theory with implied equivalent depths in the range of 12-50 m. Cross-spectrum analysis with the satellite-based microwave sounding unit deep-layer temperature data shows that these spectral peaks in the OLR are ''coupled'' with this dynamical field. The equivalent depths of the convectively coupled waves are shallower than those typical of equatorial waves uncoupled with convection. Such a small equivalent depth is thought to be a result of the interaction between convection and the dynamics. The convectively coupled equatorial waves identified correspond to the Kelvin, n 1 equatorial Rossby, mixed Rossby-gravity, n 0 eastward inertio-gravity, n 1 westward inertio-gravity (WIG), and n 2 WIG waves. Additionally, the Madden-Julian oscillation and tropical depression-type disturbances are present in the OLR spectra. These latter two features are unlike the convectively coupled equatorial waves due to their location away from the equatorial wave dispersion curves in the wavenumber-frequency domain. Extraction of the different convectively coupled disturbances in the time-longitude domain is performed by filtering the OLR dataset for very specific zonal wavenumbers and frequencies. The geographical distribution of the variance of these filtered data gives further evidence that some of the spectral peaks correspond to particular equatorial wave modes. The results have implications for the cumulus parameterization problem, for the excitation of equatorial waves in the lower stratosphere, and for extended-range forecasting in the Tropics.},
	number = {3},
	journal = {Journal of the Atmospheric Sciences},
	author = {Wheeler, Matthew and Kiladis, George N.},
	month = feb,
	year = {1999},
	pages = {374--399},
}

@article{lin_tropical_2006,
	title = {Tropical {Intraseasonal} {Variability} in 14 {IPCC} {AR4} {Climate} {Models}. {Part} {I}: {Convective} {Signals}},
	volume = {19},
	issn = {1520-0442},
	url = {http://journals.ametsoc.org/doi/10.1175/JCLI3735.1},
	doi = {10.1175/JCLI3735.1},
	abstract = {{\textless}p{\textgreater}This study evaluates the tropical intraseasonal variability, especially the fidelity of Madden–Julian oscillation (MJO) simulations, in 14 coupled general circulation models (GCMs) participating in the Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report (AR4). Eight years of daily precipitation from each model’s twentieth-century climate simulation are analyzed and compared with daily satellite-retrieved precipitation. Space–time spectral analysis is used to obtain the variance and phase speed of dominant convectively coupled equatorial waves, including the MJO, Kelvin, equatorial Rossby (ER), mixed Rossby–gravity (MRG), and eastward inertio–gravity (EIG) and westward inertio–gravity (WIG) waves. The variance and propagation of the MJO, defined as the eastward wavenumbers 1–6, 30–70-day mode, are examined in detail.{\textless}/p{\textgreater}},
	number = {12},
	urldate = {2023-02-20},
	journal = {Journal of Climate},
	author = {Lin, Jia-Lin and Kiladis, George N. and Mapes, Brian E. and Weickmann, Klaus M. and Sperber, Kenneth R. and Lin, Wuyin and Wheeler, Matthew C. and Schubert, Siegfried D. and Del Genio, Anthony and Donner, Leo J. and Emori, Seita and Gueremy, Jean-Francois and Hourdin, Frederic and Rasch, Philip J. and Roeckner, Erich and Scinocca, John F.},
	month = jun,
	year = {2006},
	pages = {2665--2690},
}

@article{ruane_6-hour_2007,
	title = {6-{Hour} to 1-year variance of five global precipitation sets},
	volume = {11},
	issn = {10873562},
	doi = {10.1175/EI225.1},
	abstract = {Three-hourly time series of precipitation from three highresolution precipitation products [Tropical Rainfall Measuring Mission (TRMM) algorithm 3B-42, the Climate Prediction Center's morphing method (CMORPH), and the Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks (PERSIANN)] and two reanalyses are examined for their frequency characteristics using broad and narrow variance categories. After isolating the diurnally forced peaks (at 24, 12, 8, and 6 h), the power spectra are divided into comprehensive broad bands comprising the annual (∼80 days-1 yr), intraseasonal (20 to ∼80 days), slow (6-20 days) and fast (36 h-6 days) synoptic, and high-frequency (6-36 h) periods. Global maps accounting for 100\% of precipitation's variance are analyzed to identify unique regional behaviors. Annual variability is strongest over regions affected by the seasonal migration of the intertropical convergence zone, as well as over monsoonal regions. The intraseasonal band displays off-equatorial evidence of the Madden-Julian oscillation (MJO), particularly in the Indian Ocean, but the MJO's rainfall is partially manifested in the slow synoptic band and at higher frequencies. The fast synoptic band is particularly strong over the oceans, while high-frequency variability is enhanced over land by more extreme surface gradients. Diurnal variance is strongest at low latitudes and is pronounced over regions with well-known diurnal circulations, including mountains and coastlines. Interproduct and intermodel differences also indicate biases of the precipitation product algorithms and convective parameterizations, including a strong bias toward low-frequency variability in the relaxed Arakawa-Schubert parameterization employed by one of the reanalyses, as well as increased white-spectral characteristics over land in the precipitation products.},
	number = {11},
	journal = {Earth Interactions},
	author = {Ruane, Alex C. and Roads, John O.},
	year = {2007},
	note = {Publisher: American Meteorological Society},
	keywords = {Diurnal effects, Intraseasonal variability, Madden-julian oscillation, Model comparison, Precipitation},
	pages = {1--29},
}

@article{kim_tropical_2013,
	title = {Tropical precipitation variability and convectively coupled equatorial waves on submonthly time scales in reanalyses and {TRMM}},
	volume = {26},
	issn = {08948755},
	doi = {10.1175/JCLI-D-12-00353.1},
	abstract = {Tropical precipitation characteristics are investigated using the Tropical Rainfall Measuring Mission (TRMM) 3-hourly estimates, and the result is compared with five reanalyses including the European Centre for Medium-Range Weather Forecasts (ECMWF) Interim Re-Analysis (ERA-Interim), Modern Era Retrospective Analysis for Research and Applications (MERRA), National Centers for Environmental Prediction (NCEP)-National Center for Atmospheric Research (NCAR) reanalysis (NCEP1), NCEP-U.S. Department of Energy (DOE) reanalysis (NCEP2), and NCEP-Climate Forecast System Reanalysis (CFSR). Precipitation characteristics are evaluated in terms of the mean, convectively coupled equatorial wave activity, frequency characteristics, diurnal cycle, and seasonality of regional precipitation variability associated with submonthly scale waves. Generally the latest reanalyses such as ERA-Interim, MERRA, and CFSR show better performances than NCEP1 and NCEP2. However, all the reanalyses are still different from observations. Besides the positive mean bias in the reanalyses, a spectral analysis revealed that the reanalyses have overreddened spectra with persistent rainfall. MERRA has the most persistent rainfall, and CFSR appears to have the most realistic variability. The diurnal cycle in NCEP1 is extremely exaggerated relative to TRMM. The low-frequency waves with the period longer than 3 days are relatively well represented in ERAInterim, MERRA, and CFSR, but all the reanalyses have significant deficiencies in representing convectively coupled equatorial waves and variability in the high-frequency range. © 2013 American Meteorological Society.},
	number = {10},
	journal = {Journal of Climate},
	author = {Kim, Ji Eun and Joan Alexander, M.},
	month = may,
	year = {2013},
	pages = {3013--3030},
}

@article{noauthor_normal_nodate,
	title = {On normal approximations for the non-central {F}-distribution},
}

@incollection{stensrud_convective_2013,
	title = {Convective parameterizations},
	booktitle = {Parameterization {Schemes}},
	publisher = {Cambridge University Press},
	author = {Stensrud, David J.},
	month = sep,
	year = {2013},
	doi = {10.1017/cbo9780511812590.007},
	pages = {185--259},
}

@article{schwarz_frequency_2021,
	title = {On the {Frequency} {Bias} of {Generative} {Models}},
	volume = {34},
	url = {https://github.com/},
	abstract = {The key objective of Generative Adversarial Networks (GANs) is to generate new data with the same statistics as the provided training data. However, multiple recent works show that state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an elevated amount of high frequencies in the spectral statistics which makes it straightforward to distinguish real and generated images. Explanations for this phenomenon are controversial: While most works attribute the artifacts to the generator, other works point to the discriminator. We take a sober look at those explanations and provide insights on what makes proposed measures against high-frequency artifacts effective. To achieve this, we first independently assess the architectures of both the generator and discriminator and investigate if they exhibit a frequency bias that makes learning the distribution of high-frequency content particularly problematic. Based on these experiments, we make the following four observations: 1) Different upsampling operations bias the generator towards different spectral properties. 2) Checkerboard artifacts introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able to compensate for these artifacts. 3) The discriminator does not struggle with detecting high frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling operations in the discriminator can impair the quality of the training signal it provides. In light of these findings, we analyze proposed measures against high-frequency artifacts in state-of-the-art GAN training but find that none of the existing approaches can fully resolve spectral artifacts yet. Our results suggest that there is great potential in improving the discriminator and that this could be key to match the distribution of the training data more closely.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Schwarz, Katja and Liao, Yiyi and Geiger, Andreas},
	year = {2021},
	pages = {18126--18136},
}

@article{dezfuli_validation_2017,
	title = {Validation of {IMERG} {Precipitation} in {Africa}},
	volume = {18},
	url = {https://doi.org/10.1175/},
	doi = {10.1175/JHM-D-17-0139.s1},
	abstract = {Understanding of hydroclimatic processes in Africa has been hindered by the lack of in situ precipitation measurements. Satellite-based observations, in particular, the TRMM Multisatellite Precipitation Analysis (TMPA) have been pivotal to filling this void. The recently released Integrated Multisatellite Retrievals for GPM (IMERG) project aims to continue the legacy of its predecessor, TMPA, and provide higher-resolution data. Here, IMERG-V04A precipitation data are validated using in situ observations from the Trans-African Hydro-Meteorological Observatory (TAHMO) project. Various evaluation measures are examined over a select number of stations in West and East Africa. In addition, continent-wide comparisons are made between IMERG and TMPA. The results show that the performance of the satellite-based products varies by season, region, and the evaluation statistics. The precipitation diurnal cycle is relatively better captured by IMERG than TMPA. Both products exhibit a better agreement with gauge data in East Africa and humid West Africa than in the southern Sahel. However, a clear advantage for IMERG is not apparent in detecting the annual cycle. Although all gridded products used here reasonably capture the annual cycle, some differences are evident during the short rains in East Africa. Direct comparison between IMERG and TMPA over the entire continent reveals that the similarity between the two products is also regionally heterogeneous. Except for Zimbabwe and Madagascar, where both satellite-based observations present a good agreement, the two products generally have their largest differences over mountainous regions. IMERG seems to have achieved a reduction in the positive bias evident in TMPA over Lake Victoria.},
	number = {10},
	journal = {Journal of Hydrometeorology},
	author = {Dezfuli, Amin K and Ichoku, Charles M and Huffman, George J and Mohr, Karen I and Selker, John S and De Giesen, Nick Van and Hochreutener, Rebecca and Annor, Frank O},
	year = {2017},
	pages = {2817--2825},
}

@article{woodhams_what_2018,
	title = {What is the added value of a convection-permitting model for forecasting extreme rainfall over tropical {East} {Africa}?},
	volume = {146},
	issn = {15200493},
	doi = {10.1175/MWR-D-17-0396.1},
	abstract = {Forecasting convective rainfall in the tropics is a major challenge for numerical weather prediction. The use of convection-permitting (CP) forecast models in the tropics has lagged behind the midlatitudes, despite the great potential of such models in this region. In the scientific literature, there is very little evaluation of CP models in the tropics, especially over an extended time period. This paper evaluates the prediction of convective storms for a period of 2 years in the Met Office operational CP model over East Africa and the global operational forecast model. A novel localized form of the fractions skill score is introduced, which shows variation in model skill across the spatial domain. Overall, the CP model and the global model both outperform a 24-h persistence forecast. The CP model shows greater skill than the global model, in particular on subdaily time scales and for storms over land. Forecasts over Lake Victoria are also improved in the CP model, with an increase in hit rate of up to 20\%. Contrary to studies in the midlatitudes, the skill of both models shows a large dependence on the time of day and comparatively little dependence on the forecast lead time within a 48-h forecast. Although these results provide more motivation for forecasters to use the CP model to produce subdaily forecasts with increased detail, there is a clear need for more in situ observations for data assimilation into the models and for verification. A move toward ensemble forecasting could have further benefits.},
	number = {9},
	journal = {Monthly Weather Review},
	author = {Woodhams, Beth J. and Birch, Cathryn E. and Marsham, John H. and Bain, Caroline L. and Roberts, Nigel M. and Boyd, Douglas F.A.},
	month = sep,
	year = {2018},
	note = {Publisher: American Meteorological Society},
	keywords = {Africa, Cloud resolving models, Inland seas/lakes, Model evaluation/performance, Rainfall, Regional models, ★},
	pages = {2757--2780},
}

@article{macleod_drivers_2021,
	title = {Drivers and {Subseasonal} {Predictability} of {Heavy} {Rainfall} in {Equatorial} {East} {Africa} and {Relationship} with {Flood} {Risk}},
	volume = {22},
	url = {https://doi.org/10.1175/JHM-D-20-},
	doi = {10.1175/JHM-D-20},
	abstract = {Equatorial East Africa (EEA) suffers from significant flood risks. These can be mitigated with preemptive action; however, currently available early warnings are limited to a few days' lead time. Extending warnings using subseasonal climate forecasts could open a window for more extensive preparedness activity. However, before these forecasts can be used, the basis of their skill and relevance for flood risk must be established. Here we demonstrate that subseasonal forecasts are particularly skillful over EEA. Forecasts can skillfully anticipate weekly upper-quintile rainfall within a season, at lead times of 2 weeks and beyond. We demonstrate the link between the Madden-Julian oscillation (MJO) and extreme rainfall events in the region, and confirm that leading forecast models accurately represent the EEA teleconnection to the MJO. The relevance of weekly rainfall totals for fluvial flood risk in the region is investigated using a long record of streamflow from the Nzoia River in western Kenya. Both heavy rainfall and high antecedent rainfall conditions are identified as key drivers of flood risk, with upper-quintile weekly rainfall shown to skillfully discriminate flood events. We additionally evaluate GloFAS global flood forecasts for the Nzoia basin. Though these are able to anticipate some flooding events with several weeks lead time, analysis suggests action based on these would result in a false alarm more than 50\% of the time. Overall, these results build on the scientific evidence base that supports the use of subseasonal forecasts in EEA, and activities to advance their use are discussed.},
	number = {4},
	journal = {Journal of Hydrometeorology},
	author = {Macleod, David A and Dankers, Rutger and Graham, Richard and Guigma, Kiswendsida and Jenkins, Luke and Todd, Martin C and Kiptum, Augustine and Kilavi, Mary and Njogu, Andrew and Mwangi, Emmah},
	year = {2021},
	keywords = {Africa, Flood events, Forecast verification/skill, Madden-Julian oscillation},
	pages = {887--903},
}

@article{engel_extreme_nodate,
	title = {Extreme {Precipitation} in the {West} {African} {Cities} of {Dakar} and {Ouagadougou}: {Atmo}-spheric {Dynamics} and {Implications} for {Flood} {Risk} {Assessments}},
	url = {https://doi.org/10.1175/JHM-D-16-0218.s1.},
	doi = {10.1175/JHM-D-16-0218.s1},
	abstract = {Two extreme, high-impact events of heavy rainfall and severe floods in West African urban areas (Ouagadougou on 1 September 2009 and Dakar on 26 August 2012) are investigated with respect to their atmospheric causes and statistical return periods. In terms of the synoptic-convective dynamics, the Ouagadougou case is truly extraordinary. A succession of two slow-moving African easterly waves (AEWs) caused record-breaking values of tropospheric moisture. The second AEW, one of the strongest in recent decades, provided the synoptic forcing for the nighttime genesis of mesoscale convective systems (MCSs). Ouagadougou was hit by two MCSs within 6 h, as the strong convergence and rotation in the AEW-related vortex allowed a swift moisture refueling. An AEW was also instrumental in the overnight development of MCSs in the Dakar case, but neither the AEW vortex nor the tropospheric moisture content was as exceptional as in the Ouagadougou case. Tropical Rainfall Measuring Mission (TRMM) 3B42 precipitation data show some promise in estimating centennial return values (RVs) using the ''peak over threshold'' approach with a generalized Pareto distribution fit, although indications for errors in estimating extreme rainfall over the arid Sahel are found. In contrast, the Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks-Climate Data Record (PERSIANN-CDR) dataset seems less suitable for this purpose despite the longer record. Notably, the Ouagadougou event demonstrates that highly unusual dynamical developments can create extremes well outside of RV estimates from century-long rainfall observations. Future research will investigate whether such developments may become more frequent in a warmer climate.},
	author = {Engel, Thomas and Fink, Andreas H and Knippertz, Peter and Pante, Gregor and Bliefernicht, Jan},
}

@article{haiden_intercomparison_2012,
	title = {Intercomparison of global model precipitation forecast skill in 2010/11 using the {SEEPS} score},
	volume = {140},
	issn = {00270644},
	doi = {10.1175/MWR-D-11-00301.1},
	abstract = {Precipitation forecasts from five global numerical weather prediction (NWP) models are verified against rain gauge observations using the new stable equitable error in probability space (SEEPS) score. It is based on a 3 × 3 contingency table and measures the ability of a forecast to discriminate between "dry," "light precipitation," and "heavy precipitation." In SEEPS, the threshold defining the boundary between the light and heavy categories varies systematically with precipitation climate. Results obtained for SEEPS are compared to those of more well-known scores, and are broken down with regard to individual contributions from the contingency table. It is found that differences in skill between the models are consistent for different scores, but are small compared to seasonal and geographical variations, which themselves can be largely ascribed to the varying prevalence of deep convection. Differences between the tropics and extratropics are quite pronounced. SEEPS scores at forecast day 1 in the tropics are similar to those at day 6 in the extratropics. It is found that the model ranking is robust with respect to choices in the score computation. The issue of observation representativeness is addressed using a "quasi-perfect model" approach. Results suggest that just under one-half of the current forecast error at day 1 in the extratropics can be attributed to the fact that gridbox values are verified against point observations. © 2012 American Meteorological Society.},
	number = {8},
	journal = {Monthly Weather Review},
	author = {Haiden, Thomas and Rodwell, Mark J. and Richardson, David S. and Okagaki, Akira and Robinson, Tom and Hewson, Tim},
	month = aug,
	year = {2012},
	keywords = {Forecast verification},
	pages = {2720--2733},
}

@article{rasp_neural_2018,
	title = {Neural {Networks} for {Postprocessing} {Ensemble} {Weather} {Forecasts}},
	volume = {146},
	url = {https://doi.org/10.1175/MWR-D-18-},
	doi = {10.1175/MWR-D-18},
	abstract = {Ensemble weather predictions require statistical postprocessing of systematic errors to obtain reliable and accurate probabilistic forecasts. Traditionally, this is accomplished with distributional regression models in which the parameters of a predictive distribution are estimated from a training period. We propose a flexible alternative based on neural networks that can incorporate nonlinear relationships between arbitrary predictor variables and forecast distribution parameters that are automatically learned in a data-driven way rather than requiring prespecified link functions. In a case study of 2-m temperature forecasts at surface stations in Germany, the neural network approach significantly outperforms benchmark postprocessing methods while being computationally more affordable. Key components to this improvement are the use of auxiliary pre-dictor variables and station-specific information with the help of embeddings. Furthermore, the trained neural network can be used to gain insight into the importance of meteorological variables, thereby challenging the notion of neural networks as uninterpretable black boxes. Our approach can easily be extended to other statistical postprocessing and forecasting problems. We anticipate that recent advances in deep learning combined with the ever-increasing amounts of model and observation data will transform the postprocessing of numerical weather forecasts in the coming decade.},
	number = {11},
	journal = {Monthly Weather Review},
	author = {Rasp, Stephan and Lerch, Sebastian},
	year = {2018},
	pages = {3885--3900},
}

@techreport{huffman_nasa_2018,
	title = {{NASA} {Global} {Precipitation} {Measurement} ({GPM}) {Integrated} {Multi}-{satellitE} {Retrievals} for {GPM} ({IMERG}) {Prepared} for: {Global} {Precipitation} {Measurement} ({GPM}) {National} {Aeronautics} and {Space} {Administration} ({NASA})},
	url = {https://pmm.nasa.gov/sites/default/files/imce/times_allsat.jpg},
	author = {Huffman, Georg J and Bolvin, David T and Braithwaite, Dan and Hsu, Kuolin and Joyce, Robert and Kidd, Christopher and Nelkin, Eric J and Sorooshian, S and Tan, J and Xie, Pingping},
	month = feb,
	year = {2018},
}

@article{bechtold_representing_2014,
	title = {Representing equilibrium and nonequilibrium convection in large-scale models},
	volume = {71},
	issn = {00224928},
	doi = {10.1175/JAS-D-13-0163.1},
	abstract = {A new diagnostic convective closure, which is dependent on convective available potential energy (CAPE), is derived under the quasi-equilibrium assumption for the free troposphere subject to boundary layer forcing. The closure involves a convective adjustment time scale for the free troposphere and a coupling coefficient between the free troposphere and the boundary layer based on different time scales over land and ocean. Earlier studies with the ECMWF Integrated Forecasting System (IFS) have already demonstrated the model's ability to realistically represent tropical convectively coupled waves and synoptic variability with use of the "standard" CAPE closure, given realistic entrainment rates. A comparison of low-resolution seasonal integrations and high-resolution short-range forecasts against complementary satellite and radar data shows that with the extended CAPE closure it is also possible, independent of model resolution and time step, to realistically represent nonequilibrium convection such as the diurnal cycle of convection and the convection tied to advective boundary layers, although representing the late night convection over land remains a challenge.Amore in-depth regional analysis of the diurnal cycle and the closure is provided for the continental United States and particularly Africa, including comparison with data from satellites and a cloud-resolving model (CRM). Consequences for global numerical weather prediction (NWP) are not only a better phase representation of convection, but also better forecasts of its spatial distribution and local intensity. © 2014 American Meteorological Society.},
	number = {2},
	journal = {Journal of the Atmospheric Sciences},
	author = {Bechtold, Peter and Semane, Noureddine and Lopez, Philippe and Chaboureau, Jean Pierre and Beljaars, Anton and Bormann, Niels},
	month = feb,
	year = {2014},
	keywords = {★},
	pages = {734--753},
}

@article{noauthor_generalized_nodate,
	title = {Generalized {Additive} {Models} versus {Linear} {Regression} in {Generating} {Probabilistic} {MOS} {Forecasts} of {Aviation} {Weather} {Parameters}},
}

@article{sweeney_reducing_2013,
	title = {Reducing errors of wind speed forecasts by an optimal combination of post-processing methods},
	volume = {20},
	issn = {14698080},
	doi = {10.1002/met.294},
	abstract = {Seven adaptive approaches to post-processing wind speed forecasts are discussed and compared. Forecasts of the wind speed over 48 h are run at horizontal resolutions of 7 and 3 km for a domain centred over Ireland. Forecast wind speeds over a 2 year period are compared to observed wind speeds at seven synoptic stations around Ireland and skill scores calculated. Two automatic methods for combining forecast streams are applied. The forecasts produced by the combined methods give bias and root mean squared errors that are better than the numerical weather prediction forecasts at all station locations. One of the combined forecast methods results in skill scores that are equal to or better than all of its component forecast streams. This method is straightforward to apply and should prove beneficial in operational wind forecasting. © 2011 Royal Meteorological Society.},
	number = {1},
	journal = {Meteorological Applications},
	author = {Sweeney, Conor P. and Lynch, Peter and Nolan, Paul},
	year = {2013},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Adaptive post-processing, Artificial neural network, Kalman filter, Numerical weather prediction},
	pages = {32--40},
}

@article{li_development_2020,
	title = {Development of a postprocessing system of daily rainfall forecasts for seasonal crop prediction in {Australia}},
	volume = {141},
	issn = {14344483},
	doi = {10.1007/s00704-020-03268-3},
	abstract = {There is a need to postprocess seasonal rainfall forecasts from physical climate models to reduce bias, improve skill and restore daily variability for use as input for crop simulation at the farm scale. We develop an extended copula postprocessing (ECPP) method to deal with daily rainfall with numerous zero occurrences. By treating rainfall as a left-censored variable, we derive likelihood estimation and adjust simulation procedure with consideration of zero rainfall occurrences. In a case study for 50 representative agricultural stations in Australia, we test our method to postprocess daily rainfall forecasts with up to 186-day lead time. We demonstrate that the ECPP improves the overall forecast skill from raw rainfall forecasts and outperforms quantile mapping (QM) by checking various verification measures. Though the forecasts for daily amounts are hardly skilful except for the first few days, the forecasts for accumulated totals can be skilful from error averaging and propagating positive skill from short lead times. We also demonstrate that the ECPP can simulate rainfall forecasts with more realistic dry day distribution and daily rainfall intensity than QM. Further research directions including several opportunities to improve ECPP are discussed.},
	number = {3-4},
	journal = {Theoretical and Applied Climatology},
	author = {Li, Ming and Jin, Huidong},
	month = aug,
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Copula, Rainfall forecast, censored data, Seasonal climate forecast, Statistical postprocessing, ★},
	pages = {1331--1349},
}

@article{vannitsem_unified_2009,
	title = {A unified linear {Model} {Output} {Statistics} scheme for both deterministic and ensemble forecasts},
	volume = {135},
	issn = {00359009},
	doi = {10.1002/qj.491},
	abstract = {An extension of the classical linear Model Output Statistics (MOS) technique is proposed allowing for the post-processing of ensemble forecasts. In this new approach, the cost function on which the least square parameter estimation is based takes into account the presence of errors in both observations and model observables (referred to as Error-in-Variables MOS, EVMOS), unlike the classical linear MOS cost function whose implicit assumption is the absence of errors in the model observables. It allows for the maintenance of an appropriate variability for the corrected forecasts, even for long lead times and for providing a framework in which both deterministic and probabilistic forecasts can be corrected. The scheme is successfully tested for ensemble correction in the context of an idealized low-order chaotic system, the Lorenz atmospheric model, in the presence of model errors, and compared with a classical technique, known as the non-homogeneous Gaussian regression (NGR) method. The potential use of this approach is also briefly discussed. © 2009 Royal Meteorological Society.},
	number = {644},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Vannitsem, S.},
	month = oct,
	year = {2009},
	keywords = {EVMOS, NGR, Post-processing, ★},
	pages = {1801--1815},
}

@article{ghil_extreme_2011,
	title = {Extreme events: {Dynamics}, statistics and prediction},
	volume = {18},
	issn = {10235809},
	doi = {10.5194/npg-18-295-2011},
	abstract = {We review work on extreme events, their causes and consequences, by a group of European and American researchers involved in a three-year project on these topics. The review covers theoretical aspects of time series analysis and of extreme value theory, as well as of the deterministic modeling of extreme events, via continuous and discrete dynamic models. The applications include climatic, seismic and socio-economic events, along with their prediction. Two important results refer to (i) the complementarity of spectral analysis of a time series in terms of the continuous and the discrete part of its power spectrum; and (ii) the need for coupled modeling of natural and socio-economic systems. Both these results have implications for the study and prediction of natural hazards and their human impacts. © 2011 Author(s).},
	number = {3},
	journal = {Nonlinear Processes in Geophysics},
	author = {Ghil, M. and Yiou, P. and Hallegatte, S. and Malamud, B. D. and Naveau, P. and Soloviev, A. and Friederichs, P. and Keilis-Borok, V. and Kondrashov, D. and Kossobokov, V. and Mestre, O. and Nicolis, C. and Rust, H. W. and Shebalin, P. and Vrac, M. and Witt, A. and Zaliapin, I.},
	year = {2011},
	pages = {295--350},
}

@article{maurer_bias_2014,
	title = {Bias correction can modify climate model simulated precipitation changes without adverse effect on the ensemble mean},
	volume = {18},
	issn = {10275606},
	doi = {10.5194/hess-18-915-2014},
	abstract = {When applied to remove climate model biases in precipitation, quantile mapping can in some settings modify the simulated difference in mean precipitation between two eras. This has important implications when the precipitation is used to drive an impacts model that is sensitive to changes in precipitation. The tendency of quantile mapping to alter model-predicted changes is demonstrated using synthetic precipitation distributions and elucidated with a simple theoretical analysis, which shows that the alteration of model-predicted changes can be controlled by the ratio of model to observed variance. To further evaluate the effects of quantile mapping in a more realistic setting, we use daily precipitation output from 11 atmospheric general circulation models (AGCMs), forced by observed sea surface temperatures, over the conterminous United States to compare precipitation differences before and after quantile mapping bias correction. The effectiveness of the bias correction is not assessed, only its effect on precipitation differences. The change in seasonal mean (winter, DJF, and summer, JJA) precipitation between two historical periods is compared to examine whether the bias correction tends to amplify or diminish an AGCM's simulated precipitation change. In some cases the trend modification can be as large as the original simulated change, though the areas where this occurs varies among AGCMs so the ensemble median shows smaller trend modification. Results show that quantile mapping improves the correspondence with observed changes in some locations and degrades it in others. While not representative of a future where natural precipitation variability is much smaller than that due to external forcing, these results suggest that at least for the next several decades the influence of quantile mapping on seasonal precipitation trends does not systematically degrade projected differences. © 2014 Author (s).},
	number = {3},
	journal = {Hydrology and Earth System Sciences},
	author = {Maurer, E. P. and Pierce, D. W.},
	month = mar,
	year = {2014},
	pages = {915--925},
}

@article{watt-meyer_correcting_2021,
	title = {Correcting {Weather} and {Climate} {Models} by {Machine} {Learning} {Nudged} {Historical} {Simulations}},
	volume = {48},
	issn = {19448007},
	doi = {10.1029/2021GL092555},
	abstract = {Due to limited resolution and inaccurate physical parameterizations, weather and climate models consistently develop biases compared to the observed atmosphere. Using the FV3GFS model at coarse resolution, we propose a method of machine learning corrective tendencies from a hindcast simulation nudged toward observational analysis. We show that a random forest can predict the nudging tendencies from this hindcast simulation with moderate skill using only the model state as input. This random forest is then coupled to FV3GFS, adding corrective tendencies of temperature, specific humidity and horizontal winds at each timestep. The coupled model shows no signs of instability in year-long simulations and has significant reductions in short-term forecast error for 500 hPa height, surface pressure and near-surface temperature. Furthermore, the root mean square error of the annual-mean precipitation is reduced by about 20\%. Biases of other variables remain similar or in some cases, like upper-atmospheric temperature, increase in the year-long simulations.},
	number = {15},
	journal = {Geophysical Research Letters},
	author = {Watt-Meyer, Oliver and Brenowitz, Noah D. and Clark, Spencer K. and Henn, Brian and Kwa, Anna and McGibbon, Jeremy and Perkins, W. Andre and Bretherton, Christopher S.},
	month = aug,
	year = {2021},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {bias correction, climate modeling, hybrid physics-ML, machine learning, parameterization, weather prediction},
}

@article{sloughter_probabilistic_2007,
	title = {Probabilistic quantitative precipitation forecasting using bayesian model averaging},
	volume = {135},
	issn = {00270644},
	doi = {10.1175/MWR3441.1},
	abstract = {Bayesian model averaging (BMA) is a statistical way of postprocessing forecast ensembles to create predictive probability density functions (PDFs) for weather quantities. It represents the predictive PDF as a weighted average of PDFs centered on the individual bias-corrected forecasts, where the weights are posterior probabilities of the models generating the forecasts and reflect the forecasts' relative contributions to predictive skill over a training period. It was developed initially for quantities whose PDFs can be approximated by normal distributions, such as temperature and sea level pressure. BMA does not apply in its original form to precipitation, because the predictive PDF of precipitation is nonnormal in two major ways: it has a positive probability of being equal to zero, and it is skewed. In this study BMA is extended to probabilistic quantitative precipitation forecasting. The predictive PDF corresponding to one ensemble member is a mixture of a discrete component at zero and a gamma distribution. Unlike methods that predict the probability of exceeding a threshold, BMA gives a full probability distribution for future precipitation. The method was applied to daily 48-h forecasts of 24-h accumulated precipitation in the North American Pacific Northwest in 2003-04 using the University of Washington mesoscale ensemble. It yielded predictive distributions that were calibrated and sharp. It also gave probability of precipitation forecasts that were much better calibrated than those based on consensus voting of the ensemble members. It gave better estimates of the probability of high-precipitation events than logistic regression on the cube root of the ensemble mean. © 2007 American Meteorological Society.},
	number = {9},
	journal = {Monthly Weather Review},
	author = {Sloughter, J. Mc Lean and Raftery, Adrian E. and Gneiting, Tilmann and Fraley, Chris},
	month = sep,
	year = {2007},
	pages = {3209--3220},
}

@article{han_deep_2021,
	title = {A {Deep} {Learning} {Method} for {Bias} {Correction} of {ECMWF} 24–240 h {Forecasts}},
	volume = {38},
	issn = {18619533},
	doi = {10.1007/s00376-021-0215-y},
	abstract = {Correcting the forecast bias of numerical weather prediction models is important for severe weather warnings. The refined grid forecast requires direct correction on gridded forecast products, as opposed to correcting forecast data only at individual weather stations. In this study, a deep learning method called CU-net is proposed to correct the gridded forecasts of four weather variables from the European Centre for Medium-Range Weather Forecast Integrated Forecasting System global model (ECMWF-IFS): 2-m temperature, 2-m relative humidity, 10-m wind speed, and 10-m wind direction, with a forecast lead time of 24 h to 240 h in North China. First, the forecast correction problem is transformed into an image-to-image translation problem in deep learning under the CU-net architecture, which is based on convolutional neural networks. Second, the ECMWF-IFS forecasts and ECMWF reanalysis data (ERA5) from 2005 to 2018 are used as training, validation, and testing datasets. The predictors and labels (ground truth) of the model are created using the ECMWF-IFS and ERA5, respectively. Finally, the correction performance of CU-net is compared with a conventional method, anomaly numerical correction with observations (ANO). Results show that forecasts from CU-net have lower root mean square error, bias, mean absolute error, and higher correlation coefficient than those from ANO for all forecast lead times from 24 h to 240 h. CU-net improves upon the ECMWF-IFS forecast for all four weather variables in terms of the above evaluation metrics, whereas ANO improves upon ECMWF-IFS performance only for 2-m temperature and relative humidity. For the correction of the 10-m wind direction forecast, which is often difficult to achieve, CU-net also improves the correction performance.},
	number = {9},
	journal = {Advances in Atmospheric Sciences},
	author = {Han, Lei and Chen, Mingxuan and Chen, Kangkai and Chen, Haonan and Zhang, Yanbiao and Lu, Bing and Song, Linye and Qin, Rui},
	month = sep,
	year = {2021},
	note = {Publisher: Science Press},
	keywords = {ECMWF, Numerical weather prediction, bias correction, deep learning},
	pages = {1444--1459},
}

@article{wang_universal_2002,
	title = {A universal image quality index},
	volume = {9},
	issn = {10709908},
	doi = {10.1109/97.995823},
	abstract = {We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu/{\textasciitilde}zwang/research/quality\_index/demo.html.},
	number = {3},
	journal = {IEEE Signal Processing Letters},
	author = {Wang, Zhou and Bovik, Alan C.},
	month = mar,
	year = {2002},
	keywords = {Human visual system (HVS), Image quality measurement, Mean squared error (MSE)},
	pages = {81--84},
}

@article{teutschbein_bias_2012,
	title = {Bias correction of regional climate model simulations for hydrological climate-change impact studies: {Review} and evaluation of different methods},
	volume = {456-457},
	issn = {00221694},
	doi = {10.1016/j.jhydrol.2012.05.052},
	abstract = {Despite the increasing use of regional climate model (RCM) simulations in hydrological climate-change impact studies, their application is challenging due to the risk of considerable biases. To deal with these biases, several bias correction methods have been developed recently, ranging from simple scaling to rather sophisticated approaches. This paper provides a review of available bias correction methods and demonstrates how they can be used to correct for deviations in an ensemble of 11 different RCM-simulated temperature and precipitation series. The performance of all methods was assessed in several ways: At first, differently corrected RCM data was compared to observed climate data. The second evaluation was based on the combined influence of corrected RCM-simulated temperature and precipitation on hydrological simulations of monthly mean streamflow as well as spring and autumn flood peaks for five catchments in Sweden under current (1961-1990) climate conditions. Finally, the impact on hydrological simulations based on projected future (2021-2050) climate conditions was compared for the different bias correction methods. Improvement of uncorrected RCM climate variables was achieved with all bias correction approaches. While all methods were able to correct the mean values, there were clear differences in their ability to correct other statistical properties such as standard deviation or percentiles. Simulated streamflow characteristics were sensitive to the quality of driving input data: Simulations driven with bias-corrected RCM variables fitted observed values better than simulations forced with uncorrected RCM climate variables and had more narrow variability bounds. © 2012 Elsevier B.V.},
	journal = {Journal of Hydrology},
	author = {Teutschbein, Claudia and Seibert, Jan},
	month = aug,
	year = {2012},
	keywords = {Bias correction, Downscaling, HBV, Hydrology, RCM, Streamflow},
	pages = {12--29},
}

@article{gutierrez_intercomparison_2019,
	title = {An intercomparison of a large ensemble of statistical downscaling methods over {Europe}: {Results} from the {VALUE} perfect predictor cross-validation experiment},
	volume = {39},
	issn = {10970088},
	doi = {10.1002/joc.5462},
	abstract = {VALUE is an open European collaboration to intercompare downscaling approaches for climate change research, focusing on different validation aspects (marginal, temporal, extremes, spatial, process-based, etc.). Here we describe the participating methods and first results from the first experiment, using “perfect” reanalysis (and reanalysis-driven regional climate model (RCM)) predictors to assess the intrinsic performance of the methods for downscaling precipitation and temperatures over a set of 86 stations representative of the main climatic regions in Europe. This study constitutes the largest and most comprehensive to date intercomparison of statistical downscaling methods, covering the three common downscaling approaches (perfect prognosis, model output statistics—including bias correction—and weather generators) with a total of over 50 downscaling methods representative of the most common techniques. Overall, most of the downscaling methods greatly improve (reanalysis or RCM) raw model biases and no approach or technique seems to be superior in general, because there is a large method-to-method variability. The main factors most influencing the results are the seasonal calibration of the methods (e.g., using a moving window) and their stochastic nature. The particular predictors used also play an important role in cases where the comparison was possible, both for the validation results and for the strength of the predictor–predictand link, indicating the local variability explained. However, the present study cannot give a conclusive assessment of the skill of the methods to simulate regional future climates, and further experiments will be soon performed in the framework of the EURO-CORDEX initiative (where VALUE activities have merged and follow on). Finally, research transparency and reproducibility has been a major concern and substantive steps have been taken. In particular, the necessary data to run the experiments are provided at http://www.value-cost.eu/data and data and validation results are available from the VALUE validation portal for further investigation: http://www.value-cost.eu/validationportal.},
	number = {9},
	journal = {International Journal of Climatology},
	author = {Gutiérrez, J. M. and Maraun, D. and Widmann, M. and Huth, R. and Hertig, E. and Benestad, R. and Roessler, O. and Wibig, J. and Wilcke, R. and Kotlarski, S. and San Martín, D. and Herrera, S. and Bedia, J. and Casanueva, A. and Manzanas, R. and Iturbide, M. and Vrac, M. and Dubrovsky, M. and Ribalaygua, J. and Pórtoles, J. and Räty, O. and Räisänen, J. and Hingray, B. and Raynaud, D. and Casado, M. J. and Ramos, P. and Zerenner, T. and Turco, M. and Bosshard, T. and Štěpánek, P. and Bartholy, J. and Pongracz, R. and Keller, D. E. and Fischer, A. M. and Cardoso, R. M. and Soares, P. M.M. and Czernecki, B. and Pagé, C.},
	month = jul,
	year = {2019},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {CORDEX, bias adjustment, downscaling, model output statistics, perfect prognosis, reproducibility, validation, weather generators, ★},
	pages = {3750--3785},
}

@article{glahn_use_1972,
	title = {The {Use} of {Model} {Output} {Statistics} ({MOS}) in {Objective} {Weather} {Forecasting}},
	volume = {11},
	number = {8},
	journal = {Journal of Applied Meteorology and Climatology},
	author = {Glahn, Harry R and Lowry, Dale A},
	year = {1972},
	pages = {1203--1211},
}

@techreport{gneiting_calibrated_2005,
	title = {Calibrated {Probabilistic} {Forecasting} {Using} {Ensemble} {Model} {Output} {Statistics} and {Minimum} {CRPS} {Estimation}},
	abstract = {Ensemble prediction systems typically show positive spread-error correlation, but they are subject to forecast bias and dispersion errors, and are therefore uncalibrated. This work proposes the use of ensemble model output statistics (EMOS), an easy-to-implement postprocessing technique that addresses both forecast bias and underdispersion and takes into account the spread-skill relationship. The technique is based on multiple linear regression and is akin to the superensemble approach that has traditionally been used for deterministic-style forecasts. The EMOS technique yields probabilistic forecasts that take the form of Gaussian predictive probability density functions (PDFs) for continuous weather variables and can be applied to gridded model output. The EMOS predictive mean is a bias-corrected weighted average of the ensemble member forecasts, with coefficients that can be interpreted in terms of the relative contributions of the member models to the ensemble, and provides a highly competitive deterministic-style forecast. The EMOS predictive variance is a linear function of the ensemble variance. For fitting the EMOS coefficients, the method of minimum continuous ranked probability score (CRPS) estimation is introduced. This technique finds the coefficient values that optimize the CRPS for the training data. The EMOS technique was applied to 48-h forecasts of sea level pressure and surface temperature over the North American Pacific Northwest in spring 2000, using the University of Washington mesoscale ensemble. When compared to the bias-corrected ensemble, deterministic-style EMOS forecasts of sea level pressure had root-mean-square error 9\% less and mean absolute error 7\% less. The EMOS predictive PDFs were sharp, and much better calibrated than the raw ensemble or the bias-corrected ensemble.},
	author = {Gneiting, Tilmann and Raftery, Adrian E and Westveld Iii, Anton H and Goldman, Tom},
	year = {2005},
}

@inproceedings{wu_image_2016,
	title = {Image quality assessment based on {Structure} {Similarity}},
	isbn = {978-1-5090-2708-8},
	doi = {10.1109/ICSPCC.2016.7753620},
	abstract = {Image Structure Similarity (SSIM) and its extended versions have been successfully used in image quality assessment. In this paper, we propose a similarity metric to evaluate image quality by extracting image sparse structure from natural scene image. A sparse dictionary trained on the data contains the basic elements for representing sparse structures, and it is insensitive to different databases. The sparse structure similarity of testing image pairs is calculated with this dictionary. The final score of image quality is obtained by counting the changed number of elements in sparse structure vector between distorted image and reference image. Experiments demonstrate that the proposed method could assess image quality effectively and outperform existing SSIM based methods.},
	booktitle = {{ICSPCC} 2016 - {IEEE} {International} {Conference} on {Signal} {Processing}, {Communications} and {Computing}, {Conference} {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Wu, Jun and Li, Huifang and Xia, Zhaoqiang},
	month = nov,
	year = {2016},
	keywords = {Image quality, dictionary learning, full-reference assessment, sparse structure},
}

@inproceedings{wang_multi-scale_2003,
	title = {Multi-scale structural similarity for image quality assessment},
	volume = {2},
	doi = {10.1109/acssc.2003.1292216},
	abstract = {The structural similarity image quality paradigm is based on the assumption that the human visual system is highly adapted for extracting structural information from the scene, and therefore a measure of structural similarity can provide a good approximation to perceived image quality. This paper proposes a multi-scale structural similarity method, which supplies more flexibility than previous single-scale methods in incorporating the variations of viewing conditions. We develop an image synthesis method to calibrate the parameters that define the relative importance of different scales. Experimental comparisons demonstrate the effectiveness of the proposed method.},
	booktitle = {Conference {Record} of the {Asilomar} {Conference} on {Signals}, {Systems} and {Computers}},
	author = {Wang, Zhou and Simoncelli, Eero P. and Bovik, Alan C.},
	year = {2003},
	note = {ISSN: 10586393},
	pages = {1398--1402},
}

@techreport{rio_inequalities_nodate,
	title = {{INEQUALITIES} {AND} {LIMIT} {THEOREMS} {FOR} {WEAKLY} {DEPENDENT} {SEQUENCES}},
	author = {Rio, Emmanuel},
}

@article{wang_evaluation_2017,
	title = {Evaluation of the {GPM} {IMERG} satellite-based precipitation products and the hydrological utility},
	volume = {196},
	issn = {01698095},
	doi = {10.1016/j.atmosres.2017.06.020},
	abstract = {Pre-occupation evaluation of latest generation satellite-based precipitation products (SPPs) is an essential step before the massive scale use. Taking the Beijiang River Basin as the case study, we used nine statistical evaluation indices and the Variable Infiltration Capacity (VIC) distributed hydrological model to quantitatively evaluate the performance and the hydrological utility of three Global Precipitation Measurement (GPM) Integrated Multi-satellitE Retrievals for GPM (IMERG) products: the near-real-time “Early” run and “Late” run IMERG products (IMERG-E and IMERG-L), and the post-real-time “Final” run IMERG product (IMERG-F) over south China during 2014–2015, with the last-generation Tropical Rainfall Measurement Mission (TRMM) Multi-satellite Precipitation Analysis (TMPA) 3B42-V7 product as comparison. The IMERG-F presents satisfactory accuracy with high correlation coefficient (CC = 0.63) and low relative bias (0.92\%), while the IMERG-E and IMERG-L performs relatively poorly featuring low correlation (with CC of 0.49 and 0.52 respectively) with the ground observations. All of the three IMERG products present apparently higher probability of detection (POD, 0.64–0.67) but have higher false alarm ratio (FAR, ≧ 0.14) than the 3B42-V7. The hydrological simulation under scenario I (model calibrated by the gauge observations) shows that, the IMERG-F, with a high Nash–Sutcliffe coefficient of efficiency (NSCE) of 0.742, presents better hydrological performance than the 3B42-V7; the IMERG-E and IMERG-L perform poorly for the whole simulation period with NSCE lower than 0.35 and relative bias higher than 28\% while perform satisfactorily during the flood season with apparently higher NSCE of 0.750 and 0.733 respectively. The hydrological simulation under scenario II (model calibrated by the 3B42-V7) shows that the performance of all the IMERG products was significantly improved. Generally, the IMERG-F has high accuracy and good hydrological utility, while the IMERG-E and IMERG-L products have satisfactory hydrological utility during the flood season and thus have great potential for the real-time application such as flood forecasting. The late-run IMERG-L presents little improvement in performance comparing with the early-run IMERG-E, therefore, the timelier IMERG-E is recommended to be firstly considered.},
	journal = {Atmospheric Research},
	author = {Wang, Zhaoli and Zhong, Ruida and Lai, Chengguang and Chen, Jiachao},
	month = nov,
	year = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {GPM IMERG, Hydrological utility, Satellite-based precipitation product, The Beijiang River Basin, Variable Infiltration Capacity model},
	pages = {151--163},
}

@article{tang_evaluation_2016,
	title = {Evaluation of {GPM} {Day}-1 {IMERG} and {TMPA} {Version}-7 legacy products over {Mainland} {China} at multiple spatiotemporal scales},
	volume = {533},
	issn = {00221694},
	doi = {10.1016/j.jhydrol.2015.12.008},
	abstract = {The post-real time product of Day-1 Integrated Multi-satellitE Retrievals for Global Precipitation Measurement (IMERG) is evaluated over Mainland China from April to December 2014 at the hourly timescale, against data from hourly ground-based observations. In addition, the IMERG product is compared with its predecessor-the Version-7 post-real-time 3B42 (3B42V7) product of Tropical Rainfall Measuring Mission (TRMM) Multisatellite Precipitation Analysis (TMPA) at its original 3-hourly and then daily timescales for the same period. All the products are cross-evaluated at gridded, regional, and national scales. Results show that: (1) the Day-1 IMERG shows appreciably better performance than 3B42V7 at both sub-daily and daily timescales, and all the three spatial scales. The gap between the two products is more significant at the sub-daily resolution; (2) Out of the six sub-regions of China, IMERG especially performs better than 3B42V7 at the mid- and high-latitudes, as well as relatively dry climate regions; (3) IMERG can better reproduce the probability density function (PDF) in terms of precipitation intensity, particularly in the low ranges; and (4) although IMERG better captures the precipitation diurnal variability, both products have room to further improve their capability, particularly in the dry climate and high-latitude regions. This study is among the earliest evaluation and comparison of IMERG and 3B42V7 products, which could be valuable in providing reference for the development of IMERG algorithms, associated global products, and various applications as well.},
	journal = {Journal of Hydrology},
	author = {Tang, Guoqiang and Ma, Yingzhao and Long, Di and Zhong, Lingzhi and Hong, Yang},
	month = feb,
	year = {2016},
	note = {Publisher: Elsevier B.V.},
	keywords = {GPM, IMERG, Precipitation, TRMM},
	pages = {152--167},
}

@article{sylla_uncertainties_2013,
	title = {Uncertainties in daily rainfall over {Africa}: {Assessment} of gridded observation products and evaluation of a regional climate model simulation},
	volume = {33},
	issn = {08998418},
	doi = {10.1002/joc.3551},
	abstract = {We intercompare three gridded observed daily rainfall datasets over Africa (FEWS (Famine Early Warning System), GPCP (Global Precipitation Climatology Project) and TRMM (Tropical Rainfall Measuring Mission)) in order to assess uncertainties in observation products towards the evaluation of the performance of a Regional Climate Model (RegCM3) in simulating daily precipitation characteristics over a domain encompassing the whole African continent. We find that different observation products exhibit substantial systematic differences in mean rainfall, but especially in higher order daily precipitation statistics, such as frequency of wet days, precipitation intensity and extremes as well as maximum length of wet and dry spells. For example, FEWS shows mostly higher frequency and lower intensity events than TRMM and GPCP. Thus, the different datasets provide quite different representations of daily precipitation behavior. As a result, although RegCM3 captures pretty well the monsoon rainband evolution and exhibits a representation of daily precipitation statistics within the range of the observations, it performs differently with respect to the various products. For instance, it simulates more intense but less frequent events over East and Southern Africa than in FEWS and vice versa compared to TRMM. We thus highlight the uncertainty in observations as a key factor preventing a rigorous and unambiguous evaluation of climate models over Africa. Improving the quality and consistency of observation products is thus paramount for a better understanding of the response of African climate to global warming. © 2012 Royal Meteorological Society.},
	number = {7},
	journal = {International Journal of Climatology},
	author = {Sylla, M. B. and Giorgi, F. and Coppola, E. and Mariotti, L.},
	month = jun,
	year = {2013},
	keywords = {Daily rainfall, Higher order statistics, Observation products, Regional climate model evaluation, Uncertainties},
	pages = {1805--1817},
}

@article{blanchard_multi-scale_2022,
	title = {A {Multi}-{Scale} {Deep} {Learning} {Framework} for {Projecting} {Weather} {Extremes}},
	url = {http://arxiv.org/abs/2210.12137},
	abstract = {Weather extremes are a major societal and economic hazard, claiming thousands of lives and causing billions of dollars in damage every year. Under climate change, their impact and intensity are expected to worsen significantly. Unfortunately, general circulation models (GCMs), which are currently the primary tool for climate projections, cannot characterize weather extremes accurately. To address this, we present a multi-resolution deep-learning framework that, firstly, corrects a GCM's biases by matching low-order and tail statistics of its output with observations at coarse scales; and secondly, increases the level of detail of the debiased GCM output by reconstructing the finer scales as a function of the coarse scales. We use the proposed framework to generate statistically realistic realizations of the climate over Western Europe from a simple GCM corrected using observational atmospheric reanalysis. We also discuss implications for probabilistic risk assessment of natural disasters in a changing climate.},
	author = {Blanchard, Antoine and Parashar, Nishant and Dodov, Boyko and Lessig, Christian and Sapsis, Themistoklis},
	month = oct,
	year = {2022},
	note = {arXiv: 2210.12137},
}

@article{murphy_hedging_1973,
	title = {Hedging and {Skill} {Scores} for {Probability} {Forecasts}},
	journal = {Journal of Applied Meteorology},
	author = {Murphy, Allan H},
	year = {1973},
	pages = {215--223},
}

@article{jolliffe_proper_2008,
	title = {Proper scores for probability forecasts can never be equitable},
	volume = {136},
	issn = {00270644},
	doi = {10.1175/2007MWR2194.1},
	abstract = {Verification is an important part of any forecasting system. It is usually achieved by computing the value of some measure or score that indicates how good the forecasts are. Many possible verification measures have been proposed, and to choose between them a number of desirable properties have been defined. For probability forecasts of a binary event, two of the best known of these properties are propriety and equitability. A proof that the two properties are incompatible for a wide class of verification measures is given in this paper, after briefly reviewing the two properties and some recent attempts to improve properties for the well-known Brier skill score. © 2008 American Meteorological Society.},
	number = {4},
	journal = {Monthly Weather Review},
	author = {Jolliffe, Ian T. and Stephenson, David B.},
	month = apr,
	year = {2008},
	keywords = {★},
	pages = {1505--1510},
}

@article{weusthoff_assessing_2010,
	title = {Assessing the benefits of convection-permitting models by neighborhood verification: {Examples} from {MAP} {D}-{PHASE}},
	volume = {138},
	issn = {00270644},
	doi = {10.1175/2010MWR3380.1},
	abstract = {High-resolution numerical weather prediction (NWP) models produce more detailed precipitation structures but the real benefit is probably the more realistic statistics gained with the higher resolution and not the information on the specific grid point. By evaluating three model pairs, each consisting of a high-resolution NWP system resolving convection explicitly and its low-resolution-driving model with parameterized convection, on different spatial scales and for different thresholds, this paper addresses the question of whether high-resolution models really perform better than their driving lower-resolution counterparts. The model pairs are evaluated by means of two fuzzy verification methods-upscaling (UP) and fractions skill score (FSS)-for the 6 months of the D-PHASE Operations Period and in a highly complex terrain. Observations are provided by the Swiss radar composite and the evaluation is restricted to the area covered by the Swiss radar stations. The high-resolution models outperform or equal the performance of their respective lower-resolution driving models. The differences between the models are significant and robust against small changes in the verification settings. An evaluation based on individual months shows that high-resolution models give better results, particularly with regard to convective, more localized precipitation events. © 2010 American Meteorological Society.},
	number = {9},
	journal = {Monthly Weather Review},
	author = {Weusthoff, Tanja and Ament, Felix and Arpagaus, Marco and Rotach, Mathias W.},
	month = sep,
	year = {2010},
	pages = {3418--3433},
}

@article{hemri_trends_2014,
	title = {Trends in the predictive performance of raw ensemble weather forecasts},
	volume = {41},
	issn = {19448007},
	doi = {10.1002/2014GL062472},
	abstract = {This study applies statistical postprocessing to ensemble forecasts of near-surface temperature, 24 h precipitation totals, and near-surface wind speed from the global model of the European Centre for Medium-Range Weather Forecasts (ECMWF). The main objective is to evaluate the evolution of the difference in skill between the raw ensemble and the postprocessed forecasts. Reliability and sharpness, and hence skill, of the former is expected to improve over time. Thus, the gain by postprocessing is expected to decrease. Based on ECMWF forecasts from January 2002 to March 2014 and corresponding observations from globally distributed stations, we generate postprocessed forecasts by ensemble model output statistics (EMOS) for each station and variable. Given the higher average skill of the postprocessed forecasts, we analyze the evolution of the difference in skill between raw ensemble and EMOS. This skill gap remains almost constant over time indicating that postprocessing will keep adding skill in the foreseeable future. Key Points Evolution of raw ensemble forecast skillFuture benefits from statistical postprocessingGlobal distribution of forecast skill development},
	number = {24},
	journal = {Geophysical Research Letters},
	author = {Hemri, S. and Scheuerer, M. and Pappenberger, F. and Bogner, K. and Haiden, T.},
	month = dec,
	year = {2014},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {EMOS, ensemble weather forecasts, model verification, statistical postprocessing, ★},
	pages = {9197--9205},
}

@article{mittermaier_how_2019,
	title = {How interpolation and resolution can affect verification scores: {A} study based on the {Fractions} {Skill} {Score}},
	volume = {28},
	issn = {16101227},
	doi = {10.1127/metz/2018/0890},
	abstract = {The Fractions Skill Score (FSS) has been used to routinely verify six-hourly UK precipitation forecasts since early 2008. For a 17-month period between May 2011 and September 2012 precipitation forecasts from the 4-km version of the Met Office Unified Model (UK4) were evaluated against two versions of radar-rainfall analyses, the 5-km product which had existed for many years, and a new 1-km product created using a revised algorithm. The standard practice is to interpolate the model forecast to the observation grid. This paper looks at the combined impact of the resolution of the verifying grid and interpolation on the magnitude of the FSS for six-hour accumulations. The 90th percentile threshold FSS mean scores based on the 1-km grid are higher for the 5-km neighbourhood. The 5- and 1-km verification grid scores are closest for the 25-km neighbourhood. For larger neighbourhoods there are several lead times and times of the day where the 5-km grid mean score is higher. In a cumulative sense the 1-km grid 90th percentile threshold FSS is 10-15 \% higher for the 5-km neighbourhood. This is relative to the 5-km grid score and based on a 180-day running mean. Individual score differences are significant at the 5 \% level. It suggests that the same 4-km model forecasts downscaled to a 1-km grid, and evaluated over a 5-km neighbourhood, score better than when upscaled (slightly) to a 5-km grid, which is close to the model grid scale. For the 25-km neighbourhood the cumulative relative score difference is reduced to less than ±3 \% in the 180-day running means, though the differences for some lead times and times of day are significant at the 10 \% level. The same trend is present for 51 and 101-km neighbourhoods, with an increasing number of lead times and times of day having a 5-km grid score which is higher. The choices made at the outset regarding interpolation and the detail (or lack thereof) in a verifying data set are influential enough to be non-neglible. For a spatial score such as the FSS the impact changes with neighbourhood size.},
	number = {3},
	journal = {Meteorologische Zeitschrift},
	author = {Mittermaier, Marion P.},
	year = {2019},
	note = {Publisher: Gebruder Borntraeger Verlagsbuchhandlung},
	keywords = {Fractions Skill Score, Interpolation, Resolution, Verification},
	pages = {181--192},
}

@article{romine_model_2013,
	title = {Model bias in a continuously cycled assimilation system and its influence on convection-permitting forecasts},
	volume = {141},
	issn = {00270644},
	doi = {10.1175/MWR-D-12-00112.1},
	abstract = {During the spring 2011 season, a real-time continuously cycled ensemble data assimilation system using the Advanced Research version of the Weather Research and Forecasting Model (WRF) coupled with the Data Assimilation Research Testbed toolkit provided initial and boundary conditions for deterministic convectionpermitting forecasts, also usingWRF, over the eastern two-thirds of the conterminousUnited States (CONUS). In this study the authors evaluate the mesoscale assimilation systemand the convection-permitting forecasts, at 15-and 3-km grid spacing, respectively. Experiments employing different physics options within the continuously cycled ensemble data assimilation systemare shown to lead to differences in the meanmesoscale analysis characteristics. Convection-permitting forecasts with a fixed model configuration are initialized from these physics-varied analyses, as well as control runs from 0.5° Global Forecast System (GFS) analysis. Systematic bias in the analysis background influences the analysis fit to observations, and when this analysis initializes convection-permitting forecasts, the forecast skill is degraded as bias in the analysis background increases. Moreover, differences in mean error characteristics associated with each physical parameterization suite lead to unique errors of spatial, temporal, and intensity aspects of convection-permitting rainfall forecasts. Observation bias by platform type is also shown to impact the analysis quality. © 2013 American Meteorological Society.},
	number = {4},
	journal = {Monthly Weather Review},
	author = {Romine, Glen S. and Schwartz, Craig S. and Snyder, Chris and Anderson, Jeff L. and Weisman, Morris L.},
	month = apr,
	year = {2013},
	pages = {1263--1284},
}

@article{ayzel_rainnet_2020,
	title = {{RainNet} v1.0: {A} convolutional neural network for radar-based precipitation nowcasting},
	volume = {13},
	issn = {19919603},
	doi = {10.5194/gmd-13-2631-2020},
	abstract = {In this study, we present RainNet, a deep convolutional neural network for radar-based precipitation nowcasting. Its design was inspired by the U-Net and SegNet families of deep learning models, which were originally designed for binary segmentation tasks. RainNet was trained to predict continuous precipitation intensities at a lead time of 5min, using several years of quality-controlled weather radar composites provided by the German Weather Service (DWD). That data set covers Germany with a spatial domain of 900 km×900km and has a resolution of 1km in space and 5min in time. Independent verification experiments were carried out on 11 summer precipitation events from 2016 to 2017. In order to achieve a lead time of 1h, a recursive approach was implemented by using RainNet predictions at 5min lead times as model inputs for longer lead times. In the verification experiments, trivial Eulerian persistence and a conventional model based on optical flow served as benchmarks. The latter is available in the rainymotion library and had previously been shown to outperform DWD's operational nowcasting model for the same set of verification events. RainNet significantly outperforms the benchmark models at all lead times up to 60min for the routine verification metrics mean absolute error (MAE) and the critical success index (CSI) at intensity thresholds of 0.125, 1, and 5mm h-1. However, rainymotion turned out to be superior in predicting the exceedance of higher intensity thresholds (here 10 and 15mm h-1). The limited ability of RainNet to predict heavy rainfall intensities is an undesirable property which we attribute to a high level of spatial smoothing introduced by the model. At a lead time of 5min, an analysis of power spectral density confirmed a significant loss of spectral power at length scales of 16km and below. Obviously, RainNet had learned an optimal level of smoothing to produce a nowcast at 5min lead time. In that sense, the loss of spectral power at small scales is informative, too, as it reflects the limits of predictability as a function of spatial scale. Beyond the lead time of 5min, however, the increasing level of smoothing is a mere artifact-an analogue to numerical diffusion-that is not a property of RainNet itself but of its recursive application. In the context of early warning, the smoothing is particularly unfavorable since pronounced features of intense precipitation tend to get lost over longer lead times. Hence, we propose several options to address this issue in prospective research, including an adjustment of the loss function for model training, model training for longer lead times, and the prediction of threshold exceedance in terms of a binary segmentation task. Furthermore, we suggest additional input data that could help to better identify situations with imminent precipitation dynamics. The model code, pretrained weights, and training data are provided in open repositories as an input for such future studies.},
	number = {6},
	journal = {Geoscientific Model Development},
	author = {Ayzel, Georgy and Scheffer, Tobias and Heistermann, Maik},
	month = jun,
	year = {2020},
	note = {Publisher: Copernicus GmbH},
	pages = {2631--2644},
}

@article{ravuri_skilful_2021,
	title = {Skilful precipitation nowcasting using deep generative models of radar},
	volume = {597},
	issn = {14764687},
	doi = {10.1038/s41586-021-03854-z},
	abstract = {Precipitation nowcasting, the high-resolution forecasting of precipitation up to two hours ahead, supports the real-world socioeconomic needs of many sectors reliant on weather-dependent decision-making1,2. State-of-the-art operational nowcasting methods typically advect precipitation fields with radar-based wind estimates, and struggle to capture important non-linear events such as convective initiations3,4. Recently introduced deep learning methods use radar to directly predict future rain rates, free of physical constraints5,6. While they accurately predict low-intensity rainfall, their operational utility is limited because their lack of constraints produces blurry nowcasts at longer lead times, yielding poor performance on rarer medium-to-heavy rain events. Here we present a deep generative model for the probabilistic nowcasting of precipitation from radar that addresses these challenges. Using statistical, economic and cognitive measures, we show that our method provides improved forecast quality, forecast consistency and forecast value. Our model produces realistic and spatiotemporally consistent predictions over regions up to 1,536 km × 1,280 km and with lead times from 5–90 min ahead. Using a systematic evaluation by more than 50 expert meteorologists, we show that our generative model ranked first for its accuracy and usefulness in 89\% of cases against two competitive methods. When verified quantitatively, these nowcasts are skillful without resorting to blurring. We show that generative nowcasting can provide probabilistic predictions that improve forecast value and support operational utility, and at resolutions and lead times where alternative methods struggle.},
	number = {7878},
	journal = {Nature},
	author = {Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and Prudden, Rachel and Mandhane, Amol and Clark, Aidan and Brock, Andrew and Simonyan, Karen and Hadsell, Raia and Robinson, Niall and Clancy, Ellen and Arribas, Alberto and Mohamed, Shakir},
	month = sep,
	year = {2021},
	pmid = {34588668},
	note = {arXiv: 2104.00954
Publisher: Nature Research},
	pages = {672--677},
}

@article{bousquet_analysis_2006,
	title = {Analysis of scale dependence of quantitative precipitation forecast verification: {A} case-study over the {Mackenzie} river basin},
	volume = {132},
	issn = {00359009},
	doi = {10.1256/qj.05.154},
	abstract = {Six-hour rainfall accumulations derived from radar observations collected during a 3-day summertime precipitation event over central Alberta (Canada) are used to assess the performance of a regional Canadian numerical weather prediction system for quantitative precipitation forecast verification. We show that radar data provide a simple and efficient way to significantly reduce model phase errors associated with misplacement of predicted precipitation patterns. Using wavelet analysis, we determine that the limiting spatial scale of predictability of the model is about six times its grid resolution for 6 h accumulated fields. The use of longer accumulation periods is shown to smooth out forecast errors that may have resulted from slight phase or time shift errors but does not change the limiting scale of predictability. The scale decomposition of the mean-square forecast error also reveals that scales which cannot be accurately reproduced by the model account for about 20\% of the total error. Using classical continuous and categorical scores, we show that significantly better model performance can be achieved by smoothing out wavelengths that cannot be predicted. © Royal Meteorological Society, 2006.},
	number = {620},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Bousquet, Olivier and Lin, Charles A. and Zawadzki, Isztar},
	month = oct,
	year = {2006},
	keywords = {Numerical weather prediction (NWP) systems, Quantitative precipitation forecast (QPF), Radar, Wavelet transform},
	pages = {2107--2125},
}

@inproceedings{mason_understanding_2008,
	title = {Understanding forecast verification statistics},
	volume = {15},
	doi = {10.1002/met.51},
	abstract = {Although there are numerous reasons for performing a verification analysis, there are usually two general questions that are of interest: are the forecasts good, and can we be confident that the estimate of forecast quality is not misleading? When calculating a verification score, it is not usually obvious how the score can answer either of these questions. Some procedures for attempting to answer the questions are reviewed, with particular focus on p-values and confidence intervals. P-values are shown to be rather unhelpful in answering either question, especially when applied to probabilistic verification scores, and confidence intervals are to be preferred. However, confidence intervals cannot reveal biases in the value of a score that arises from an inadequate experimental design for testing on truly out-of-sample observations. Some specific problems with cross validation are highlighted. Finally, in the interests of increasing the insight into forecast strengths and weaknesses and in pointing towards methods for improving forecast quality, a plea is made for a more discriminating selection of verification procedures than has been adopted to date. Copyright © 2008 Royal Meteorological Society.},
	booktitle = {Meteorological {Applications}},
	publisher = {John Wiley and Sons Ltd},
	author = {Mason, Simon J.},
	year = {2008},
	note = {Issue: 1
ISSN: 14698080},
	keywords = {Bootstrap, Cross validation, Effectiveness, Equitability, Locality, P-values, Permutation, Propriety},
	pages = {31--40},
}

@article{mittermaier_long-term_2013,
	title = {A long-term assessment of precipitation forecast skill using the {Fractions} {Skill} {Score}},
	volume = {20},
	issn = {14698080},
	doi = {10.1002/met.296},
	abstract = {The Fractions Skill Score (FSS) is a spatial verification metric routinely computed in the operational verification suite. It enables the comparison of forecasts of different resolutions against a common spatial truth (radar rainfall analyses) in such a way that high-resolution forecasts are not penalized for representativeness errors that arise from the 'double penalty' problem. Officially Met Office model precipitation forecast accuracy is monitored using the Equitable Threat Score (ETS) at gauge locations. These precipitation scores form part of a basket of measures assessing six surface parameters known as the UK index, which forms the basis for making decisions regarding model upgrades (especially over the UK). It is used to monitor the impact of continuous model improvements. This framework and the methodology underlying it, is less appropriate for high-resolution forecasts for reasons as described above. For precipitation forecasts in particular, a new framework for long-term monitoring is necessary and the FSS provides such a potential framework. This paper provides an objective critique of FSS results to date. It has been shown that the 'convection-permitting' (4 km) Unified Model (MetUM) forecasts are better than the 12 km MetUM (significant at the 5\% level). The scale at which the models have sufficient practical skill is typically 10 km better for the high-resolution forecasts, and are better at forecasting afternoon convection exceeding 4 mm (6 h)-1. The use of frequency (percentile) thresholds is recommended because of the implicit bias removal this approach provides, as any rain in a forecast period is treated as 'the event of interest'. © 2011 British Crown Copyright, the Met Office.},
	number = {2},
	journal = {Meteorological Applications},
	author = {Mittermaier, Marion and Roberts, Nigel and Thompson, Simon A.},
	year = {2013},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {High-resolution NWP, Long-term monitoring against radar, Spatial verification methods, precipitation},
	pages = {176--186},
}

@article{fierro_impact_2015,
	title = {Impact of storm-scale lightning data assimilation on {WRF}-{ARW} precipitation forecasts during the 2013 warm season over the contiguous {United} {States}},
	volume = {143},
	issn = {15200493},
	doi = {10.1175/MWR-D-14-00183.1},
	abstract = {This work evaluates the performance of a recently developed cloud-scale lightning data assimilation technique implemented within the Weather Research and Forecasting Model running at convection-allowing scales (4-km grid spacing). Data provided by the Earth Networks Total Lightning Network for the contiguous United States (CONUS) were assimilated in real time over 67 days spanning the 2013 warm season (May-July). The lightning data were assimilated during the first 2 h of simulations each day. Bias-corrected, neighborhood-based, equitable threat scores (BC-ETSs) were the chief metric used to quantify the skill of the forecasts utilizing this assimilation scheme. Owing to inferior observational data quality over mountainous terrain, this evaluation focused on the eastern two-thirds of the United States. During the first 3 h following the assimilation (i.e., 3-h forecasts), all the simulations suffered from a high wet bias in forecasted accumulated precipitation (APCP), particularly for the lightning assimilation run (LIGHT). Forecasts produced by LIGHT, however, had a noticeable, statistically significant (α = 0.05) improvement over those by the control run (CTRL) up to 6 h into the forecast with BC-ETS differences often exceeding 0.4. This improvement was seen independently of the APCP threshold (ranging from 2.5 to 50 mm) and the neighborhood radius (ranging from 0 to 40 km) selected. Past 6 h of the forecast, the APCP fields from LIGHT progressively converged to that of CTRL probably due to the longer-term evolution being bounded by the large-scale model environment. Thus, this computationally inexpensive lightning assimilation scheme shows considerable promise for routinely improving short-term (≤6 h) forecasts of high-impact weather by convection-allowing forecast models.},
	number = {3},
	journal = {Monthly Weather Review},
	author = {Fierro, Alexandre O. and Clark, Adam J. and Mansell, Edward R. and Macgorman, Donald R. and Dembek, Scott R. and Ziegler, Conrad L.},
	year = {2015},
	note = {Publisher: American Meteorological Society},
	keywords = {Cloud microphysics, Cloud resolving models, Data assimilation, Lightning, Model evaluation/performance, Numerical weather prediction/forecasting},
	pages = {757--777},
}

@article{woodhams_what_2018,
	title = {What is the added value of a convection-permitting model for forecasting extreme rainfall over tropical {East} {Africa}?},
	volume = {146},
	issn = {15200493},
	doi = {10.1175/MWR-D-17-0396.1},
	abstract = {Forecasting convective rainfall in the tropics is a major challenge for numerical weather prediction. The use of convection-permitting (CP) forecast models in the tropics has lagged behind the midlatitudes, despite the great potential of such models in this region. In the scientific literature, there is very little evaluation of CP models in the tropics, especially over an extended time period. This paper evaluates the prediction of convective storms for a period of 2 years in the Met Office operational CP model over East Africa and the global operational forecast model. A novel localized form of the fractions skill score is introduced, which shows variation in model skill across the spatial domain. Overall, the CP model and the global model both outperform a 24-h persistence forecast. The CP model shows greater skill than the global model, in particular on subdaily time scales and for storms over land. Forecasts over Lake Victoria are also improved in the CP model, with an increase in hit rate of up to 20\%. Contrary to studies in the midlatitudes, the skill of both models shows a large dependence on the time of day and comparatively little dependence on the forecast lead time within a 48-h forecast. Although these results provide more motivation for forecasters to use the CP model to produce subdaily forecasts with increased detail, there is a clear need for more in situ observations for data assimilation into the models and for verification. A move toward ensemble forecasting could have further benefits.},
	number = {9},
	journal = {Monthly Weather Review},
	author = {Woodhams, Beth J. and Birch, Cathryn E. and Marsham, John H. and Bain, Caroline L. and Roberts, Nigel M. and Boyd, Douglas F.A.},
	month = sep,
	year = {2018},
	note = {Publisher: American Meteorological Society},
	keywords = {Africa, Cloud resolving models, Inland seas/lakes, Model evaluation/performance, Rainfall, Regional models},
	pages = {2757--2780},
}

@article{nachamkin_applying_2015,
	title = {Applying a neighborhood fractions sampling approach as a diagnostic tool},
	volume = {143},
	issn = {15200493},
	doi = {10.1175/MWR-D-14-00411.1},
	abstract = {The fractions skill score (FSS) belongs to a class of spatial neighborhood techniques that measures forecast skill from samples of gridded forecasts and observations at increasing spatial scales. Each sample contains the fraction of the predicted and observed quantities that exist above a threshold value. Skill is gauged by the rate that the observed and predicted fractions converge with increasing scale. In this study, neighborhood sampling is applied to diagnose the performance of high-resolution (1.67 km) precipitation forecasts over central Florida. Reliability diagrams derived from the spatial fractions indicate that the FSS can be influenced by small, low-predictability events. Further tests indicate the FSS is subtly affected by samples from points on and near the grid boundaries. Inclusion of these points tends to reduce the magnitude and sensitivity of the FSS, especially at large scales. An attempt to mine data from the set of neighborhood fractions was moderately successful at obtaining descriptive information about the precipitation fields. The width of the distribution of the fractions at each scale provided information concerning forecast resolution and sharpness. The rate at which the distribution of the fractions converged toward the domain mean with increasing scale was found to be sensitive to the uniformity of coverage of precipitation through the domain. Generally, the 6-h forecasts possessed greater spatial skill than those at 12 h. High-FSS values at 12 h were mostly associated with evenly distributed precipitation patterns, while the 6-h forecasts also performed well for several nonuniform cases.},
	number = {11},
	journal = {Monthly Weather Review},
	author = {Nachamkin, Jason E. and Schmidt, Jerome},
	year = {2015},
	note = {Publisher: American Meteorological Society},
	keywords = {Forecast verification/skill, Mesoscale forecasting},
	pages = {4736--4749},
}

@techreport{faggian_fast_2015,
	title = {Fast calculation of the fractions skill score},
	abstract = {The forecast verification metric known as the Fractions Skill Score (FSS) is typicallycomputed using sliding window operators, which can be computationally expensive. A keycomponent of the score is the computation of fractional event frequencies, which is equivalent to a weighted summation of sub-grids (windows) commonly realized as a convolutionoperation. An alternative approach is to use "summed area tables", which have been used incomputer graphics as a means to quickly compute summations of sub-grids in texture fields.In this paper we describe how a summed area table can effectively reduce the computationtime of the FSS while also allowing the score to generalize to include the time dimension.We demonstrate the methodology on idealized cases from the Spatial Verification MethodsInter-comparison Project and explore the properties of the score on a high-resolution NWPdataset.},
	author = {Faggian, Nathan and Roux, Belinda and Steinle, Peter and Ebert, Beth},
	year = {2015},
	note = {Publication Title: MAUSAM
Volume: 66
Issue: 3},
	keywords = {Fractions skill score (FSS), NWP},
	pages = {457--466},
}

@article{simecek-beatty_oil_2021,
	title = {Oil spill forecast assessment using {Fractions} {Skill} {Score}},
	volume = {164},
	issn = {0025326X},
	doi = {10.1016/j.marpolbul.2021.112041},
	journal = {Marine Pollution Bulletin},
	author = {Simecek-Beatty, Debra and Lehr, William J.},
	month = mar,
	year = {2021},
	pages = {112041},
}

@article{manzato_behaviour_2017,
	title = {Behaviour of verification measures for deterministic binary forecasts with respect to random changes and thresholding},
	volume = {143},
	issn = {1477870X},
	doi = {10.1002/qj.3050},
	abstract = {There are many possible verification measures when deterministic binary forecasts are made. In deciding which to use, various properties of the measures are of interest. One type of property explores what happens when certain types of change are made to the forecasts. In this article, two types of change and their effects on various verification measures are examined. Underlying some desirable properties of measures, such as propriety, equitability and consistency, is the desire that it should be impossible to improve the value, or expected value, of a measure by changing the original forecast from the forecaster's ‘true belief’. This behaviour is often called ‘hedging’. Among the types of change that ought not to lead to improved forecasts are random changes of subsets of the true binary forecasts. The effect of such changes is discussed here. A second type of change examined here arises when a continuous forecast is thresholded to produce the binary forecast. When this type of ‘thresholding’ is practiced a posteriori, it can be seen as a fine-tuning of the forecasting system, in order to improve its verification performance. In the thresholding framework, one can optimize some verification performance measures with respect to others. In particular, this article discusses the optimization of bias and of the Peirce skill score.},
	number = {705},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Manzato, Agostino and Jolliffe, Ian},
	month = apr,
	year = {2017},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Peirce skill score, bias, binary classifiers, forecast verification, hedging, posterior probability, ★},
	pages = {1903--1915},
}

@article{mittermaier_meta_2021,
	title = {A “{Meta}” {Analysis} of the {Fractions} {Skill} {Score}: {The} {Limiting} {Case} and {Implications} for {Aggregation}},
	volume = {149},
	issn = {0027-0644},
	doi = {10.1175/mwr-d-18-0106.1},
	abstract = {The fractions skill score (FSS) is arguably one of the most popular spatial verification metrics in use today. The fraction of grid points exceeding a threshold within a forecast and observed field neighborhood are examined to compute a score. By definition a perfect forecast has an FSS of 1, and a “no skill” forecast has a score of 0. It is shown that the denominator defines the score’s characteristics. The FSS is undefined for instances in which both the forecast and the observed field do not exceed a threshold. In the limiting case, the FSS for a perfect null (zero) forecast is also undefined, unless a threshold of  0 is used, in which case it would be 1 (i.e., perfect). Furthermore the FSS is 0 if either the forecast or the observed field does not exceed a threshold. This symmetry means it cannot differentiate between what are traditionally referred to as false alarms or misses. Additional supplementary information is required. The FSS is greater than 0 if and only if there are values exceeding a given threshold in both the forecast and the observed field. The magnitude of an overall score computed over many forecasts is sensitive to the pooling method. Zero scores are nontrivial. Excluding them implies excluding all situations associated with false alarms or misses. Omitting near-zero scores is a more credible decision, but only if it can be proven that these are related to spurious artifacts in the observed field. To avoid ambiguity the components of the FSS should be aggregated separately for computing an overall score for most applications and purposes.},
	number = {10},
	journal = {Monthly Weather Review},
	author = {Mittermaier, M. P.},
	month = feb,
	year = {2021},
	note = {Publisher: American Meteorological Society},
	pages = {3491--3504},
}

@article{wolff_beyond_2014,
	title = {Beyond the basics: {Evaluating} model-based precipitation forecasts using traditional, spatial, and object-based methods},
	volume = {29},
	issn = {15200434},
	doi = {10.1175/WAF-D-13-00135.1},
	abstract = {While traditional verification methods are commonly used to assess numerical model quantitative precipitation forecasts (QPFs) using a grid-to-grid approach, they generally offer little diagnostic information or reasoning behind the computed statistic. On the other hand, advanced spatial verification techniques, such as neighborhood and object-based methods, can provide more meaningful insight into differences between forecast and observed features in terms of skill with spatial scale, coverage area, displacement, orientation, and intensity. To demonstrate the utility of applying advanced verification techniques to mid- and coarseresolution models, the Developmental Testbed Center (DTC) applied several traditional metrics and spatial verification techniques to QPFs provided by the Global Forecast System (GFS) and operational North American Mesoscale Model (NAM). Along with frequency bias and Gilbert skill score (GSS) adjusted for bias, both the fractions skill score (FSS) and Method for Object-Based Diagnostic Evaluation (MODE) were utilized for this study with careful consideration given to how these methods were applied and how the results were interpreted. By illustrating the types of forecast attributes appropriate to assess with the spatial verification techniques, this paper provides examples of how to obtain advanced diagnostic information to help identify what aspects of the forecast are or are not performing well.},
	number = {6},
	journal = {Weather and Forecasting},
	author = {Wolff, Jamie K. and Harrold, Michelle and Fowler, Tressa and Gotway, John Halley and Nance, Louisa and Brown, Barbara G.},
	year = {2014},
	note = {Publisher: American Meteorological Society},
	keywords = {Forecast verification/skill, Model comparison, Model evaluation/performance, ★},
	pages = {1451--1472},
}

@article{marsham_role_2013,
	title = {The role of moist convection in the {West} {African} monsoon system: {Insights} from continental-scale convection-permitting simulations},
	volume = {40},
	issn = {00948276},
	doi = {10.1002/grl.50347},
	abstract = {Predicting the West African monsoon (WAM) remains a major challenge for weather and climate models. We compare multiday continental-scale simulations of the WAM that explicitly resolve moist convection with simulations which parameterize convection. Simulations with the same grid spacing but differing representations of convection isolate the impact of the representation of convection. The more realistic explicit convection gives greater latent and radiative heating farther north, with latent heating later in the day. This weakens the Sahel-Sahara pressure gradient and the monsoon flow, delaying its diurnal cycle and changing interactions between the monsoon and boundary layer convection. In explicit runs, cold storm outflows provide a significant component of the monsoon flux. In an operational global model, biases resemble those in our parameterized case. Improved parameterizations of convection that better capture storm structures, their diurnal cycle, and rainfall intensities will therefore substantially improve predictions of the WAM and coupled aspects of the Earth system. Key PointsSimulations of West African Monsoon with explicit and parameterized convectionMore realistic explicit convection weakens monsoon and delays diurnal cycleCold storm outflows are a significant component of the monsoon in explicit runs ©2013. American Geophysical Union. All Rights Reserved.},
	number = {9},
	journal = {Geophysical Research Letters},
	author = {Marsham, John H. and Dixon, Nick S. and Garcia-Carreras, Luis and Lister, Grenville M.S. and Parker, Douglas J. and Knippertz, Peter and Birch, Cathryn E.},
	month = may,
	year = {2013},
	keywords = {Explicit and parameterized convection, Moist convection, West African monsoon, cold-pool outflows, ★},
	pages = {1843--1849},
}

@article{ebert_progress_2013,
	title = {Progress and challenges in forecast verification},
	volume = {20},
	issn = {14698080},
	doi = {10.1002/met.1392},
	abstract = {Verification scientists and practitioners came together at the 5th International Verification Methods Workshop in Melbourne, Australia, in December2011 to discuss methods for evaluating forecasts within a wide variety of applications. Progress has been made in many areas including improved verification reporting, wider use of diagnostic verification, development of new scores and techniques for difficult problems, and evaluation of forecasts for applications using meteorological information. There are many interesting challenges, particularly the improvement of methods to verify high resolution ensemble forecasts, seamless predictions spanning multiple spatial and temporal scales, and multivariate forecasts. Greater efforts are needed to make best use of new observations, forge greater links between data assimilation and verification, and develop better and more intuitive forecast verification products for end-users. © 2013 Royal Meteorological Society.},
	number = {2},
	journal = {Meteorological Applications},
	author = {Ebert, E. and Wilson, L. and Weigel, A. and Mittermaier, M. and Nurmi, P. and Gill, P. and Göber, M. and Joslyn, S. and Brown, B. and Fowler, T. and Watkins, A.},
	year = {2013},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Challenges, Evaluation, Forecast verification, ★},
	pages = {130--139},
}

@article{harvey_spatial_2016,
	title = {Spatial evaluation of volcanic ash forecasts using satellite observations},
	volume = {16},
	issn = {16807324},
	doi = {10.5194/acp-16-861-2016},
	abstract = {The decision to close airspace in the event of a volcanic eruption is based on hazard maps of predicted ash extent. These are produced using output from volcanic ash transport and dispersion (VATD) models. In this paper the fractions skill score has been used for the first time to evaluate the spatial accuracy of VATD simulations relative to satellite retrievals of volcanic ash. This objective measure of skill provides more information than traditional point-by-point metrics, such as success index and Pearson correlation coefficient, as it takes into the account spatial scale over which skill is being assessed. The FSS determines the scale over which a simulation has skill and can differentiate between a "near miss" and a forecast that is badly misplaced. The idealized scenarios presented show that even simulations with considerable displacement errors have useful skill when evaluated over neighbourhood scales of 200-700 (km)2. This method could be used to compare forecasts produced by different VATDs or using different model parameters, assess the impact of assimilating satellite-retrieved ash data and evaluate VATD forecasts over a long time period.},
	number = {2},
	journal = {Atmospheric Chemistry and Physics},
	author = {Harvey, N. J. and Dacre, H. F.},
	month = jan,
	year = {2016},
	note = {Publisher: Copernicus GmbH},
	pages = {861--872},
}

@article{skok_analysis_2016,
	title = {Analysis of {Fractions} {Skill} {Score} properties for random precipitation fields and {ECMWF} forecasts},
	volume = {142},
	issn = {1477870X},
	doi = {10.1002/qj.2849},
	abstract = {The Fractions Skill Score (FSS) is a spatial verification measure that is used for assessing the performance of precipitation forecasts from numerical weather prediction models. Previous studies have shown that the FSS is able to give a direct measure of the error in the placement of the rain. This article takes the approach further and derives analytical expressions and uses Monte-Carlo simulations for randomly positioned observed and forecast rainfall to reveal further characteristics of the FSS in both infinite and bounded domains. It reveals that the definition of an FSS value that determines the minimum scale at which a forecast should be deemed ‘useful’ (useful forecast criteria) is a meaningful concept and shows how this value increases with increasing fractional rainfall coverage. A study of real forecast data is also presented using 8 years of European Centre for Medium-Range Weather Forecasts (ECMWF) model forecasts, out to a lead time of 9 days, over domains of differing sizes covering parts of Europe and North Africa. The FSS is examined using different strategies for dealing with the domain boundary and is compared with the analytical study. The findings give practical guidance on how to use the FSS. For most situations a FSS value of {\textgreater}0.5 serves as a good indicator of a useful forecast. The choice of domain size for rainfall forecast verification should consider the typical spatial errors of the forecast. For a domain that is large compared to the typical spatial error, the boundaries have little adverse affect, but this is not the case if the spatial errors start to become comparable to the size of the domain. The evaluation of ECMWF forecasts reveals the extent of the spatial errors that emerge for medium-range forecasts and show the value of verifying those forecasts using the FSS over an appropriately sized region.},
	number = {700},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Skok, Gregor and Roberts, Nigel},
	month = oct,
	year = {2016},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {FSS, Fractions Skill Score, precipitation, verification},
	pages = {2599--2610},
}

@article{mittermaier_intercomparison_2010,
	title = {Intercomparison of spatial forecast verification methods: {Identifying} skillful spatial scales using the fractions skill score},
	volume = {25},
	issn = {08828156},
	doi = {10.1175/2009WAF2222260.1},
	abstract = {The fractions skill score (FSS) was one of the measures that formed part of the Intercomparison of Spatial Forecast Verification Methods project. The FSS was used to assess a common dataset that consisted of real and perturbed Weather Research and Forecasting (WRF) model precipitation forecasts, as well as geometric cases. These datasets are all based on the NCEP 240 grid, which translates to approximately 4-km resolution over the contiguous United States. The geometric cases showed that the FSS can provide a truthful assessment of displacement errors and forecast skill. In addition, the FSS can be used to determine the scale at which an acceptable level of skill is reached and this usage is perhaps more helpful than interpreting the actual FSS value. This spatial-scale approach is becoming more popular for monitoring operational forecast performance. The study also shows how the FSS responds to forecast bias.Amore biased forecast always gives lower FSS values at large scales and usually at smaller scales. It is possible, however, for a more biased forecast to give a higher score at smaller scales, when additional rain overlaps the observed rain. However, given a sufficiently large sample of forecasts, a more biased forecast system will score lower. The use of percentile thresholds can remove the impacts of the bias. When the proportion of the domain that is "wet" (the wet-area ratio) is small, subtle differences introduced through near-threshold misses can lead to large changes in FSS magnitude in individual cases (primarily because the bias is changed). Reliable statistics for small wet-area ratios require a larger sample of forecasts. Care needs to be taken in the choice of verification domain. For high-resolution models, the domain should be large enough to encompass the length scale of the typical mesoscale forcing (e.g., upper-level troughs or squall lines). If the domain is too large, the wet-area ratios will always be small. If the domain is too small, fluctuations in the wet-area ratio can be large and larger spatial errors may be missed. The FSS is a good measure of the spatial accuracy of precipitation forecasts. Different methods are needed to determine other patterns of behavior. © 2010 American Meteorological Society.},
	number = {1},
	journal = {Weather and Forecasting},
	author = {Mittermaier, Marion and Roberts, Nigel},
	month = feb,
	year = {2010},
	pages = {343--354},
}

@article{skok_analysis_2015,
	title = {Analysis of {Fraction} {Skill} {Score} properties for a displaced rainband in a rectangular domain},
	volume = {22},
	issn = {14698080},
	doi = {10.1002/met.1478},
	abstract = {A compact analytical expression of the Fraction Skill Score (FSS) is derived for a case with a single displaced rainband in a rectangular domain. The rainband is oriented parallel to the border and displaced perpendicularly to its orientation. An analytical solution is used to determine some of the properties of the FSS which might also be applicable in other cases. The solution is independent of the length of the rainband (it is valid also for a displaced rainy grid point). The position of the borders perpendicular to the rainband orientation does not influence the FSS value. The position of the other two borders does however influence the FSS value in a complex way; moving a border closer to the rainbands can either increase or decrease the FSS value depending on the location of the borders. FSS is shown to be a monotonically increasing function of the neighbourhood size (regardless of the position of the borders). If the FSS value for a displacement that is half the neighbourhood size is used to define a 'useful' FSS value then the usefulness criterion is somewhat different than presented in the original FSS paper (there is no dependence on the frequency of the observations/forecasts). 'Useful' FSS values are always {\textgreater}1/2 but depend on the position of the borders and the size of the displacement.},
	number = {3},
	journal = {Meteorological Applications},
	author = {Skok, Gregor},
	month = jul,
	year = {2015},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Fraction Skill Score, Precipitation, Rainband, Verification},
	pages = {477--484},
}

@article{skok_estimating_2018,
	title = {Estimating the displacement in precipitation forecasts using the {Fractions} {Skill} {Score}},
	volume = {144},
	issn = {1477870X},
	doi = {10.1002/qj.3212},
	abstract = {The Fractions Skill Score (FSS) is a popular spatial verification metric commonly used for precipitation verification. In this study we focus on analysing the ability of FSS to provide meaningful information about the displacement between precipitation in one field compared to another. A simple overlap-adjusted use of the FSS is introduced and a number of relevant idealized cases are analysed that show that the FSS can indeed be used to determine displacement in a meaningful way. It was found that the displacement provided by the FSS is directly related to the true displacements of precipitation but with larger contiguous precipitation objects having a much larger influence. Overall, the displacement provided via the FSS compares well with the average distance to the closest neighbouring precipitation object (assuming the objects are of similar size). It is recommended that the user should use a frequency (percentile) threshold when focussing on spatial differences unless biases are known to be small and adopt the overlap-adjusted variant of the FSS displacement. If the frequency bias is very large the FSS-derived displacements become less reliable. The same is true of any spatial comparison. A recipe for the use of the FSS for determining displacements is provided.},
	number = {711},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Skok, Gregor and Roberts, Nigel},
	month = jan,
	year = {2018},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {FSS, Fractions Skill Score, precipitation, spatial displacement, verification},
	pages = {414--425},
}

@article{roberts_scale-selective_2008,
	title = {Scale-selective verification of rainfall accumulations from high-resolution forecasts of convective events},
	volume = {136},
	issn = {00270644},
	doi = {10.1175/2007MWR2123.1},
	abstract = {The development of NWP models with grid spacing down to ∼1 km should produce more realistic forecasts of convective storms. However, greater realism does not necessarily mean more accurate precipitation forecasts. The rapid growth of errors on small scales in conjunction with preexisting errors on larger scales may limit the usefulness of such models. The purpose of this paper is to examine whether improved model resolution alone is able to produce more skillful precipitation forecasts on useful scales, and how the skill varies with spatial scale. A verification method will be described in which skill is determined from a comparison of rainfall forecasts with radar using fractional coverage over different sized areas. The Met Office Unified Model was run with grid spacings of 12, 4, and 1 km for 10 days in which convection occurred during the summers of 2003 and 2004. All forecasts were run from 12-km initial states for a clean comparison. The results show that the 1-km model was the most skillful over all but the smallest scales (approximately {\textless}10-15 km). A measure of acceptable skill was defined; this was attained by the 1-km model at scales around 40-70 km, some 10-20 km less than that of the 12-km model. The biggest improvement occurred for heavier, more localized rain, despite it being more difficult to predict. The 4-km model did not improve much on the 12-km model because of the difficulties of representing convection at that resolution, which was accentuated by the spinup from 12-km fields.},
	number = {1},
	journal = {Monthly Weather Review},
	author = {Roberts, Nigel M. and Lean, Humphrey W.},
	month = jan,
	year = {2008},
	pages = {78--97},
}

@article{vogel_skill_2018,
	title = {Skill of {Global} {Raw} and {Postprocessed} {Ensemble} {Predictions} of {Rainfall} over {Northern} {Tropical} {Africa}},
	volume = {33},
	url = {https://doi.org/10.1175/WAF-D-17-},
	doi = {10.1175/WAF-D-17},
	abstract = {Accumulated precipitation forecasts are of high socioeconomic importance for agriculturally dominated societies in northern tropical Africa. In this study, the performance of nine operational global ensemble prediction systems (EPSs) is analyzed relative to climatology-based forecasts for 1-5-day accumulated precipitation based on the monsoon seasons during 2007-14 for three regions within northern tropical Africa. To assess the full potential of raw ensemble forecasts across spatial scales, state-of-the-art statistical post-processing methods were applied in the form of Bayesian model averaging (BMA) and ensemble model output statistics (EMOS), and results were verified against station and spatially aggregated, satellite-based gridded observations. Raw ensemble forecasts are uncalibrated and unreliable, and often underperform relative to climatology, independently of region, accumulation time, monsoon season, and ensemble. The differences between raw ensemble and climatological forecasts are large and partly stem from poor prediction for low precipitation amounts. BMA and EMOS postprocessed forecasts are calibrated, reliable, and strongly improve on the raw ensembles but, somewhat disappointingly, typically do not outperform climatology. Most EPSs exhibit slight improvements over the period 2007-14, but overall they have little added value compared to climatology. The suspicion is that parameterization of convection is a potential cause for the sobering lack of ensemble forecast skill in a region dominated by mesoscale convective systems.},
	number = {2},
	journal = {Weather and Forecasting},
	author = {Vogel, Peter and Knippertz, Peter and Fink, Andreas H and Schlueter, Andreas},
	year = {2018},
	pages = {369--388},
}

@article{nicholson_climate_2017,
	title = {Climate and climatic variability of rainfall over eastern {Africa}},
	volume = {55},
	number = {3},
	journal = {Reviews of Geophysics},
	author = {Nicholson, Sharon E},
	year = {2017},
	pages = {590--635},
}

@article{ahijevych_application_2009,
	title = {Application of spatial verification methods to idealized and {NWP}-gridded precipitation forecasts},
	volume = {24},
	issn = {08828156},
	doi = {10.1175/2009WAF2222298.1},
	abstract = {Several spatial forecast verification methods have been developed that are suited for high-resolution precipitation forecasts. They can account for the spatial coherence of precipitation and give credit to a forecast that does not necessarily match the observation at any particular grid point. The methods were grouped into four broad categories (neighborhood, scale separation, features based, and field deformation) for the Spatial Forecast Verification Methods Intercomparison Project (ICP). Participants were asked to apply their new methods to a set of artificial geometric and perturbed forecasts with prescribed errors, and a set of real forecasts of convective precipitation on a 4-km grid. This paper describes the intercomparison test cases, summarizes results from the geometric cases, and presents subjective scores and traditional scores from the real cases. All the new methods could detect bias error, and the features-based and field deformation methods were also able to diagnose displacement errors of precipitation features. The best approach for capturing errors in aspect ratio was field deformation. When comparing model forecasts with real cases, the traditional verification scores did not agree with the subjective assessment of the forecasts. © 2009 American Meteorological Society.},
	number = {6},
	journal = {Weather and Forecasting},
	author = {Ahijevych, David and Gilleland, Eric and Brown, Barbara G. and Ebert, Elizabeth E.},
	month = dec,
	year = {2009},
	pages = {1485--1497},
}

@inproceedings{casati_forecast_2008,
	title = {Forecast verification: {Current} status and future directions},
	volume = {15},
	doi = {10.1002/met.52},
	abstract = {Research and development of new verification strategies and reassessment of traditional forecast verification methods has received a groat deal of attention from the scientific community in the last decade. This scientific effort has arisen from the need to respond to changes encompassing several aspects of the verification process, such as the evolution of forecasting systems, or the desire for more meaningful verification approaches that address specific forecast user requirements. Verification techniques that account for the spatial structure and the presence of features in forecast fields, and which are designed specifically for high-resolution forecasts have been developed. The advent of ensemble forecasts has motivated the re-evaluation of some of the traditional scores and the development of new verification methods for probability forecasts. The expected climatological increase of extreme events and their potential socio-economical impacts have revitalized research studies addressing the challenges concerning extreme event verification. Verification issues encountered in the operational forecasting environment have been widely discussed, verification needs for different user communities have been identified, and models to assess the forecast value for specific users have been proposed. Proper verification practice and correct interpretation of verification statistics has been extensively promoted with recent publications and books, tutorials and workshops, and the development of open-source software and verification tools. This paper addresses some of the current issues in forecast verification, reviews some of the most recently developed verification techniques, and provides recommendations for future research. Copyright © 2008 Royal Meteorological Society and Crown in the right of Canada.},
	booktitle = {Meteorological {Applications}},
	publisher = {John Wiley and Sons Ltd},
	author = {Casati, Barbara and Wilson, L. J. and Stephenson, D. B. and Nurmi, P. and Ghelli, Anna and Pocernich, M. and Damrath, U. and Ebert, E. E. and Brown, B. G. and Mason, S.},
	year = {2008},
	note = {Issue: 1
ISSN: 14698080},
	keywords = {Extreme events verification, Operational verification, Probability forecasts and ensemble verification, Spatial verification approaches, User-oriented verification, Value, Verification packages},
	pages = {3--18},
}

@article{wilkinson_technique_2017,
	title = {A technique for verification of convection-permitting {NWP} model deterministic forecasts of lightning activity},
	volume = {32},
	issn = {15200434},
	doi = {10.1175/WAF-D-16-0106.1},
	abstract = {This manuscript introduces a new technique for evaluating lightning forecasts from convection-permitting models. In recent years, numerical weather prediction models at the convection-permitting scales (horizontal grid resolutions of 1-5 km) have been able to produce realistic-looking forecasts of lightning activity when compared with observations. However, it is challenging to assess what value these forecasts add above standard large-scale indices. Examining this problem, it is found that existing skill scores and neighborhood verification methods are unable to cope with both the double-penalty effect and the model's variable frequency bias. A displacement distance and a quasi-symmetric distance score are introduced based on the distance between the model and the observations, the latter showing any improvement the forecast has over a completely "hedged" forecast. This can be combined with a domain-improved contingency table and comparisons between modeled and observed lightning flashes to evaluate the forecast performance in three important dimensions: coverage, distance, and intensity. The verification metric is illustrated with a single case, which shows that the convective-scale U.K. variable resolution model (UKV) delivers improved forecasts compared with the large-scale indices in both coverage and distance. Additionally, a month-long analysis is performed, which reveals that the coverage of lightning is in good agreement with the observations; lightning is displaced by the model by a distance on the order of 50-75 km, but the model overpredicts the lightning intensity by at least a factor of 6 after observational detection efficiencies have been considered.},
	number = {1},
	journal = {Weather and Forecasting},
	author = {Wilkinson, Jonathan M.},
	year = {2017},
	note = {Publisher: American Meteorological Society},
	keywords = {Cloud microphysics, Clouds, Convective storms, Forecast verification/skill, Lightning, Numerical weather prediction/forecasting},
	pages = {97--115},
}

@article{noauthor_e3f74f77558daa5801da08ede29e8f36c8bc_nodate,
	title = {e3f74f77558daa5801da08ede29e8f36c8bc},
}

@article{yan_decoherence_2022,
	title = {Decoherence factor as a convolution: an interplay between a {Gaussian} and an exponential coherence loss},
	issn = {13672630},
	doi = {10.1088/1367-2630/ac9fe8},
	abstract = {We identify and investigate the origin and nature of the transition between Gaussian and exponential forms of decoherence: The decoherence factor (that controls the time dependence of the off-diagonal terms of the density matrix expressed in the pointer basis representation) is the convolution of the Fourier transforms of the spectral density and of the overlap (between the eigenstates the environment with and without couplings to the system). Spectral density alone tends to lead to the (approximately) Gaussian decay of coherence while the overlap alone results in a (largely) exponential decay. We show that these two contributions combine as a convolution, their relative importance controlled by the strength of the system- environment coupling. The resultant decoherence factor in the strong and weak coupling limits leads to predominantly Gaussian or exponential decay, respectively, as is demonstrated with two paradigmatic examples of decoherence—a spin-bath model and the quantum Brownian motion.},
	journal = {New Journal of Physics},
	author = {Yan, Bin and Zurek, Wojciech},
	month = nov,
	year = {2022},
	note = {arXiv: 2110.09463
Publisher: IOP Publishing},
}

@article{zepeda-arce_space-time_2000,
	title = {Space-time rainfall organization and its role in validating quantitative precipitation forecasts},
	volume = {105},
	issn = {01480227},
	doi = {10.1029/1999JD901087},
	abstract = {The scope of this paper is to introduce a suite of new multiscale statistical measures which can be used, in addition to traditional measures, to compare observed and model-predicted patterns for model validation. Recent research on analysis of observed precipitation patterns at a multitude of scales has revealed interesting spatial and spatiotemporal organizations which have often been related to physical properties of the storm environment. By testing whether this multiscale statistical organization is also reproduced in the model-predicted patterns or whether there are significant biases and disagreements in such comparisons is conjectured to hold promise for understanding model performance and guiding future model improvements. Results from application of the developed methodologies to the May 7-8, 1995, multisquall line storm over central Oklahoma are presented and discussed in light of the additional information gained by the new validation measures as compared to traditional measures. Copyright 2000 by the American Geophysical Union.},
	number = {D8},
	journal = {Journal of Geophysical Research Atmospheres},
	author = {Zepeda-Arce, Jesus and Foufoula-Georgiou, Efi and Droegemeier, Kelvin K.},
	month = apr,
	year = {2000},
	note = {Publisher: Blackwell Publishing Ltd},
	pages = {10129--10146},
}

@article{marsigli_spatial_2008,
	title = {A spatial verification method applied to the evaluation of high-resolution ensemble forecasts},
	volume = {15},
	issn = {14698080},
	doi = {10.1002/met.65},
	abstract = {The verification of ensemble systems is being operationally carried out in several meteorological centres. However, the main operational ensemble systems have a coarser spatial resolution with respect to the deterministic runs. Only recently, high-resolution limited-area ensembles have started to be run on a regular basis. Their verification requires combining the usual probabilistic evaluation with the statistical verification techniques which are being developed for high-resolution model forecasts (1-10 km). These techniques permit to evaluate a deterministic forecast in a probabilistic manner, by taking into account the spatio-temporal distribution of the forecast at different scales. In this work, a spatial verification technique, called 'distributional method', is used to verify the Consortium for Small-scale MOdeling Limited-area Ensemble Prediction System (COSMO-LEPS) ensemble system, a mesoscale ensemble with 10 km horizontal resolution. The system is mainly designed to give probabilistic assistance in the forecast of severe weather, in particular of intense precipitation possibly leading to floods, hence verification is focused on the ability of the system in forecasting precipitation at high spatial resolution. The methodology is based on a comparison of forecasts and observations in terms of some parameters of their distributions, evaluated after the values are aggregated over boxes of selected size. In particular, performances in terms of average, a few percentiles and maximum forecast value in a box are considered. The system is compared against European Centre for Medium-Range Weather Forecasts Ensemble Prediction System (ECMWF EPS), addressing the issues of an intercomparison between a higher-resolution smaller-size ensemble and a lower-resolution larger-size one. Results show that when the forecast of the average amount of precipitation over an area is concerned, COSMO-LEPS is more skilful than the Ensemble Prediction System (EPS) only from the resolution point of view. Therefore, although not properly calibrated, it is more capable of distinguishing between events and non-events, especially for moderate and high precipitation. Furthermore, COSMO-LEPS has skill in forecasting the occurrence of precipitation peaks over an area, irrespective of the exact location. The analysis of the score behaviour as a function of the distribution parameter shows that EPS has the maximum skill in reproducing the central part of the observed precipitation distribution over an area of about 10 000 km2, while COSMO-LEPS is more skilful in reproducing the tail of the observed precipitation distribution. The problem of the predictability of precipitation at different spatial scales is also investigated, showing the role of different system resolutions. Copyright © 2008 Royal Meteorological Society.},
	number = {1},
	journal = {Meteorological Applications},
	author = {Marsigli, Chiara and Montani, Andrea and Paccangnella, Tiziana},
	month = mar,
	year = {2008},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Ensemble, High-resolution, Verification},
	pages = {125--143},
}

@techreport{skok_supplementary_nodate,
	title = {Supplementary material to: {Analysis} of {Fractions} {Skill} {Score} properties for random precipitation fields and {ECMWF} forecasts},
	author = {Skok, Gregor and Roberts, Nigel},
}

@article{murphy_general_1987,
	title = {A {General} {Framework} for {Forecast} {Verification}},
	volume = {115},
	journal = {Monthly Weather Review},
	author = {Murphy, Allan H and Winkler, Robert L},
	year = {1987},
	pages = {1330--1338},
}

@article{tibshirani_estimating_2001,
	title = {Estimating the number of clusters in a data set via the gap statistic},
	volume = {63},
	issn = {13697412},
	doi = {10.1111/1467-9868.00293},
	abstract = {We propose a method (the 'gap statistic') for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
	year = {2001},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {Clustering, Groups, Hierarchy, K-means, Uniform distribution},
	pages = {411--423},
}

@techreport{wang_incorporating_nodate,
	title = {{INCORPORATING} {SYMMETRY} {INTO} {DEEP} {DYNAMICS} {MODELS} {FOR} {IMPROVED} {GENERALIZATION}},
	url = {https://github.com/Rose-STL-Lab/Equivariant-Net.},
	abstract = {Recent work has shown deep learning can accelerate the prediction of physical dynamics relative to numerical solvers. However, limited physical accuracy and an inability to generalize under distributional shift limits its applicability to the real world. We propose to improve accuracy and generalization by incorporating symmetries into convolutional neural networks. Specifically, we employ a variety of methods each tailored to enforce a different symmetry. Our models are both theoretically and experimentally robust to distributional shift by symmetry group transformations and enjoy favorable sample complexity. We demonstrate the advantage of our approach on a variety of physical dynamics including Rayleigh-Bénard convection and real-world ocean currents and temperatures. Compared with image or text applications, our work is a significant step towards applying equivariant neural networks to high-dimensional systems with complex dynamics. We open-source our simulation, data and code at https://github.com/Rose-STL-Lab/Equivariant-Net.},
	author = {Wang, Rui and Walters, Robin and Yu, Rose},
	keywords = {★},
}

@article{pasaric_comments_2011,
	title = {Comments on "{Applying} a general analytic method for assessing bias sensitivity to bias-adjusted threat and equitable threat scores"},
	volume = {26},
	issn = {08828156},
	doi = {10.1175/2010WAF2222453.1},
	number = {1},
	journal = {Weather and Forecasting},
	author = {Pasarić, Zoran and Juras, Josip},
	month = feb,
	year = {2011},
	keywords = {Bias, Error analysis, Forecast verification, Statistics, ★},
	pages = {122--125},
}

@techreport{ling_conditioned_nodate,
	title = {Conditioned {Spatial} {Downscaling} of {Climate} {Variables}},
	url = {https://github.com/evbecker/climate-spatial-downscaling},
	abstract = {Global Climate Models (GCM) play a vital role in assessing the large-scale impacts of climate change. Downscaling methods can translate coarse-resolution climate information from GCM to high-resolution predictions to forecast regional effects. Unfortunately, current downscaling methods struggle to fully take into account spatial relationships among variables, especially at long distances. In this work, we propose an instance-conditional pixel synthesis generative adversarial network (ICPS-GAN), wherein conditioning on spatial information is an explicit way of providing the GAN with previous high-resolution and current low-resolution data, resulting in an enhancement of the general performance. Experimental results on precipitation forecast for US region data outperform both traditional and other learning-based methods when extrapolating in space. The code is available at https://github.com/evbecker/climate-spatial-downscaling},
	author = {Ling, Alex and Hung, Yu and Becker, Evan and Zadouri, Ted and Grover, Aditya},
	keywords = {★},
}

@inproceedings{roberts_assessing_2008,
	title = {Assessing the spatial and temporal variation in the skill of precipitation forecasts from an {NWP} model},
	volume = {15},
	doi = {10.1002/met.57},
	abstract = {It is becoming increasingly important to be able to verify the spatial accuracy of precipitation forecasts, especially with the advent of high-resolution numerical weather prediction (NWP) models. In this article, the fractions skill score (FSS) approach has been used to perform a scale-selective evaluation of precipitation forecasts during 2003 from the Met Office mesoscale model (12 km grid length). The investigation shows how skill varies with spatial scale, the scales over which the data assimilation (DA) adds most skill, and how the loss of that skill is dependent on both the spatial scale and the rainfall coverage being examined. Although these results come from a specific model, they demonstrate how this verification approach can provide a quantitative assessment of the spatial behaviour of new finer-resolution models and DA techniques. Copyright © 2008 Royal Meteorological Society.},
	booktitle = {Meteorological {Applications}},
	publisher = {John Wiley and Sons Ltd},
	author = {Roberts, Nigel},
	year = {2008},
	note = {Issue: 1
ISSN: 14698080},
	keywords = {Forecasts, Precipitation, Scale-selective, Verification},
	pages = {163--169},
}

@techreport{sawyer_department_nodate,
	title = {{DEPARTMENT} {OF} {COMMERCE} {WEATHER} {BUREAU} {MONTHLY} {WEATHER} {REVIEW} {VERIFICATION} {OF} {FORECASTS} {EXPRESSED} {IN} {TERMS} {OF} {PROBABILITY}},
	author = {Sawyer, Charles and Reichelderfer, F W and Editor, James E and Caskey, J R and Brie, Glenn W},
}

@techreport{noauthor_approximations_nodate,
	title = {Approximations for {Mean} and {Variance} of a {Ratio}},
}

@article{orskaug_evaluation_2011,
	title = {Evaluation of a dynamic downscaling of precipitation over the {Norwegian} mainland},
	issn = {0280-6495},
	doi = {10.3402/tellusa.v63i4.15859},
	abstract = {In order to assess the potential of regional climate models to be used to project future weather events, a first step is to study the regional model forced by actual weather, or more precisely by reanalysis of weather data. In this paper we investigate how well the Norwegian regional model HIRHAM, forced by ERA-40 reanalysis data, compares to observed precipitation data from the Norwegian Meteorological Institute over Norwegian mainland. This paper aims to show how standard methods of statistical testing may be used to assess dynamic downscaling. Methods considered are the Kolmogorov-Smirnov two-sample test, a Fisher exact test for equality of quantiles, an Extreme Value Theory test, where equality of the 1-yr return levels are tested, and equality of wet-day frequency. All tests are performed seasonally. The regional model is skillful in describing the lower quartile of the precipitation distribution, but underestimates higher levels of precipitation. Our results indicate that the regional model has too many but too small rain events for all seasons.},
	journal = {Tellus A},
	author = {Orskaug, E. and Scheel, I. and Frigessi, A. and Guttorp, P. and Haugen, J. E. and Tveito, O. E. and Haug, O.},
	month = aug,
	year = {2011},
	note = {Publisher: Stockholm University Press},
}

@techreport{hamill_hypothesis_1999,
	title = {Hypothesis {Tests} for {Evaluating} {Numerical} {Precipitation} {Forecasts}},
	abstract = {When evaluating differences between competing precipitation forecasts, formal hypothesis testing is rarely performed. This may be due to the difficulty in applying common tests given the spatial correlation of and non-normality of errors. Possible ways around these difficulties are explored here. Two datasets of precipitation forecasts are evaluated, a set of two competing gridded precipitation forecasts from operational weather prediction models and sets of competing probabilistic quantitative precipitation forecasts from model output statistics and from an ensemble of forecasts. For each test, data from each competing forecast are collected into one sample for each case day to avoid problems with spatial correlation. Next, several possible hypothesis test methods are evaluated: the paired t test, the nonparametric Wilcoxon signed-rank test, and two resampling tests. The more involved resampling test methodology is the most appropriate when testing threat scores from nonprobabilistic forecasts. The simpler paired t test or Wilcoxon test is appropriate to use in testing the skill of probabilistic forecasts evaluated with the ranked probability score.},
	author = {Hamill, Thomas M},
	year = {1999},
}

@techreport{hinkley_ratio_1969,
	title = {On the {Ratio} of {Two} {Correlated} {Normal} {Random} {Variables}},
	url = {https://www.jstor.org/stable/2334671},
	author = {Hinkley, D V},
	year = {1969},
	note = {Volume: 56
Issue: 3},
	pages = {635--639},
}

@article{north_assessment_2013,
	title = {An assessment of the {SEEPS} and {SEDI} metrics for the verification of 6h forecast precipitation accumulations},
	volume = {20},
	issn = {14698080},
	doi = {10.1002/met.1405},
	abstract = {An evaluation of the performance of the Stable Equitable Error in Probability Space (SEEPS) and the Symmetric Extremal Dependence Index (SEDI) as monitoring metrics for 6h precipitation forecasts from regional NWP is presented. These scores provide complementary assessments of forecast performance. SEEPS quantifies general performance in the prediction of dry weather and precipitation amount. SEDI focuses on higher threshold events. By using the climatological distribution of precipitation at each location to define thresholds, both scores assess the locally important aspects of the forecast. Both scores can also be aggregated over climatologically diverse regions to obtain an area-mean performance measure. From the perspective of forecast system development, an important aspect of both scores is their resistance to hedging. Each score is calculated for forecasts coming from two model systems, one a 12km regional configuration of the Met Office Unified Model (MetUM), the other is the independent European Centre for Medium-range Weather Forecasts (ECMWF) global model; here interpolated to a 25km grid. Results suggest that SEEPS can be meaningfully applied to 6h accumulations. Diurnal variations in SEEPS highlight timing errors in the development of convection. A decomposition in the contributions to SEEPS highlights the over-prediction of drizzle near midday, and difficulties to predict convection during the evening. SEDI results suggest that performance trends in 90\% percentile events are detectable over a 3year period. However, results do suggest that higher model resolution improves the skill and frequency bias for higher percentile events. © 2013 Royal Meteorological Society and British Crown copyright, the Met Office.},
	number = {2},
	journal = {Meteorological Applications},
	author = {North, Rachel and Trueman, Matthew and Mittermaier, Marion and Rodwell, Mark J.},
	year = {2013},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Precipitation verification, SEDI, SEEPS, ★},
	pages = {164--175},
}

@techreport{rodwell_new_2010,
	title = {A {New} {Equitable} {Score} {Suitable} for {Verifying} {Precipitation} in {NWP}},
	url = {http://www.ecmwf.int/publications/},
	abstract = {A new equitable score is developed for monitoring precipitation forecasts and for guiding forecast system development. To accommodate the difficult distribution of precipitation, the score measures error in 'prob-ability space' through use of the climatological cumulative distribution function. For sufficiently skillful forecasting systems, the new score is less sensitive to sampling uncertainty than other established scores. It is therefore called here 'Stable Equitable Error in Probability Space' (SEEPS). Weather is partitioned into three categories: 'dry', 'light precipitation' and 'heavy precipitation'. SEEPS adapts to the climate of the region in question so that it assesses the salient aspects of the local weather, encouraging 'refinement' and 'discrimination' and discouraging 'hedging'. To permit continuous monitoring of a system whose resolution is increasing with time, forecasts are verified against point observations. With some careful choices, observation error and lack of representativeness of model grid-box averages are found to have minimal impact. SEEPS can identify key forecasting errors including the over-prediction of drizzle, failure to predict heavy large-scale precipitation, and incorrectly locating convective cells. Area-averages are calculated taking into account the observation density, so that all sub-regions are treated more equally. A gain of ∼2 days, at lead-times 3-9 days, over the last 14 years is found in extratropical scores of forecasts made at the European Centre for Medium-range Weather Forecasts (ECMWF). This gain is due to system improvements, not the increased amount of data assimilated. SEEPS may also be applicable for verifying other quantities that suffer from difficult spatio-temporal distributions.},
	author = {Rodwell, M J and Richardson, D S and Hewson, T D},
	year = {2010},
	note = {Publication Title: Press in Quart. J. Roy. Meteorol. Soc},
	keywords = {★},
}

@article{gandin_equitable_nodate,
	title = {Equitable {Skill} {Scores} for {Categorical} {Forecasts}},
	author = {Gandin, Lev and Murphy, Allan},
}

@techreport{singh_short-range_2005,
	title = {Short-range forecasts of global precipitation using deep learning-augmented numerical weather prediction},
	abstract = {Precipitation drives the hydroclimate of Earth and its spatiotemporal changes on a day to day basis have one of the most notable socioeconomic impacts. The success of numerical weather prediction (NWP) is measured by the improvement of forecasts for various physical fields such as temperature and pressure. Large biases however exist in the precipitation predictions. Pure deep learning based approaches lack the advancements acheived by NWP in the past two to three decades. Hybrid methodology using NWP outputs as inputs to the deep learning based refinement tool offer an attractive means taking advantage of both NWP and state of the art deep learning algorithms. Augmenting the output from a well-known NWP model: Coupled Forecast System ver.2 (CFSv2) with deep learning for the first time, we demonstrate a hybrid model capability (DeepNWP) which shows substantial skill improvements for short-range global precipitation at 1-, 2-and 3-days lead time. To achieve this hybridization, we address the sphericity of the global data by using modified DLWP-CS architecture which transforms all the fields to cubed-sphere projection. The dynamical model outputs corresponding to precipitation and surface temperature are ingested to a UNET for predicting the target ground truth precipitation. While the dynamical model CFSv2 shows Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022. a bias in the range of +5 to +7 mm/day over land, the multivariate deep learning model reduces it to-1 to +1 mm/day over global land areas. We validate the results by taking examples from},
	author = {Singh, Manmeet and Sb, Vaisakh and Acharya, Nachiketa and Grover, Aditya and Rao, Suryachandra A and Kumar, Bipin and Yang, Zong-Liang and Niyogi, Dev},
	year = {2005},
	keywords = {★},
}

@techreport{singh_short-range_2005-1,
	title = {Short-range forecasts of global precipitation using deep learning-augmented numerical weather prediction},
	abstract = {Precipitation drives the hydroclimate of Earth and its spatiotemporal changes on a day to day basis have one of the most notable socioeconomic impacts. The success of numerical weather prediction (NWP) is measured by the improvement of forecasts for various physical fields such as temperature and pressure. Large biases however exist in the precipitation predictions. Pure deep learning based approaches lack the advancements acheived by NWP in the past two to three decades. Hybrid methodology using NWP outputs as inputs to the deep learning based refinement tool offer an attractive means taking advantage of both NWP and state of the art deep learning algorithms. Augmenting the output from a well-known NWP model: Coupled Forecast System ver.2 (CFSv2) with deep learning for the first time, we demonstrate a hybrid model capability (DeepNWP) which shows substantial skill improvements for short-range global precipitation at 1-, 2-and 3-days lead time. To achieve this hybridization, we address the sphericity of the global data by using modified DLWP-CS architecture which transforms all the fields to cubed-sphere projection. The dynamical model outputs corresponding to precipitation and surface temperature are ingested to a UNET for predicting the target ground truth precipitation. While the dynamical model CFSv2 shows Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022. a bias in the range of +5 to +7 mm/day over land, the multivariate deep learning model reduces it to-1 to +1 mm/day over global land areas. We validate the results by taking examples from},
	author = {Singh, Manmeet and Sb, Vaisakh and Acharya, Nachiketa and Grover, Aditya and Rao, Suryachandra A and Kumar, Bipin and Yang, Zong-Liang and Niyogi, Dev},
	year = {2005},
	keywords = {★},
}

@techreport{graubner_calibration_nodate,
	title = {Calibration of {Large} {Neural} {Weather} {Models}},
	abstract = {Uncertainty quantification of weather forecasts is a necessity for reliably planning for and responding to extreme weather events in a warming world. This motivates the need for well-calibrated ensembles in probabilistic weather forecasting. We present initial results for the calibration of large-scale deep neural weather models for data-driven probabilistic weather forecasting. By explicitly accounting for uncertainties about the forecast's initial condition and model parameters, we generate ensemble forecasts that show promising results on standard diagnostics for probabilistic forecasts. Specifically, we are approaching the Integrated Forecasting System (IFS), the gold standard on probabilistic weather forecasting, on: (i) the spread-error agreement; and (ii) the Continuous Ranked Probability Score (CRPS). Our approach scales to state-of-the-art data-driven weather models, enabling cheap post-hoc calibration of pretrained models with tens of millions of parameters and paving the way towards the next generation of well-calibrated data-driven weather models.},
	author = {Graubner, Andre and Azizzadenesheli, Kamyar and Pathak, Jaideep and Mardani, Morteza and Pritchard, Mike and Kashinath, Karthik},
}

@article{dagon_machine_2022,
	title = {Machine {Learning}‐{Based} {Detection} of {Weather} {Fronts} and {Associated} {Extreme} {Precipitation} in {Historical} and {Future} {Climates}},
	volume = {127},
	issn = {2169-897X},
	doi = {10.1029/2022jd037038},
	abstract = {Abstract Extreme precipitation events, including those associated with weather fronts, have wide-ranging impacts across the world. Here we use a deep learning algorithm to identify weather fronts in high resolution Community Earth System Model (CESM) simulations over the contiguous United States (CONUS), and evaluate the results using observational and reanalysis products. We further compare results between CESM simulations using present-day and future climate forcing, to study how these features might change with climate change. We find that detected front frequencies in CESM have seasonally varying spatial patterns and responses to climate change and are found to be associated with modeled changes in large scale circulation such as the jet stream. We also associate the detected fronts with precipitation and find that total and extreme frontal precipitation mostly decreases with climate change, with some seasonal and regional differences. Decreases in Northern Hemisphere summer frontal precipitation are largely driven by changes in the frequency of different front types, especially cold and stationary fronts. On the other hand, Northern Hemisphere winter exhibits some regional increases in frontal precipitation that are largely driven by changes in frontal precipitation intensity. While CONUS mean and extreme precipitation generally increase during all seasons in these climate change simulations, the likelihood of frontal extreme precipitation decreases, demonstrating that extreme precipitation has seasonally varying sources and mechanisms that will continue to evolve with climate change.},
	number = {21},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Dagon, Katherine and Truesdale, John and Biard, James C. and Kunkel, Kenneth E. and Meehl, Gerald A. and Molina, Maria J.},
	month = nov,
	year = {2022},
	note = {Publisher: American Geophysical Union (AGU)},
}

@techreport{ioffe_batch_nodate,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making nor-malization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Nor-malization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	author = {Ioffe, Sergey and Szegedy, Christian},
}

@article{kilavi_extreme_2018,
	title = {Extreme rainfall and flooding over {Central} {Kenya} {Including} {Nairobi} {City} during the long-rains season 2018: {Causes}, predictability, and potential for early warning and actions},
	volume = {9},
	issn = {20734433},
	doi = {10.3390/atmos9120472},
	abstract = {The Long-Rains wet season of March-May (MAM) over Kenya in 2018 was one of the wettest on record. This paper examines the nature, causes, impacts, and predictability of the rainfall events, and considers the implications for flood risk management. The exceptionally high monthly rainfall totals in March and April resulted from several multi-day heavy rainfall episodes, rather than from distinct extreme daily events. Three intra-seasonal rainfall events in particular resulted in extensive flooding with the loss of lives and livelihoods, a significant displacement of people, major disruption to essential services, and damage to infrastructure. The rainfall events appear to be associated with the combined effects of active Madden-Julian Oscillation (MJO) events in MJO phases 2-4, and at shorter timescales, tropical cyclone events over the southwest Indian Ocean. These combine to drive an anomalous westerly low-level circulation over Kenya and the surrounding region, which likely leads to moisture convergence and enhanced convection. We assessed how predictable such events over a range of forecast lead times. Long-lead seasonal forecast products for MAM 2018 showed little indication of an enhanced likelihood of heavy rain over most of Kenya, which is consistent with the low predictability of MAM Long-Rains at seasonal lead times. At shorter lead times of a few weeks, the seasonal and extended-range forecasts provided a clear signal of extreme rainfall, which is likely associated with skill in MJO prediction. Short lead weather forecasts from multiple models also highlighted enhanced risk. The flood response actions during the MAM 2018 events are reviewed. Implications of our results for forecasting and flood preparedness systems include: (i) Potential exists for the integration of sub-seasonal and short-term weather prediction to support flood risk management and preparedness action in Kenya, notwithstanding the particular challenge of forecasting at small scales. (ii) We suggest that forecasting agencies provide greater clarity on the difference in potentially useful forecast lead times between the two wet seasons in Kenya and East Africa. For the MAM Long-Rains, the utility of sub-seasonal to short-term forecasts should be emphasized; while at seasonal timescales, skill is currently low, and there is the challenge of exploiting new research identifying the primary drivers of variability. In contrast, greater seasonal predictability of the Short-Rains in the October-December season means that greater potential exists for early warning and preparedness over longer lead times. (iii) There is a need for well-developed and functional forecast-based action systems for heavy rain and flood risk management in Kenya, especially with the relatively short windows for anticipatory action during MAM.},
	number = {12},
	journal = {Atmosphere},
	author = {Kilavi, Mary and MacLeod, Dave and Ambani, Maurine and Robbins, Joanne and Dankers, Rutger and Graham, Richard and Helen, Titley and Salih, Abubakr A.M. and Todd, Martin C.},
	month = nov,
	year = {2018},
	note = {Publisher: MDPI AG},
	keywords = {East Africa, Extreme rainfall, Flood, Forecast based action, Forecasting, Kenya, Long-Rains, MJO, Predictability, S2S, Seasonal sub-seasonal variability},
}

@article{hsieh_robustness_2019,
	title = {On the {Robustness} of {Self}-{Attentive} {Models}},
	author = {Hsieh, Yu-lun and Cheng, Minhao and Juan, Da-cheng and Wei, Wei and Hsu, Wen-lian and Hsieh, Cho-jui},
	year = {2019},
	pages = {1520--1529},
}

@article{ribeiro_why_2016,
	title = {"{Why} should i trust you?" {Explaining} the predictions of any classifier},
	volume = {13-17-Augu},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2016},
	note = {arXiv: 1602.04938
ISBN: 9781450342322},
	pages = {1135--1144},
}

@article{madsen_post-hoc_2021,
	title = {Post-hoc {Interpretability} for {Neural} {NLP}: {A} {Survey}},
	url = {http://arxiv.org/abs/2108.04840},
	abstract = {Natural Language Processing (NLP) models have become increasingly more complex and widespread. With recent developments in neural networks, a growing concern is whether it is responsible to use these models. Concerns such as safety and ethics can be partially addressed by providing explanations. Furthermore, when models do fail, providing explanations is paramount for accountability purposes. To this end, interpretability serves to provide these explanations in terms that are understandable to humans. Central to what is understandable is how explanations are communicated. Therefore, this survey provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth. Furthermore, the survey focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic. A common concern for this class of methods is whether they accurately reflect the model. Hence, how these post-hoc methods are evaluated is discussed throughout the paper.},
	author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
	year = {2021},
	note = {arXiv: 2108.04840},
	keywords = {interpretability, neural networks, nlp, post-hoc, survey},
}

@article{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	year = {2021},
	note = {arXiv: 2107.03374},
}

@article{winkler_methods_2002,
	title = {Methods for {Record} {Linkage} and {Bayesian} {Networks}},
	abstract = {Although terminology differs, there is considerable overlap between record linkage methods based on the Fellegi-Sunter model (JASA 1969) and Bayesian networks used in machine learning (Mitchell 1997). Both are based on formal probabilistic models that can be shown to be equivalent in many situations (Winkler 2000). When no missing data are present in identifying fields and training data are available, then both can efficiently estimate parameters of interest. When missing data are present, the EM algorithm can be used for parameter estimation in Bayesian Networks when there are training data (Friedman 1997) and in record linkage when there are no training data (unsupervised learning). EM and MCMC methods can be used for automatically estimating error rates in some of the record linkage situations (Belin and Rubin 1995, Larsen and Rubin 2001).},
	number = {Jasa 1969},
	journal = {Networks},
	author = {Winkler, William E},
	year = {2002},
	note = {ISBN: Statistics 2002-5},
	keywords = {bayesian nets, em algorithm, likelihood ratio},
	pages = {29},
}

@article{chiang_pre-training_2020,
	title = {Pre-training a language model without human language},
	issn = {23318422},
	abstract = {In this paper, we study how the intrinsic nature of pre-training data contributes to the fine-tuned downstream performance. To this end, we pre-train different transformer-based masked language models on several corpora with certain features, and we fine-tune those language models on GLUE benchmarks. We find that models pre-trained on unstructured data beat those trained directly from scratch on downstream tasks. Our results also show that pre-training on structured data does not always make the model acquire ability that can be transferred to natural language downstream tasks. To our great astonishment, we uncover that pre-training on certain non-human language data gives GLUE performance close to performance pre-trained on another non-English language.},
	journal = {arXiv},
	author = {Chiang, Cheng Han and Lee, Hung Yi},
	year = {2020},
	note = {arXiv: 2012.11995},
}

@article{papadimitriou_learning_2020,
	title = {Learning {Music} {Helps} {You} {Read}: {Using} {Transfer} to {Study} {Linguistic} {Structure} in {Language} {Models}},
	doi = {10.18653/v1/2020.emnlp-main.554},
	abstract = {We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads to the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.},
	author = {Papadimitriou, Isabel and Jurafsky, Dan},
	year = {2020},
	note = {arXiv: 2004.14601},
	pages = {6829--6839},
}

@article{rennie_tackling_2003,
	title = {Tackling the {Poor} {Assumptions} of {Naive} {Bayes} {Text} {Classifiers}},
	volume = {2},
	abstract = {Naive Bayes is often used as a baseline in text classification because it is fast and easy to implement. Its severe assumptions make such efficiency possible but also adversely affect the quality of its results. In this paper we propose simple, heuristic solutions to some of the problems with Naive Bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model. We find that our simple corrections result in a fast algorithm that is competitive with state-of-the-art text classification algorithms such as the Support Vector Machine.},
	number = {1973},
	journal = {Proceedings, Twentieth International Conference on Machine Learning},
	author = {Rennie, Jason D.M. and Shih, Lawrence and Teevan, Jaime and Karger, David},
	year = {2003},
	note = {ISBN: 1577351894},
	pages = {616--623},
}

@article{cohen_learning_2002,
	title = {Learning to match and cluster large high-dimensional data sets for data integration},
	doi = {10.1145/775047.775116},
	abstract = {Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities. In integrating databases found on the Web or obtained by using information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a particular domain. An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems, and is nearly always competitive with the best baseline system.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Cohen, William W. and Richman, Jacob},
	year = {2002},
	note = {ISBN: 158113567X},
	keywords = {Clustering, Large datasets, Learning, Text mining},
	pages = {475--480},
}

@article{bhattacharya_latent_2006,
	title = {A {Latent} {Dirichlet} model for unsupervised entity resolution},
	volume = {2006},
	doi = {10.1137/1.9781611972764.5},
	abstract = {Entity resolution has received considerable attention in recent years. Given many references to underlying entities, the goal is to predict which references correspond to the same entity. We show how to extend the Latent Dirichlet Allocation model for this task and propose a probabilistic model for collective entity resolution for relational domains where references are connected to each other. Our approach differs from other recently proposed entity resolution approaches in that it is a) generative, b) does not make pair-wise decisions and c) captures relations between entities through a hidden group variable. We propose a novel sampling algorithm for collective entity resolution which is unsupervised and also takes entity relations into account. Additionally, we do not assume the domain of entities to be known and show how to infer the number of entities from the data. We demonstrate the utility and practicality of our relational entity resolution approach for author resolution in two real-world bibliographic datasets. In addition, we present preliminary results on characterizing conditions under which relational information is useful.},
	journal = {Proceedings of the Sixth SIAM International Conference on Data Mining},
	author = {Bhattacharya, Indrajit and Getoor, Lise},
	year = {2006},
	note = {ISBN: 089871611X},
	pages = {47--58},
}

@article{card_little_2020,
	title = {With {Little} {Power} {Comes} {Great} {Responsibility}},
	doi = {10.18653/v1/2020.emnlp-main.745},
	abstract = {Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75\% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.},
	author = {Card, Dallas and Henderson, Peter and Khandelwal, Urvashi and Jia, Robin and Mahowald, Kyle and Jurafsky, Dan},
	year = {2020},
	note = {arXiv: 2010.06595},
	pages = {9263--9274},
}

@article{christophides_end--end_2019,
	title = {End-to-end entity resolution for big data: {A} survey},
	volume = {53},
	abstract = {One of the most important tasks for improv-ing data quality and the reliability of data analytics results is Entity Resolution (ER). ER aims to identify different descriptions that refer to the same real-world entity, and remains a challenging problem. While pre-vious works have studied specific aspects of ER (and mostly in traditional settings), in this survey, we pro-vide for the first time an end-to-end view of modern ER workows, and of the novel aspects of entity indexing and matching methods in order to cope with more than one of the Big Data characteristics simultaneously. We present the basic concepts, processing steps and exe-cution strategies that have been proposed by different communities, i.e., database, semanticWeb and machine learning, in order to cope with the loose structured-ness, extreme diversity, high speed and large scale of entity descriptions used by real-world applications. Fi-nally, we provide a synthetic discussion of the existing approaches, and conclude with a detailed presentation of open research directions.},
	number = {6},
	journal = {arXiv},
	author = {Christophides, Vassilis and Efthymiou, Vasilis and Palpanas, Themis and Papadakis, George and Stefanidis, Kostas},
	year = {2019},
	note = {arXiv: 1905.06397},
	keywords = {Batch and Online Entity Resolution Workows, Block Processing, Entity Blocking and Matching, Strongly and Near Similar Entities},
}

@article{lai_ridging_2019,
	title = {Ridging the domain gap in cross - lingual document classification},
	author = {Lai, Guokun and Oguz, Barlas and Yang, Yiming and Stoyanov, Veselin},
	year = {2019},
	note = {arXiv: 1909.07009v2},
	pages = {1--13},
}

@article{joulin_bag_2015,
	title = {Bag of {Tricks} for {Efficient} {Text} {Classification}},
	author = {Joulin, Armand},
	year = {2015},
	note = {arXiv: 1607.01759v3},
}

@article{wang_transformer_nodate,
	title = {Transformer on a {Diet}},
	author = {Wang, Chenguang and Smola, Alexander J},
	note = {arXiv: 2002.06170v1},
	pages = {1--6},
}

@article{asai_l_2020,
	title = {L {EARNING} {TO} {R} {ETRIEVE} {R} {EASONING} {P} {ATHS} {OVER}},
	author = {Asai, Akari and Hashimoto, Kazuma and Hajishirzi, Hannaneh and Socher, Richard and Xiong, Caiming},
	year = {2020},
	note = {arXiv: 1911.10470v2},
	pages = {1--22},
}

@article{torfi_deep_nodate,
	title = {Deep {Learning} : {A} {Survey}},
	author = {Torfi, Amirsina and Shirvani, Rouzbeh A and Keneshloo, Yaser and Tavvaf, Nader},
	note = {arXiv: 2003.01200v1},
	pages = {1--21},
}

@article{ma_hierarchical_nodate,
	title = {A {Hierarchical} {Fine}-{Tuning} {Approach} {Based} on {Joint} {Embedding} of {Words} and {Parent} {Categories} for {Hierarchical} {Multi}-label {Text} {Classification}},
	author = {Ma, Yinglong and Zhao, Jingpeng and Jin, Beihong},
	keywords = {hierarchical fine tuning, multi-label classification, text classification, word embedding},
	pages = {1--12},
}

@article{raiman_deeptype_2018,
	title = {{DeepType} : {Multilingual} {Entity} {Linking} by {Neural} {Type} {System} {Evolution}},
	author = {Raiman, Jonathan and Raiman, Olivier},
	year = {2018},
	keywords = {Natural Language Processing and Machine Learning T},
	pages = {5406--5413},
}

@article{rogers_primer_2019,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	year = {2019},
	note = {arXiv: 2002.12327v1},
}

@article{barrey_multilingual_2007,
	title = {Multilingual {Alignment} of {Contextual} {Word} {Representations}},
	volume = {61},
	doi = {10.1227/01.NEU.0000280112.98151.6D},
	abstract = {This paper focuses on the strategic alignment model and how it has been operationalized to enable assessment of an organization’s business and technology strategies into one of twelve defined alignment perspectives using a web-based model. Analysis of data from a multi-year study suggests that certain industries favor specific alignment perspectives. Further analysis of longitudinal data appears to yield distinct patterns of strategy development among industries.},
	number = {5},
	journal = {Neurosurgery},
	author = {Barrey, Cédric and Jund, Jérôme and Perrin, Gilles and Roussouly, Pierre},
	year = {2007},
	note = {arXiv: 2002.03518v2},
	keywords = {0000280112, 01, 10, 1227, 2007, 6d, 981, 98151, 986, com, doi, lumbar lordosis, lumbar stenosis, neu, neurosurgery 61, neurosurgery-online, pelvic incidence, pelvis, sagittal balance, spondylolisthesis, www},
	pages = {981--986},
}

@article{xu_bert--theseus_2020,
	title = {{BERT}-of-{Theseus}: {Compressing} {BERT} by {Progressive} {Module} {Replacing}},
	url = {http://arxiv.org/abs/2002.02925},
	abstract = {In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models, and smooths the training process. Compared to the previous knowledge distillation approaches for BERT compression, our approach leverages only one loss function and one hyper-parameter, liberating human effort from hyper-parameter tuning. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.},
	author = {Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Wei, Furu and Zhou, Ming},
	year = {2020},
	note = {arXiv: 2002.02925},
}

@article{almasian_word_2019,
	title = {Word embeddings for entity-annotated texts},
	volume = {11437 LNCS},
	issn = {16113349},
	doi = {10.1007/978-3-030-15712-8_20},
	abstract = {Learned vector representations of words are useful tools for many information retrieval and natural language processing tasks due to their ability to capture lexical semantics. However, while many such tasks involve or even rely on named entities as central components, popular word embedding models have so far failed to include entities as first-class citizens. While it seems intuitive that annotating named entities in the training corpus should result in more intelligent word features for downstream tasks, performance issues arise when popular embedding approaches are naïvely applied to entity annotated corpora. Not only are the resulting entity embeddings less useful than expected, but one also finds that the performance of the non-entity word embeddings degrades in comparison to those trained on the raw, unannotated corpus. In this paper, we investigate approaches to jointly train word and entity embeddings on a large corpus with automatically annotated and linked entities. We discuss two distinct approaches to the generation of such embeddings, namely the training of state-of-the-art embeddings on raw-text and annotated versions of the corpus, as well as node embeddings of a co-occurrence graph representation of the annotated corpus. We compare the performance of annotated embeddings and classical word embeddings on a variety of word similarity, analogy, and clustering evaluation tasks, and investigate their performance in entity-specific tasks. Our findings show that it takes more than training popular word embedding models on an annotated corpus to create entity embeddings with acceptable performance on common test cases. Based on these results, we discuss how and when node embeddings of the co-occurrence graph representation of the text can restore the performance.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Almasian, Satya and Spitz, Andreas and Gertz, Michael},
	year = {2019},
	note = {arXiv: 1902.02078
ISBN: 9783030157111},
	keywords = {Entity embeddings, Entity graph, Word embeddings},
	pages = {307--322},
}

@article{oliver_bayesian_1998,
	title = {A {Bayesian} {Analysis} of the {Doomsday} {Argument}},
	abstract = {The doomsday argument purports to show that starting solely from the information of what your birth rank is amongst some population the extinction of that population is near at hand, or at any rate signiicantly nearer than you had previously supposed. Various authors have suggested that this is a straightforward consequence of Bayesian reasoning. We show that Bayesian reasoning is both more subtle and more plausible than the doomsdayers' understanding of it.},
	number = {Dd},
	author = {Oliver, Jonathan and Korb, Kevin},
	year = {1998},
	pages = {1--12},
}

@article{ruder_survey_2019,
	title = {A {Survey} of {Cross}-lingual {Word} {Embedding} {Models}},
	volume = {65},
	issn = {1076-9757},
	doi = {10.1613/jair.1.11640},
	abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
	journal = {Journal of Artificial Intelligence Research},
	author = {Ruder, Sebastian and Vulić, Ivan and Søgaard, Anders},
	year = {2019},
	note = {arXiv: 1706.04902},
	pages = {569--631},
}

@article{raunak_effective_2019,
	title = {Effective {Dimensionality} {Reduction} for {Word} {Embeddings}},
	doi = {10.18653/v1/w19-4328},
	abstract = {Word embeddings have become the basic building blocks for several natural language processing and information retrieval tasks. Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on further improving the pre-trained word vectors through post-processing algorithms. One such area of improvement is the dimensionality reduction of the word embeddings. Reducing the size of word embeddings through dimensionality reduction can improve their utility in memory constrained devices, benefiting several real-world applications. In this work, we present a novel algorithm that effectively combines PCA based dimensionality reduction with a recently proposed post-processing algorithm, to construct word embeddings of lower dimensions. Empirical evaluations on 12 standard word similarity benchmarks show that our algorithm reduces the embedding dimensionality by 50\%, while achieving similar or (more often) better performance than the higher dimension embeddings.},
	number = {Nips},
	author = {Raunak, Vikas and Gupta, Vivek and Metze, Florian},
	year = {2019},
	note = {arXiv: 1708.03629},
	pages = {235--243},
}

@article{ruder_unsupervised_2019,
	title = {Unsupervised {Cross}-{Lingual} {Representation} {Learning}},
	doi = {10.18653/v1/p19-4007},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8\% average accuracy on XNLI, +12.3\% average F1 score on MLQA, and +2.1\% average F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 11.8\% in XNLI accuracy for Swahili and 9.2\% for Urdu over the previous XLM model. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data, and models publicly available.},
	author = {Ruder, Sebastian and Søgaard, Anders and Vulić, Ivan},
	year = {2019},
	note = {arXiv: 1911.02116},
	pages = {31--38},
}

@article{mehdiyev_topic_2019,
	title = {Topic subject creation using unsupervised learning for topic modeling},
	url = {http://arxiv.org/abs/1912.08868},
	abstract = {We describe the use of Non-Negative Matrix Factorization (NMF) and Latent Dirichlet Allocation (LDA) algorithms to perform topic mining and labelling applied to retail customer communications in attempt to characterize the subject of customers inquiries. In this paper we compare both algorithms in the topic mining performance and propose methods to assign topic subject labels in an automated way.},
	author = {Mehdiyev, Rashid and Nava, Jean and Sodhi, Karan and Acharya, Saurav and Rana, Annie Ibrahim},
	year = {2019},
	note = {arXiv: 1912.08868},
}

@article{wan_long-length_2014,
	title = {Long-length {Legal} {Document} {Classification}},
	author = {Wan, Lulu and Llp, Clifford Chance and Papageorgiou, George and Llp, Clifford Chance and Seddon, Michael and Llp, Clifford Chance},
	year = {2014},
	note = {arXiv: 1912.06905v1},
}

@article{speer_ensemble_2016,
	title = {An {Ensemble} {Method} to {Produce} {High}-{Quality} {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1604.01692},
	abstract = {A currently successful approach to computational semantics is to represent words as embeddings in a machine-learned vector space. We present an ensemble method that combines embeddings produced by GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al., 2013), merging their information into a common representation with a large, multilingual vocabulary. The embeddings it produces achieve state-of-the-art performance on many word-similarity evaluations. Its score of \${\textbackslash}rho = .596\$ on an evaluation of rare words (Luong et al., 2013) is 16\% higher than the previous best known system.},
	author = {Speer, Robert and Chin, Joshua},
	year = {2016},
	note = {arXiv: 1604.01692},
}

@article{adhikari_rethinking_2019,
	title = {Rethinking {Complex} {Neural} {Network} {Architectures} for {Document} {Classification}},
	doi = {10.18653/v1/n19-1408},
	abstract = {Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective. We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly, our simple model is able to achieve these results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification.},
	author = {Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Lin, Jimmy},
	year = {2019},
	pages = {4046--4051},
}

@article{wang_baselines_2012,
	title = {Baselines and bigrams: {Simple}, good sentiment and topic classification},
	volume = {2},
	abstract = {Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/ dataset. We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets. Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level. © 2012 Association for Computational Linguistics.},
	number = {July},
	journal = {50th Annual Meeting of the Association for Computational Linguistics, ACL 2012 - Proceedings of the Conference},
	author = {Wang, Sida and Manning, Christopher D.},
	year = {2012},
	note = {ISBN: 9781937284251},
	pages = {90--94},
}

@article{eban_scalable_2017,
	title = {Scalable learning of non-decomposable objectives},
	volume = {54},
	abstract = {Modern retrieval systems are often driven by an underlying machine learning model. The goal of such systems is to identify and possibly rank the few most relevant items for a given query or context. Thus, such systems are typically evaluated using a ranking-based performance metric such as the area under the precision-recall curve, the Fβ score, precision at fixed recall, etc. Obviously, it is desirable to train such systems to optimize the metric of interest. In practice, due to the scalability limitations of existing approaches for optimizing such objectives, large-scale retrieval systems are instead trained to maximize classification accuracy, in the hope that performance as measured via the true objective will also be favorable. In this work we present a unified framework that, using straightforward building block bounds, allows for highly scalable optimization of a wide range of ranking-based objectives. We demonstrate the advantage of our approach on several real-life retrieval problems that are significantly larger than those considered in the literature, while achieving substantial improvement in performance over the accuracy-objective baseline.},
	journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
	author = {Eban, Elad and Schain, Mariano and Mackey, Alan and Gordon, Ariel and Saurous, Rif A. and Elidan, Gal},
	year = {2017},
	note = {arXiv: 1608.04802},
}

@article{hand_note_2014,
	title = {A note on using the {F}-measure for evaluating record linkage algorithms (and classification and information retrieval systems)},
	abstract = {Record linkage is the process of identifying and linking records about the same entities from one or more databases. Record linkage can be viewed as a classification problem where the aim is to decide if a pair of records is a match (i.e. two records refer to the same real-world entity) or a non-match (two records refer to two different entities). Various classification techniques-including supervised, unsupervised, semi-supervised and active learning based-have been employed for record linkage. If ground truth data in the form of known true matches and non-matches are available, the quality of classified links can be evaluated. Due to the generally high class imbalance in record linkage problems , standard accuracy or misclassification rate are not meaningful for assessing the quality of a set of linked records. Instead, precision and recall, as commonly used in information retrieval and machine learning, are used. These are often combined into the popular F-measure, which is the harmonic mean of precision and recall. We show that the F-measure can also be expressed as a weighted sum of precision and recall, with weights which depend on the linkage method being used. This reformulation reveals that the F-measure has a major conceptual weakness: the relative importance assigned to precision and recall should be an aspect of the problem and the researcher or user, but not of the particular linkage method being used. We suggest alternative measures which do not suffer from this fundamental flaw.},
	author = {Hand, David and Christen, Peter},
	year = {2014},
	keywords = {()},
}

@article{kiela_dynamic_2019,
	title = {Dynamic {Meta}-{Embeddings} for {Improved} {Sentence} {Representations}},
	doi = {10.18653/v1/d18-1176},
	abstract = {While one of the first steps in many NLP systems is selecting what pre-trained word embeddings to use, we argue that such a step is better left for neural networks to figure out by themselves. To that end, we introduce dynamic meta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to state-of-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems.},
	author = {Kiela, Douwe and Wang, Changhan and Cho, Kyunghyun},
	year = {2019},
	note = {arXiv: 1804.07983},
	pages = {1466--1477},
}

@article{gupta_retrieve_2019,
	title = {Retrieve and {Re}-rank: {A} {Simple} and {Effective} {IR} {Approach} to {Simple} {Question} {Answering} over {Knowledge} {Graphs}},
	doi = {10.18653/v1/w18-5504},
	abstract = {SimpleQuestions is a commonly used benchmark},
	author = {Gupta, Vishal and Chinnakotla, Manoj and Shrivastava, Manish},
	year = {2019},
	pages = {22--27},
}

@inproceedings{howard_universal_2018,
	title = {Universal language model fine-tuning for text classification},
	volume = {1},
	isbn = {978-1-948087-32-2},
	doi = {10.18653/v1/p18-1031},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We open-source our pretrained models and code1},
	booktitle = {{ACL} 2018 - 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}, {Proceedings} of the {Conference} ({Long} {Papers})},
	author = {Howard, Jeremy and Ruder, Sebastian},
	year = {2018},
	note = {arXiv: 1801.06146},
	pages = {328--339},
}

@article{shen_modeling_2017,
	title = {Modeling {Large}-{Scale} {Structured} {Relationships} with {Shared} {Memory} for {Knowledge} {Base} {Completion}},
	doi = {10.18653/v1/w17-2608},
	abstract = {Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed triplets could be costly. Hence, a manually designed procedure is often used when training the models. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform multi-step inference implicitly through a controller and shared memory. Without a human-designed inference procedure, IRNs use training data to learn to perform multi-step inference in an embedding neural space through the shared memory and controller. While the inference procedure does not explicitly operate on top of observed triplets, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7\%.},
	author = {Shen, Yelong and Huang, Po-Sen and Chang, Ming-Wei and Gao, Jianfeng},
	year = {2017},
	pages = {57--68},
}

@article{hwang_comprehensive_2019,
	title = {A {Comprehensive} {Exploration} on {WikiSQL} with {Table}-{Aware} {Word} {Contextualization}},
	number = {NeurIPS},
	author = {Hwang, Wonseok and Yim, Jinyeong and Park, Seunghyun and Seo, Minjoon},
	year = {2019},
	note = {arXiv: 1902.01069v2},
	pages = {1--34},
}

@article{arora_simple_2016,
	title = {A simple but tough-to-beat {Baseline} for {Sentence} {Embeddings}},
	volume = {15},
	url = {https://openreview.net/forum?id=SyK00v5xx%0Ahttps://github.com/PrincetonML/SIF},
	abstract = {The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embed-dings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsuper-vised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10\% to 30\% in textual similarity tasks, and beats sophisticated supervised methods including RNN's and LSTM's. It even improves Wieting et al.'s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsu-pervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new "smoothing" terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts.},
	journal = {Iclr},
	author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
	year = {2016},
	pages = {416--424},
}

@article{de_model_2015,
	title = {Model {Compression}},
	volume = {54},
	issn = {0218-0014},
	doi = {10.1111/j.1467-985x.2009.00624_10.x},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
	number = {1},
	journal = {IEEE Communications Magazine},
	author = {De, Soham and Goldstein, Tom and Ahmed, Faraz and Erman, Jeffrey and Ge, Zihui and Liu, Alex X. and Wang, Jiannan Jing Jia and Yan, He and Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming and Wang, Shiqiang Shangguang and Tuor, Tiffany and Salonidis, Theodoros and Leung, Kin K. and Makaya, Christian and He, Ting and Chan, Kevin and Zhao, Yue and Li, Meng Ming and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas and Liu, Lumin and Zhang, Jun Jihui and Song, S H and Letaief, Khaled B and Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin and McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Agüera y and Wang, Nan and Zhao, Xibin and Jiang, Yu and Gao, Yue Yang and Vluymans, Sarah and Bader-El-Den, Mohamed and Teitei, Eleman and Adda, Mo and SUN, YANMIN and WONG, ANDREW K. C. and KAMEL, MOHAMED S. and Liu, Ji and Wright, Stephen J. and Ré, Christopher and Bittorf, Victor and Sridhar, Srikrishna and Li, Wenbin and Huo, Jing and Shi, Yinghuan and Gao, Yue Yang and Wang, Lei and Luo, Jiebo and SUN, SHILIANG and CHEN, QIAONA and Globerson, Amir and Roweis, Sam T and Habrard, Amaury and Curien, Laboratoire Hubert and Duan, Yueqi and Zheng, Wenzhao and Lin, Xudong and Lu, Jiwen and Zhou, Jie and Davis, Jason V and Xing, Eric P and Jordan, Michael I and Russell, Stuart and Ng, Andrew Y and Jordan, Michael I and Russell, Stuart and Deriglazov, Alexei A. and Ramírez, Walberto Guzmán and Byrd, Richard H and Chin, Gillian M and Nocedal, Jorge and Wu, Yuchen and Adams, Niall and {Unknown} and Bauer, Julia and Franke, Nikolaus and 中島, 一憲 and Xu, Jinliang and Wang, Shiqiang Shangguang and Member, Senior and Zhang, Ning and Yang, Fangchun and Member, Senior and Shen, Xuemin Sherman and Al-Ayyoub, Mahmoud and Gupta, Himanshu and Li, Meng Ming and Li, Pan and Pan, Miao and Sun, Jinyuan and Aydin, Bahadir and Yilmaz, Yavuz and Li, Yaliang and Li, Qi and Gao, Jing and Demirbas, Murat and Berend, Daniel and Kontorovich, Aryeh and Chandra, Praphul and Narahari, Yadati and Mandal, Debmalya and Chen, Ning and Deng, Xiaotie and Tang, Bo and Zhang, Hongyang Hao and Chen, Yanjiao and Li, Baochun and Zhang, Qian and Conitzer, Vincent and Yokoo, Makoto and Dasgupta, Anirban and Ghosh, Arpita and Gao, Yue Yang and Chen, Yanjiao and Liu, K J Ray and Guo, Bin and Chen, Chao and Zhang, Daqing and Yu, Zhiwen and Chin, Alvin and Ipeirotis, Panagiotis G and Provost, Foster and Sheng, Victor S and Wang, Jiannan Jing Jia and Jurca, Radu and Faltings, Boi and {others} and Kamar, Ece and Horvitz, Eric and Li, Guoliang and Wang, Jiannan Jing Jia and Zheng, Yudian and Franklin, Michael and Liang, Sihua and Zhang, Jun Jihui and Liu, Xuan and Lu, Meiyu and Ooi, Beng Chin and Shen, Yanyan and Wu, Sai and Zhang, Meihui and Miller, Nolan and Resnick, Paul and Zeckhauser, Richard and Prelec, Dražen Drazen and Seung, Sebastian and Radanovic, Goran and Faltings, Boi and Raykar, Vikas C and Yu, Shipeng and Zhao, Linda H and Valadez, Gerardo Hermosillo and Florin, Charles and Bogoni, Luca and Moy, Linda and Ren, Ju and Zhang, Yuqing Yaoxue and Zhang, Kuan and Shen, Xuemin Sherman and Rokicki, Markus and Zerr, Sergej and Siersdorfer, Stefan and Shah, Nihar Bhadresh and Zhou, Denny Dengyong and Peres, Yuval and Silverman, Bernard W and Simpson, Edwin and Roberts, Stephen and Sun, Xiaoqian and Shen, Huawei and Cheng, XueQi and Zhang, Yuqing Yaoxue and Von Ahn, Luis and Dabbish, Laura and Vullioud, Colin and Clément, Fabrice and Scott-Phillips, Thom and Mercier, Hugo and Vuurens, Jeroen and de Vries, Arjen P and Eickhoff, Carsten and Wang, Jiannan Jing Jia and Faridani, Siamak and Ipeirotis G., Panagiotis and Witkowski, Jens and Bachrach, Yoram and Key, Peter and Parkes, David Christopher and C. Parkes, David and Seuken, Sven and Yang, Kan and Zhang, Kuan and Ren, Ju and Shen, Xuemin Sherman and Zhang, Hongyang Hao and Sugiyama, Masashi and Zhou, Denny Dengyong and Liu, Qiang and Platt, John C and Meek, Christopher and Mishra, Asit and Marr, Debbie and Labs, Intel and Bucil, Cristian and Caruana, Rich and Dean, Jeff},
	year = {2015},
	pmid = {12839992},
	note = {arXiv: 1706.02677
ISBN: 9781509053360},
	keywords = {Edge Intellignce, Federated Learning, Index Terms-Mobile Edge Computing, Machine Learning: Classification, Machine Learning: Machine Learning, asynchronous parallel optimization, co, com, deep metric learning, http, journals, jphs email, manuscriptcentral, mc, metric learning, model compression, online metric learning, spla-peerreview, stochastic coordinate descent, supervised learning, tandf, uk, url},
	pages = {1--9},
}

@article{graves_symbolic_2016,
	title = {Symbolic {Reasoning} with {Differentiable} {Neural} {Computers}},
	abstract = {manipulate it to solve tasks. Conventional computers, on the other hand, can easily be pro-12 grammed to store and process large data structures in memory, but cannot learn to recognise 13 complex patterns. This work aims to combine the advantages of neural and computational 14 processing by providing a neural network with read-write access to an external memory. We 15 refer to the resulting architecture as a Differentiable Neural Computer (DNC). Memory access 16 is sparse, minimising interference among memoranda and enabling long-term storage 12, 13 , 17 and the entire system can be trained with gradient descent, allowing the network to learn how 18 to operate and organise the memory in a goal-directed manner. We demonstrate DNC's abil-19 ity to manipulate large data structures by applying it to a set of synthetic question-answering 20 tasks involving graphs, such as finding shortest paths and inferring missing links. We then 21 show that DNC can learn, based solely on behavioral reinforcement 14, 15 , to carry out com-22 plex symbolic instructions in a game environment 16. Taken together, these results suggest 23 1},
	journal = {Nature},
	author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Gomez, Sergio and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Puigdomènech and Moritz Hermann, Karl and Zwols, Yori and Ostrovski, Georg and Cain, Adam and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
	year = {2016},
}

@article{moreo_word-class_2019,
	title = {Word-class embeddings for {Multiclass} {Text} {Classification}},
	author = {Moreo, Alejandro and Esuli, Andrea and Sebastiani, Fabrizio},
	year = {2019},
	note = {arXiv: 1911.11506v1},
}

@techreport{yik_exploring_nodate,
	title = {Exploring {Randomly} {Wired} {Neural} {Networks} for {Climate} {Model} {Emulation}},
	abstract = {Exploring the climate impacts of various anthropogenic emissions scenarios is key to making informed decisions for climate change mitigation and adaptation. State-of-the-art Earth system models can provide detailed insight into these impacts, but have a large associated computational cost on a per-scenario basis. This large computational burden has driven recent interest in developing cheap machine learning models for the task of climate model emulation. In this manuscript, we explore the efficacy of randomly wired neural networks for this task. We describe how they can be constructed and compare them to their standard feedforward counterparts using the ClimateBench dataset. Specifically, we replace the dense layers in multilayer perceptrons, convolutional neural networks, and convolutional long short-term memory networks with randomly wired ones and assess the impact on model performance for models with 1 million and 10 million parameters. We find average performance improvements of 4.2\% across model complexities and prediction tasks, with substantial performance improvements of up to 16.4\% in some cases. Furthermore, we find no significant difference in prediction speed between networks with standard feedforward dense layers and those with randomly wired layers. These findings indicate that randomly wired neural networks may be suitable direct replacements for traditional dense layers in many standard models.},
	author = {Yik, William and Silva, Sam and Geiss, Andrew and Watson-Parris, Duncan},
	keywords = {★},
}

@techreport{blanchard_multi-scale_nodate,
	title = {A {Multi}-{Scale} {Deep} {Learning} {Framework} for {Projecting} {Weather} {Extremes}},
	abstract = {Weather extremes are a major societal and economic hazard, claiming thousands of lives and causing billions of dollars in damage every year. Under climate change, their impact and intensity are expected to worsen significantly. Unfortunately, general circulation models (GCMs), which are currently the primary tool for climate projections, cannot characterize weather extremes accurately. To address this, we present a multi-resolution deep-learning framework that, firstly, corrects a GCM's biases by matching low-order and tail statistics of its output with observations at coarse scales; and secondly, increases the level of detail of the debiased GCM output by reconstructing the finer scales as a function of the coarse scales. We use the proposed framework to generate statistically realistic realizations of the climate over Western Europe from a simple GCM corrected using observational atmospheric reanalysis. We also discuss implications for probabilistic risk assessment of natural disasters in a changing climate.},
	author = {Blanchard, Antoine and Parashar, Nishant and Dodov, Boyko and Lessig, Christian},
	keywords = {★},
}

@techreport{sospedra-alfonso_deep_nodate,
	title = {Deep learning-based bias adjustment of decadal climate predictions},
	abstract = {Decadal climate predictions are key to inform adaptation strategies in a warming climate. Coupled climate models used for decadal predictions are, however, imperfect representations of the climate system causing forecast biases. Biases can also result from a poor model initialization that, when combined with forecast drift, can produce errors depending non-linearly on lead time. We propose a deep learning-based bias correction approach for post-processing gridded forecasts to enhance the accuracy of decadal climate predictions. 1 Motivation and problem statement Decadal or near-term climate prediction refers to climate forecasts on the range of a year to a decade. Unlike climate projections, which simulate the climate response to external forcing such as changes in greenhouse gas concentrations and aerosols, decadal predictions also simulate the climate response to unforced variations such as El Niño-Southern Oscillation (ENSO) and other modes of internal climate variability. As part of the Wold Climate Research Program (WCRP), the Decadal Climate Prediction Project (DCPP) [1] offers quasi-real-time decadal forecasts for potential users, whereas the World Meteorological Organization (WMO) Global Annual to Decadal Climate Update (GADCU) is produced annually to inform society on the state of the climate for the next 5 years [2]. Decadal forecasts typically drift from their observation-based initial conditions toward the uncon-strained model climatology, which may be far from observations. Consequently, operational decadal predictions often require some form of data post-processing to attain skill. This is often done using simple linear methods. Given the importance of climate predictions for informed adaptation strategies, the exploration of novel post-processing methods to correct forecast bias and drift is an important step to improve adaptation. We propose a deep learning model as a data post-processing tool for gridded climate predictions to enhance forecast skill. 2 Background and previous work While many studies describe adjustments of weather and subseasonal-to-seasonal (S2S) forecasts, there is limited work devoted to adjustments of decadal predictions, partly due to their relatively recent use, unique long-time range, drifts, and potential for erroneous trends. A simple approach is climatological bias correction, for which the difference between the modeled and observed clima-Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.},
	author = {Sospedra-Alfonso, Reinel and Exenberger, Johannes and Mcgraw, Marie C and Dang, Trung Kien},
	keywords = {★},
}

@article{veldkamp_statistical_2021,
	title = {Statistical postprocessing of wind speed forecasts using convolutional neural networks},
	volume = {149},
	issn = {15200493},
	doi = {10.1175/MWR-D-20-0219.1},
	abstract = {Current statistical postprocessing methods for probabilistic weather forecasting are not capable of using full spatial patterns from the numerical weather prediction (NWP) model. In this paper, we incorporate spatial wind speed information by using convolutional neural networks (CNNs) and obtain probabilistic wind speed forecasts in the Netherlands for 48 h ahead, based on KNMI's deterministic HARMONIE-AROME NWP model. The probabilistic forecasts from the CNNs are shown to have higher Brier skill scores for medium to higher wind speeds, as well as a better continuous ranked probability score (CRPS) and logarithmic score, than the forecasts from fully connected neural networks and quantile regression forests. As a secondary result, we have compared the CNNs using three different density estimation methods [quantized softmax (QS), kernel mixture networks, and fitting a truncated normal distribution], and found the probabilistic forecasts based on the QS method to be best.},
	number = {4},
	journal = {Monthly Weather Review},
	author = {Veldkamp, Simon and Whan, Kirien and Dirksen, Sjoerd and Schmeits, Maurice},
	year = {2021},
	note = {arXiv: 2007.04005
Publisher: American Meteorological Society},
	keywords = {Deep learning, Forecast verification/skill, Machine learning, Model output statistics, Neural networks, Probability forecasts/models/distribution, ★},
	pages = {1141--1152},
}

@article{walker_skill_2019,
	title = {Skill of dynamical and {GHACOF} consensus seasonal forecasts of {East} {African} rainfall},
	volume = {53},
	issn = {14320894},
	doi = {10.1007/s00382-019-04835-9},
	abstract = {Seasonal forecasts of rainfall are considered the priority timescale by many users in the tropics. In East Africa, the primary operational seasonal forecast for the region is produced by the Greater Horn of Africa Climate Outlook Forum (GHACOF), and issued ahead of each rainfall season. This study evaluates and compares the GHACOF consensus forecasts with dynamical model forecasts from the UK Met Office GloSea5 seasonal prediction system for the two rainy seasons. GloSea demonstrates positive skill (r = 0.69) for the short rains at 1 month lead. In contrast, skill is low for the long rains due to lack of predictability of driving factors. For both seasons GHACOF forecasts show generally lower levels of skill than GloSea. Several systematic errors within the GHACOF forecasts are identified; the largest being the tendency to over-estimate the likelihood of near normal rainfall, with over 70\% (80\%) of forecasts giving this category the highest probability in the short (long) rains. In a more detailed evaluation of GloSea, a large wet bias, increasing with forecast lead time, is identified in the short rains. This bias is attributed to a developing cold SST bias in the eastern Indian Ocean, driving an easterly wind bias across the equatorial Indian Ocean. These biases affect the mean state moisture availability, and could act to reduce the ability of the dynamical model in predicting interannual variability, which may also be relevant to predictions from coupled models on longer timescales.},
	number = {7-8},
	journal = {Climate Dynamics},
	author = {Walker, Dean P. and Birch, Cathryn E. and Marsham, John H. and Scaife, Adam A. and Graham, Richard J. and Segele, Zewdu T.},
	month = oct,
	year = {2019},
	note = {Publisher: Springer Verlag},
	keywords = {Consensus outlooks, East Africa, Precipitation, Probabilistic verification, Seasonal climate forecasts},
	pages = {4911--4935},
}

@techreport{khayatkhoei_spatial_2022,
	title = {Spatial {Frequency} {Bias} in {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {www.aaai.org},
	abstract = {Understanding the capability of Generative Adversarial Networks (GANs) in learning the full spectrum of spatial frequencies , that is, beyond the low-frequency dominant spectrum of natural images, is critical for assessing the reliability of GAN-generated data in any detail-sensitive application. In this work, we show that the ability of convolutional GANs to learn an image distribution depends on the spatial frequency of the underlying carrier signal, that is, they have a bias against learning high spatial frequencies. Our findings are consistent with the recent observations of high-frequency artifacts in GAN-generated images, but further suggest that such artifacts are the consequence of an underlying bias. We also provide a theoretical explanation for this bias as the manifestation of linear dependencies present in the spectrum of filters of a typical generative Convolutional Neural Network (CNN). Finally, by proposing a proof-of-concept method that can effectively manipulate this bias towards other spatial frequencies , we show that the bias is not fixed and can be exploited to explicitly direct computational resources towards any specific spatial frequency of interest in a dataset, with minimal computational overhead.},
	author = {Khayatkhoei, Mahyar and Elgammal, Ahmed},
	year = {2022},
	keywords = {FT: Computer Vision (CV), FT: Machine Learning (ML), ★},
}

@article{vannitsem_predictability_2017,
	title = {Predictability of large-scale atmospheric motions: {Lyapunov} exponents and error dynamics},
	volume = {27},
	issn = {10541500},
	doi = {10.1063/1.4979042},
	abstract = {The deterministic equations describing the dynamics of the atmosphere (and of the climate system) are known to display the property of sensitivity to initial conditions. In the ergodic theory of chaos, this property is usually quantified by computing the Lyapunov exponents. In this review, these quantifiers computed in a hierarchy of atmospheric models (coupled or not to an ocean) are analyzed, together with their local counterparts known as the local or finite-time Lyapunov exponents. It is shown in particular that the variability of the local Lyapunov exponents (corresponding to the dominant Lyapunov exponent) decreases when the model resolution increases. The dynamics of (finite-amplitude) initial condition errors in these models is also reviewed, and in general found to display a complicated growth far from the asymptotic estimates provided by the Lyapunov exponents. The implications of these results for operational (high resolution) atmospheric and climate modelling are also discussed.},
	number = {3},
	journal = {Chaos},
	author = {Vannitsem, Stéphane},
	month = mar,
	year = {2017},
	pmid = {28364758},
	note = {arXiv: 1703.04284
Publisher: American Institute of Physics Inc.},
	keywords = {★},
}

@article{vaughan_convolutional_2022,
	title = {Convolutional conditional neural processes for local climate downscaling},
	volume = {15},
	issn = {19919603},
	doi = {10.5194/gmd-15-251-2022},
	abstract = {A new model is presented for multisite statistical downscaling of temperature and precipitation using convolutional conditional neural processes (convCNPs). ConvCNPs are a recently developed class of models that allow deep-learning techniques to be applied to off-the-grid spatio-temporal data. In contrast to existing methods that map from low-resolution model output to high-resolution predictions at a discrete set of locations, this model outputs a stochastic process that can be queried at an arbitrary latitude-longitude coordinate. The convCNP model is shown to outperform an ensemble of existing downscaling techniques over Europe for both temperature and precipitation taken from the VALUE intercomparison project. The model also outperforms an approach that uses Gaussian processes to interpolate single-site downscaling models at unseen locations. Importantly, substantial improvement is seen in the representation of extreme precipitation events. These results indicate that the convCNP is a robust downscaling model suitable for generating localised projections for use in climate impact studies.},
	number = {1},
	journal = {Geoscientific Model Development},
	author = {Vaughan, Anna and Tebbutt, Will and Hosking, J. Scott and Turner, Richard E.},
	month = jan,
	year = {2022},
	note = {arXiv: 2101.07950
Publisher: Copernicus GmbH},
	keywords = {★},
	pages = {251--268},
}

@techreport{ling_conditioned_nodate,
	title = {Conditioned {Spatial} {Downscaling} of {Climate} {Variables}},
	url = {https://github.com/evbecker/climate-spatial-downscaling},
	abstract = {Global Climate Models (GCM) play a vital role in assessing the large-scale impacts of climate change. Downscaling methods can translate coarse-resolution climate information from GCM to high-resolution predictions to forecast regional effects. Unfortunately, current downscaling methods struggle to fully take into account spatial relationships among variables, especially at long distances. In this work, we propose an instance-conditional pixel synthesis generative adversarial network (ICPS-GAN), wherein conditioning on spatial information is an explicit way of providing the GAN with previous high-resolution and current low-resolution data, resulting in an enhancement of the general performance. Experimental results on precipitation forecast for US region data outperform both traditional and other learning-based methods when extrapolating in space. The code is available at https://github.com/evbecker/climate-spatial-downscaling},
	author = {Ling, Alex and Hung, Yu and Becker, Evan and Zadouri, Ted and Grover, Aditya},
	keywords = {★},
}

@article{ji_clgan_nodate,
	title = {{CLGAN}: {A} {GAN}-based video prediction model for precipitation nowcasting},
	url = {https://doi.org/10.5194/egusphere-2022-859},
	doi = {10.5194/egusphere-2022-859},
	abstract = {The prediction of precipitation patterns at high spatio-temporal resolution up to two hours ahead, also known as precipitation nowcasting, is of great relevance in weather-dependant decision-making and early warning systems. In this study, we are aiming to provide an efficient and easy-to-understand model-CLGAN, to improve the nowcasting skills of heavy precipitation events with deep neural networks for video prediction. The model constitutes a Generative Adversarial Network (GAN) architecture whose generator is built upon an u-shaped encoder-decoder network (U-Net) equipped with recurrent 5 LSTM cells to capture spatio-temporal features. A comprehensive comparison among CLGAN, and baseline models optical flow model DenseRotation as well as the advanced video prediction model PredRNN-v2 is performed. We show that CLGAN outperforms in terms of scores for dichotomous events and object-based diagnostics. The ablation study indicates that the GAN-based architecture helps to capture heavy precipitation events. The results encourage future work based on the proposed CLGAN architecture to improve the precipitation nowcasting and early-warning systems.},
	author = {Ji, Yan and Gong, Bing and Langguth, Michael and Mozaffari, Amirpasha and Zhi, Xiefei},
	keywords = {★},
}

@article{castro-camilo_spliced_2019,
	title = {A {Spliced} {Gamma}-{Generalized} {Pareto} {Model} for {Short}-{Term} {Extreme} {Wind} {Speed} {Probabilistic} {Forecasting}},
	volume = {24},
	issn = {15372693},
	doi = {10.1007/s13253-019-00369-z},
	abstract = {Renewable sources of energy such as wind power have become a sustainable alternative to fossil fuel-based energy. However, the uncertainty and fluctuation of the wind speed derived from its intermittent nature bring a great threat to the wind power production stability, and to the wind turbines themselves. Lately, much work has been done on developing models to forecast average wind speed values, yet surprisingly little has focused on proposing models to accurately forecast extreme wind speeds, which can damage the turbines. In this work, we develop a flexible spliced Gamma-Generalized Pareto model to forecast extreme and non-extreme wind speeds simultaneously. Our model belongs to the class of latent Gaussian models, for which inference is conveniently performed based on the integrated nested Laplace approximation method. Considering a flexible additive regression structure, we propose two models for the latent linear predictor to capture the spatio-temporal dynamics of wind speeds. Our models are fast to fit and can describe both the bulk and the tail of the wind speed distribution while producing short-term extreme and non-extreme wind speed probabilistic forecasts. Supplementary materials accompanying this paper appear online.},
	number = {3},
	journal = {Journal of Agricultural, Biological, and Environmental Statistics},
	author = {Castro-Camilo, Daniela and Huser, Raphaël and Rue, Håvard},
	month = sep,
	year = {2019},
	note = {arXiv: 1810.04099
Publisher: Springer New York LLC},
	keywords = {Extreme-value theory, INLA, Latent Gaussian models, SPDE, Threshold-based inference, Wind speed forecasting, ★},
	pages = {517--534},
}

@article{hayatbini_conditional_2019,
	title = {Conditional generative adversarial networks ({cGANs}) for near real-time precipitation estimation from multispectral {GOES}-16 satellite imageries-{PERSIANN}-{cGAN}},
	volume = {11},
	issn = {20724292},
	doi = {10.3390/rs11192193},
	abstract = {In this paper, we present a state-of-the-art precipitation estimation framework which leverages advances in satellite remote sensing as well as Deep Learning (DL). The framework takes advantage of the improvements in spatial, spectral and temporal resolutions of the Advanced Baseline Imager (ABI) onboard the GOES-16 platform along with elevation information to improve the precipitation estimates. The procedure begins by first deriving a Rain/No Rain (R/NR) binary mask through classification of the pixels and then applying regression to estimate the amount of rainfall for rainy pixels. A Fully Convolutional Network is used as a regressor to predict precipitation estimates. The network is trained using the non-saturating conditional Generative Adversarial Network (cGAN) and Mean Squared Error (MSE) loss terms to generate results that better learn the complex distribution of precipitation in the observed data. Common verification metrics such as Probability Of Detection (POD), False Alarm Ratio (FAR), Critical Success Index (CSI), Bias, Correlation and MSE are used to evaluate the accuracy of both R/NR classification and real-valued precipitation estimates. Statistics and visualizations of the evaluation measures show improvements in the precipitation retrieval accuracy in the proposed framework compared to the baseline models trained using conventional MSE loss terms. This framework is proposed as an augmentation for PERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network- Cloud Classification System) algorithm for estimating global precipitation.},
	number = {19},
	journal = {Remote Sensing},
	author = {Hayatbini, Negin and Kong, Bailey and Hsu, Kuo Lin and Nguyen, Phu and Sorooshian, Soroosh and Stephens, Graeme and Fowlkes, Charless and Nemani, Ramakrishna and Ganguly, Sangram},
	month = oct,
	year = {2019},
	note = {Publisher: MDPI AG},
	keywords = {Convolutional neural networks (CNNs), Generative adversarial networks (GANs), Machine learning, Multispectral satellite imagery, Precipitation, ★},
}

@techreport{wilson_bayesian_nodate,
	title = {Bayesian {Deep} {Learning} and a {Probabilistic} {Perspective} of {Generalization}},
	url = {https://github.com/izmailovpavel/understandingbdl.},
	abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility.},
	author = {Wilson, Andrew Gordon and Izmailov, Pavel},
	keywords = {★},
}

@article{leinonen_stochastic_2020,
	title = {Stochastic {Super}-{Resolution} for {Downscaling} {Time}-{Evolving} {Atmospheric} {Fields} {With} a {Generative} {Adversarial} {Network}},
	volume = {59},
	issn = {0196-2892},
	doi = {10.1109/tgrs.2020.3032790},
	abstract = {Generative adversarial networks (GANs) have been recently adopted for super-resolution, an application closely related to what is referred to as "downscaling" in the atmospheric sciences: improving the spatial resolution of low-resolution images. The ability of conditional GANs to generate an ensemble of solutions for a given input lends itself naturally to stochastic downscaling, but the stochastic nature of GANs is not usually considered in super-resolution applications. Here, we introduce a recurrent, stochastic super-resolution GAN that can generate ensembles of time-evolving high-resolution atmospheric fields for an input consisting of a low-resolution sequence of images of the same field. We test the GAN using two datasets, one consisting of radar-measured precipitation from Switzerland, the other of cloud optical thickness derived from the Geostationary Earth Observing Satellite 16 (GOES-16). We find that the GAN can generate realistic, temporally consistent super-resolution sequences for both datasets. The statistical properties of the generated ensemble are analyzed using rank statistics, a method adapted from ensemble weather forecasting; these analyses indicate that the GAN produces close to the correct amount of variability in its outputs. As the GAN generator is fully convolutional, it can be applied after training to input images larger than the images used to train it. It is also able to generate time series much longer than the training sequences, as demonstrated by applying the generator to a three-month dataset of the precipitation radar data. The source code to our GAN is available at https://github.com/jleinonen/downscaling-rnn-gan.},
	number = {9},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Leinonen, Jussi and Nerini, Daniele and Berne, Alexis},
	month = nov,
	year = {2020},
	note = {arXiv: 2005.10374
Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {7211--7223},
}

@article{vandal_intercomparison_2019,
	title = {Intercomparison of machine learning methods for statistical downscaling: the case of daily and extreme precipitation},
	volume = {137},
	issn = {14344483},
	url = {https://link.springer.com/article/10.1007/s00704-018-2613-3},
	doi = {10.1007/s00704-018-2613-3},
	abstract = {Statistical downscaling of Global Climate Models (GCMs) allows researchers to study local climate change effects decades into the future. A wide range of statistical models have been applied to downscaling GCMs but recent advances in machine learning have not been explored compared to traditional approaches. In this paper, we compare five Perfect Prognosis (PP) approaches, Ordinary Least Squares, Elastic-Net, and Support Vector Machine along with two machine learning methods Multi-task Sparse Structure Learning (MSSL) and Autoencoder Neural Networks. In addition, we introduce a hybrid Model Output Statistics and PP approach by modeling the residuals of Bias Correction Spatial Disaggregation (BCSD) with MSSL. Metrics to evaluate each method’s ability to capture daily anomalies, large-scale climate shifts, and extremes are analyzed. Generally, we find inconsistent performance between PP methods in their ability to predict daily anomalies and extremes as well as monthly and annual precipitation. However, results suggest that L1 sparsity constraints aid in reducing error through internal feature selection. The MSSL+BCSD coupling, when compared with BCSD, improved daily, monthly, and annual predictability but decreased performance at the extremes. Hence, these results suggest that the direct application of state-of-the-art machine learning methods to statistical downscaling does not provide direct improvements over simpler, longstanding approaches.},
	number = {1-2},
	urldate = {2022-11-16},
	journal = {Theoretical and Applied Climatology},
	author = {Vandal, Thomas and Kodra, Evan and Ganguly, Auroop R.},
	month = jul,
	year = {2019},
	note = {arXiv: 1702.04018
Publisher: Springer-Verlag Wien},
	pages = {557--570},
}

@article{kashinath_physics-informed_2021,
	title = {Physics-informed machine learning: {Case} studies for weather and climate modelling},
	volume = {379},
	issn = {1364503X},
	doi = {10.1098/rsta.2020.0093},
	abstract = {Machine learning (ML) provides novel and powerful ways of accurately and efficiently recognizing complex patterns, emulating nonlinear dynamics, and predicting the spatio-temporal evolution of weather and climate processes. Off-the-shelf ML models, however, do not necessarily obey the fundamental governing laws of physical systems, nor do they generalize well to scenarios on which they have not been trained. We survey systematic approaches to incorporating physics and domain knowledge into ML models and distill these approaches into broad categories. Through 10 case studies, we show how these approaches have been used successfully for emulating, downscaling, and forecasting weather and climate processes. The accomplishments of these studies include greater physical consistency, reduced training time, improved data efficiency, and better generalization. Finally, we synthesize the lessons learned and identify scientific, diagnostic, computational, and resource challenges for developing truly robust and reliable physics-informed ML models for weather and climate processes. This article is part of the theme issue 'Machine learning for weather and climate modelling'.},
	number = {2194},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Kashinath, K. and Mustafa, M. and Albert, A. and Wu, J. L. and Jiang, C. and Esmaeilzadeh, S. and Azizzadenesheli, K. and Wang, R. and Chattopadhyay, A. and Singh, A. and Manepalli, A. and Chirila, D. and Yu, R. and Walters, R. and White, B. and Xiao, H. and Tchelepi, H. A. and Marcus, P. and Anandkumar, A. and Hassanzadeh, P. and {Prabhat}},
	month = apr,
	year = {2021},
	pmid = {33583262},
	note = {Publisher: Royal Society Publishing},
	keywords = {neural networks, physical constraints, physics-informed machine learning, turbulent flows, weather and climate modeling, ★},
}

@article{rampal_high-resolution_2022,
	title = {High-resolution downscaling with interpretable deep learning: {Rainfall} extremes over {New} {Zealand}},
	volume = {38},
	issn = {22120947},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212094722001049},
	doi = {10.1016/j.wace.2022.100525},
	journal = {Weather and Climate Extremes},
	author = {Rampal, Neelesh and Gibson, Peter B. and Sood, Abha and Stuart, Stephen and Fauchereau, Nicolas C. and Brandolino, Chris and Noll, Ben and Meyers, Tristan},
	month = dec,
	year = {2022},
	pages = {100525},
}

@inproceedings{vandal_deepsd_2017,
	address = {New York, NY, USA},
	title = {{DeepSD}: {Generating} {High} {Resolution} {Climate} {ChangeProjections} through {Single} {Image} {Super}-{Resolution}},
	isbn = {978-1-4503-4887-4},
	doi = {10.1145/3097983.3098004},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Vandal, Thomas and Kodra, Evan and Ganguly, Sangram and Michaelis, Andrew and Nemani, Ramakrishna and Ganguly, Auroop R.},
	month = aug,
	year = {2017},
	keywords = {★},
	pages = {1663--1672},
}

@article{steininger_convmos_2022,
	title = {{ConvMOS}: climate model output statistics with deep learning},
	issn = {1384-5810},
	url = {https://link.springer.com/10.1007/s10618-022-00877-6},
	doi = {10.1007/s10618-022-00877-6},
	abstract = {{\textless}p{\textgreater}Climate models are the tool of choice for scientists researching climate change. Like all models they suffer from errors, particularly systematic and location-specific representation errors. One way to reduce these errors is model output statistics (MOS) where the model output is fitted to observational data with machine learning. In this work, we assess the use of convolutional Deep Learning climate MOS approaches and present the ConvMOS architecture which is specifically designed based on the observation that there are systematic and location-specific errors in the precipitation estimates of climate models. We apply ConvMOS models to the simulated precipitation of the regional climate model REMO, showing that a combination of per-location model parameters for reducing location-specific errors and global model parameters for reducing systematic errors is indeed beneficial for MOS performance. We find that ConvMOS models can reduce errors considerably and perform significantly better than three commonly used MOS approaches and plain ResNet and U-Net models in most cases. Our results show that non-linear MOS models underestimate the number of extreme precipitation events, which we alleviate by training models specialized towards extreme precipitation events with the imbalanced regression method DenseLoss. While we consider climate MOS, we argue that aspects of ConvMOS may also be beneficial in other domains with geospatial data, such as air pollution modeling or weather forecasts.{\textless}/p{\textgreater}},
	journal = {Data Mining and Knowledge Discovery},
	author = {Steininger, Michael and Abel, Daniel and Ziegler, Katrin and Krause, Anna and Paeth, Heiko and Hotho, Andreas},
	month = oct,
	year = {2022},
	keywords = {★},
}

@article{rummukainen_state---art_2010,
	title = {State-of-the-art with regional climate models},
	volume = {1},
	url = {https://wires.onlinelibrary.wiley.com/doi/10.1002/wcc.8},
	doi = {10.1002/wcc.008},
	abstract = {Regional climate models are used by a large number of groups, for more or less all regions of the world. Regional climate models are complementary to global climate models. A typical use of regional climate models is to add further detail to global climate analyses or simulations, or to study climate processes in more detail than global models allow. The relationship between global and regional climate models is much akin to that of global and regional weather forecasting models. Over the past 20 years, the development of regional climate models has led to increased resolution, longer model runs, and steps towards regional climate system models. During recent years, community efforts have started to emerge in earnest, which can be expected to further advance the state-of-the-art in regional climate modeling. Applications of regional climate models span both the past and possible future climates, facilitating climate impact studies, information and support to climate policy, and adaptation.  2010 John Wiley \& Sons, Ltd. WIREs Clim Change 2010 1 82-96 G lobal climate models (GCMs) are a fundamental research tool for the understanding of climate. Regional climate models (RCMs) are a complementary research method, allowing more detailed process studies and simulation of regional and even local conditions. In so doing, they provide key input to climate impact studies as well as to adaptation planning, dealing with possible damages and opportunities related to climate variability and change. RCMs are thus vehicles for both research and applications. RCMs are not a new concept. They are at their core limited area models that are used in numerical weather prediction (NWP). The pioneering regional climate modeling efforts were those of Refs 1 and 2 For more information on the earlier developments, the reader is referred to Refs 3-7. Today, regional climate modeling encompasses a large international community and covers most geographical regions of the globe (see Figure 1). This article describes the essential principles of RCMs, outlining their potential and acknowledges fundamental limitations, for the interested interdis-ciplinary readership. Consideration is also given to the role of RCMs vis-` a-vis applications to climate projections. Other major uses are mentioned in brief, such as climate process and climate system studies. The references provided are not exhaustive and the discussion does not venture into deep detail. Weather and seasonal forecasting applications are outside the scope of this review. THE DOWNSCALING CONCEPT The climate system is global. Observations, theory, and models are all needed in climate research. Comprehensive climate models are based on physical laws and allow for numerical simulations. The climate system is characterized by a broad range of spatial scales and timescales. Consequently, GCMs can effectively address large-scale climate features such as the general circulation of the atmosphere and the ocean, and sub-continental patterns of, for example, temperature and precipitation. Their formal resolution (grid scale) is at best around 100-200 km. 8 Their real resolution is more like 6-8 grid distances, i.e., of the order of 1000 km. 9 This falls short of many key regional and local climate aspects, e.g. intensive precipitation. Very high global model resolution would of course give rise to simulation of regional and local aspects, see e.g., Ref 10. GCMs of this kind are, however, still not feasible due to their high computational cost. Other methods are therefore needed, which is the backdrop to downscaling (cf. Figure 2).},
	journal = {John Wiley \& Sons, Ltd},
	author = {Rummukainen, Markku},
	year = {2010},
}

@article{kumar_deep_nodate,
	title = {Deep learning-based downscaling of summer monsoon rainfall data over {Indian} region},
	url = {https://doi.org/10.1007/s00704-020-03489-6},
	doi = {10.1007/s00704-020-03489-6/Published},
	abstract = {Downscaling is necessary to generate high-resolution observation data to validate the climate model forecast or monitor rainfall at the micro-regional level operationally. Available observations generated by automated weather stations or meteorological observatories are often limited in spatial resolution resulting in misrepresentation or absence of rainfall information at these levels. Dynamical and statistical downscaling models are often used to get information at high-resolution gridded data over larger domains. As rainfall variability is dependent on the complex spatio-temporal process leading to non-linear or chaotic spatio-temporal variations, no single downscaling method can be considered efficient enough. In the domains dominated by complex topographies, quasi-periodicities, and non-linearities, deep learning (DL)-based methods provide an efficient solution in down-scaling rainfall data for regional climate forecasting and real-time rainfall observation data at high spatial resolutions. We employed three deep learning-based algorithms derived from the super-resolution convolutional neural network (SRCNN) methods in this work. Summer monsoon season data from India Meteorological Department (IMD) and the tropical rainfall measuring mission (TRMM) data set were downscaled up to 4 times higher resolution using these methods. High-resolution data derived from deep learning-based models provide better results than linear interpolation for up to 4 times higher resolution. Among the three algorithms, namely, SRCNN, stacked SRCNN, and DeepSD, employed here, the best spatial distribution of rainfall amplitude and minimum root-mean-square error is produced by DeepSD-based downscaling. Hence, the use of the DeepSD algorithm is advocated for future use. We found that spatial discontinuity in amplitude and intensity rainfall patterns is the main obstacle in the downscaling of precipitation. Furthermore, we applied these methods for model data post-processing, in particular, ERA5 reanalysis data. Downscaled ERA5 rainfall data show a much better distribution of spatial covariance and temporal variance when compared with observation. This study is the first step towards developing deep learning-based weather data downscaling model for Indian summer monsoon rainfall data.},
	author = {Kumar, Bipin and Chattopadhyay, Rajib and Singh, Manmeet and Chaudhari, Niraj and Kodari, Karthik and Barve, Amit},
	keywords = {Deep learning method, Rainfall data over Indian region, Statistical downscaling, Super-resolution},
}

@article{wang_deep_2021,
	title = {Deep {Learning} for {Daily} {Precipitation} and {Temperature} {Downscaling}},
	volume = {57},
	issn = {19447973},
	doi = {10.1029/2020WR029308},
	abstract = {Downscaling is a critical step to bridge the gap between large-scale climate information and local-scale impact assessment. This study presents a novel deep learning approach: Super Resolution Deep Residual Network (SRDRN) for downscaling daily precipitation and temperature. This approach was constructed based on an advanced deep convolutional neural network with residual blocks and batch normalizations. The data augmentation technique was utilized to address overfitting that is due to highly imbalanced precipitation and nonprecipitation days and sparse precipitation extremes. Synthetic experiments were designed to downscale daily maximum/minimum temperature and precipitation data from coarse resolutions (25, 50, and 100 km) to a high resolution (4 km). The results showed that, during the validation period, the SRDRN approach not only captured the spatial and temporal patterns remarkably well, but also reproduced both precipitation and temperature extremes in different locations and time at the local scale. Through transfer learning, the trained SRDRN model in one region was directly applied to downscale precipitation in another region with a different environment, and the results showed notable improvement compared to classic statistical downscaling methods. The outstanding performance of the SRDRN approach stemmed from its ability to fully extract spatial features without suffering from degradation and overfitting issues due to the incorporations of residual blocks, batch normalizations, and data augmentations. The SRDRN approach is thus a powerful tool for downscaling daily precipitation and temperature and can potentially be leveraged to downscale any hydrologic, climate, and earth system data.},
	number = {4},
	journal = {Water Resources Research},
	author = {Wang, Fang and Tian, Di and Lowe, Lisa and Kalin, Latif and Lehrter, John},
	month = apr,
	year = {2021},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {deep learning, downscaling, precipitation, temperature},
}

@article{hess_physically_2022,
	title = {Physically {Constrained} {Generative} {Adversarial} {Networks} for {Improving} {Precipitation} {Fields} from {Earth} {System} {Models}},
	url = {http://arxiv.org/abs/2209.07568},
	abstract = {Precipitation results from complex processes across many scales, making its accurate simulation in Earth system models (ESMs) challenging. Existing post-processing methods can improve ESM simulations locally, but cannot correct errors in modelled spatial patterns. Here we propose a framework based on physically constrained generative adversarial networks (GANs) to improve local distributions and spatial structure simultaneously. We apply our approach to the computationally efficient ESM CM2Mc-LPJmL. Our method outperforms existing ones in correcting local distributions, and leads to strongly improved spatial patterns especially regarding the intermittency of daily precipitation. Notably, a double-peaked Intertropical Convergence Zone, a common problem in ESMs, is removed. Enforcing a physical constraint to preserve global precipitation sums, the GAN can generalize to future climate scenarios unseen during training. Feature attribution shows that the GAN identifies regions where the ESM exhibits strong biases. Our method constitutes a general framework for correcting ESM variables and enables realistic simulations at a fraction of the computational costs.},
	journal = {Nature Machine Intelligence},
	author = {Hess, Philipp and Drüke, Markus and Petri, Stefan and Strnad, Felix M. and Boers, Niklas},
	month = aug,
	year = {2022},
	note = {arXiv: 2209.07568},
}

@article{geiss_strict_2020,
	title = {Strict {Enforcement} of {Conservation} {Laws} and {Invertibility} in {CNN}-{Based} {Super} {Resolution} for {Scientific} {Datasets}},
	url = {http://arxiv.org/abs/2011.05586},
	abstract = {Recently, deep Convolutional Neural Networks (CNNs) have revolutionized image super-resolution (SR), dramatically outperforming past methods for enhancing image resolution. They could be a boon for the many scientific fields that involve image or gridded datasets: satellite remote sensing, radar meteorology, medical imaging, numerical modeling etc. Unfortunately, while SR-CNNs produce visually compelling outputs, they may break physical conservation laws when applied to scientific datasets. Here, a method for ``Downsampling Enforcement" in SR-CNNs is proposed. A differentiable operator is derived that, when applied as the final transfer function of a CNN, ensures the high resolution outputs exactly reproduce the low resolution inputs under 2D-average downsampling while improving performance of the SR schemes. The method is demonstrated across seven modern CNN-based SR schemes on several benchmark image datasets, and applications to weather radar, satellite imager, and climate model data are also shown. The approach improves training time and performance while ensuring physical consistency between the super-resolved and low resolution data.},
	author = {Geiss, Andrew and Hardin, Joseph C.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.05586},
	keywords = {★},
}

@techreport{palmer_extended-range_nodate,
	title = {Extended-range atmospheric prediction and the {Lorenz} model},
	abstract = {The physical basis for extended-range prediction is explored using the famous three-component Lorenz convection model, taken as a conceptual representation of the chaotic extratropical circulation , and extended by coupling to a linear oscillator to represent large-scale tropical-extratropical interactions. The model is used to analyze the roles of time averaging and ensemble forecasting, and, in extended form, the impact of both anomalous tropical sea surface temperature and anomalous extratropical sea surface temperature. The conceptual paradigms and analytic calculations presented are used to interpret results from numerical weather prediction and general circulation model experiments. Some remarks on the relevance of predictability studies for the climate change problem are given.},
	author = {Palmer, TN},
}

@techreport{wikipedia_numerical_nodate,
	title = {Numerical {Weather} {Prediction} ({Wikipedia})},
	author = {{Wikipedia}},
}

@article{bauer_quiet_2015,
	title = {The quiet revolution of numerical weather prediction},
	volume = {525},
	issn = {14764687},
	doi = {10.1038/nature14956},
	abstract = {Advances in numerical weather prediction represent a quiet revolution because they have resulted from a steady accumulation of scientific knowledge and technological advances over many years that, with only a few exceptions, have not been associated with the aura of fundamental physics breakthroughs. Nonetheless, the impact of numerical weather prediction is among the greatest of any area of physical science. As a computational problem, global weather prediction is comparable to the simulation of the human brain and of the evolution of the early Universe, and it is performed every day at major operational centres across the world.},
	number = {7567},
	journal = {Nature},
	author = {Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
	month = sep,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	pages = {47--55},
}

@article{leutbecher_ensemble_2008,
	title = {Ensemble forecasting},
	volume = {227},
	issn = {10902716},
	doi = {10.1016/j.jcp.2007.02.014},
	abstract = {Numerical weather prediction models as well as the atmosphere itself can be viewed as nonlinear dynamical systems in which the evolution depends sensitively on the initial conditions. The fact that estimates of the current state are inaccurate and that numerical models have inadequacies, leads to forecast errors that grow with increasing forecast lead time. The growth of errors depends on the flow itself. Ensemble forecasting aims at quantifying this flow-dependent forecast uncertainty. The sources of uncertainty in weather forecasting are discussed. Then, an overview is given on evaluating probabilistic forecasts and their usefulness compared with single forecasts. Thereafter, the representation of uncertainties in ensemble forecasts is reviewed with an emphasis on the initial condition perturbations. The review is complemented by a detailed description of the methodology to generate initial condition perturbations of the Ensemble Prediction System (EPS) of the European Centre for Medium-Range Weather Forecasts (ECMWF). These perturbations are based on the leading part of the singular value decomposition of the operator describing the linearised dynamics over a finite time interval. The perturbations are flow-dependent as the linearisation is performed with respect to a solution of the nonlinear forecast model. The extent to which the current ECMWF ensemble prediction system is capable of predicting flow-dependent variations in uncertainty is assessed for the large-scale flow in mid-latitudes. © 2007 Elsevier Inc. All rights reserved.},
	number = {7},
	journal = {Journal of Computational Physics},
	author = {Leutbecher, M. and Palmer, T. N.},
	month = mar,
	year = {2008},
	note = {Publisher: Academic Press Inc.},
	keywords = {Numerical weather prediction, Predictability, Uncertainty},
	pages = {3515--3539},
}

@article{slingo_uncertainty_2011,
	title = {Uncertainty in weather and climate prediction},
	volume = {369},
	issn = {1364503X},
	doi = {10.1098/rsta.2011.0161},
	abstract = {Following Lorenz's seminal work on chaos theory in the 1960s, probabilistic approaches to prediction have come to dominate the science of weather and climate forecasting. This paper gives a perspective on Lorenz's work and how it has influenced the ways in which we seek to represent uncertainty in forecasts on all lead times from hours to decades. It looks at how model uncertainty has been represented in probabilistic prediction systems and considers the challenges posed by a changing climate. Finally, the paper considers how the uncertainty in projections of climate change can be addressed to deliver more reliable and confident assessments that support decision-making on adaptation and mitigation. This journal is © 2011 The Royal Society.},
	number = {1956},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Slingo, Julia and Palmer, Tim},
	month = dec,
	year = {2011},
	note = {Publisher: Royal Society},
	keywords = {Climate prediction, Ensemble prediction system, Probabilities, Uncertainty, Weather forecasting},
	pages = {4751--4767},
}

@article{beucler_enforcing_2019,
	title = {Enforcing {Analytic} {Constraints} in {Neural}-{Networks} {Emulating} {Physical} {Systems}},
	url = {http://arxiv.org/abs/1909.00912},
	doi = {10.1103/PhysRevLett.126.098302},
	abstract = {Neural networks can emulate nonlinear physical systems with high accuracy, yet they may produce physically-inconsistent results when violating fundamental constraints. Here, we introduce a systematic way of enforcing nonlinear analytic constraints in neural networks via constraints in the architecture or the loss function. Applied to convective processes for climate modeling, architectural constraints enforce conservation laws to within machine precision without degrading performance. Enforcing constraints also reduces errors in the subsets of the outputs most impacted by the constraints.},
	journal = {Physical Review Letters},
	author = {Beucler, Tom and Pritchard, Michael and Rasp, Stephan and Ott, Jordan and Baldi, Pierre and Gentine, Pierre},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.00912},
}

@article{pacchiardi_probabilistic_2021,
	title = {Probabilistic {Forecasting} with {Generative} {Networks} via {Scoring} {Rule} {Minimization}},
	url = {http://arxiv.org/abs/2112.08217},
	abstract = {Generative networks are often trained to minimize a statistical divergence between the reference distribution and the generative one in an adversarial setting. Some works trained instead generative networks to minimize Scoring Rules, functions assessing how well the generative distribution matches each training sample individually. We show how the Scoring Rule formulation easily extends to the so-called prequential (predictive-sequential) score, whose minimization allows performing probabilistic forecasting with generative networks. This objective leads to adversarial-free training, therefore easily avoiding uncertainty underestimation due to mode collapse, which is a common issue in the adversarial setting and undesirable for probabilistic forecasting. We provide consistency guarantees for the minimizer of the prequential score and employ that to perform probabilistic forecasting for two chaotic dynamical models and a benchmark dataset of global weather observations. For this last example, we define scoring rules for spatial data by drawing from the relevant literature, with which we obtain better uncertainty quantification with little hyperparameter tuning compared to adversarial training.},
	author = {Pacchiardi, Lorenzo and Adewoyin, Rilwan and Dueben, Peter and Dutta, Ritabrata},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.08217},
	keywords = {★},
}

@article{larsen_autoencoding_2015,
	title = {Autoencoding beyond pixels using a learned similarity metric},
	url = {http://arxiv.org/abs/1512.09300},
	abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
	author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.09300},
	keywords = {★},
}

@techreport{su_pixel-adaptive_nodate,
	title = {Pixel-{Adaptive} {Convolutional} {Neural} {Networks}},
	abstract = {Convolutions are the fundamental building blocks of CNNs. The fact that their weights are spatially shared is one of the main reasons for their widespread use, but it is also a major limitation, as it makes convolutions content-agnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet effective modification of standard convolutions, in which the filter weights are multiplied with a spatially varying kernel that depends on learnable, local pixel features. PAC is a generalization of several popular filtering techniques and thus can be used for a wide range of use cases. Specifically, we demonstrate state-of-the-art performance when PAC is used for deep joint image upsampling. PAC also offers an effective alternative to fully-connected CRF (Full-CRF), called PAC-CRF, which performs competitively compared to Full-CRF, while being considerably faster. In addition, we also demonstrate that PAC can be used as a drop-in replacement for convolution layers in pre-trained networks, resulting in consistent performance improvements.},
	author = {Su, Hang and Jampani, Varun and Sun, Deqing and Gallo, Orazio and Learned-Miller, Erik and Kautz, Jan and Amherst, UMass},
}

@article{clare_combining_2021,
	title = {Combining distribution‐based neural networks to predict weather forecast probabilities},
	volume = {147},
	issn = {0035-9009},
	doi = {10.1002/qj.4180},
	number = {741},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Clare, Mariana C.A. and Jamil, Omar and Morcrette, Cyril J.},
	month = oct,
	year = {2021},
	keywords = {★},
	pages = {4337--4357},
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	author = {Kingma, Diederik P and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
}

@article{reichstein_deep_2019,
	title = {Deep learning and process understanding for data-driven {Earth} system science},
	volume = {566},
	issn = {14764687},
	doi = {10.1038/s41586-019-0912-1},
	abstract = {Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning.},
	number = {7743},
	journal = {Nature},
	author = {Reichstein, Markus and Camps-Valls, Gustau and Stevens, Bjorn and Jung, Martin and Denzler, Joachim and Carvalhais, Nuno and {Prabhat}},
	month = feb,
	year = {2019},
	pmid = {30760912},
	note = {Publisher: Nature Publishing Group},
	pages = {195--204},
}

@incollection{vallis_effects_2017,
	title = {Effects of {Rotation} and {Stratification}},
	booktitle = {Atmospheric and {Oceanic} {Fluid} {Dynamics}},
	publisher = {Cambridge University Press},
	author = {Vallis, Geoffrey K.},
	month = may,
	year = {2017},
	doi = {10.1017/9781107588417.003},
	pages = {55--104},
}

@incollection{vallis_equations_2017,
	title = {Equations of {Motion}},
	booktitle = {Atmospheric and {Oceanic} {Fluid} {Dynamics}},
	publisher = {Cambridge University Press},
	author = {Vallis, Geoffrey K.},
	month = may,
	year = {2017},
	doi = {10.1017/9781107588417.002},
	pages = {3--54},
}

@article{bochenek_machine_2022,
	title = {Machine {Learning} in {Weather} {Prediction} and {Climate} {Analyses}—{Applications} and {Perspectives}},
	volume = {13},
	issn = {20734433},
	doi = {10.3390/atmos13020180},
	abstract = {In this paper, we performed an analysis of the 500 most relevant scientific articles published since 2018, concerning machine learning methods in the field of climate and numerical weather prediction using the Google Scholar search engine. The most common topics of interest in the abstracts were identified, and some of them examined in detail: in numerical weather prediction research—photovoltaic and wind energy, atmospheric physics and processes; in climate research— parametrizations, extreme events, and climate change. With the created database, it was also possible to extract the most commonly examined meteorological fields (wind, precipitation, temperature, pressure, and radiation), methods (Deep Learning, Random Forest, Artificial Neural Networks, Support Vector Machine, and XGBoost), and countries (China, USA, Australia, India, and Germany) in these topics. Performing critical reviews of the literature, authors are trying to predict the future research direction of these fields, with the main conclusion being that machine learning methods will be a key feature in future weather forecasting.},
	number = {2},
	journal = {Atmosphere},
	author = {Bochenek, Bogdan and Ustrnul, Zbigniew},
	month = feb,
	year = {2022},
	note = {Publisher: MDPI},
	keywords = {Climate, Machine learning, Numerical weather prediction, Weather},
}

@article{lloyd_quantum_2020,
	title = {Quantum algorithm for nonlinear differential equations},
	url = {http://arxiv.org/abs/2011.06571},
	abstract = {Quantum computers are known to provide an exponential advantage over classical computers for the solution of linear differential equations in high-dimensional spaces. Here, we present a quantum algorithm for the solution of nonlinear differential equations. The quantum algorithm provides an exponential advantage over classical algorithms for solving nonlinear differential equations. Potential applications include the Navier-Stokes equation, plasma hydrodynamics, epidemiology, and more.},
	author = {Lloyd, Seth and De Palma, Giacomo and Gokler, Can and Kiani, Bobak and Liu, Zi-Wen and Marvian, Milad and Tennie, Felix and Palmer, Tim},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.06571},
}

@article{espeholt_deep_2022,
	title = {Deep learning for twelve hour precipitation forecasts},
	volume = {13},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32483-x},
	doi = {10.1038/s41467-022-32483-x},
	abstract = {{\textless}p{\textgreater}Existing weather forecasting models are based on physics and use supercomputers to evolve the atmosphere into the future. Better physics-based forecasts require improved atmospheric models, which can be difficult to discover and develop, or increasing the resolution underlying the simulation, which can be computationally prohibitive. An emerging class of weather models based on neural networks overcome these limitations by learning the required transformations from data instead of relying on hand-coded physics and by running efficiently in parallel. Here we present a neural network capable of predicting precipitation at a high resolution up to 12 h ahead. The model predicts raw precipitation targets and outperforms for up to 12 h of lead time state-of-the-art physics-based models currently operating in the Continental United States. The results represent a substantial step towards validating the new class of neural weather models.{\textless}/p{\textgreater}},
	number = {1},
	journal = {Nature Communications},
	author = {Espeholt, Lasse and Agrawal, Shreya and Sønderby, Casper and Kumar, Manoj and Heek, Jonathan and Bromberg, Carla and Gazen, Cenk and Carver, Rob and Andrychowicz, Marcin and Hickey, Jason and Bell, Aaron and Kalchbrenner, Nal},
	month = sep,
	year = {2022},
	pages = {5145},
}

@article{rasp_data-driven_2020,
	title = {Data-driven medium-range weather prediction with a {Resnet} pretrained on climate simulations: {A} new model for {WeatherBench}},
	url = {http://arxiv.org/abs/2008.08626},
	doi = {10.1029/2020MS002405},
	abstract = {Numerical weather prediction has traditionally been based on physical models of the atmosphere. Recently, however, the rise of deep learning has created increased interest in purely data-driven medium-range weather forecasting with first studies exploring the feasibility of such an approach. To accelerate progress in this area, the WeatherBench benchmark challenge was defined. Here, we train a deep residual convolutional neural network (Resnet) to predict geopotential, temperature and precipitation at 5.625 degree resolution up to 5 days ahead. To avoid overfitting and improve forecast skill, we pretrain the model using historical climate model output before fine-tuning on reanalysis data. The resulting forecasts outperform previous submissions to WeatherBench and are comparable in skill to a physical baseline at similar resolution. We also analyze how the neural network creates its predictions and find that, with some exceptions, it is compatible with physical reasoning. Finally, we perform scaling experiments to estimate the potential skill of data-driven approaches at higher resolutions.},
	author = {Rasp, Stephan and Thuerey, Nils},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.08626},
}

@article{espeholt_skillful_2021,
	title = {Skillful {Twelve} {Hour} {Precipitation} {Forecasts} using {Large} {Context} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2111.07470},
	abstract = {The problem of forecasting weather has been scientifically studied for centuries due to its high impact on human lives, transportation, food production and energy management, among others. Current operational forecasting models are based on physics and use supercomputers to simulate the atmosphere to make forecasts hours and days in advance. Better physics-based forecasts require improvements in the models themselves, which can be a substantial scientific challenge, as well as improvements in the underlying resolution, which can be computationally prohibitive. An emerging class of weather models based on neural networks represents a paradigm shift in weather forecasting: the models learn the required transformations from data instead of relying on hand-coded physics and are computationally efficient. For neural models, however, each additional hour of lead time poses a substantial challenge as it requires capturing ever larger spatial contexts and increases the uncertainty of the prediction. In this work, we present a neural network that is capable of large-scale precipitation forecasting up to twelve hours ahead and, starting from the same atmospheric state, the model achieves greater skill than the state-of-the-art physics-based models HRRR and HREF that currently operate in the Continental United States. Interpretability analyses reinforce the observation that the model learns to emulate advanced physics principles. These results represent a substantial step towards establishing a new paradigm of efficient forecasting with neural networks.},
	author = {Espeholt, Lasse and Agrawal, Shreya and Sønderby, Casper and Kumar, Manoj and Heek, Jonathan and Bromberg, Carla and Gazen, Cenk and Hickey, Jason and Bell, Aaron and Kalchbrenner, Nal},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.07470},
}

@article{lim_time_2020,
	title = {Time {Series} {Forecasting} {With} {Deep} {Learning}: {A} {Survey}},
	url = {http://arxiv.org/abs/2004.13408},
	doi = {10.1098/rsta.2020.0209},
	abstract = {Numerous deep learning architectures have been developed to accommodate the diversity of time series datasets across different domains. In this article, we survey common encoder and decoder designs used in both one-step-ahead and multi-horizon time series forecasting -- describing how temporal information is incorporated into predictions by each model. Next, we highlight recent developments in hybrid deep learning models, which combine well-studied statistical models with neural network components to improve pure methods in either category. Lastly, we outline some ways in which deep learning can also facilitate decision support with time series data.},
	author = {Lim, Bryan and Zohren, Stefan},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.13408},
}

@article{lara-benitez_experimental_2021,
	title = {An {Experimental} {Review} on {Deep} {Learning} {Architectures} for {Time} {Series} {Forecasting}},
	volume = {31},
	issn = {17936462},
	doi = {10.1142/S0129065721300011},
	abstract = {In recent years, deep learning techniques have outperformed traditional models in many machine learning tasks. Deep neural networks have successfully been applied to address time series forecasting problems, which is a very important topic in data mining. They have proved to be an effective solution given their capacity to automatically learn the temporal dependencies present in time series. However, selecting the most convenient type of deep neural network and its parametrization is a complex task that requires considerable expertise. Therefore, there is a need for deeper studies on the suitability of all existing architectures for different forecasting tasks. In this work, we face two main challenges: a comprehensive review of the latest works using deep learning for time series forecasting and an experimental study comparing the performance of the most popular architectures. The comparison involves a thorough analysis of seven types of deep learning models in terms of accuracy and efficiency. We evaluate the rankings and distribution of results obtained with the proposed models under many different architecture configurations and training hyperparameters. The datasets used comprise more than 50,000 time series divided into 12 different forecasting problems. By training more than 38,000 models on these data, we provide the most extensive deep learning study for time series forecasting. Among all studied models, the results show that long short-term memory (LSTM) and convolutional networks (CNN) are the best alternatives, with LSTMs obtaining the most accurate forecasts. CNNs achieve comparable performance with less variability of results under different parameter configurations, while also being more efficient.},
	number = {3},
	journal = {International Journal of Neural Systems},
	author = {Lara-Benítez, Pedro and Carranza-García, Manuel and Riquelme, José C.},
	month = mar,
	year = {2021},
	pmid = {33588711},
	note = {arXiv: 2103.12057
Publisher: World Scientific},
	keywords = {Deep learning, forecasting, review, time series},
}

@article{liu_time_2021,
	title = {Time {Series} is a {Special} {Sequence}: {Forecasting} with {Sample} {Convolution} and {Interaction}},
	url = {http://arxiv.org/abs/2106.09305},
	abstract = {Time series is a special type of sequence data, a set of observations collected at even time intervals and ordered chronologically. Existing deep learning techniques use generic sequence models (e.g., recurrent neural network, Transformer model, or temporal convolutional network) for time series analysis, which ignore some of its unique properties. In particular, three components characterize time series: trend, seasonality, and irregular components, and the former two components enable us to perform forecasting with reasonable accuracy. Other types of sequence data do not have such characteristics. Motivated by the above, in this paper, we propose a novel neural network architecture that conducts sample convolution and interaction for temporal modeling and apply it for the time series forecasting problem, namely {\textbackslash}textbf\{SCINet\}. Compared to conventional dilated causal convolution architectures, the proposed downsample-convolve-interact architecture enables multi-resolution analysis besides expanding the receptive field of the convolution operation, which facilitates extracting temporal relation features with enhanced predictability. Experimental results show that SCINet achieves significant prediction accuracy improvement over existing solutions across various real-world time series forecasting datasets.},
	author = {Liu, Minhao and Zeng, Ailing and Xu, Zhijian and Lai, Qiuxia and Xu, Qiang},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09305},
}

@article{borgeaud_improving_2021,
	title = {Improving language models by retrieving from trillions of tokens},
	url = {http://arxiv.org/abs/2112.04426},
	abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\${\textbackslash}times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
	author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
	year = {2021},
	note = {arXiv: 2112.04426},
}

@article{baylor_tfx_2017,
	title = {{TFX}: {A} {TensorFlow}-based production-scale machine learning platform},
	volume = {Part F1296},
	doi = {10.1145/3097983.3098021},
	abstract = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components - a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2\% increase in app installs resulting from improved data and model analysis.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Baylor, Denis and Breck, Eric and Cheng, Heng Tze and Fiedel, Noah and Foo, Chuan Yu and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and Koo, Chiu Yuen and Lew, Lukasz and Mewald, Clemens and Modi, Akshay Naresh and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and Whang, Steven Euijong and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and Zinkevich, Martin},
	year = {2017},
	note = {ISBN: 9781450348874},
	keywords = {Continuous training, End-to-end platform, Large-scale machine learning},
	pages = {1387--1395},
}

@article{liu_adversarial_2019,
	title = {Adversarial {Training} for {Large} {Neural} {Language} {Models}},
	author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
	year = {2019},
	note = {arXiv: 2004.08994v2},
	pages = {1--13},
}

@article{elazar_text-based_2021,
	title = {Text-based {NP} {Enrichment}},
	url = {http://arxiv.org/abs/2109.12085},
	abstract = {Understanding the relations between entities denoted by NPs in text is a critical part of human-like natural language understanding. However, only a fraction of such relations is covered by NLP tasks and models nowadays. In this work, we establish the task of text-based NP enrichment (TNE), that is, enriching each NP with all the preposition-mediated relations that hold between this and the other NPs in the text. The relations are represented as triplets, each denoting two NPs linked via a preposition. Humans recover such relations seamlessly, while current state-of-the-art models struggle with them due to the implicit nature of the problem. We build the first large-scale dataset for the problem, provide the formal framing and scope of annotation, analyze the data, and report the result of fine-tuned neural language models on the task, demonstrating the challenge it poses to current technology. We created a webpage with the data, data-exploration UI, code, models, and demo to foster further research into this challenging text understanding problem at yanaiela.github.io/TNE/.},
	number = {4},
	author = {Elazar, Yanai and Basmov, Victoria and Goldberg, Yoav and Tsarfaty, Reut},
	year = {2021},
	note = {arXiv: 2109.12085},
}

@article{rebuffi_data_2021,
	title = {Data {Augmentation} {Can} {Improve} {Robustness}},
	url = {http://arxiv.org/abs/2111.05328},
	abstract = {Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overfitting by using common data augmentation schemes. We demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Furthermore, we compare various augmentations techniques and observe that spatial composition techniques work the best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against \${\textbackslash}ell\_{\textbackslash}infty\$ and \${\textbackslash}ell\_2\$ norm-bounded perturbations of size \${\textbackslash}epsilon = 8/255\$ and \${\textbackslash}epsilon = 128/255\$, respectively. We show large absolute improvements of +2.93\% and +2.16\% in robust accuracy compared to previous state-of-the-art methods. In particular, against \${\textbackslash}ell\_{\textbackslash}infty\$ norm-bounded perturbations of size \${\textbackslash}epsilon = 8/255\$, our model reaches 60.07\% robust accuracy without using any external data. We also achieve a significant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TinyImageNet.},
	number = {NeurIPS},
	author = {Rebuffi, Sylvestre-Alvise and Gowal, Sven and Calian, Dan A. and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
	year = {2021},
	note = {arXiv: 2111.05328},
}

@article{engstrom_adversarial_2019,
	title = {Adversarial {Robustness} as a {Prior} for {Learned} {Representations}},
	url = {http://arxiv.org/abs/1906.00945},
	abstract = {An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .},
	author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Madry, Aleksander},
	year = {2019},
	note = {arXiv: 1906.00945},
}

@article{zaragoza_confidence_nodate,
	title = {Confidence {Measures} for {Neural} {Network} {Classifiers} 2 {Confidence} {Measures} for {Neural} {Net}- work {Classifiers}},
	volume = {05},
	author = {Zaragoza, Hugo and Alché-buc, Florence and Pierre, Université},
}

@article{mandelbaum_distance-based_nodate,
	title = {Distance-based {Confidence} {Score} for {Neural} {Network} {Classifiers}},
	author = {Mandelbaum, Amit and Weinshall, Daphna},
	note = {arXiv: 1709.09844v1},
}

@article{karamcheti_mind_2019,
	title = {Mind {Your} {Outliers}! {Investigating} the {Negative} {Impact} of {Outliers} on {Active} {Learning} for {Visual} {Question} {Answering}},
	author = {Karamcheti, Siddharth and Krishna, Ranjay and Christopher, Li Fei-fei},
	year = {2019},
	note = {arXiv: 2107.02331v1},
}

@article{bossert_impossibility_2000,
	title = {{AN} {IMPOSSIBILITY} {THEOREM} {FOR}},
	volume = {16},
	number = {January 1996},
	author = {Bossert, Walter and Broome, John and Bykvist, Krister and Carlson, Erik and Danielsson, Sven and Donaldson, David and Parfit, Derek and Igboemeka, Adeze and Rabinowicz, Wlodzimierz and Sobel, Howard and Sumner, Wayne and Gibson, John and Goldstick, Danny and Fleurbaey, Marc and Hammond, Peter and Hooker, Brad and Hurka, Tom and Jensen, Karsten Klint and Latus, Andrew and Odelstad, Jan and Orleans, New and Congress, Societies},
	year = {2000},
	pages = {247--266},
}

@article{mar_impossibility_nodate,
	title = {Impossibility and {Uncertainty} {Theorems} in {AI} {Value} {Alignment}},
	author = {Mar, A I and Eckersley, Peter},
	note = {arXiv: 1901.00064v3},
}

@article{lu_probit_2021,
	title = {A {Probit} {Tensor} {Factorization} {Model} {For} {Relational} {Learning}},
	author = {Lu, Wenbin},
	year = {2021},
	note = {arXiv: 2111.03943v2},
	pages = {1--30},
}

@article{noauthor_discriminative_nodate,
	title = {On discriminative vs. generative classifiers: {A} comparison of logistic regression and naive bayes},
	abstract = {This study departs from the reality of gender relations within the family institution which has to be dikhotomis, causing the pole inequality relations between men and women. Therefore, in this study wanted to dismantle the detail view of some theories, both social and feminist about gender relations in the family. Each of these theories (structural functional, conflict and feminist) has their own viewpoint about the pattern of gender relations in the family. However, simultaneously acknowledging that the social construction of culture remained significant influence on the division of roles are played by men (husbands) and women (wife) in the family institution. This means that the social construction of culture was instrumental in the creation of relations contribute between men and women equally or otherwise occurred inequality.},
}

@article{klie_zero_2020,
	title = {From {Zero} to {Hero}: {Human}-{In}-{The}-{Loop} {Entity} {Linking} in {Low} {Resource} {Domains}},
	doi = {10.18653/v1/2020.acl-main.624},
	abstract = {Entity linking (EL) is concerned with disam-biguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data. However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 \% compared to annotating without interactive support; users report that they strongly prefer our system. An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 .},
	author = {Klie, Jan-Christoph and Eckart de Castilho, Richard and Gurevych, Iryna},
	year = {2020},
	pages = {6982--6993},
}

@article{wiegreffe_attention_2020,
	title = {Attention is not not explanation},
	doi = {10.18653/v1/d19-1002},
	abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that 'Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
	journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
	author = {Wiegreffe, Sarah and Pinter, Yuval},
	year = {2020},
	note = {arXiv: 1908.04626
ISBN: 9781950737901},
	pages = {11--20},
}

@article{wiegreffe_attention_2020-1,
	title = {Attention is not not explanation},
	doi = {10.18653/v1/d19-1002},
	abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that 'Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
	journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
	author = {Wiegreffe, Sarah and Pinter, Yuval},
	year = {2020},
	note = {arXiv: 1908.04626
ISBN: 9781950737901},
	pages = {11--20},
}

@article{zhagorina_personal_2018,
	title = {Personal names popularity estimation and its application to record linkage},
	volume = {909},
	issn = {18650929},
	doi = {10.1007/978-3-030-00063-9_9},
	abstract = {In this study, we investigate several statistical techniques for personal name popularity estimation and perform a record linkage experiment guided by name popularity estimates. The results show that name popularity can leverage personal name matching in databases and be of interest for many other domains.},
	journal = {Communications in Computer and Information Science},
	author = {Zhagorina, Ksenia and Braslavski, Pavel and Gusev, Vladimir},
	year = {2018},
	note = {arXiv: 1811.05361
ISBN: 9783030000622},
	keywords = {Name distribution, Personal name matching, Record linkage},
	pages = {71--79},
}

@article{atanasova_diagnostic_2020,
	title = {A {Diagnostic} {Study} of {Explainability} {Techniques} for {Text} {Classification}},
	doi = {10.18653/v1/2020.emnlp-main.263},
	abstract = {Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.},
	author = {Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
	year = {2020},
	note = {arXiv: 2009.13295},
	pages = {3256--3274},
}

@article{lovett_inferring_2020,
	title = {Inferring proximity from {Bluetooth} {Low} {Energy} {RSSI} with {Unscented} {Kalman} {Smoothers}},
	issn = {2331-8422},
	url = {http://arxiv.org/abs/2007.05057},
	abstract = {The Covid-19 pandemic has resulted in a variety of approaches for managing infection outbreaks in international populations. One example is mobile phone applications, which attempt to alert infected individuals and their contacts by automatically inferring two key components of infection risk: the proximity to an individual who may be infected, and the duration of proximity. The former component, proximity, relies on Bluetooth Low Energy (BLE) Received Signal Strength Indicator(RSSI) as a distance sensor, and this has been shown to be problematic; not least because of unpredictable variations caused by different device types, device location on-body, device orientation, the local environment and the general noise associated with radio frequency propagation. In this paper, we present an approach that infers posterior probabilities over distance given sequences of RSSI values. Using a single-dimensional Unscented Kalman Smoother (UKS) for non-linear state space modelling, we outline several Gaussian process observation transforms, including: a generative model that directly captures sources of variation; and a discriminative model that learns a suitable observation function from training data using both distance and infection risk as optimisation objective functions. Our results show that good risk prediction can be achieved in \${\textbackslash}mathcal\{O\}(n)\$ time on real-world data sets, with the UKS outperforming more traditional classification methods learned from the same training data.},
	author = {Lovett, Tom and Briers, Mark and Charalambides, Marcos and Jersakova, Radka and Lomax, James and Holmes, Chris},
	year = {2020},
	note = {arXiv: 2007.05057},
	pages = {1--21},
}

@article{xia_graph_2021,
	title = {Graph {Learning}: {A} {Survey}},
	volume = {00},
	url = {http://arxiv.org/abs/2105.00696%0Ahttp://dx.doi.org/10.1109/TAI.2021.3076021},
	doi = {10.1109/TAI.2021.3076021},
	abstract = {Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.},
	number = {0},
	author = {Xia, Feng and Sun, Ke and Yu, Shuo and Aziz, Abdul and Wan, Liangtian and Pan, Shirui and Liu, Huan},
	year = {2021},
	note = {arXiv: 2105.00696},
	pages = {1--19},
}

@article{jaleniauskiene_strategies_2009,
	title = {The {Strategies} for {Translating} {Proper} {Names} in {Children}'s {Literature}},
	volume = {15},
	issn = {1648-2824},
	abstract = {The translation of proper names is one of the most challenging activities every translator faces. While working on children's literature, the translation is especially complicated since proper names usually have various allusions indicating sex, age, geographical belonging, history, specific meaning, playfulness of language and cultural connotations. The goal of this article is to draw attention to strategic choices for the translation of proper names in children's literature. First, the article presents the theoretical considerations that deal with different aspects of proper names in literary works and the issue of their translation. Second, the translation strategies provided by the translation theorist Eirlys E. Davies used for this research are explained. In addition, the principles of adaptation of proper names provided the State Commission of the Lithuanian Language are presented. Then, the discussion proceeds to the quantitative analysis of the translated proper names with an emphasis on providing and explaining numerous examples. The research has been carried out on four popular fantasy books translated from English and German by three Lithuanian translators. After analyzing the strategies of preservation, localization, transformation and creation, the strategy of localization has proved to be the most frequent one in all translations.},
	number = {15},
	journal = {Studies Abouot Languages},
	author = {Jaleniauskienė, Evelina and Čičelytė, Vilma},
	year = {2009},
	keywords = {children's literature, culture-specific items, domestication, foreignization, proper names, translation strategies},
	pages = {31--42},
}

@article{juzellnienl_translation_2016,
	title = {The {Translation} of {Proper} {Names} from {English} to {Lithuanian} in ⿿{Steve} {Jobs}⿿ by {W}. {Isaacson}},
	volume = {232},
	issn = {18770428},
	doi = {10.1016/j.sbspro.2016.10.108},
	abstract = {The translation of proper names is one of the most challenging activities faced by translators. All languages have particular personal names, some of which are deeply rooted in the culture of the speakers of the specific language; consequently, they can pose unique difficulties in the comprehension of culture-specific texts. It is interesting to note that some personal names have various allusions indicating sex, age, geographical belonging, history, specific meaning, playfulness of language and cultural connotations when omitting this implied information results in unacceptable translation. The goal of the research is to draw attention to the strategic choices for the translation of proper names in S. Jobs⿿ biography by Isaacson (2011) and its translation into Lithuanian by AmbrazeviĿius (2012). S. Jobs stands as the ultimate icon of inventiveness and applied imagination; by connecting creativity with technology he started the era known as Computer age or Digital Age. Findings and results: the paper first gives a short overview of the concept of proper names and of the techniques that are applied when translating them. Second, the translation strategies and principles provided by the translation theorists Davies (2003) and Venuti (1995) used for the research are explained. In addition, the principles of adaptation of proper names provided by the State Commission of the Lithuanian Language are presented. Subsequently, the discussion proceeds to the quantitative analysis of the translated proper names with the emphasis on providing and explaining numerous examples. The emphasis is drawn to the strategies of localization, preservation, transformation and creation.},
	number = {April},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {JuzelĿnienĿ, SaulĿ and PetronienĿ, SaulĿ and Kopylova, Ksenija},
	year = {2016},
	pages = {800--805},
}

@article{yamada_luke_2020,
	title = {{LUKE}: {Deep} contextualized entity representations with entity-aware self-attention},
	issn = {23318422},
	doi = {10.18653/v1/2020.emnlp-main.523},
	abstract = {Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer (Vaswani et al., 2017). The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT (Devlin et al., 2019). The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.},
	journal = {arXiv},
	author = {Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Matsumoto, Yuji},
	year = {2020},
	note = {arXiv: 2010.01057},
}

@article{winkler_matching_2011,
	title = {Matching and {Record} {Linkage}},
	doi = {10.1002/9781118150504.ch20},
	abstract = {This chapter contains sections titled: * Terminology and Definition of Errors * Improved Computer-Assisted Matching Methods * Standardization and Parsing * Matching Decision Rules * Evaluating the Quality of Lists * Estimation of Error Rates and Adjustment for Matching Error * Computing Resources and Automation * Concluding Remarks},
	author = {Winkler, William E.},
	year = {2011},
	pages = {353--384},
}

@article{memobust_handbook_method_2014,
	title = {Method: {Fellegi}-{Sunter} and {Jaro} {Approach} to {Record} {Linkage} {Contents}},
	number = {March},
	author = {{Memobust Handbook}},
	year = {2014},
}

@article{gu_record_2003,
	title = {Record linkage: {Current} practice and future directions},
	abstract = {Record linkage is the task of quickly and accurately identi- fying records corresponding to the same entity from one or more data sources. Record linkage is also known as data cleaning, entity reconcilia- tion or identification and the merge/purge problem. This paper presents the “standard” probabilistic record linkage model and the associated algorithm. Recent work in information retrieval, federated database sys- tems and data mining have proposed alternatives to key components of the standard algorithm. The impact of these alternatives on the stan- dard approach are assessed. The key question is whether and how these new alternatives are better in terms of time, accuracy and degree of automation for a particular record linkage application.},
	journal = {Cmis},
	author = {Gu, Lifang and Baxter, Rohan},
	year = {2003},
	keywords = {data cleaning, entity, entity identification, list washing, merge, object isomerism, purge, reconciliation, record linkage},
	pages = {03/83},
}

@book{brimicombe_data_2020,
	title = {Data and information quality issues},
	isbn = {978-3-319-24104-3},
	author = {Brimicombe, Allan},
	year = {2020},
	doi = {10.1201/9781482264654-14},
	note = {Publication Title: GIS Environmental Modelling and Engineering},
}

@article{winkler_using_2014,
	title = {Using the {EM} {Algorithm} for {Weight} {Computation} in the {Fellegi}-{Sunter} {Model}},
	volume = {12},
	issn = {14787954},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/25734004%0Ahttp://dx.doi.org/10.1016/S2214-109X(17)30297-8%0Ahttp://citeseer.ist.psu.edu/511528.html%5Cnpapers2://publication/uuid/BC906E09-49FD-4A25-88D1-0DA3649447EB},
	abstract = {The Agincourt Health and Demographic Surveillance System has since 2001 conducted a{\textbackslash}nbiannual household asset survey in order to quantify household socio-economic status (SES){\textbackslash}nin a rural population living in northeast South Africa. The data contain binary, ordinal and{\textbackslash}nnominal items. We aim to describe the SES landscape in the study population by clustering{\textbackslash}nthe households into homogeneous groups based on their asset status.{\textbackslash}nA model-based approach to clustering, based on latent variable models, is proposed. In{\textbackslash}nthe case of modeling binary or ordinal items, item response models are employed. For nominal{\textbackslash}nsurvey items, a factor analysis model, similar in nature to a multinomial probit model, is used.{\textbackslash}nBoth model types have an underlying latent variable structure \{\vphantom{\}} this similarity is exploited{\textbackslash}nand the models are combined to produce a hybrid model capable of handling mixed data{\textbackslash}ntypes. Further, a mixture of the hybrid models is considered to provide clustering capabilities{\textbackslash}nwithin the context of mixed binary, ordinal and nominal response data. The proposed model{\textbackslash}nis termed the mixture for factor analyzers for mixed data (MFA-MD).{\textbackslash}nThe MFA-MD model is applied to the SES data to cluster households into homogeneous{\textbackslash}ngroups. The model is estimated within the Bayesian paradigm, using a Markov chain Monte{\textbackslash}nCarlo algorithm. Intuitive groupings result providing insight to the dierent socio-economic{\textbackslash}nstrata within the Agincourt region},
	number = {1},
	journal = {Global Health Action},
	author = {Winkler, W and Serwaa-Bonsu, Adwoa and Herbst, AbrahamJ. and Reniers, Georges and Ijaa, Wilfred and Clark, Benjamin and Kabudula, Chodziwadziwa Whiteson and Sankoh, Osman and Rentsch, Christopher T. and Kabudula, Chodziwadziwa Whiteson and Catlett, Jason and Beckles, David and Machemba, Richard and Mtenga, Baltazar and Masilela, Nkosinathi and Michael, Denna and Natalis, Redempta and Urassa, Mark and Todd, Jim and Zaba, Basia and Reniers, Georges and McParland, Damien and Gormley, Isobel Claire and Mccormick, Tyler H. and Clark, Samuel J. and Kabudula, Chodziwadziwa Whiteson and Collinson, Mark A. and Li, Author Zehang and Mccormick, Tyler H. and Clark, Samuel J. and Byass, Peter and Kabudula, Chodziwadziwa Whiteson and Tollman, Stephen M. and Mee, Paul and Ngobeni, Sizzy and Silaule, Bernard and Xavier Gómez-Olivé, F. and Collinson, Mark A. and Kahn, Kathleen and Byass, Peter and Joubert, Jané D. and Tuoane-Nkhasi, Maletela and Kahn, Kathleen and Rao, Chalapati and Gómez-Olivé, Francesc Xavier and Mee, Paul and Tollman, Stephen M. and Lopez, Alan D. and Vos, Theo and Bradshaw, Debbie and Houle, Brian and Collinson, Mark A. and Kahn, Kathleen and Gómez-Olivé, Francesc Xavier and Tollman, Stephen M. and Clark, Samuel J. and Tollman, Stephen M. and Dusetzina, SB and Tyree, S and Meyer, AM and Green, L and Carpenter, WR and Campbell, Kevin M. and Deck, Dennis and Krupski, Antoinette and Byass, Peter and Kabudula, Chodziwadziwa Whiteson and Mee, Paul and Ngobeni, Sizzy and Silaule, Bernard and Gómez-Olivé, Francesc Xavier and Collinson, Mark A. and Tugendhaft, Aviva and Wagner, Ryan G. and Twine, Rhian and Hofman, Karen and Tollman, Stephen M. and Kahn, Kathleen and Herbst, K and Fottrell, E and Ali, M M and Odhiambo, F and Amek, N and Hamel, M J and Laserson, K F and Kahn, Kathleen and Kabudula, Chodziwadziwa Whiteson and Mee, Paul and Bird, J and Jakob, R and Sankoh, Osman and Tollman, Stephen M. and Borman, Sean},
	year = {2014},
	pmid = {25392892},
	note = {arXiv: 1401.5343v2
ISBN: 0387952845},
	keywords = {24 february 2010, 26 january 2010, 26 october 2009, Africa, Agincourt, Agincourt HDSS, Cause composition, Civil registration system, Clustering, Death registration, Deterministic record linkage, Electronic patient records, HIV/AIDS, HIV/Aids, Health and demographic surveillance system (HDSS), INDEPTH Network, InterVA, Item response theory, Metropolis-within-Gibbs, Mixed data, Mortality, Non-communicable diseases, Probabilistic record linkage, Record linkage, Rural, South Africa, Tuberculosis, Verbal autopsy, accepted, biometrics, ealth and demographic surveillance, epidemiological transition, fingerprint, for track-, hdsss, health and demographic sur, indepth network, provide a useful framework, published, received, record linkage, revised, systems, v eillance systems},
	pages = {2120},
}

@article{ivan_p_fellegi_theory_1969,
	title = {A {Theory} for {Record} {Linkage}},
	volume = {64},
	number = {328},
	journal = {Journal of the American Statistical Association},
	author = {{Ivan P. Fellegi} and {Alan B. Sunter}},
	year = {1969},
	pages = {1183--1210},
}

@article{wieting_no_2019,
	title = {No training required: {Exploring} random encoders for sentence classification},
	issn = {23318422},
	abstract = {We explore various methods for computing sentence representations from pretrained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods-as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward-which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.},
	journal = {arXiv},
	author = {Wieting, John and Kiela, Douwe},
	year = {2019},
	note = {arXiv: 1901.10444},
	pages = {1--16},
}

@article{mou_how_2016,
	title = {How transferable are neural networks in {NLP} applications?},
	doi = {10.18653/v1/d16-1046},
	abstract = {Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.},
	journal = {EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
	author = {Mou, Lili and Meng, Zhao and Yan, Rui and Li, Ge and Xu, Yan and Zhang, Lu and Jin, Zhi},
	year = {2016},
	note = {arXiv: 1603.06111
ISBN: 9781945626258},
	pages = {479--489},
}

@article{rountree_initialising_2006,
	title = {Initialising {Neural} {Networks} with {Prior} {Knowledge}},
	number = {September},
	author = {Rountree, Nathan},
	year = {2006},
}

@article{nur_artificial_2014,
	title = {Artificial {Neural} {Network} {Weight} {Optimization}: {A} {Review}},
	volume = {12},
	issn = {2302-4046},
	doi = {10.11591/telkomnika.v12i9.6264},
	number = {9},
	journal = {TELKOMNIKA Indonesian Journal of Electrical Engineering},
	author = {Nur, Abdirashid Salad and Mohd Radzi, Nor Haizan and Ibrahim, Ashraf Osman},
	year = {2014},
}

@article{des_combes_learning_2018,
	title = {On the learning dynamics of deep neural networks},
	issn = {23318422},
	abstract = {While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features' frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize gradient starvation where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.},
	number = {2016},
	journal = {arXiv},
	author = {Des Combes, Remi Tachet and Pezeshki, Mohammad and Shabanian, Samira and Courville, Aaron and Bengio, Yoshua},
	year = {2018},
	note = {arXiv: 1809.06848},
	pages = {1--21},
}

@article{ramos_quantitative_2017,
	title = {Quantitative measures to evaluate neural network weight initialization strategies},
	doi = {10.1109/CCWC.2017.7868389},
	abstract = {It has been reported numerous times in the neural network research literature that weight initialization in neural networks affects the learning rate, the convergence rate and the probability of correct classification. In this research paper we develop a theory for objectively testing various weight initialization strategies. Our theory provides a quantitative measure for each available weight initialization strategy. Thus for each initialization strategy and each epoch we estimate the conditional probability distribution function of correct classification given the epoch number. For each initialization strategy and for a given epoch the conditional probability is a random variable with certain probability distribution function and certain mean and variance. Based on multivariate analysis, statistics of extremes, analysis of variance and estimation theory we develop an objective framework and measurements to assess if one strategy is better than another or if the differences between strategies are not significant but they are due to random fluctuations.},
	number = {3},
	journal = {2017 IEEE 7th Annual Computing and Communication Workshop and Conference, CCWC 2017},
	author = {Ramos, Ernesto Zamora and Nakakuni, Masanori and Yfantis, Evangelos},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509042289},
	keywords = {backpropagation, convergence rate, machine learning, neural network, parameter initialization, weight initialization},
}

@article{daniely_toward_2016,
	title = {Toward deeper understanding of neural networks: {The} power of initialization and a dual view on expressivity},
	issn = {10495258},
	abstract = {We develop a general duality between neural networks and compositional kernel Hilbert spaces. We introduce the notion of a computation skeleton, an acyclic graph that succinctly describes both a family of neural networks and a kernel space. Random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights. The kernel space consists of functions that arise by compositions, averaging, and non-linear transformations governed by the skeleton's graph topology and activation functions. We prove that random networks induce representations which approximate the kernel space. In particular, it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
	year = {2016},
	note = {arXiv: 1602.05897},
	pages = {2261--2269},
}

@article{sinha_masked_2021,
	title = {Masked {Language} {Modeling} and the {Distributional} {Hypothesis}: {Order} {Word} {Matters} {Pre}-training for {Little}},
	url = {http://arxiv.org/abs/2104.06644},
	abstract = {A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks -- including on tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.},
	author = {Sinha, Koustuv and Jia, Robin and Hupkes, Dieuwke and Pineau, Joelle and Williams, Adina and Kiela, Douwe},
	year = {2021},
	note = {arXiv: 2104.06644},
}

@article{kumar_weight_2017,
	title = {On weight initialization in deep neural networks},
	issn = {23318422},
	abstract = {A proper initialization of the weights in a neural network is critical to its convergence. Current insights into weight initialization come primarily from linear activation functions. In this paper, I develop a theory for weight initializations with non-linear activations. First, I derive a general weight initialization strategy for any neural network using activation functions differentiable at 0. Next, I derive the weight initialization strategy for the Rectified Linear Unit (RELU), and provide theoretical insights into why the Xavier initialization is a poor choice with RELU activations. My analysis provides a clear demonstration of the role of non-linearities in determining the proper weight initializations.},
	journal = {arXiv},
	author = {Kumar, Siddharth Krishna},
	year = {2017},
	note = {arXiv: 1704.08863},
	pages = {1--9},
}

@article{arakelyan_complex_2019,
	title = {{COMPLEX} {QUERY} {ANSWERING} {WITH} {NEURAL} {LINK} {PREDICTORS}},
	author = {Arakelyan, Erik and Daza, Daniel and Minervini, Pasquale and Cochez, Michael and Kingdom, United and Amsterdam, Vrije Universiteit},
	year = {2019},
	note = {arXiv: 2011.03459v2},
	pages = {1--14},
}

@article{lazaridou_pitfalls_2021,
	title = {Pitfalls of {Static} {Language} {Modelling}},
	url = {http://arxiv.org/abs/2102.01951},
	abstract = {Our world is open-ended, non-stationary and constantly evolving; thus what we talk about and how we talk about it changes over time. This inherent dynamic nature of language comes in stark contrast to the current static language modelling paradigm, which constructs training and evaluation sets from overlapping time periods. Despite recent progress, we demonstrate that state-of-the-art Transformer models perform worse in the realistic setup of predicting future utterances from beyond their training period -- a consistent pattern across three datasets from two domains. We find that, while increasing model size alone -- a key driver behind recent progress -- does not provide a solution for the temporal generalization problem, having models that continually update their knowledge with new information can indeed slow down the degradation over time. Hence, given the compilation of ever-larger language modelling training datasets, combined with the growing list of language-model-based NLP applications that require up-to-date knowledge about the world, we argue that now is the right time to rethink our static language modelling evaluation protocol, and develop adaptive language models that can remain up-to-date with respect to our ever-changing and non-stationary world.},
	author = {Lazaridou, Angeliki and Kuncoro, Adhiguna and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and d'Autume, Cyprien de Masson and Ruder, Sebastian and Yogatama, Dani and Cao, Kris and Kocisky, Tomas and Young, Susannah and Blunsom, Phil},
	year = {2021},
	note = {arXiv: 2102.01951},
}

@misc{noauthor_knowledgevaultpdf_nodate,
	title = {{KnowledgeVault}.pdf},
}

@article{scholkopf_towards_2021,
	title = {Towards {Causal} {Representation} {Learning}},
	issn = {15582256},
	url = {http://arxiv.org/abs/2102.11107},
	doi = {10.1109/JPROC.2021.3058954},
	abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
	author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
	year = {2021},
	note = {arXiv: 2102.11107},
	pages = {1--24},
}

@article{iyyer_deep_2015,
	title = {Deep unordered composition rivals syntactic methods for text classification},
	volume = {1},
	doi = {10.3115/v1/p15-1162},
	abstract = {Many existing deep learning models for natural language processing tasks focus on learning the compositionality of their inputs, which requires many expensive computations. We present a simple deep neural network that competes with and, in some cases, outperforms such models on sentiment analysis and factoid question answering tasks while taking only a fraction of the training time. While our model is syntactically-ignorant, we show significant improvements over previous bag-of-words models by deepening our network and applying a novel variant of dropout. Moreover, our model performs better than syntactic models on datasets with high syntactic variance. We show that our model makes similar errors to syntactically-Aware models, indicating that for the tasks we consider, nonlinearly transforming the input is more important than tailoring a network to incorporate word order and syntax.},
	journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
	author = {Iyyer, Mohit and Manjunatha, Varun and Boyd-Graber, Jordan and Daumé, Hal},
	year = {2015},
	note = {ISBN: 9781941643723},
	pages = {1681--1691},
}

@article{tierney_fee_2013,
	title = {Fee note},
	number = {December},
	author = {Tierney, Mazars},
	year = {2013},
	pages = {58324},
}

@article{sadinle_bayesian_2017,
	title = {Bayesian {Estimation} of {Bipartite} {Matchings} for {Record} {Linkage}},
	volume = {112},
	issn = {1537274X},
	doi = {10.1080/01621459.2016.1148612},
	abstract = {The bipartite record linkage task consists of merging two disparate datafiles containing information on two overlapping sets of entities. This is nontrivial in the absence of unique identifiers and it is important for a wide variety of applications given that it needs to be solved whenever we have to combine information from different sources. Most statistical techniques currently used for record linkage are derived from a seminal article by Fellegi and Sunter in 1969. These techniques usually assume independence in the matching statuses of record pairs to derive estimation procedures and optimal point estimators. We argue that this independence assumption is unreasonable and instead target a bipartite matching between the two datafiles as our parameter of interest. Bayesian implementations allow us to quantify uncertainty on the matching decisions and derive a variety of point estimators using different loss functions. We propose partial Bayes estimates that allow uncertain parts of the bipartite matching to be left unresolved. We evaluate our approach to record linkage using a variety of challenging scenarios and show that it outperforms the traditional methodology. We illustrate the advantages of our methods merging two datafiles on casualties from the civil war of El Salvador. Supplementary materials for this article are available online.},
	number = {518},
	journal = {Journal of the American Statistical Association},
	author = {Sadinle, Mauricio},
	year = {2017},
	note = {arXiv: 1601.06630},
	keywords = {Assignment problem, Bayes estimate, Data matching, Fellegi–Sunter decision rule, Mixture model, Reject option},
	pages = {600--612},
}

@article{fellegi_theory_1969,
	title = {A {Theory} for {Record} {Linkage}},
	volume = {64},
	issn = {1537274X},
	doi = {10.1080/01621459.1969.10501049},
	abstract = {A mathematical model is developed to provide a theoretical framework for a computer-oriented solution to the problem of recognizing those records in two files which represent identical persons, objects or events (said to be matched). A comparison is to be made between the recorded characteristics and values in two records (one from each file) and a decision made as to whether or not the members of the comparison-pair represent the same person or event, or whether there is insufficient evidence to justify either of these decisions at stipulated levels of error. These three decisions are referred to as link (A1), a non-link (A3), and a possible link (A2). The first two decisions are called positive dispositions. The two types of error are defined as the error of the decision A1 when the members of the comparison pair are in fact unmatched, and the error of the decision A3 when the members of the comparison pair are, in fact matched. The probabilities of these errors are defined as and respectively where u(γ), m(γ) are the probabilities of realizing γ (a comparison vector whose components are the coded agreements and disagreements on each characteristic) for unmatched and matched record pairs respectively. The summation is over the whole comparison space r of possible realizations. A linkage rule assigns probabilities P(A1{\textbar}γ), and P(A2{\textbar}γ), and P(A3{\textbar}γ) to each possible realization of γ ε Γ. An optimal linkage rule L (μ, λ, Γ) is defined for each value of (μ, λ) as the rule that minimizes P(A2) at those error levels. In other words, for fixed levels of error, the rule minimizes the probability of failing to make positive dispositions. A theorem describing the construction and properties of the optimal linkage rule and two corollaries to the theorem which make it a practical working tool are given. © Taylor \& Francis Group, LLC.},
	number = {328},
	journal = {Journal of the American Statistical Association},
	author = {Fellegi, Ivan P. and Sunter, Alan B.},
	year = {1969},
	pages = {1183--1210},
}

@article{steorts_bayesian_2016,
	title = {A {Bayesian} {Approach} to {Graphical} {Record} {Linkage} and {Deduplication}},
	volume = {111},
	issn = {1537274X},
	doi = {10.1080/01621459.2015.1105807},
	abstract = {We propose an unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation involves the representation of the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate transitive linkage probabilities across records (and represent this visually), and propagate the uncertainty of record linkage into later analyses. Our method makes it particularly easy to integrate record linkage with post-processing procedures such as logistic regression, capture–recapture, etc. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously record linkage approaches, despite the high-dimensional parameter space. We illustrate our method using longitudinal data from the National Long Term Care Survey and with data from the Italian Survey on Household and Wealth, where we assess the accuracy of our method and show it to be better in terms of error rates and empirical scalability than other approaches in the literature. Supplementary materials for this article are available online.},
	number = {516},
	journal = {Journal of the American Statistical Association},
	author = {Steorts, Rebecca C. and Hall, Rob and Fienberg, Stephen E.},
	year = {2016},
	note = {arXiv: 1312.4645},
	keywords = {Bayesian methods, Blocking, Clustering, Entity resolution, Hybrid Markov chain Monte Carlo, Linkage structure},
	pages = {1660--1672},
}

@article{steorts_entity_2015,
	title = {Entity resolution with empirically motivated priors},
	volume = {10},
	issn = {19316690},
	doi = {10.1214/15-BA965SI},
	abstract = {Databases often contain corrupted, degraded, and noisy data with duplicate entries across and within each database. Such problems arise in citations, medical databases, genetics, human rights databases, and a variety of other applied settings. The target of statistical inference can be viewed as an unsupervised problem of determining the edges of a bipartite graph that links the observed records to unobserved latent entities. Bayesian approaches provide attractive benefits, naturally providing uncertainty quantification via posterior probabilities. We propose a novel record linkage approach based on empirical Bayesian principles. Specifically, the empirical Bayesian-type step consists of taking the empirical distribution function of the data as the prior for the latent entities. This approach improves on the earlier HB approach not only by avoiding the prior specification problem but also by allowing both categorical and string-valued variables. Our extension to string-valued variables also involves the proposal of a new probabilistic mechanism by which observed record values for string fields can deviate from the values of their associated latent entities. Categorical fields that deviate from their corresponding true value are simply drawn from the empirical distribution function. We apply our proposed methodology to a simulated data set of German names and an Italian household survey on income and wealth, showing our method performs favorably compared to several standard methods in the literature. We also consider the robustness of our methods to changes in the hyper-parameters.},
	number = {4},
	journal = {Bayesian Analysis},
	author = {Steorts, Rebecca C.},
	year = {2015},
	note = {arXiv: 1409.0643},
	pages = {849--875},
}

@article{dorneles_approximate_2011,
	title = {Approximate data instance matching: {A} survey},
	volume = {27},
	issn = {02191377},
	doi = {10.1007/s10115-010-0285-0},
	abstract = {Approximate data matching is a central problem in several data management processes, such as data integration, data cleaning, approximate queries, similarity search and so on. An approximate matching process aims at defining whether two data represent the same real-world object. For atomic values (strings, dates, etc), similarity functions have been defined for several value domains (person names, addresses, and so on). For matching aggregated values, such as relational tuples and XML trees, approaches alternate from the definition of simple functions that combine values of similarity of record attributes to sophisticated techniques based on machine learning, for example. For complex data comparison, including structured and semistructured documents, existing approaches use both structure and data for the comparison, by either considering or not considering data semantics. This survey presents terminology and concepts that base approximated data matching, as well as discusses related work on the use of similarity functions in such a subject. © 2010 Springer-Verlag London Limited.},
	number = {1},
	journal = {Knowledge and Information Systems},
	author = {Dorneles, Carina Friedrich and Gonçalves, Rodrigo and dos Santos Mello, Ronaldo},
	year = {2011},
	note = {ISBN: 1011501002850},
	keywords = {Duplicate detection, Entity resolution, Instance data matching, Object matching, Record linkage, Record matching, Similarity function, Similarity matching},
	pages = {1--21},
}

@article{dufter_identifying_2020,
	title = {Identifying {Elements} {Essential} for {BERT}’s {Multilinguality} ¨},
	author = {Dufter, Philipp},
	year = {2020},
	note = {arXiv: 2005.00396v2},
}

@article{uteuov_combined_2018,
	title = {Combined document embedding and hierarchical topic model for social media texts analysis},
	volume = {136},
	issn = {18770509},
	url = {https://doi.org/10.1016/j.procs.2018.08.285},
	doi = {10.1016/j.procs.2018.08.285},
	abstract = {Exploring customer interests from open source information has become a significant issue. On the one hand, consumers deepen their engagement with the brands which values matter to them. On the other hand, annoying marketing calls and polls do not reflect real customers' needs and wants. This article considers topic modeling in application to social media analysis. We have received interpretable topics related to users preferences. Crawled posts texts and texts obtaining from images by an optical character recognition were used as datasets. Focusing on two approaches: probabilistic (LDA, ARTM) and neural network based (doc2vec, word2vec), we suggest the combined model deARTM. Hierarchical ARTM model allows us to obtain relations between texts in several abstraction levels which we used as vector representation. To avoid misspelling sensitivity, our model includes document embedding. In the experimental part, we show that our model can improve results of topic modeling on social media datasets.},
	journal = {Procedia Computer Science},
	author = {Uteuov, Amir and Kalyuzhnaya, Anna},
	year = {2018},
	note = {Publisher: Elsevier B.V.},
	keywords = {Additive regularization of topic models, document embedding, information retrieval, social media, topic modelling},
	pages = {293--303},
}

@article{angelov_top2vec_2020,
	title = {Top2vec: {Distributed} representations of topics},
	issn = {23318422},
	abstract = {Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present top2vec, which leverages joint document and word semantic embedding to find topic vectors. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that top2vec finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.},
	journal = {arXiv},
	author = {Angelov, Dimo},
	year = {2020},
	note = {arXiv: 2008.09470},
	pages = {1--25},
}

@article{du_plessis_analysis_2014,
	title = {Analysis of learning from positive and unlabeled data},
	volume = {1},
	issn = {10495258},
	abstract = {Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than 2√2 times the fully supervised case. These theoretical findings are also validated through experiments.},
	number = {January},
	journal = {Advances in Neural Information Processing Systems},
	author = {Du Plessis, Marthinus C. and Niu, Gang and Sugiyama, Masashi},
	year = {2014},
	pages = {703--711},
}

@article{jacovi_scalable_2019,
	title = {Scalable evaluation and improvement of document set expansion via neural positive-unlabeled learning},
	issn = {23318422},
	abstract = {We consider the situation in which a user has collected a small set of documents on a cohesive topic, and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query, and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning-i.e., learning binary classifiers from only positive and unlabeled data, where the positive data corresponds to the query documents, and the unlabeled data is the results returned by the IR engine. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting, including an unknown class prior, extremely imbalanced data and large-scale accurate evaluation of models, and we propose solutions and empirically validate them. We demonstrate the effectiveness of the method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics. We demonstrate improvements over the base IR solution and other baselines. Implementation is available at https://github.com/sayaendo/document-set-expansion-pu.},
	journal = {arXiv},
	author = {Jacovi, Alon and Niu, Gang and Goldberg, Yoav and Sugiyama, Masashi},
	year = {2019},
	note = {arXiv: 1910.13339},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling} : {The} {Two} {Cultures}},
	volume = {16},
	number = {3},
	author = {Breiman, Leo},
	year = {2001},
	pages = {199--231},
}

@article{rudin_study_nodate,
	title = {A {Study} in {Rashomon} {Curves} and {Volumes} : {A} {New} {Perspective} on {Generalization} and {Model} {Simplicity} {arXiv} : 1908 . 01755v2 [ cs . {LG} ] 27 {Mar} 2020},
	author = {Rudin, Cynthia and Parr, Ronald},
	note = {arXiv: 1908.01755v2},
	pages = {1--83},
}

@article{adlam_underspecification_nodate,
	title = {Underspecification {Presents} {Challenges} for {Credibility} in},
	author = {Adlam, Ben and Hoffman, Matthew D and Houlsby, Neil and Mclean, Cory and Veitch, Victor},
	note = {arXiv: 2011.03395v1},
}

@article{guu_realm_2019,
	title = {{REALM} : {Retrieval}-{Augmented} {Language} {Model} {Pre}-{Training}},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-wei},
	year = {2019},
	note = {arXiv: 2002.08909v1},
}

@article{allen_tucker_1980,
	title = {{TuckER}: {Tensor} {Factorization} for {Knowledge} {Graph} {Completion}},
	author = {Allen, Carl and Hospedales, Timothy M},
	year = {1980},
	note = {arXiv: 1901.09590v2},
}

@article{ramsauer_hopfield_nodate,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Gruber, Lukas and Holzleitner, Markus and Pavlovi, Milena and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	note = {arXiv: 2008.02217v1},
}

@article{suresh_hybrid_nodate,
	title = {A {Hybrid} {Model} for {Learning} {Embeddings} and {Logical} {Rules} {Simultaneously} from {Knowledge} {Graphs}},
	author = {Suresh, Susheel and Neville, Jennifer},
	note = {arXiv: 2009.10800v1},
}

@article{johanna_statistical_2011,
	title = {Statistical {Schema} {Induction}},
	author = {Johanna, V and Niepert, Mathias},
	year = {2011},
	keywords = {association rule mining, linked data, ontologies},
	pages = {124--138},
}

@article{galarraga_amie_nodate,
	title = {{AMIE} : {Association} {Rule} {Mining} under {Incomplete} {Evidence} in {Ontological} {Knowledge} {Bases}},
	author = {Galárraga, Luis and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian M},
	note = {ISBN: 9781450320351},
	keywords = {ilp, inductive logic programming, rule mining},
}

@article{wang_knowledge_2015,
	title = {Knowledge {Base} {Completion} {Using} {Embeddings} and {Rules}},
	number = {Ijcai},
	author = {Wang, Quan and Wang, Bin and Guo, Li},
	year = {2015},
	keywords = {Technical Papers — Relational Learning},
	pages = {1859--1865},
}

@article{feng_gake_2016,
	title = {{GAKE} : {Graph} {Aware} {Knowledge} {Embedding}},
	author = {Feng, Jun and Huang, Minlie and Yang, Yang and Zhu, Xiaoyan},
	year = {2016},
	pages = {641--651},
}

@article{lin_modeling_2015,
	title = {Modeling {Relation} {Paths} for {Representation} {Learning} of {Knowledge} {Bases}},
	number = {September},
	author = {Lin, Yankai and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong and Rao, Siwei and Liu, Song},
	year = {2015},
	pages = {705--714},
}

@article{li_hybrid_1900,
	title = {Hybrid reasoning in knowledge graphs : {Combing} symbolic reasoning and statistical reasoning},
	volume = {0},
	number = {0},
	author = {Li, Weizhuo and Qi, Guilin and Ji, Qiu},
	year = {1900},
	keywords = {combination, embedding, hybrid reasoning, knowledge graphs},
}

@article{tan_dgl-ke_nodate,
	title = {{DGL}-{KE} : {Training} {Knowledge} {Graph} {Embeddings} at {Scale}},
	author = {Tan, Zeyuan and Karypis, George},
	note = {arXiv: 2004.08532v1},
	keywords = {all or part of, classroom use is granted, copies are not made, distributed training, knowledge graph, large scale, or, or distributed, or hard copies of, permission to make digital, this work for personal, without fee provided that},
}

@article{fitzgerald_matching_2019,
	title = {Matching the {Blanks} : {Distributional} {Similarity} for {Relation} {Learning}},
	number = {2013},
	author = {Fitzgerald, Nicholas and Ling, Jeffrey},
	year = {2019},
	pages = {2895--2905},
}

@article{bosselut_comet_nodate,
	title = {{COMET} : {Commonsense} {Transformers} for {Automatic} {Knowledge} {Graph} {Construction}},
	author = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
	note = {arXiv: 1906.05317v2},
}

@article{mining_real-world_1998,
	title = {Real-world {Data} is {Dirty} : {Data} {Cleansing} and {The} {Merge} / {Purge} {Problem} {Real}-world {Data} is {Dirty} : {Data} {Cleansing} and {The} {Merge} / {Purge} {Problem}},
	doi = {10.1023/A},
	number = {January},
	author = {Mining, Data and Stolfo, Salvatore J},
	year = {1998},
}

@article{baxter_comparison_2001,
	title = {A {Comparison} of {Fast} {Blocking} {Methods} for {Record} {Linkage}},
	author = {Baxter, Rohan and Christen, Peter},
	year = {2001},
	keywords = {and, anu, data integration, febrl record linkage system, funded by the australian, national university, object reconciliation, record linkage, the development of the, was},
	pages = {6--8},
}

@article{kopcke_data_2009,
	title = {Data \& {Knowledge} {Engineering} {Frameworks} for entity matching : {A} comparison},
	issn = {0169-023X},
	url = {http://dx.doi.org/10.1016/j.datak.2009.10.003},
	doi = {10.1016/j.datak.2009.10.003},
	journal = {DATE \& KNOWLEDEGE ENGINEERING},
	author = {Köpcke, Hanna and Rahm, Erhard},
	year = {2009},
	note = {Publisher: Elsevier B.V.},
}

@article{bohm_linda_nodate,
	title = {{LINDA} : {Distributed} {Web}-of-{Data}-{Scale} {Entity} {Matching}},
	number = {1},
	author = {Böhm, Christoph and Melo, Gerard De and Naumann, Felix and Weikum, Gerhard},
	note = {ISBN: 9781450311564},
}

@article{nickel_review_2015,
	title = {A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}},
	author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
	year = {2015},
	note = {arXiv: 1503.00759v3},
	pages = {1--23},
}

@article{ojha_kgeval_2017,
	title = {{KGEval} : {Accuracy} {Estimation} of {Automatically} {Constructed} {Knowledge} {Graphs}},
	author = {Ojha, Prakhar},
	year = {2017},
	pages = {1741--1750},
}

@article{kontokostas_test-driven_nodate,
	title = {Test-driven {Evaluation} of {Linked} {Data} {Quality} {Categories} and {Subject} {Descriptors}},
	author = {Kontokostas, Dimitris and Westphal, Patrick and Cornelissen, Roland and Bibliotheek, Stichting and Hellmann, Sebastian and Lehmann, Jens},
	note = {ISBN: 9781450327442},
	keywords = {comprises an unprecedented vol-, however, linked open data, lod, metamatter, nl, on the web, roland, these datasets, ume of structured data},
	pages = {747--757},
}

@article{gao_efficient_2019,
	title = {Efficient {Knowledge} {Graph} {Accuracy} {Evaluation} ( {Technical} {Report} {Version} ) ∗},
	volume = {12},
	number = {xxx},
	author = {Gao, Junyang},
	year = {2019},
}

@article{ringler_one_2012,
	title = {One {Knowledge} {Graph} to {Rule} them {All} ? {Analyzing} the {Differences} between},
	author = {Ringler, Daniel and Paulheim, Heiko},
	year = {2012},
}

@article{paulheim_towards_2012,
	title = {Towards {Profiling} {Knowledge} {Graphs}},
	author = {Paulheim, Heiko},
	year = {2012},
}

@article{liu_measuring_2017,
	title = {Measuring {Accuracy} of {Triples} in {Knowledge} {Graphs} {Conference}},
	doi = {10.1007/978-3-319-59888-8},
	author = {Liu, Shuangyan and d’Aquin, Mathieu and Motta, Enrico},
	year = {2017},
	note = {ISBN: 9783319598888},
}

@article{kertkeidkachorn_t2kg_2012,
	title = {{T2KG} : {An} {End}-to-{End} {System} for {Creating} {Knowledge} {Graph} from {Unstructured} {Text}},
	author = {Kertkeidkachorn, Natthawut and Ichise, Ryutaro},
	year = {2012},
	keywords = {Knowledge-Based Techniques for Problem Solving and},
	pages = {743--749},
}

@article{zaveri_quality_2012,
	title = {Quality {Assessment} for {Linked} {Open} {Data} : {A} {Survey}},
	volume = {1},
	author = {Zaveri, Amrapali and Rula, Anisa and Maurino, Andrea and Pietrobon, Ricardo and Lehmann, Jens},
	year = {2012},
	keywords = {assessment, data quality, linked open data, survey},
	pages = {1--5},
}

@article{paulheim_improving_nodate,
	title = {Improving the {Quality} of {Linked} {Data} {Using} {Statistical} {Distributions}},
	author = {Paulheim, Heiko and Bizer, Christian},
	keywords = {data quality, error detection, noisy data, semi-structured data, type completion},
}

@article{cimiano_knowledge_2016,
	title = {Knowledge {Graph} {Refinement} : {A} {Survey} of {Approaches} and {Evaluation} {Methods}},
	volume = {0},
	author = {Cimiano, Philipp and Bielefeld, Universität},
	year = {2016},
	keywords = {completion, correction, error detection, evaluation, knowledge graphs, refinement},
}

@article{history_tac_2019,
	title = {{TAC} / {TRECVID} {Streaming} {Multimedia} {KBP} for {AIDA}},
	volume = {2019},
	author = {History, Revision},
	year = {2019},
	pages = {1--46},
}

@article{discovery_tac_2019,
	title = {{TAC} {KBP2019} {Ultra}-{Fine}-{Grained} {Name} {Tagging} for {Entity} {Types}},
	author = {Discovery, Tac-kbp Entity and Aida, Darpa and Personnel, Military},
	year = {2019},
	pages = {1--8},
}

@article{dong_knowledge_2014,
	title = {Knowledge {Vault} : {A} {Web}-{Scale} {Approach} to {Probabilistic} {Knowledge} {Fusion}},
	author = {Dong, Xin Luna and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Lao, Ni and Murphy, Kevin and Strohmann, Thomas and Sun, Shaohua and Zhang, Wei},
	year = {2014},
	keywords = {els, information extraction, knowledge bases, machine learning, probabilistic mod-},
}

@article{flaxman_estimating_2020,
	title = {Estimating the number of infections and the impact of non- pharmaceutical interventions on {COVID}-19 in 11 {European} countries},
	number = {March},
	author = {Flaxman, Seth and Mishra, Swapnil and Gandy, Axel and Unwin, H Juliette T and Coupland, Helen and Mellan, Thomas A and Berah, Tresnia and Eaton, Jeffrey W and Guzman, Pablo N P and Schmit, Nora and Cilloni, Lucia and Ainslie, Kylie E C and Blake, Isobel and Boonyasiri, Adhiratha and Boyd, Olivia and Cattarino, Lorenzo and Ciavarella, Constanze and Cooper, Laura and Cucunubá, Zulma and Cuomo-dannenburg, Gina and Dighe, Amy and Djaafara, Bimandra and Dorigatti, Ilaria and Elsland, Sabine Van},
	year = {2020},
	pages = {1--35},
}

@article{ferguson_impact_2020,
	title = {Impact of non-pharmaceutical interventions ( {NPIs} ) to reduce {COVID}- 19 mortality and healthcare demand},
	number = {March},
	author = {Ferguson, Neil M and Laydon, Daniel and Nedjati-gilani, Gemma and Imai, Natsuko and Ainslie, Kylie and Baguelin, Marc and Bhatia, Sangeeta and Boonyasiri, Adhiratha and Cucunubá, Zulma and Cuomo-dannenburg, Gina and Dighe, Amy and Fu, Han and Gaythorpe, Katy and Green, Will and Hamlet, Arran and Hinsley, Wes and Okell, Lucy C and Van, Sabine and Thompson, Hayley and Verity, Robert and Volz, Erik and Wang, Haowei and Wang, Yuanrong and Walker, Patrick G T and Walters, Caroline and Winskill, Peter and Whittaker, Charles and Donnelly, Christl A and Riley, Steven and Ghani, Azra C},
	year = {2020},
}

@article{kilian_founding_nodate,
	title = {Founding {Cryptography} on {Oblivious} {Transfer} 1 {Introduction}},
	author = {Kilian, Joe},
}

@article{garg_candidate_2013,
	title = {Candidate {Indistinguishability} {Obfuscation} and {Functional} {Encryption} for all circuits},
	author = {Garg, Sanjam and Raykova, Mariana and Gentry, Craig and Waters, Brent},
	year = {2013},
}

@article{frankle_lottery_2019,
	title = {The lottery ticket hypothesis: {Finding} sparse, trainable neural networks},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	journal = {7th International Conference on Learning Representations, ICLR 2019},
	author = {Frankle, Jonathan and Carbin, Michael},
	year = {2019},
	note = {arXiv: 1803.03635},
	pages = {1--42},
}

@article{yang_differentiable_2017,
	title = {Differentiable learning of logical rules for knowledge base reasoning},
	volume = {2017-Decem},
	issn = {10495258},
	abstract = {We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.},
	number = {Nips},
	journal = {Advances in Neural Information Processing Systems},
	author = {Yang, Fan and Yang, Zhilin and Cohen, William W.},
	year = {2017},
	note = {arXiv: 1702.08367},
	pages = {2320--2329},
}

@misc{noauthor_knowledge_nodate,
	title = {Knowledge {Vault} : {A} {Web}-{Scale} {Approach} to {Probabilistic} {Knowledge} {Fusion}},
}

@article{nakashole_patty_2012,
	title = {{PATTY}: {A} taxonomy of relational patterns with semantic types},
	abstract = {This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semanti-cally typed and organized into a subsumption taxonomy. The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7\%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75\%. The PATTY resource is freely available for interactive access and download. © 2012 Association for Computational Linguistics.},
	number = {July},
	journal = {EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference},
	author = {Nakashole, Ndapandula and Weikum, Gerhard and Suchanek, Fabian},
	year = {2012},
	note = {ISBN: 9781937284435},
	pages = {1135--1145},
}

@article{devlin_bert_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {Mlm},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	note = {arXiv: 1810.04805},
}

@article{lample_cross-lingual_2019,
	title = {Cross-lingual {Language} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/1901.07291},
	abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
	author = {Lample, Guillaume and Conneau, Alexis},
	year = {2019},
	note = {arXiv: 1901.07291},
}

@article{adhikari_docbert_2019,
	title = {{DocBERT}: {BERT} for {Document} {Classification}},
	url = {http://arxiv.org/abs/1904.08398},
	abstract = {We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.},
	author = {Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Lin, Jimmy},
	year = {2019},
	note = {arXiv: 1904.08398},
}

@article{platanios_competence-based_2019,
	title = {Competence-based {Curriculum} {Learning} for {Neural} {Machine} {Translation}},
	doi = {10.18653/v1/n19-1119},
	abstract = {Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70\% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.},
	author = {Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Poczos, Barnabas and Mitchell, Tom},
	year = {2019},
	note = {arXiv: 1903.09848},
	pages = {1162--1172},
}

@article{karthikeyan_cross-lingual_2019,
	title = {Cross-lingual {Ability} of {Multilingual} {BERT}: {An} {Empirical} {Study}},
	abstract = {Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT)-surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and of the learning objectives. The experimental study is done in the context of three typologically different languages-Spanish, Hindi, and Russian-and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an important part of it.},
	journal = {ICLR 2020 Submission},
	author = {Karthikeyan, K and Wang, Zihan and Mayhew, Stephen and Roth, Dan},
	year = {2019},
	note = {arXiv: 1912.07840v1},
	pages = {1--13},
}

@article{al-khatib_simple_nodate,
	title = {A {Simple} and {Effective} {Approach} for {Fine} {Tuning} {Pre}-trained {Word} {Embeddings} for {Improved} {Text} {Classification}},
	author = {Al-khatib, Amr and El-beltagy, Samhaa R},
}

@article{liu_roberta:_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	number = {1},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	year = {2019},
	note = {arXiv: 1907.11692},
}

@article{zhang_sentence_2017,
	title = {Sentence simplification with deep reinforcement learning},
	doi = {10.18653/v1/d17-1062},
	abstract = {Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.1},
	journal = {EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
	author = {Zhang, Xingxing and Lapata, Mirella},
	year = {2017},
	note = {arXiv: 1703.10931
ISBN: 9781945626838},
	pages = {584--594},
}

@article{warstadt_blimp:_2019,
	title = {{BLiMP}: {A} {Benchmark} of {Linguistic} {Minimal} {Pairs} for {English}},
	url = {http://arxiv.org/abs/1912.00582},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP), a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars, and aggregate human agreement with the labels is 96.4\%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs. We find that state-of-the-art models identify morphological contrasts reliably, but they struggle with semantic restrictions on the distribution of quantifiers and negative polarity items and subtle syntactic phenomena such as extraction islands.},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	year = {2019},
	note = {arXiv: 1912.00582},
}

@article{karami_flatm_nodate,
	title = {{FLATM} : {A} {Fuzzy} {Logic} {Approach} {Topic} {Model} for {Medical} {Documents}},
	author = {Karami, Amir and Gangopadhyay, Aryya and Zhou, Bin},
	note = {arXiv: 1911.10953v1},
}

@article{takahashi_hardness_2013,
	title = {Hardness of {Classically} {Simulating} {Quantum} {Circuits} with {Unbounded} {Toffoli} and {Fan}-{Out} {Gates}},
	author = {Takahashi, Yasuhiro and Yamazaki, Takeshi and Tanaka, Kazuyuki},
	year = {2013},
	pages = {801--812},
}

@article{massar_estimating_2011,
	title = {Estimating preselected and postselected ensembles},
	volume = {84},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.84.052106},
	doi = {10.1103/PhysRevA.84.052106},
	number = {5},
	urldate = {2014-12-05},
	journal = {Physical Review A},
	author = {Massar, Serge and Popescu, Sandu},
	month = nov,
	year = {2011},
	pages = {052106},
}

@article{mahadev_rational_2014,
	title = {Rational approximations and quantum algorithms with postselection},
	url = {http://arxiv.org/abs/1401.0912v2},
	urldate = {2014-12-05},
	author = {Mahadev, Urmila and de Wolf, Ronald},
	month = jan,
	year = {2014},
	note = {arXiv: 1401.0912},
	pages = {1--12},
}

@article{takahashi_commuting_2014,
	title = {Commuting {Quantum} {Circuits} with {Few} {Outputs} are {Unlikely} to be {Classically} {Simulatable}},
	url = {http://arxiv.org/abs/1409.6792},
	urldate = {2014-11-28},
	journal = {arXiv preprint arXiv: …},
	author = {Takahashi, Yasuhiro and Tani, Seiichiro and Yamazaki, T and Tanaka, K},
	year = {2014},
	note = {arXiv: 1409.6792v1},
	pages = {1--18},
}

@article{zheng_fault-tolerant_2014,
	title = {Fault-tolerant {Holonomic} {Quantum} {Computation} in {Surface} {Codes}},
	url = {http://arxiv.org/abs/1411.4248},
	urldate = {2014-11-28},
	journal = {arXiv preprint arXiv:1411.4248},
	author = {Zheng, YC and Brun, TA},
	year = {2014},
	note = {arXiv: 1411.4248v3},
}

@article{kielpinski_entanglement_2001,
	title = {Entanglement and {Decoherence} in a {Trapped}-{Ion}},
	author = {Kielpinski, David},
	year = {2001},
}

@article{bravyi_quantum_2008,
	title = {Quantum {Simulation} of {Many}-{Body} {Hamiltonians} {Using} {Perturbation} {Theory} with {Bounded}-{Strength} {Interactions}},
	volume = {101},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.101.070503},
	doi = {10.1103/PhysRevLett.101.070503},
	number = {7},
	urldate = {2014-11-01},
	journal = {Physical Review Letters},
	author = {Bravyi, Sergey and DiVincenzo, David and Loss, Daniel and Terhal, Barbara},
	month = aug,
	year = {2008},
	pages = {070503},
}

@article{hauser_b553_2012,
	title = {B553 {Lecture} 4 : {Gradient} {Descent}},
	number = {1},
	author = {Hauser, Kris},
	year = {2012},
	pages = {1--6},
}

@article{borhani_two-spin_2010,
	title = {Two-spin relaxation of {P} dimers in silicon},
	volume = {82},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.82.241302},
	doi = {10.1103/PhysRevB.82.241302},
	number = {24},
	urldate = {2014-10-30},
	journal = {Physical Review B},
	author = {Borhani, Massoud and Hu, Xuedong},
	month = dec,
	year = {2010},
	pages = {241302},
}

@article{hill_global_2005,
	title = {Global control and fast solid-state donor electron spin quantum computing},
	volume = {72},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.72.045350},
	doi = {10.1103/PhysRevB.72.045350},
	number = {4},
	urldate = {2014-10-30},
	journal = {Physical Review B},
	author = {Hill, C. and Hollenberg, L. and Fowler, a. and Wellard, C. and Greentree, a. and Goan, H.-S.},
	month = jul,
	year = {2005},
	pages = {045350},
}

@article{cavin_future_2005,
	title = {Future {Devices} for {Information} {Processing} {Abstract} : {Barrier} control},
	author = {Cavin, Ralph K and Zhirnov, Victor V},
	year = {2005},
	note = {ISBN: 0780392051},
	pages = {7--12},
}

@article{sasanian_technology_2012,
	title = {Technology mapping and optimization for reversible and quantum circuits},
	url = {https://dspace.library.uvic.ca/handle/1828/4324},
	urldate = {2014-10-27},
	author = {Sasanian, Z},
	year = {2012},
}

@article{frank_introduction_2005,
	title = {Introduction to reversible computing: motivation, progress, and challenges},
	url = {http://dl.acm.org/citation.cfm?id=1062324},
	urldate = {2014-10-27},
	journal = {Proceedings of the 2nd Conference on Computing …},
	author = {Frank, MP},
	year = {2005},
	note = {ISBN: 1595930191},
	keywords = {computer, computing, digital logic technologies, field-effect devices, high-performance computing, limits of, low-power computing, power management, reversible computing, reversible logic, unconventional computing, vlsi},
	pages = {385--390},
}

@article{havinga_design_2000,
	title = {Design techniques for low-power systems},
	volume = {46},
	issn = {13837621},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1383762198000575},
	doi = {10.1016/S1383-7621(98)00057-5},
	number = {1},
	journal = {Journal of Systems Architecture},
	author = {Havinga, Paul J.M and Smit, Gerard J.M},
	month = jan,
	year = {2000},
	keywords = {low power, mobile computing, system architecture, wireless communication},
	pages = {1--21},
}

@article{schrom_ultra-low-power_1991,
	title = {Ultra-{Low}-{Power} {CMOS} {Technologies}},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Ultra-+Low-Power+CMOS+Technologies#0},
	number = {1},
	urldate = {2014-10-27},
	author = {Schrom, G. and Selberherr, S.},
	year = {1991},
}

@article{frank_power-_2002,
	title = {Power- constrained {CMOS} scaling limits},
	volume = {46},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5388987},
	number = {2},
	urldate = {2014-10-27},
	journal = {IBM Journal of Research and Development},
	author = {Frank, DJ},
	year = {2002},
	pages = {235--244},
}

@article{zhirnov_limits_2003,
	title = {Limits to {Binary} {Logic} {Switch} {Scaling} — {A} {Gedanken} {Model}},
	volume = {91},
	number = {11},
	author = {Zhirnov, Victor V and Cavin, Ralph K and Hutchby, James A and Member, Senior and Bourianoff, George I},
	year = {2003},
	keywords = {closely packed devices, device scaling limits, digital, heat removal, integrated circuits, nanotechnology, power consump-},
}

@article{noauthor_international_2013,
	title = {{INTERNATIONAL} {TECHNOLOGY} {ROADMAP} {FOR} {SEMICONDUCTORS} (2013). {EMERGING} {RESEARCH} {DEVICES}},
	urldate = {2014-10-27},
	year = {2013},
}

@article{platzman_quantum_1999,
	title = {Quantum {Computing} with {Electrons} {Floating} on {Liquid} {Helium}},
	volume = {284},
	issn = {00368075},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.284.5422.1967},
	doi = {10.1126/science.284.5422.1967},
	number = {5422},
	urldate = {2014-10-22},
	journal = {Science},
	author = {Platzman, P. M.},
	month = jun,
	year = {1999},
	pages = {1967--1969},
}

@article{piacente_generic_2004,
	title = {Generic properties of a quasi-one-dimensional classical {Wigner} crystal},
	volume = {69},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.69.045324},
	doi = {10.1103/PhysRevB.69.045324},
	number = {4},
	urldate = {2014-10-18},
	journal = {Physical Review B},
	author = {Piacente, G. and Schweigert, I. and Betouras, J. and Peeters, F.},
	month = jan,
	year = {2004},
	pages = {045324},
}

@article{bravyi_complexity_2014,
	title = {On complexity of the quantum {Ising} model},
	url = {http://arxiv.org/abs/1410.0703v1},
	urldate = {2014-10-06},
	author = {Bravyi, Sergey and Hastings, Matthew},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.0703},
	pages = {1--48},
}

@article{roger_magnetism_1983,
	title = {Magnetism in solid {3He}},
	url = {http://journals.aps.org/rmp/abstract/10.1103/RevModPhys.55.1},
	urldate = {2014-08-15},
	journal = {Reviews of Modern Physics},
	author = {Roger, M and Hetherington, JH and Delrieu, JM},
	year = {1983},
}

@article{bravyi_complexity_2008,
	title = {The {Complexity} of {Stoquastic} {Local} {Hamiltonian} {Problems}},
	author = {Bravyi, Sergey and Divincenzo, David P and Oliveira, Roberto and Terhal, Barbara M},
	year = {2008},
	note = {arXiv: quant-ph/0606140v4},
	pages = {1--21},
}

@article{delgado_storage_2012,
	title = {Storage of {Classical} {Information} in {Quantum} {Spins}},
	volume = {108},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.108.196602},
	doi = {10.1103/PhysRevLett.108.196602},
	number = {19},
	urldate = {2014-07-14},
	journal = {Physical Review Letters},
	author = {Delgado, F. and Fernández-Rossier, J.},
	month = may,
	year = {2012},
	pages = {196602},
}

@article{Search,
	title = {Exchange frequencies in two-dimensional solids},
	volume = {9099},
	url = {http://iopscience.iop.org/0953-8984/14/40/304},
	urldate = {2014-06-25},
	journal = {Journal of Physics: Condensed Matter},
	author = {Bernu, B and Ceperley, DM},
	year = {2002},
}

@article{Matveev2004,
	title = {Conductance of a quantum wire at low electron density},
	url = {http://prb.aps.org/abstract/PRB/v70/i24/e245319},
	urldate = {2014-03-11},
	journal = {Physical Review B},
	author = {Matveev, KA},
	year = {2004},
	note = {arXiv: cond-mat/0405542v1},
}

@article{Scarola2005,
	title = {Exchange gate in solid-state spin-quantum computation: {The} applicability of the {Heisenberg} model},
	volume = {71},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.71.032340},
	doi = {10.1103/PhysRevA.71.032340},
	number = {3},
	urldate = {2014-03-11},
	journal = {Physical Review A},
	author = {Scarola, V. and Das Sarma, S.},
	month = mar,
	year = {2005},
	pages = {032340},
}

@article{Pedersen2010,
	title = {A path integral study of the role of correlation in exchange coupling of spins in double quantum dots and optical lattices.},
	volume = {22},
	issn = {1361-648X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21389524},
	doi = {10.1088/0953-8984/22/14/145301},
	abstract = {We explore exchange coupling of a pair of spins in a double dot and in an optical lattice, using the frequency of exchanges in a bosonic path integral, evaluated using Monte Carlo simulation. The algorithm gives insights into the role of correlation through visualization of two-particle probability densities, instantons, and the correlation hole. We map the problem to the Hubbard model and see that exchange and correlation renormalize the model parameters, dramatically reducing the effective on-site repulsion at larger separations.},
	number = {14},
	urldate = {2014-03-11},
	journal = {Journal of physics. Condensed matter : an Institute of Physics journal},
	author = {Pedersen, Jesper Goor and Zhang, Lei and Gilbert, M J and Shumway, J},
	month = apr,
	year = {2010},
	pmid = {21389524},
	pages = {145301},
}

@article{Voelker2001,
	title = {Multiparticle ring exchange in the {Wigner} glass and its possible relevance to strongly interacting two-dimensional electron systems in the presence of disorder},
	volume = {64},
	issn = {0163-1829},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.64.235125},
	doi = {10.1103/PhysRevB.64.235125},
	number = {23},
	urldate = {2014-03-06},
	journal = {Physical Review B},
	author = {Voelker, Klaus and Chakravarty, Sudip},
	month = nov,
	year = {2001},
	pages = {235125},
}

@article{hen_quantum_2014,
	title = {Quantum {Adiabatic} {Circuits}},
	author = {Hen, Itay},
	year = {2014},
	note = {arXiv: 1401.5172v1},
	keywords = {adiabatic quantum computing, quantum adiabatic algorithm},
	pages = {1--5},
}

@article{cubitt_complexity_2013,
	title = {Complexity classification of local {Hamiltonian} problems},
	url = {http://arxiv.org/abs/1311.3161},
	urldate = {2014-01-29},
	journal = {arXiv preprint arXiv:1311.3161},
	author = {Cubitt, Toby and Montanaro, Ashley},
	year = {2013},
	note = {arXiv: 1311.3161v1},
	pages = {1--43},
}

@article{iblisdir_low_2012,
	title = {Low depth quantum circuits for {Ising} models},
	author = {Iblisdir, S and Cirio, M and Boada, O and Brennen, G K},
	year = {2012},
	note = {arXiv: 1208.3918v3},
	pages = {1--28},
}

@article{bravyi_complexity_nodate,
	title = {Complexity of stoquastic frustration-free hamiltonians},
	author = {Bravyi, Sergey and Terhal, Barbara},
	note = {arXiv: 0806.1746v1},
	keywords = {adiabatic quantum computing, non-negative matrices, randomized algorithms},
	pages = {1--22},
}

@article{osborne_hamiltonian_2011,
	title = {Hamiltonian complexity},
	author = {Osborne, Tobias J},
	year = {2011},
	note = {arXiv: 1106.5875v1},
	pages = {1--14},
}

@article{aaronson_quantum_2004,
	title = {Quantum {Computing}, {Postselection}, and {Probabilistic} {Polynomial}-{Time}},
	url = {http://arxiv.org/abs/quant-ph/0412187v1},
	urldate = {2014-12-05},
	author = {Aaronson, Scott},
	month = dec,
	year = {2004},
	note = {arXiv: quant-ph/0412187},
	keywords = {computational complexity, postselection, quantum computing},
	pages = {1--8},
}

@article{viola_complexity_2012,
	title = {The complexity of distributions},
	volume = {41},
	url = {http://epubs.siam.org/doi/abs/10.1137/100814998},
	number = {1},
	urldate = {2014-11-28},
	journal = {SIAM Journal on Computing},
	author = {Viola, Emanuele},
	year = {2012},
	keywords = {10, 100814998, 1137, 68p05, 68q17, 97k99, ams subject classifications, decision tree, distribution, doi, local, lower bound, pseudorandom generator, sampling, succinct data structure},
	pages = {191--218},
}

@article{kaniewski_query_nodate,
	title = {Query complexity in expectation},
	author = {Kaniewski, Jedrzej and Lee, Troy and Wolf, Ronald De},
	note = {arXiv: 1411.7280v1},
}

@article{schwarz_simulating_2013,
	title = {Simulating {Quantum} {Circuits} with {Sparse} {Output} {Distributions}},
	url = {http://arxiv.org/abs/1310.6749},
	urldate = {2014-11-28},
	journal = {arXiv preprint arXiv:1310.6749},
	author = {Schwarz, Martin and Nest, MV},
	year = {2013},
	note = {arXiv: 1310.6749v1},
	pages = {1--21},
}

@article{roadmap_solid_2004,
	title = {Solid {State} {Approaches} to {Quantum} {Information} {Processing} {Quantum} {Computing} {A} {Quantum} {Information} {Science} and},
	author = {Roadmap, Technology},
	year = {2004},
}

@article{wineland_ion_2004,
	title = {Ion {Trap} {Approaches} to {Quantum} {Information} {Processing} {Quantum} {Computing} {A} {Quantum} {Information} {Science} and {Compiled} by :},
	author = {Wineland, David},
	year = {2004},
}

@article{marzoli_experimental_2009,
	title = {Experimental and theoretical challenges for the trapped electron quantum computer},
	volume = {42},
	issn = {0953-4075},
	url = {http://stacks.iop.org/0953-4075/42/i=15/a=154010?key=crossref.d0965d44b03322049fe1e9eb71d9717e},
	doi = {10.1088/0953-4075/42/15/154010},
	number = {15},
	urldate = {2014-11-03},
	journal = {Journal of Physics B: Atomic, Molecular and Optical Physics},
	author = {Marzoli, I and Tombesi, P and Ciaramicoli, G and Werth, G and Bushev, P and Stahl, S and Schmidt-Kaler, F and Hellwig, M and Henkel, C and Marx, G and Jex, I and Stachowska, E and Szawiola, G and Walaszyk, a},
	month = aug,
	year = {2009},
	pages = {154010},
}

@article{clark_progress_2003,
	title = {Progress in silicon-based quantum computing.},
	volume = {361},
	issn = {1364-503X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12869321},
	doi = {10.1098/rsta.2003.1221},
	abstract = {We review progress at the Australian Centre for Quantum Computer Technology towards the fabrication and demonstration of spin qubits and charge qubits based on phosphorus donor atoms embedded in intrinsic silicon. Fabrication is being pursued via two complementary pathways: a 'top-down' approach for near-term production of few-qubit demonstration devices and a 'bottom-up' approach for large-scale qubit arrays with sub-nanometre precision. The 'top-down' approach employs a low-energy (keV) ion beam to implant the phosphorus atoms. Single-atom control during implantation is achieved by monitoring on-chip detector electrodes, integrated within the device structure. In contrast, the 'bottom-up' approach uses scanning tunnelling microscope lithography and epitaxial silicon overgrowth to construct devices at an atomic scale. In both cases, surface electrodes control the qubit using voltage pulses, and dual single-electron transistors operating near the quantum limit provide fast read-out with spurious-signal rejection.},
	number = {1808},
	urldate = {2014-10-30},
	journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
	author = {Clark, R G and Brenner, R and Buehler, T M and Chan, V and Curson, N J and Dzurak, a S and Gauja, E and Goan, H S and Greentree, a D and Hallam, T and Hamilton, a R and Hollenberg, L C L and Jamieson, D N and McCallum, J C and Milburn, G J and O'Brien, J L and Oberbeck, L and Pakes, C I and Prawer, S D and Reilly, D J and Ruess, F J and Schofield, S R and Simmons, M Y and Stanley, F E and Starrett, R P and Wellard, C and Yang, C},
	month = jul,
	year = {2003},
	pmid = {12869321},
	pages = {1451--71},
}

@article{hill_robust_2007,
	title = {Robust {Controlled}-{NOT} {Gates} from {Almost} {Any} {Interaction}},
	volume = {98},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.98.180501},
	doi = {10.1103/PhysRevLett.98.180501},
	number = {18},
	urldate = {2014-10-30},
	journal = {Physical Review Letters},
	author = {Hill, Charles},
	month = may,
	year = {2007},
	pages = {180501},
}

@article{fei_mediated_2012,
	title = {Mediated gates between spin qubits},
	volume = {86},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.86.062328},
	doi = {10.1103/PhysRevA.86.062328},
	number = {6},
	urldate = {2014-10-19},
	journal = {Physical Review A},
	author = {Fei, Jianjia and Zhou, Dong and Shim, Yun-Pil and Oh, Sangchul and Hu, Xuedong and Friesen, Mark},
	month = dec,
	year = {2012},
	pages = {062328},
}

@article{kalra_robust_2014,
	title = {Robust {Two}-{Qubit} {Gates} for {Donors} in {Silicon} {Controlled} by {Hyperfine} {Interactions}},
	volume = {4},
	issn = {2160-3308},
	url = {http://link.aps.org/doi/10.1103/PhysRevX.4.021044},
	doi = {10.1103/PhysRevX.4.021044},
	number = {2},
	urldate = {2014-09-03},
	journal = {Physical Review X},
	author = {Kalra, Rachpon and Laucht, Arne and Hill, Charles D. and Morello, Andrea},
	month = jun,
	year = {2014},
	keywords = {nanophysics, quantum physics},
	pages = {021044},
}

@article{caroe_design_2012,
	title = {Design of {Reversible} {Computing} {Systems}},
	urldate = {2014-10-27},
	journal = {diku.dk},
	author = {Carøe, MK},
	year = {2012},
}

@article{likharev_classical_1982,
	title = {Classical and quantum limitations on energy consumption in computation},
	volume = {21},
	issn = {0020-7748},
	url = {http://link.springer.com/10.1007/BF01857733},
	doi = {10.1007/BF01857733},
	number = {3-4},
	journal = {International Journal of Theoretical Physics},
	author = {Likharev, K. K.},
	month = apr,
	year = {1982},
	pages = {311--326},
}

@article{ceperley_calculation_1987,
	title = {Calculation of {Exchange} {Frequencies} in bcc {3He} with the {Path}-{Integral} {Monte} {Carlo} {Method}},
	volume = {58},
	url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.58.1648},
	number = {16},
	urldate = {2014-08-15},
	journal = {Physical review letters},
	author = {Ceperley, DM and Jacucci, G},
	year = {1987},
}

@article{Hirashima2001,
	title = {Multiple-spin exchange in a two-dimensional {Wigner} crystal in a magnetic field},
	volume = {63},
	issn = {0163-1829},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.63.125340},
	doi = {10.1103/PhysRevB.63.125340},
	number = {12},
	urldate = {2014-05-02},
	journal = {Physical Review B},
	author = {Hirashima, D. and Kubo, Katsunori},
	month = mar,
	year = {2001},
	pages = {125340},
}

@article{Hausler1995,
	title = {Correlations in quantum dots},
	volume = {560},
	url = {http://link.springer.com/article/10.1007/BF02769980},
	urldate = {2014-03-11},
	journal = {Zeitschrift für Physik B Condensed Matter},
	author = {Häusler, W},
	year = {1995},
	pages = {551--560},
}

@article{Ashizawa2000,
	title = {{WKB} calculation of multiple spin exchange in monolayer solid {3He}},
	volume = {62},
	url = {http://prb.aps.org/abstract/PRB/v62/i14/p9413_1},
	number = {14},
	urldate = {2014-03-11},
	journal = {Physical Review B},
	author = {Ashizawa, Hisayuki and Hirashima, DS},
	year = {2000},
	pages = {9413--9417},
}

@article{Klironomos2007,
	title = {Spin coupling in zigzag {Wigner} crystals},
	volume = {76},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.76.075302},
	doi = {10.1103/PhysRevB.76.075302},
	number = {7},
	urldate = {2014-03-11},
	journal = {Physical Review B},
	author = {Klironomos, A. and Meyer, J. and Hikihara, T. and Matveev, K.},
	month = aug,
	year = {2007},
	pages = {075302},
}

@article{Candido2011,
	title = {Effect of long cyclic exchanges on the magnetic properties of bcc {\textasciicircum}\{3\}{He}},
	volume = {84},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.84.064515},
	doi = {10.1103/PhysRevB.84.064515},
	number = {6},
	urldate = {2014-03-11},
	journal = {Physical Review B},
	author = {Cândido, Ladir and Hai, G.-Q. and Ceperley, D. M.},
	month = aug,
	year = {2011},
	pages = {064515},
}

@article{Roger1984,
	title = {Multiple exchange in {He} and in the {Wigner} solid},
	volume = {30},
	url = {http://prb.aps.org/abstract/PRB/v30/i11/p6432_1},
	number = {11},
	urldate = {2014-03-07},
	journal = {Physical Review B},
	author = {Roger, M},
	year = {1984},
}

@article{Klironomos2006,
	title = {Spontaneous spin polarization in quantum wires},
	volume = {74},
	issn = {0295-5075},
	url = {http://stacks.iop.org/0295-5075/74/i=4/a=679?key=crossref.7cf9681b714596d7c14ff3a222999dbc},
	doi = {10.1209/epl/i2006-10024-x},
	number = {4},
	urldate = {2014-03-06},
	journal = {Europhysics Letters (EPL)},
	author = {Klironomos, a. D and Meyer, J. S and Matveev, K. a},
	month = may,
	year = {2006},
	pages = {679--685},
}

@article{smith_three_2006,
	title = {Three counterexamples refuting {Kieu}’s plan for “quantum adiabatic hypercomputation”; and some uncomputable quantum mechanical tasks},
	volume = {178},
	issn = {00963003},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0096300305008507},
	doi = {10.1016/j.amc.2005.09.078},
	number = {1},
	urldate = {2014-02-07},
	journal = {Applied Mathematics and Computation},
	author = {Smith, Warren D.},
	month = jul,
	year = {2006},
	pages = {184--193},
}

@article{oi_unitary_2014,
	title = {Unitary {Holonomies} by {Degenerate} {Projections}},
	author = {Oi, Daniel K L},
	year = {2014},
	note = {arXiv: 1402.1104v1},
	pages = {1--4},
}

@article{zeuch_analytic_nodate,
	title = {Analytic {Pulse} {Sequence} {Construction} for {Exchange}-{Only} {Quantum} {Computation}},
	volume = {1},
	author = {Zeuch, Daniel and Cipri, R and Bonesteel, N E},
	note = {arXiv: 1402.0551v1},
	pages = {1--14},
}

@article{derzhko_jordan-wigner_2001,
	title = {Jordan-{Wigner} fermionization for spin-1/2 systems in two dimensions: {A} brief review},
	volume = {0},
	url = {http://arxiv.org/abs/cond-mat/0101188},
	number = {1},
	urldate = {2014-02-12},
	journal = {arXiv preprint cond-mat/0101188},
	author = {Derzhko, Oleg},
	year = {2001},
	note = {arXiv: cond-mat/0101188v1},
	keywords = {2d heisenberg model, 2d xy model, jordan, wigner fermionization},
}

@article{howard_contextuality_2014,
	title = {Contextuality supplies the magic for quantum computation},
	author = {Howard, Mark and Wallman, Joel and Veitch, Victor and Emerson, Joseph},
	year = {2014},
	note = {arXiv: 1401.4174v1},
	pages = {1--9},
}

@article{aharonov_detectability_2008,
	title = {The {Detectability} {Lemma} and {Quantum} {Gap} {Amplification}},
	author = {Aharonov, Dorit},
	year = {2008},
	note = {arXiv: 0811.3412v1},
	pages = {1--30},
}

@article{fujii_quantum_2013,
	title = {Quantum {Commuting} {Circuits} and {Complexity} of {Ising}},
	author = {Fujii, Keisuke and Morimae, Tomoyuki and Honmachi, Yoshida},
	year = {2013},
	note = {arXiv: 1311.2128v1},
	pages = {1--33},
}

@article{ambainis_physical_nodate,
	title = {On physical problems that are slightly more difficult than {QM} {A}},
	author = {Ambainis, Andris},
	note = {arXiv: 1312.4758v1},
	pages = {1--25},
}

@article{jafari_topological_2013,
	title = {Topological {Phase} {Transition} in the {Extended} {Cluster} {Compass} {Ladder}},
	author = {Jafari, R and Mahdavifar, S},
	year = {2013},
	note = {arXiv: 1310.5501v1},
	pages = {1--12},
}

@article{Search2003,
	title = {Optically driven silicon-based quantum gates with potential for high-temperature operation},
	volume = {447},
	url = {http://iopscience.iop.org/0953-8984/15/27/102},
	urldate = {2014-05-28},
	journal = {Journal of Physics: …},
	author = {Stoneham, AM and Fisher, AJ and Greenland, PT},
	year = {2003},
}

@article{Meyer2009,
	title = {Wigner crystal physics in quantum wires {arXiv}},
	url = {http://iopscience.iop.org/0953-8984/21/2/023203},
	urldate = {2014-03-11},
	journal = {Journal of Physics: Condensed Matter},
	author = {Meyer, JS and Matveev, KA},
	year = {2009},
	note = {arXiv: 0808.2076v2},
}

@article{dynomant_doc2vec_nodate,
	title = {{Doc2Vec} on the {PubMed} corpus: study of a new approach to generate related articles ´},
	author = {Dynomant, Emeric},
	note = {arXiv: 1911.11698v1},
	pages = {1--12},
}

@article{peng_cross-sentence_2017,
	title = {Cross-{Sentence} {N} -ary {Relation} {Extraction} with {Graph} {LSTMs}},
	volume = {5},
	issn = {2307-387X},
	doi = {10.1162/tacl_a_00049},
	abstract = {Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Peng, Nanyun and Poon, Hoifung and Quirk, Chris and Toutanova, Kristina and Yih, Wen-tau},
	year = {2017},
	note = {arXiv: 1708.03743},
	pages = {101--115},
}

@article{evans_canneural_2018,
	title = {Canneural networks understand logical entailment?},
	abstract = {We introduce a new dataset of logical entailments for the purpose of measuring models’ ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class—PossibleWorldNets—which computes entailment as a “convolution over possible worlds”. Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.},
	journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
	author = {Evans, Richard and Saxton, David and Amos, David and Kohli, Pushmeet and Grefenstette, Edward},
	year = {2018},
	note = {arXiv: 1802.08535},
	pages = {1--15},
}

@article{gui_long_2019,
	title = {Long {Short}-{Term} {Memory} with {Dynamic} {Skip} {Connections}},
	volume = {33},
	issn = {2159-5399},
	doi = {10.1609/aaai.v33i01.33016481},
	abstract = {In recent years, long short-term memory (LSTM) has been successfully used to model sequential data of variable length. However, LSTM can still experience difficulty in capturing long-term dependencies. In this work, we tried to alleviate this problem by introducing a dynamic skip connection, which can learn to directly connect two dependent words. Since there is no dependency information in the training data, we propose a novel reinforcement learning-based method to model the dependency relationship and connect dependent words. The proposed model computes the recurrent transition functions based on the skip connections, which provides a dynamic skipping advantage over RNNs that always tackle entire sentences sequentially. Our experimental results on three natural language processing tasks demonstrate that the proposed method can achieve better performance than existing methods. In the number prediction experiment, the proposed model outperformed LSTM with respect to accuracy by nearly 20\%.},
	number = {2016},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Gui, Tao and Zhang, Qi and Zhao, Lujun and Lin, Yaosong and Peng, Minlong and Gong, Jingjing and Huang, Xuanjing},
	year = {2019},
	note = {arXiv: 1811.03873},
	pages = {6481--6488},
}

@article{kiros_inferlite:_2019,
	title = {{InferLite}: {Simple} {Universal} {Sentence} {Representations} from {Natural} {Language} {Inference} {Data}},
	doi = {10.18653/v1/d18-1524},
	abstract = {Natural language inference has been shown to be an effective supervised task for learning generic sentence embeddings. In order to better understand the components that lead to effective representations, we propose a lightweight version of InferSent (Conneau et al., 2017), called InferLite, that does not use any recurrent layers and operates on a collection of pre-trained word embeddings. We show that a simple instance of our model that makes no use of context, word ordering or position can still obtain competitive performance on the majority of downstream prediction tasks, with most performance gaps being filled by adding local contextual information through temporal convolutions. Our models can be trained in under 1 hour on a single GPU and allows for fast inference of new representations. Finally we describe a semantic hash-ing layer that allows our model to learn generic binary codes for sentences.},
	author = {Kiros, Jamie and Chan, William},
	year = {2019},
	pages = {4868--4874},
}

@article{mikolov_efficient_2013,
	title = {Efficient estimation of word representations in vector space},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	journal = {1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year = {2013},
	note = {arXiv: 1301.3781},
	pages = {1--12},
}

@misc{noauthor_whatdoesbertlookat.pdf_nodate,
	title = {{WhatDoesBERTLookAt}.pdf},
}

@article{conneau_what_2018,
	title = {What you can cram into a single \$\&!\#* vector: {Probing} sentence embeddings for linguistic properties},
	volume = {1},
	doi = {10.18653/v1/p18-1198},
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
	year = {2018},
	note = {arXiv: 1805.01070
ISBN: 9781948087322},
	pages = {2126--2136},
}

@article{pagliardini_unsupervised_2018,
	title = {Unsupervised {Learning} of {Sentence} {Embeddings} {Using} {Compositional} n-{Gram} {Features}},
	doi = {10.18653/v1/n18-1049},
	abstract = {The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.},
	author = {Pagliardini, Matteo and Gupta, Prakhar and Jaggi, Martin},
	year = {2018},
	note = {arXiv: 1703.02507},
	pages = {528--540},
}

@article{xie_unsupervised_2019,
	title = {Unsupervised {Data} {Augmentation} for {Consistency} {Training}},
	url = {http://arxiv.org/abs/1904.12848},
	abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 2.7\% with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
	author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
	year = {2019},
	note = {arXiv: 1904.12848},
	pages = {1--19},
}

@article{peters_tune_2019,
	title = {To {Tune} or {Not} to {Tune}? {Adapting} {Pretrained} {Representations} to {Diverse} {Tasks}},
	doi = {10.18653/v1/w19-4302},
	abstract = {While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.},
	author = {Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.},
	year = {2019},
	note = {arXiv: 1903.05987},
	pages = {7--14},
}

@article{see_get_2017,
	title = {Get to the point: {Summarization} with pointer-generator networks},
	volume = {1},
	doi = {10.18653/v1/P17-1099},
	abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
	journal = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
	author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
	year = {2017},
	note = {arXiv: 1704.04368
ISBN: 9781945626753},
	pages = {1073--1083},
}

@article{kiros_skip-thought_2015,
	title = {Skip-thought vectors},
	volume = {2015-Janua},
	issn = {10495258},
	abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.},
	number = {786},
	journal = {Advances in Neural Information Processing Systems},
	author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	year = {2015},
	note = {arXiv: 1506.06726},
	pages = {3294--3302},
}

@article{chronopoulou_embarrassingly_2019,
	title = {An {Embarrassingly} {Simple} {Approach} for {Transfer} {Learning} from {Pretrained} {Language} {Models}},
	doi = {10.18653/v1/n19-1213},
	abstract = {A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.},
	author = {Chronopoulou, Alexandra and Baziotis, Christos and Potamianos, Alexandros},
	year = {2019},
	note = {arXiv: 1902.10547},
	pages = {2089--2095},
}

@article{ceglarek_semantic_2014,
	title = {Semantic compression for text document processing},
	volume = {8615},
	issn = {16113349},
	doi = {10.1007/978-3-662-44509-9.2},
	abstract = {Ongoing research on novel methods and tools that can be applied in Natural Language Processing tasks has resulted in the design of a semantic compression mechanism. Semantic compression is a technique that allows for correct generalization of terms in some given context. Thanks to this generalization a common thought can be detected. The rules governing the generalization process are based on a data structure which is referred to as a domain frequency dictionary. Having established the domain for a given text fragment the disambiguation of possibly many hypernyms becomes a feasible task. Semantic compression, thus an informed generalization, is possible through the use of semantic networks as a knowledge representation structure. In the given overview, it is worth noting that the semantic compression allows for a number of improvements in comparison to already established Natural Language Processing techniques. These improvements, along with a detailed discussion of the various elements of algorithms and data structures that are necessary to make semantic compression a viable solution, are the core of this work. Semantic compression can be applied in a variety of scenarios, e.g. in detection of plagiarism. With increasing effort being spent on developing semantic compression, new domains of application have been discovered. What is more, semantic compression itself has evolved and has been refined by the introduction of new solutions that boost the level of disambiguation efficiency. Thanks to the remodeling of already existing data sources to suit algorithms enabling semantic compression, it has become possible to use semantic compression as a base for automata that, thanks to the exploration of hypernym-hyponym and synonym relations, new concepts that may be included in the knowledge representation structures can now be discovered.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Ceglarek, Dariusz},
	year = {2014},
	note = {ISBN: 9783662445099},
	keywords = {Information retrieval, Knowledge acquisition, Natural language processing, Plagiarism detection, Semantic compression, Semantic network, Text clustering},
	pages = {20--48},
}

@article{nimishakavi_relation_2016,
	title = {Relation schema induction using tensor factorization with side information},
	doi = {10.18653/v1/d16-1040},
	abstract = {Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster).},
	journal = {EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
	author = {Nimishakavi, Madhav and Saini, Uday Singh and Talukdar, Partha},
	year = {2016},
	note = {arXiv: 1605.04227
ISBN: 9781945626258},
	pages = {414--423},
}

@article{sudhahar_reasoning_2019,
	title = {Reasoning {Over} {Paths} via {Knowledge} {Base} {Completion}},
	url = {http://arxiv.org/abs/1911.00492},
	abstract = {Reasoning over paths in large scale knowledge graphs is an important problem for many applications. In this paper we discuss a simple approach to automatically build and rank paths between a source and target entity pair with learned embeddings using a knowledge base completion model (KBC). We assembled a knowledge graph by mining the available biomedical scientific literature and extracted a set of high frequency paths to use for validation. We demonstrate that our method is able to effectively rank a list of known paths between a pair of entities and also come up with plausible paths that are not present in the knowledge graph. For a given entity pair we are able to reconstruct the highest ranking path 60\% of the time within the the top 10 ranked paths and achieve 49\% mean average precision. Our approach is compositional since any KBC model that can produce vector representations of entities can be used.},
	author = {Sudhahar, Saatviga and Roberts, Ian and Pierleoni, Andrea},
	year = {2019},
	note = {arXiv: 1911.00492},
}

@article{killoran_continuous-variable_2018,
	title = {Continuous-variable quantum neural networks},
	url = {http://arxiv.org/abs/1806.06871%0Ahttp://dx.doi.org/10.1103/PhysRevResearch.1.033063},
	doi = {10.1103/PhysRevResearch.1.033063},
	abstract = {We introduce a general method for building neural networks on quantum computers. The quantum neural network is a variational quantum circuit built in the continuous-variable (CV) architecture, which encodes quantum information in continuous degrees of freedom such as the amplitudes of the electromagnetic field. This circuit contains a layered structure of continuously parameterized gates which is universal for CV quantum computation. Affine transformations and nonlinear activation functions, two key elements in neural networks, are enacted in the quantum network using Gaussian and non-Gaussian gates, respectively. The non-Gaussian gates provide both the nonlinearity and the universality of the model. Due to the structure of the CV model, the CV quantum neural network can encode highly nonlinear transformations while remaining completely unitary. We show how a classical network can be embedded into the quantum formalism and propose quantum versions of various specialized model such as convolutional, recurrent, and residual networks. Finally, we present numerous modeling experiments built with the Strawberry Fields software library. These experiments, including a classifier for fraud detection, a network which generates Tetris images, and a hybrid classical-quantum autoencoder, demonstrate the capability and adaptability of CV quantum neural networks.},
	author = {Killoran, Nathan and Bromley, Thomas R. and Arrazola, Juan Miguel and Schuld, Maria and Quesada, Nicolás and Lloyd, Seth},
	year = {2018},
	note = {arXiv: 1806.06871},
	pages = {1--21},
}

@article{renter_siamese_2016,
	title = {Siamese {CBOW}: {Optimizing} word embeddings for sentence representations},
	volume = {2},
	doi = {10.18653/v1/p16-1089},
	abstract = {We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation of highquality sentence embeddings. Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings. However, word embeddings trained with the methods currently available are not optimized for the task of sentence representation, and, thus, likely to be suboptimal. Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged. The underlying neural network learns word embeddings by predicting, from a sentence representation, its surrounding sentences. We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources.},
	journal = {54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers},
	author = {Renter, Tom and Borisov, Alexey and De Rijke, Maarten},
	year = {2016},
	note = {arXiv: 1606.04640
ISBN: 9781510827585},
	pages = {941--951},
}

@article{fan_reducing_2019,
	title = {Reducing {Transformer} {Depth} on {Demand} with {Structured} {Dropout}},
	volume = {103},
	url = {http://arxiv.org/abs/1909.11556},
	abstract = {Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.},
	author = {Fan, Angela and Grave, Edouard and Joulin, Armand},
	year = {2019},
	note = {arXiv: 1909.11556},
	pages = {1--15},
}

@article{pujara_knowledge_2013,
	title = {Knowledge graph identification},
	volume = {8218 LNCS},
	issn = {03029743},
	doi = {10.1007/978-3-642-41335-3_34},
	abstract = {Large-scale information processing systems are able to extract massive collections of interrelated facts, but unfortunately transforming these candidate facts into useful knowledge is a formidable challenge. In this paper, we show how uncertain extractions about entities and their relations can be transformed into a knowledge graph. The extractions form an extraction graph and we refer to the task of removing noise, inferring missing information, and determining which candidate facts should be included into a knowledge graph as knowledge graph identification. In order to perform this task, we must reason jointly about candidate facts and their associated extraction confidences, identify co-referent entities, and incorporate ontological constraints. Our proposed approach uses probabilistic soft logic (PSL), a recently introduced probabilistic modeling framework which easily scales to millions of facts. We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL project containing over 1M extractions and 70K ontological relations. We show that compared to existing methods, our approach is able to achieve improved AUC and F1 with significantly lower running time. © 2013 Springer-Verlag.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Pujara, Jay and Miao, Hui and Getoor, Lise and Cohen, William},
	year = {2013},
	note = {ISBN: 9783642413346},
	pages = {542--557},
}

@article{sinoara_knowledge-enhanced_2019,
	title = {Knowledge-enhanced document embeddings for text classification},
	volume = {163},
	issn = {09507051},
	url = {https://doi.org/10.1016/j.knosys.2018.10.026},
	doi = {10.1016/j.knosys.2018.10.026},
	abstract = {Accurate semantic representation models are essential in text mining applications. For a successful application of the text mining process, the text representation adopted must keep the interesting patterns to be discovered. Although competitive results for automatic text classification may be achieved with traditional bag of words, such representation model cannot provide satisfactory classification performances on hard settings where richer text representations are required. In this paper, we present an approach to represent document collections based on embedded representations of words and word senses. We bring together the power of word sense disambiguation and the semantic richness of word- and word-sense embedded vectors to construct embedded representations of document collections. Our approach results in semantically enhanced and low-dimensional representations. We overcome the lack of interpretability of embedded vectors, which is a drawback of this kind of representation, with the use of word sense embedded vectors. Moreover, the experimental evaluation indicates that the use of the proposed representations provides stable classifiers with strong quantitative results, especially in semantically-complex classification scenarios.},
	journal = {Knowledge-Based Systems},
	author = {Sinoara, Roberta A. and Camacho-Collados, Jose and Rossi, Rafael G. and Navigli, Roberto and Rezende, Solange O.},
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Document embeddings, Semantic representation, Text classification, Text mining},
	pages = {955--971},
}

@article{zhou_improving_2019,
	title = {Improving {BERT} {Fine}-tuning with {Embedding} {Normalization}},
	url = {http://arxiv.org/abs/1911.03918},
	abstract = {Large pre-trained sentence encoders like BERT start a new chapter in natural language processing. A common practice to apply pre-trained BERT to sequence classification tasks (e.g., classification of sentences or sentence pairs) is by feeding the embedding of [CLS] token (in the last layer) to a task-specific classification layer, and then fine tune the model parameters of BERT and classifier jointly. In this paper, we conduct systematic analysis over several sequence classification datasets to examine the embedding values of [CLS] token before the fine tuning phase, and present the biased embedding distribution issue---i.e., embedding values of [CLS] concentrate on a few dimensions and are non-zero centered. Such biased embedding brings challenge to the optimization process during fine-tuning as gradients of [CLS] embedding may explode and result in degraded model performance. We further propose several simple yet effective normalization methods to modify the [CLS] embedding during the fine-tuning. Compared with the previous practice, neural classification model with the normalized embedding shows improvements on several text classification tasks, demonstrates the effectiveness of our method.},
	author = {Zhou, Wenxuan and Du, Junyi and Ren, Xiang},
	year = {2019},
	note = {arXiv: 1911.03918},
}

@article{mukherjee_distilling_2019,
	title = {Distilling {Transformers} into {Simple} {Neural} {Networks} with {Unlabeled} {Transfer} {Data}},
	url = {http://arxiv.org/abs/1910.01769},
	abstract = {Recent advances in pre-training huge models on large amounts of text through self supervision have obtained state-of-the-art results in various natural language processing tasks. However, these huge and expensive models are difficult to use in practise for downstream tasks. Some recent efforts use knowledge distillation to compress these models. However, we see a gap between the performance of the smaller student models as compared to that of the large teacher. In this work, we leverage large amounts of in-domain unlabeled transfer data in addition to a limited amount of labeled training instances to bridge this gap. We show that simple RNN based student models even with hard distillation can perform at par with the huge teachers given the transfer set. The student performance can be further improved with soft distillation and leveraging teacher intermediate representations. We show that our student models can compress the huge teacher by up to 26x while still matching or even marginally exceeding the teacher performance in low-resource settings with small amount of labeled data.},
	author = {Mukherjee, Subhabrata and Awadallah, Ahmed Hassan},
	year = {2019},
	note = {arXiv: 1910.01769},
}

@article{xu_discourse-aware_2019,
	title = {Discourse-{Aware} {Neural} {Extractive} {Model} for {Text} {Summarization}},
	url = {http://arxiv.org/abs/1910.14142},
	abstract = {Recently BERT has been adopted in state-of-the-art text summarization models for document encoding. However, such BERT-based extractive models use the sentence as the minimal selection unit, which often results in redundant or uninformative phrases in the generated summaries. As BERT is pre-trained on sentence pairs, not documents, the long-range dependencies between sentences are not well captured. To address these issues, we present a graph-based discourse-aware neural summarization model - DiscoBert. By utilizing discourse segmentation to extract discourse units (instead of sentences) as candidates, DiscoBert provides a fine-grained granularity for extractive selection, which helps reduce redundancy in extracted summaries. Based on this, two discourse graphs are further proposed: (\$i\$) RST Graph based on RST discourse trees; and (\$ii\$) Coreference Graph based on coreference mentions in the document. DiscoBert first encodes the extracted discourse units with BERT, and then uses a graph convolutional network to capture the long-range dependencies among discourse units through the constructed graphs. Experimental results on two popular summarization datasets demonstrate that DiscoBert outperforms state-of-the-art methods by a significant margin.},
	number = {Liu},
	author = {Xu, Jiacheng and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	year = {2019},
	note = {arXiv: 1910.14142},
}

@article{lin_d_2001,
	title = {D {I} {R} {T} - {Discovery} of {Inference} {Rules} from {Text}},
	author = {Lin, Dekang and Pantel, Patrick},
	year = {2001},
	pages = {323--328},
}

@article{han_deep_2016,
	title = {Deep compression: {Compressing} deep neural networks with pruning, trained quantization and {Huffman} coding},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce “deep compression”, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.},
	journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	year = {2016},
	note = {arXiv: 1510.00149},
	pages = {1--14},
}

@article{palangi_deep_2016,
	title = {Deep {Sentence} embedding using long short-term memory networks: {Analysis} and application to information retrieval},
	volume = {24},
	issn = {23299290},
	doi = {10.1109/TASLP.2016.2520371},
	abstract = {This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks (RNN) with Long Short-Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detect the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTMRNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms Paragraph Vector method for web document retrieval task.},
	number = {4},
	journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
	author = {Palangi, Hamid and Deng, Li and Shen, Yelong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Song, Xinying and Ward, Rabab},
	year = {2016},
	note = {arXiv: 1502.06922},
	keywords = {Deep learning, Long short-term memory, Sentence embedding},
	pages = {694--707},
}

@article{hill_learning_2016,
	title = {Learning distributed representations of sentences from unlabelled data},
	doi = {10.18653/v1/n16-1162},
	abstract = {Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-bilinear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.},
	journal = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
	author = {Hill, Felix and Cho, Kyunghyun and Korhonen, Anna},
	year = {2016},
	note = {arXiv: 1602.03483
ISBN: 9781941643914},
	pages = {1367--1377},
}

@article{angelino_learning_2018,
	title = {Learning certifiably optimal rule lists for categorical data},
	volume = {18},
	issn = {15337928},
	abstract = {We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm produces rule lists with optimal training performance, according to the regularized empirical risk, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. Our results indicate that it is possible to construct optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary risk prediction tool on data from Broward County, Florida, but that are completely interpretable. This framework is a novel alternative to CART and other decision tree methods for interpretable modeling.},
	journal = {Journal of Machine Learning Research},
	author = {Angelino, Elaine and Larus-Stone, Nicholas and Alabi, Daniel and Seltzer, Margo and Rudin, Cynthia},
	year = {2018},
	note = {arXiv: 1704.01701},
	keywords = {Criminal justice applications, Decision trees, Interpretable models, Optimization, Rule lists},
	pages = {1--78},
}

@article{galarraga_canonicalizing_2018,
	title = {Canonicalizing {Open} {Knowledge} {Bases} {To} cite this version : {HAL} {Id} : hal-01699884 {Canonicalizing} {Open} {Knowledge} {Bases}},
	author = {Galárraga, Luis and Heitz, Geremy and Murphy, Kevin and Suchanek, Fabian M and Galárraga, Luis and Heitz, Geremy and Murphy, Kevin and Suchanek, Fabian M and Open, Canonicalizing and Galárraga, Luis and Murphy, Kevin and Suchanek, Fabian},
	year = {2018},
}

@article{elwany_bert_2019,
	title = {{BERT} {Goes} to {Law} {School}: {Quantifying} the {Competitive} {Advantage} of {Access} to {Large} {Legal} {Corpora} in {Contract} {Understanding}},
	url = {http://arxiv.org/abs/1911.00473},
	abstract = {Fine-tuning language models, such as BERT, on domain specific corpora has proven to be valuable in domains like scientific papers and biomedical text. In this paper, we show that fine-tuning BERT on legal documents similarly provides valuable improvements on NLP tasks in the legal domain. Demonstrating this outcome is significant for analyzing commercial agreements, because obtaining large legal corpora is challenging due to their confidential nature. As such, we show that having access to large legal corpora is a competitive advantage for commercial applications, and academic research on analyzing contracts.},
	author = {Elwany, Emad and Moore, Dave and Oberoi, Gaurav},
	year = {2019},
	note = {arXiv: 1911.00473},
}

@article{brendel_approximating_2019,
	title = {Approximating cnns with bag-of-local-features models works surprisingly well on {Imagenet}},
	abstract = {Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6\% top-5 for 33 × 33 px features and Alexnet performance for 17 × 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.},
	journal = {7th International Conference on Learning Representations, ICLR 2019},
	author = {Brendel, Wieland and Bethge, Matthias},
	year = {2019},
	note = {arXiv: 1904.00760},
	pages = {1--15},
}

@article{kim_power_2019,
	title = {The {Power} of {Communities}: {A} {Text} {Classification} {Model} with {Automated} {Labeling} {Process} {Using} {Network} {Community} {Detection}},
	url = {http://arxiv.org/abs/1909.11706},
	abstract = {Text classification is one of the most critical areas in machine learning and artificial intelligence research. It has been actively adopted in many business applications such as conversational intelligence systems, news articles categorizations, sentiment analysis, emotion detection systems, and many other recommendation systems in our daily life. One of the problems in supervised text classification models is that the models' performance depends heavily on the quality of data labeling that is typically done by humans. In this study, we propose a new network community detection-based approach to automatically label and classify text data into multiclass value spaces. Specifically, we build a network with sentences as the network nodes and pairwise cosine similarities between TFIDF vector representations of the sentences as the network link weights. We use the Louvain method to detect the communities in the sentence network. We train and test Support Vector Machine and Random Forest models on both the human-labeled data and network community detection labeled data. Results showed that models with the data labeled by network community detection outperformed the models with the human-labeled data by 2.68-3.75\% of classification accuracy. Our method may help developments of more accurate conversational intelligence and other text classification systems.},
	author = {Kim, Minjun and Sayama, Hiroki},
	year = {2019},
	note = {arXiv: 1909.11706},
	keywords = {detection sentence, network science network community},
	pages = {1--13},
}

@article{michel_are_2019,
	title = {Are {Sixteen} {Heads} {Really} {Better} than {One}?},
	url = {http://arxiv.org/abs/1905.10650},
	abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
	number = {NeurIPS},
	author = {Michel, Paul and Levy, Omer and Neubig, Graham},
	year = {2019},
	note = {arXiv: 1905.10650},
	pages = {1--13},
}

@article{agibetov_fast_2018,
	title = {Fast and scalable neural embedding models for biomedical sentence classification},
	volume = {19},
	issn = {14712105},
	doi = {10.1186/s12859-018-2496-4},
	abstract = {Background: Biomedical literature is expanding rapidly, and tools that help locate information of interest are needed. To this end, a multitude of different approaches for classifying sentences in biomedical publications according to their coarse semantic and rhetoric categories (e.g., Background, Methods, Results, Conclusions) have been devised, with recent state-of-the-art results reported for a complex deep learning model. Recent evidence showed that shallow and wide neural models such as fastText can provide results that are competitive or superior to complex deep learning models while requiring drastically lower training times and having better scalability. We analyze the efficacy of the fastText model in the classification of biomedical sentences in the PubMed 200k RCT benchmark, and introduce a simple pre-processing step that enables the application of fastText on sentence sequences. Furthermore, we explore the utility of two unsupervised pre-training approaches in scenarios where labeled training data are limited. Results: Our fastText-based methodology yields a state-of-the-art F1 score of.917 on the PubMed 200k benchmark when sentence ordering is taken into account, with a training time of only 73 s on standard hardware. Applying fastText on single sentences, without taking sentence ordering into account, yielded an F1 score of.852 (training time 13 s). Unsupervised pre-training of N-gram vectors greatly improved the results for small training set sizes, with an increase of F1 score of.21 to.74 when trained on only 1000 randomly picked sentences without taking sentence ordering into account. Conclusions: Because of it's ease of use and performance, fastText should be among the first choices of tools when tackling biomedical text classification problems with large corpora. Unsupervised pre-training of N-gram vectors on domain-specific corpora also makes it possible to apply fastText when labeled training data are limited.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Agibetov, Asan and Blagec, Kathrin and Xu, Hong and Samwald, Matthias},
	year = {2018},
	note = {Publisher: BMC Bioinformatics},
	keywords = {FastText, Natural language processing, Neural networks, Scientific abstracts, Text classification, Word vector models},
	pages = {1--9},
}

@article{cer_universal_2018,
	title = {Universal sentence encoder for {English}},
	doi = {10.18653/v1/d18-2029},
	abstract = {We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.},
	journal = {EMNLP 2018 - Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Proceedings},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng yi and Hua, Nan and Limtiaco, Nicole and St. John, Rhomni and Constant, Noah and Guajardo-Céspedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun Hsuan and Strope, Brian and Kurzweil, Ray},
	year = {2018},
	note = {arXiv: 1803.11175
ISBN: 9781948087858},
	pages = {169--174},
}

@article{raffel_exploring_2019,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2019},
	note = {arXiv: 1910.10683},
	pages = {1--53},
}

@article{andreotti_code-shift-keying_2015,
	title = {Code-{Shift}-{Keying} ({CSK}) with advanced {FEC} coding for {GNSS} applications in satellite multipath channel},
	doi = {10.1109/NAVITEC.2014.7045139},
	abstract = {In this paper, Code-Shift-Keying (CSK) is investigated in combination with Forward Error Correction (FEC) coding techniques, with an additional block interleaving scheme. The target of this analysis is to evaluate the CSK capability to increase data rate or data demodulation sensitivity in GNSS scenarios as compared to the use of traditional BPSK A particular set of advanced signal baselines is reported as example of application to SBAS signal formats, assessing the data delivery performance in Additive White Gaussian Noise (AWGN) channel, two-ray multipath and Land Mobile Satellite (LMS) channel in terms of Bit Error Rate (BER).},
	journal = {2014 7th ESA Workshop on Satellite Navigation Technologies and European Workshop on GNSS Signals and Signal Processing, NAVITEC 2014 - Proceedings},
	author = {Andreotti, Riccardo and Emmanuele, Andrea and Fontanella, Diana and Zanier, Francesca and Luise, Marco},
	year = {2015},
	note = {ISBN: 9781479965298},
	keywords = {Code-Shift-Keying (CSK), EGNOS, GNSS, LDPC, Land Mobile Satellite (LMS), SBAS, SOTC, bit error rate (BER), coding, data demodulation},
	pages = {1--9},
}

@article{lebret_neural_2016,
	title = {Neural text generation from structured data with application to the biography domain},
	doi = {10.18653/v1/d16-1128},
	abstract = {This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. It generates biographical sentences from fact tables on a new dataset of biographies from Wikipedia. This set is an order of magnitude larger than existing resources with over 700k samples and a 400k vocabulary. Our model builds on conditional neural language models for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. To deal with structured data, we allow the model to embed words differently depending on the data fields in which they occur. Our neural model significantly outperforms a Templated Kneser-Ney language model by nearly 15 BLEU.},
	journal = {EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
	author = {Lebret, Rémi and Grangier, David and Auli, Michael},
	year = {2016},
	note = {arXiv: 1603.07771
ISBN: 9781945626258},
	pages = {1203--1213},
}

@article{dunning_accurate_1993,
	title = {Accurate {Methods} for the {Statistics} of {Surprise} and {Coincidence}},
	volume = {19},
	issn = {0891-2017},
	abstract = {Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.},
	number = {1},
	journal = {Computational linguistics},
	author = {Dunning, Ted},
	year = {1993},
	keywords = {Contingency table, Rare events},
	pages = {61},
}

@article{peters_deep_2018,
	title = {Deep {Contextualized} {Word} {Representations}},
	doi = {10.18653/v1/n18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	year = {2018},
	note = {arXiv: 1802.05365},
	pages = {2227--2237},
}

@article{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	journal = {OpenAI},
	author = {Radford, Alec and Salimans, Tim},
	year = {2018},
	pages = {1--12},
}

@article{liu_generating_2018,
	title = {Generating wikipedia by summarizing long sequences},
	abstract = {We show that generating English Wikipedia articles can be approached as a multi-document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
	journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
	author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Łukasz and Shazeer, Noam},
	year = {2018},
	note = {arXiv: 1801.10198},
	pages = {1--18},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	year = {2015},
	note = {arXiv: 1503.02531},
	pages = {1--9},
}

@article{lan_albert:_2019,
	title = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
	url = {http://arxiv.org/abs/1909.11942},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.The code and the pretrained models are available at https://github.com/google-research/google-research/tree/master/albert.},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	year = {2019},
	note = {arXiv: 1909.11942},
	pages = {1--16},
}

@article{wang_glue:_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} platform for {NLU}},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
	journal = {Iclr},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
	year = {2019},
	keywords = {DeepMind},
	pages = {1--20},
}

@article{sun_patient_2019,
	title = {Patient {Knowledge} {Distillation} for {BERT} {Model} {Compression}},
	url = {http://arxiv.org/abs/1908.09355},
	abstract = {Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (\$i\$) PKD-Last: learning from the last \$k\$ layers; and (\$ii\$) PKD-Skip: learning from every \$k\$ layers. These two patient distillation schemes enable the exploitation of rich information in the teacher's hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with significant gain in training efficiency, without sacrificing model accuracy.},
	author = {Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
	year = {2019},
	note = {arXiv: 1908.09355},
}

@article{tian_contrastive_2019,
	title = {Contrastive {Representation} {Distillation}},
	url = {http://arxiv.org/abs/1910.10699},
	abstract = {Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: http://github.com/HobbitLong/RepDistiller.},
	number = {2015},
	author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	year = {2019},
	note = {arXiv: 1910.10699},
}

@article{sanh_distilbert_2019,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	number = {NeurIPS},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	year = {2019},
	note = {arXiv: 1910.01108},
	pages = {1--5},
}

@article{tang_distilling_2019,
	title = {Distilling {Task}-{Specific} {Knowledge} from {BERT} into {Simple} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1903.12136},
	abstract = {In the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.},
	author = {Tang, Raphael and Lu, Yao and Liu, Linqing and Mou, Lili and Vechtomova, Olga and Lin, Jimmy},
	year = {2019},
	note = {arXiv: 1903.12136},
}

@article{vaswani_attention_2017,
	title = {Attention is all you need},
	volume = {2017-Decem},
	issn = {10495258},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	number = {Nips},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	note = {arXiv: 1706.03762},
	pages = {5999--6009},
}

@article{das_go_2018,
	title = {Go for a walk and arrive at the answer: {Reasoning} over paths in knowledge bases using reinforcement learning},
	abstract = {Knowledge bases (KB), both automatically and manually constructed, are often incomplete — many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities, or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm, MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with unknown destination and combinatorially many paths from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths. On a comprehensive evaluation on seven knowledge base datasets, we found MINERVA to be competitive with many current state-of-the-art methods.},
	journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
	author = {Das, Rajarshi and Dhuliawala, Shehzaad and Zaheer, Manzil and Vilnis, Luke and Durugkar, Ishan and Krishnamurthy, Akshay and Smola, Alex and McCallum, Andrew},
	year = {2018},
	note = {arXiv: 1711.05851},
}

@article{zhao_extreme_2019,
	title = {Extreme {Language} {Model} {Compression} with {Optimal} {Subwords} and {Shared} {Projections}},
	url = {http://arxiv.org/abs/1909.11687},
	abstract = {Pre-trained deep neural network language models such as ELMo, GPT, BERT and XLNet have recently achieved state-of-the-art performance on a variety of language understanding tasks. However, their size makes them impractical for a number of scenarios, especially on mobile and edge devices. In particular, the input word embedding matrix accounts for a significant proportion of the model's memory footprint, due to the large input vocabulary and embedding dimensions. Knowledge distillation techniques have had success at compressing large neural network models, but they are ineffective at yielding student models with vocabularies different from the original teacher models. We introduce a novel knowledge distillation technique for training a student model with a significantly smaller vocabulary as well as lower embedding and hidden state dimensions. Specifically, we employ a dual-training mechanism that trains the teacher and student models simultaneously to obtain optimal word embeddings for the student vocabulary. We combine this approach with learning shared projection matrices that transfer layer-wise knowledge from the teacher model to the student model. Our method is able to compress the BERT\_BASE model by more than 60x, with only a minor drop in downstream task metrics, resulting in a language model with a footprint of under 7MB. Experimental results also demonstrate higher compression efficiency and accuracy when compared with other state-of-the-art compression techniques.},
	author = {Zhao, Sanqiang and Gupta, Raghav and Song, Yang and Zhou, Denny},
	year = {2019},
	note = {arXiv: 1909.11687},
	pages = {1--11},
}

@article{shazeer_fast_2019,
	title = {Fast {Transformer} {Decoding}: {One} {Write}-{Head} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/1911.02150},
	abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
	author = {Shazeer, Noam},
	year = {2019},
	note = {arXiv: 1911.02150},
	pages = {1--9},
}

@article{sadeghian_drum:_2019,
	title = {{DRUM}: {End}-{To}-{End} {Differentiable} {Rule} {Mining} {On} {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/1911.00055},
	abstract = {In this paper, we study the problem of learning probabilistic logical rules for inductive and interpretable link prediction. Despite the importance of inductive link prediction, most previous works focused on transductive link prediction and cannot manage previously unseen entities. Moreover, they are black-box models that are not easily explainable for humans. We propose DRUM, a scalable and differentiable approach for mining first-order logical rules from knowledge graphs which resolves these problems. We motivate our method by making a connection between learning confidence scores for each rule and low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful information across the tasks of learning rules for different relations. We also empirically demonstrate the efficiency of DRUM over existing rule mining methods for inductive link prediction on a variety of benchmark datasets.},
	author = {Sadeghian, Ali and Armandpour, Mohammadreza and Ding, Patrick and Wang, Daisy Zhe},
	year = {2019},
	note = {arXiv: 1911.00055},
	pages = {1--13},
}

@article{brown_quantum_2016,
	title = {Quantum memories at finite temperature},
	volume = {88},
	issn = {15390756},
	doi = {10.1103/RevModPhys.88.045005},
	abstract = {To use quantum systems for technological applications we first need to preserve their coherence for macroscopic timescales, even at finite temperature. Quantum error correction has made it possible to actively correct errors that affect a quantum memory. An attractive scenario is the construction of passive storage of quantum information with minimal active support. Indeed, passive protection is the basis of robust and scalable classical technology, physically realized in the form of the transistor and the ferromagnetic hard disk. The discovery of an analogous quantum system is a challenging open problem, plagued with a variety of no-go theorems. Several approaches have been devised to overcome these theorems by taking advantage of their loopholes. Here we review the state-of-the-art developments in this field in an informative and pedagogical way. We give the main principles of self-correcting quantum memories and we analyze several milestone examples from the literature of two-, three- and higher-dimensional quantum memories.},
	number = {4},
	journal = {Reviews of Modern Physics},
	author = {Brown, Benjamin J. and Loss, Daniel and Pachos, Jiannis K. and Self, Chris N. and Wootton, James R.},
	year = {2016},
	note = {arXiv: 1411.6643},
}

@article{breuckmann_space-time_2013,
	title = {Space-{Time} {Circuit}-to-{Hamiltonian} {Construction} and {Its} {Applications}},
	url = {http://arxiv.org/abs/1311.6101},
	urldate = {2014-04-23},
	journal = {arXiv preprint arXiv:1311.6101},
	author = {Breuckmann, NP and Terhal, BM},
	year = {2013},
	note = {arXiv: 1311.6101v3},
	pages = {1--27},
}

@article{narayanan_quantum_2000,
	title = {Quantum artificial neural network architectures and components},
	volume = {128},
	issn = {00200255},
	doi = {10.1016/S0020-0255(00)00055-4},
	abstract = {It is shown by classical simulation and experimentation that quantum artificial neural networks (QUANNs) are more efficient and in some cases more powerful than classical artificial neural networks (CLANNs) for a variety of experimental tasks. This effect is particularly noticeable with larger and more complex domains. The gain in efficiency is achieved with no generalization loss in most cases. QUANNs are also more powerful than CLANNs, again for some of the tasks examined, in terms of what the network can learn. What is more, it appears that not all components of a QUANN architecture need to be quantum for these advantages to surface. It is demonstrated that a fully quantum neural network has no advantage over a partly quantum network and may in fact produce worse results. Overall, this work provides a first insight into the expected behaviour of individual components of QUANNs, if and when quantum hardware is ever built, and raises questions about the interface between quantum and classical components of future QUANNs.},
	number = {3},
	journal = {Information sciences},
	author = {Narayanan, Ajit and Menneer, Tammy},
	year = {2000},
	pages = {231--255},
}

@article{rebentrost_quantum_2016,
	title = {Quantum singular value decomposition of non-sparse low-rank matrices},
	url = {http://arxiv.org/abs/1607.05404},
	abstract = {In this work, we present a method to exponentiate non-sparse indefinite low-rank matrices on a quantum computer. Given an operation for accessing the elements of the matrix, our method allows singular values and associated singular vectors to be found quantum mechanically in a time exponentially faster in the dimension of the matrix than known classical algorithms. The method extends to non-Hermitian and non-square matrices via embedding matrices. In the context of the generic singular value decomposition of a matrix, we discuss the Procrustes problem of finding a closest isometry to a given matrix.},
	journal = {arXiv:1607.05404},
	author = {Rebentrost, Patrick and Steffens, Adrian and Lloyd, Seth},
	year = {2016},
	note = {arXiv: 1607.05404},
	pages = {1--5},
}

@article{wittek_quantum_2014,
	title = {Quantum {Machine} {Learning}},
	issn = {87567016},
	url = {http://www.sciencedirect.com/science/article/pii/B9780128009536000025},
	doi = {10.1016/B978-0-12-800953-6.00002-5},
	abstract = {Machine learning is a vast area of research that is primarily concerned with finding patterns in empirical data. We restrict our attention to a limited number of core concepts that are most relevant for quantum learning algorithms. We discuss the importance of the data-driven approach, compared with the formal modeling of traditional artificial intelligence. We outline how to build high-dimensional feature spaces that will correspond to quantum states in quantum computing. An algorithm's ability to generalize beyond training data is intrinsically linked with its complexity—this is the foundation of the Vapnik-Chervonenkis theory, which we briefly explain. There is no free lunch, however: a single algorithm will not work in every learning scenario, which leads to combinations of learners and ensembles. Dependence on the dimensions and volume of the data defines computational complexity for learning methods, which often do not scale well.},
	journal = {Quantum Machine Learning},
	author = {Wittek, Peter},
	year = {2014},
	pmid = {20236947},
	note = {arXiv: 0-387-31073-8
ISBN: 9780128009536},
	keywords = {Bagging, Boosting, Computational complexity, Data-driven models, Ensembles, Feature extraction, Feature filtering, Feature space, Generalization performance, No-free-lunch theorem, VC dimension},
	pages = {11--24},
}

@article{ricks_training_2004,
	title = {Training a {Quantum} {Neural} {Network}},
	url = {http://papers.nips.cc/paper/2363-training-a-quantum-neural-network},
	abstract = {Quantum learning holds great promise for the field of machine intelli-gence. The most studied quantum learning algorithm is the quantum neural network. Many such models have been proposed, yet none has become a standard. In addition, these models usually leave out many details, often excluding how they intend to train their networks. This pa-per discusses one approach to the problem and what advantages it would have over classical networks.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ricks, Bob and Ventura, Dan},
	year = {2004},
	pages = {1019--1026},
}

@article{da_silva_weightless_2016,
	title = {Weightless neural network parameters and architecture selection in a quantum computer},
	volume = {183},
	issn = {18728286},
	doi = {10.1016/j.neucom.2015.05.139},
	abstract = {Training artificial neural networks requires a tedious empirical evaluation to determine a suitable neural network architecture. To avoid this empirical process several techniques have been proposed to automatise the architecture selection process. In this paper, we propose a method to perform parameter and architecture selection for a quantum weightless neural network (qWNN). The architecture selection is performed through the learning procedure of a qWNN with a learning algorithm that uses the principle of quantum superposition and a non-linear quantum operator. The main advantage of the proposed method is that it performs a global search in the space of qWNN architecture and parameters rather than a local search.},
	journal = {Neurocomputing},
	author = {da Silva, Adenilton J. and de Oliveira, Wilson R. and Ludermir, Teresa B.},
	year = {2016},
	pmid = {26878722},
	note = {arXiv: 1601.03277},
	keywords = {Architecture selection, Quantum learning, Quantum neural networks, Quantum weightless neural networks},
	pages = {13--22},
}

@article{gupta_quantum_2001,
	title = {Quantum {Neural} {Networks}},
	volume = {63},
	issn = {00220000},
	url = {http://www.idealibrary.com},
	doi = {10.1006},
	abstract = {This paper initiates the study of quantum computing within the constraints of using a polylogarithmic (O(log k n), k {\textbackslash} 1) number of qubits and a polylogarithmic number of computation steps. The current research in the literature has focussed on using a polynomial number of qubits. A new mathematical model of computation called Quantum Neural Networks (QNNs) is defined, building on Deutsch's model of quantum computational network. The model introduces a nonlinear and irreversible gate, similar to the speculative operator defined by Abrams and Lloyd. The precise dynamics of this operator are defined and while giving examples in which nonlinear Schrödinger's equations are applied, we speculate on its possible implemen-tation. The many practical problems associated with the current model of quantum computing are alleviated in the new model. It is shown that QNNs of logarithmic size and constant depth have the same computational power as threshold circuits, which are used for modeling neural networks. QNNs of polylogarithmic size and polylogarithmic depth can solve the problems in NC, the class of problems with theoretically fast parallel solutions. Thus, the new model may indeed provide an approach for building scalable parallel computers.},
	journal = {Journal of Computer and System Sciences},
	author = {Gupta, Sanjay and Zia, R K P},
	year = {2001},
	pmid = {18255670},
	note = {arXiv: quant-ph/0107012
ISBN: 3790812765},
	keywords = {Church-Turing thesis, parallel computation, quantum computing, theoretical computer science, threshold circuits},
	pages = {355--383},
}

@article{schuld_simulating_2015,
	title = {Simulating a perceptron on a quantum computer},
	volume = {379},
	issn = {03759601},
	doi = {10.1016/j.physleta.2014.11.061},
	abstract = {Perceptrons are the basic computational unit of artificial neural networks, as they model the activation mechanism of an output neuron due to incoming signals from its neighbours. As linear classifiers, they play an important role in the foundations of machine learning. In the context of the emerging field of quantum machine learning, several attempts have been made to develop a corresponding unit using quantum information theory. Based on the quantum phase estimation algorithm, this paper introduces a quantum perceptron model imitating the step-activation function of a classical perceptron. This scheme requires resources in O(n) (where n is the size of the input) and promises efficient applications for more complex structures such as trainable quantum neural networks.},
	number = {7},
	journal = {Physics Letters, Section A: General, Atomic and Solid State Physics},
	author = {Schuld, Maria and Sinayskiy, Ilya and Petruccione, Francesco},
	year = {2015},
	note = {arXiv: 1412.3635
ISBN: 0375-9601},
	keywords = {Linear classification, Quantum computing, Quantum machine learning, Quantum neural network},
	pages = {660--663},
}

@article{bremner_average-case_2015,
	title = {Average-case complexity versus approximate simulation of commuting quantum computations},
	url = {http://arxiv.org/abs/1504.07999%0Ahttp://arxiv.org/abs/1504.07999},
	doi = {10.1103/PhysRevLett.117.080501},
	abstract = {We use the class of commuting quantum computations known as IQP (Instantaneous Quantum Polynomial time) to strengthen the conjecture that quantum computers are hard to simulate classically. We show that, if either of two plausible average-case hardness conjectures holds, then IQP computations are hard to simulate classically up to constant additive error. One conjecture relates to the hardness of estimating the complex-temperature partition function for random instances of the Ising model; the other concerns approximating the number of zeroes of random low-degree polynomials. We observe that both conjectures can be shown to be valid in the setting of worst-case complexity. We arrive at these conjectures by deriving spin-based generalisations of the Boson Sampling problem that avoid the so-called permanent anticoncentration conjecture.},
	author = {Bremner, Michael J. and Montanaro, Ashley and Shepherd, Dan J.},
	year = {2015},
	note = {arXiv: 1504.07999},
	pages = {1--10},
}

@article{lund_quantum_2017,
	title = {Quantum {Sampling} {Problems}, {BosonSampling} and {Quantum} {Supremacy}},
	url = {http://arxiv.org/abs/1702.03061},
	abstract = {There is a large body of evidence for the potential of greater computational power using information carriers that are quantum mechanical over those governed by the laws of classical mechanics. But the question of the exact nature of the power contributed by quantum mechanics remains only partially answered. Furthermore, there exists doubt over the practicality of achieving a large enough quantum computation that definitively demonstrates quantum supremacy. Recently the study of computational problems that produce samples from probability distributions has added to both our understanding of the power of quantum algorithms and lowered the requirements for demonstration of fast quantum algorithms. The proposed quantum sampling problems do not require a quantum computer capable of universal operations and also permit physically realistic errors in their operation. This is an encouraging step towards an experimental demonstration of quantum algorithmic supremacy. In this paper, we will review sampling problems and the arguments that have been used to deduce when sampling problems are hard for classical computers to simulate. Two classes of quantum sampling problems that demonstrate the supremacy of quantum algorithms are BosonSampling and IQP Sampling. We will present the details of these classes and recent experimental progress towards demonstrating quantum supremacy in BosonSampling.},
	author = {Lund, a. P. and Bremner, Michael J. and Ralph, T. C.},
	year = {2017},
	note = {arXiv: 1702.03061},
	pages = {1--10},
}

@article{gao_quantum_2016,
	title = {Quantum {Supremacy} for {Simulating} {A} {Translation}-{Invariant} {Ising} {Spin} {Model}},
	issn = {0031-9007},
	url = {http://arxiv.org/abs/1607.04947},
	doi = {10.1103/PhysRevLett.118.040502},
	abstract = {We introduce an intermediate quantum computing model built from translation-invariant Ising-interacting spins. Despite being non-universal, the model cannot be classically efficiently simulated unless the polynomial hierarchy collapses. Equipped with the intrinsic one-instance property, a single fixed unitary evolution in our model is sufficient to produce classically intractable results, compared to several other models that rely on average-case or worst-case hardness. We propose an implementation scheme for our Hamiltonian model using cold atoms in a square lattice, with the requirements well within the reach of current experimental technology. We formulate a scheme to certify the correct functioning of this quantum machine. The certification requires only a polynomial number of local measurements assuming measurement imperfections are sufficiently small.},
	number = {1},
	journal = {arXiv:1607.04947 [quant-ph]},
	author = {Gao, Xun and Wang, Sheng-Tao and Duan, Lu-Ming},
	year = {2016},
	note = {arXiv: 1607.04947},
	keywords = {Quantum Physics},
	pages = {1--13},
}

@article{bouland_complexity_2016,
	title = {Complexity classification of two-qubit commuting hamiltonians},
	volume = {39},
	issn = {18688969},
	doi = {10.4230/LIPIcs.CCC.2016.28},
	abstract = {We classify two-qubit commuting Hamiltonians in terms of their computational complexity. Suppose one has a two-qubit commuting Hamiltonian H which one can apply to any pair of qubits, starting in a computational basis state. We prove a dichotomy theorem: either this model is efficiently classically simulable or it allows one to sample from probability distributions which cannot be sampled from classically unless the polynomial hierarchy collapses. Furthermore, the only simulable Hamiltonians are those which fail to generate entanglement. This shows that generic two-qubit commuting Hamiltonians can be used to perform computational tasks which are intractable for classical computers under plausible assumptions. Our proof makes use of new postselection gadgets and Lie theory.},
	number = {39},
	journal = {arXiv},
	author = {Bouland, Adam and Mančinska, Laura and Zhang, Xue},
	year = {2016},
	note = {arXiv: 1602.04145v1
ISBN: 9783959770088},
	keywords = {()},
	pages = {1--34},
}

@article{kempe_complexity_2006,
	title = {The complexity of the local {Hamiltonian} problem},
	volume = {35},
	number = {5},
	journal = {SIAM J. Comput.},
	author = {Kempe, Julia and Kitaev, Alexei and Regev, Oded},
	year = {2006},
	keywords = {adiabatic, complete problems, local hamiltonian problem, quantum computation},
	pages = {1070--1097},
}

@article{leggett_kitaev_2009,
	title = {The {Kitaev} models},
	volume = {1},
	number = {2},
	author = {Leggett, a. J.},
	year = {2009},
	pages = {1--13},
}

@article{usher_non-unitary_2017,
	title = {Non-{Unitary} {Quantum} {Computation} in the {Ground} {Space} of {Local} {Hamiltonians}},
	url = {http://arxiv.org/abs/1703.08118},
	abstract = {A central result in the study of Quantum Hamiltonian Complexity is that the k-Local hamiltonian problem is QMA-complete. In that problem, we must decide if the lowest eigenvalue of a Hamiltonian is bounded below some value, or above another, promised one of these is true. Given the ground state of the Hamiltonian, a quantum computer can determine this question, even if the ground state itself may not be efficiently quantum preparable. Kitaev's proof of QMA-completeness encodes a unitary quantum circuit in QMA into the ground space of a Hamiltonian. However, we now have quantum computing models based on measurement instead of unitary evolution, furthermore we can use post-selected measurement as an additional computational tool. In this work, we generalise Kitaev's construction to allow for non-unitary evolution including post-selection. Furthermore, we consider a type of post-selection under which the construction is consistent, which we call tame post-selection. We consider the computational complexity consequences of this construction and then consider how the probability of an event upon which we are post-selecting affects the gap between the ground state energy and the energy of the first excited state of its corresponding Hamiltonian. We provide numerical evidence that the two are not immediately related, by giving a family of circuits where the probability of an event upon which we post-select is exponentially small, but the gap in the energy levels of the Hamiltonian decreases as a polynomial.},
	author = {Usher, Naïri and Hoban, Matty J. and Browne, Dan E.},
	year = {2017},
	note = {arXiv: 1703.08118},
	pages = {1--14},
}

@article{adcock_advances_2015,
	title = {Advances in quantum machine learning {arXiv} : 1512 . 02900v1 [ quant-ph ] 9 {Dec} 2015},
	author = {Adcock, J C and Allen, E and Day, M and Frick, S and Hinchliff, J and Johnson, M and Pallister, S and Price, A B and Stanisic, S},
	year = {2015},
	note = {arXiv: 1512.02900v1},
}

@article{ni_commuting_2012,
	title = {Commuting quantum circuits: efficient classical simulations versus hardness results},
	issn = {15337146},
	url = {http://arxiv.org/abs/1204.4570},
	abstract = {The study of quantum circuits composed of commuting gates is particularly useful to understand the delicate boundary between quantum and classical computation. Indeed, while being a restricted class, commuting circuits exhibit genuine quantum effects such as entanglement. In this paper we show that the computational power of commuting circuits exhibits a surprisingly rich structure. First we show that every 2-local commuting circuit acting on d-level systems and followed by single-qudit measurements can be efficiently simulated classically with high accuracy. In contrast, we prove that such strong simulations are hard for 3-local circuits. Using sampling methods we further show that all commuting circuits composed of exponentiated Pauli operators e\{{\textasciicircum}\}\{\{\}i\${\textbackslash}\$theta P\{\}\} can be simulated efficiently classically when followed by single-qubit measurements. Finally, we show that commuting circuits can efficiently simulate certain non-commutative processes, related in particular to constant-depth quantum circuits. This gives evidence that the power of commuting circuits goes beyond classical computation.},
	journal = {arXiv:1204.4570},
	author = {Ni, Xiaotong and Van den Nest, Maarten},
	year = {2012},
	note = {arXiv: 1204.4570},
	pages = {19},
}

@article{behrman_simulations_2000,
	title = {Simulations of quantum neural networks},
	volume = {128},
	issn = {00200255},
	doi = {10.1016/S0020-0255(00)00056-6},
	abstract = {We explore by simulation ways in which an array of quantum dot molecules could serve as a quantum neural computer. First, we show that a single quantum dot molecule evolving in real time can act as a recurrent temporal quantum neural network. Inputs are prepared by fixing the initial states of a quantum dot molecule, and outputs determined by reading its value at a given time T later. The nodes of the network are the instantaneous states of the molecule at successive time slices. The nodes interact indirectly through their mutual interaction with local and phononic modes of the substrate. These modes can be preferentially excited optically, and, therefore, controlled externally. The number of excitations can thus be used as trainable `weight' parameters for a neural network. This network is shown to perform classical logic gates. By preparing the input state as a superposition state, multiple inputs can be encoded as a single initial state. Second, we simulate the possibility of a spatial, rather than temporal, design, as a Hopfield net. The network consists of a regular array of quantum dot molecules on a suitable substrate. The molecules interact indirectly as before, and, now, with each other directly through Coulombic interactions. Both of the quantum networks have none of the `wiring problems' of traditional neural nets: the necessary connections are supplied by the physical system itself. Computation is performed by the intrinsic physics of the physical system. The long range character of the phononic interactions takes the net beyond traditional local connectionist structures. The hypothesized increase in complexity and power, in going to the quantum regime, is demonstrated. We train the quantum Hopfield net using simultaneous recurrent backpropagation.},
	number = {3},
	journal = {Information sciences},
	author = {Behrman, E. C. and Nash, L. R. and Steck, J. E. and Chandrashekar, V. G. and Skinner, S. R.},
	year = {2000},
	note = {ISBN: 0020-0255},
	pages = {257--269},
}

@article{gonz_tesis_2011,
	title = {{TESIS} {DOCTORAL} {T} ´ ıtulo de la tesis : {Independent} {Component} {Analysis} for {Time} {Series} {Autor} :},
	author = {Gonz, Ester},
	year = {2011},
}

@article{weigang_entangled_2008,
	title = {Entangled neural networks},
	journal = {Department of Computer Science, University of Brasilia …},
	author = {Weigang, Li},
	year = {2008},
	keywords = {consciousness, entanglement, networks, neuron a is orientated, quantum computing, teleportation, with intelligence},
	pages = {1--10},
}

@article{altaisky_quantum_2001,
	title = {Quantum neural network},
	issn = {00220000},
	url = {http://arxiv.org/abs/quant-ph/0107012},
	doi = {10.1006/jcss.2001.1769},
	abstract = {It is suggested that a quantum neural network (QNN), a type of artificial neural network, can be built using the principles of quantum information processing. The input and output qubits in the QNN can be implemented by optical modes with different polarization, the weights of the QNN can be implemented by optical beam splitters and phase shifters},
	number = {1},
	journal = {arXiv:quant-ph/0107012},
	author = {Altaisky, M. V.},
	year = {2001},
	note = {arXiv: quant-ph/0107012},
	pages = {1--4},
}

@article{assareh_bayesian_2013,
	title = {Bayesian change point estimation in {Poisson}-based control charts},
	volume = {9:32},
	issn = {2251-712X},
	doi = {10.1186/2251-712X-9-32},
	abstract = {Precise identification of the time when a process has changed enables process engineers to search for a potential special cause more effectively. In this paper, we develop change point estimation methods for a Poisson process in a Bayesian framework.We apply Bayesian hierarchicalmodels to formulate the change point where there exists a step change, a linear trend and a known multiple number of changes in the Poisson rate. The Markov chain Monte Carlo is used to obtain posterior distributions of the change point parameters and corresponding probabilistic intervals and inferences. The performance of the Bayesian estimator is investigated through simulations and the result shows that precise estimates can be obtained when they are used in conjunction with the well-known c-, Poisson exponentially weighted moving average (EWMA) and Poisson cumulative sum (CUSUM) control charts for different change type scenarios. We also apply the Deviance Information Criterion as amodel selection criterion in the Bayesian context, to find the best change point model for a given dataset where there is no prior knowledge about the change type in the process. In comparison with built-in estimators of EWMA and CUSUM charts and ML based estimators, the Bayesian estimator performs reasonably well and remains a strong alternative. These superiorities are enhanced when probability quantification, flexibility and generalizability of the Bayesian change point detectionmodel are also considered. Keywords:},
	number = {Montgomery 2008},
	journal = {Journal of Industrial Engineering International 2013, http://www.jiei-tsb.com/content/9/1/32 ORIGINAL},
	author = {Assareh, Hassan and Noorossana, Rassoul and Mengersen, Kerrie L},
	year = {2013},
	keywords = {bayesian hierarchical model, change point, control charts, markov chain monte carlo, poisson process},
	pages = {1--13},
}

@article{hyvarinen_independent_2001,
	title = {Independent {Component} {Analysis}},
	volume = {21},
	issn = {10635203},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520306000509\nhttp://doi.wiley.com/10.1002/0471221317},
	doi = {10.1002/0471221317},
	abstract = {A comprehensive introduction to ICA for students and practitioners Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more. Independent Component Analysis is divided into four sections that cover: General mathematical concepts utilized in the book The basic ICA model and its solution Various extensions of the basic ICA model Real-world applications for ICA models Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Hyvärinen, Aapo and Karhunen, Juha and Oja, Erkki},
	year = {2001},
	pmid = {20421937},
	note = {ISBN: 047140540X},
	pages = {135--144},
}

@article{schuld_quest_2014,
	title = {The quest for a {Quantum} {Neural} {Network}},
	volume = {13},
	issn = {15700755},
	doi = {10.1007/s11128-014-0809-8},
	abstract = {With the overwhelming success in the field of quantum information in the last decades, the "quest" for a Quantum Neural Network (QNN) model began in order to combine quantum computing with the striking properties of neural computing. This article presents a systematic approach to QNN research, which so far consists of a conglomeration of ideas and proposals. It outlines the challenge of combining the nonlinear, dissipative dynamics of neural computing and the linear, unitary dynamics of quantum computing. It establishes requirements for a meaningful QNN and reviews existing literature against these requirements. It is found that none of the proposals for a potential QNN model fully exploits both the advantages of quantum physics and computing in neural networks. An outlook on possible ways forward is given, emphasizing the idea of Open Quantum Neural Networks based on dissipative quantum computing.},
	number = {11},
	journal = {Quantum Information Processing},
	author = {Schuld, Maria and Sinayskiy, Ilya and Petruccione, Francesco},
	year = {2014},
	note = {arXiv: 1408.7005
ISBN: 1570-0755},
	keywords = {Artificial neural networks, Open quantum systems, Quantum Neural Networks, Quantum computing},
	pages = {2567--2586},
}

@article{analysis_18_nodate,
	title = {18 18.1},
	author = {Analysis, Time-series},
}

@article{mining_springer_2009,
	title = {Springer {Series} in {Statistics} {The} {Elements} of},
	volume = {27},
	issn = {03436993},
	doi = {10.1007/b94608},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
	number = {2},
	journal = {The Mathematical Intelligencer},
	author = {Mining, Data},
	year = {2009},
	pmid = {15512507},
	note = {ISBN: 9780387848570},
	pages = {83--85},
}

@article{kamenev_introduction_nodate,
	title = {Introduction to the {Keldysh} {NEGF}.pdf},
	author = {Kamenev, Alex},
}

@article{feynman_theory_1963,
	title = {The {Theory} of a {General} a {Linear} {Quantum} {Dissipative} {System} {System} {Interacting} with},
	volume = {173},
	author = {Feynman, R P and Vernon, F L},
	year = {1963},
	pages = {118--173},
}

@article{johnson_triplet-singlet_2005,
	title = {Triplet-singlet spin relaxation via nuclei in a double quantum dot.},
	volume = {435},
	issn = {0028-0836},
	doi = {10.1038/nature03815},
	abstract = {The spin of a confined electron, when oriented originally in some direction, will lose memory of that orientation after some time. Physical mechanisms leading to this relaxation of spin memory typically involve either coupling of the electron spin to its orbital motion or to nuclear spins. Relaxation of confined electron spin has been previously measured only for Zeeman or exchange split spin states, where spin-orbit effects dominate relaxation; spin flips due to nuclei have been observed in optical spectroscopy studies. Using an isolated GaAs double quantum dot defined by electrostatic gates and direct time domain measurements, we investigate in detail spin relaxation for arbitrary splitting of spin states. Here we show that electron spin flips are dominated by nuclear interactions and are slowed by several orders of magnitude when a magnetic field of a few millitesla is applied. These results have significant implications for spin-based information processing.},
	number = {7044},
	journal = {Nature},
	author = {Johnson, a C and Petta, J R and Taylor, J M and Yacoby, a and Lukin, M D and Marcus, C M and Hanson, M P and Gossard, a C},
	year = {2005},
	pmid = {15944715},
	note = {arXiv: cond-mat/0503687
ISBN: 0028-0836},
	pages = {925--928},
}

@article{segal_numerically_2010,
	title = {Numerically exact path-integral simulation of nonequilibrium quantum transport and dissipation},
	volume = {82},
	issn = {10980121},
	doi = {10.1103/PhysRevB.82.205323},
	abstract = {We develop an iterative, numerically exact approach for the treatment of nonequilibrium quantum transport and dissipation problems that avoids the real-time sign problem associated with standard Monte Carlo techniques. The method requires a well-defined decorrelation time of the non-local influence functional for proper convergence to the exact limit. Since finite decorrelation times may arise either from temperature or from a voltage drop at zero temperature, the approach is well suited for the description of the real-time dynamics of single-molecule devices and quantum dots driven to a steady-state via interaction with two or more electron leads. We numerically investigate two non-trivial models: the evolution of the nonequilibrium population of a two-level system coupled to two electronic reservoirs, and quantum transport in the nonequilibrium Anderson model. For the latter case, two distinct formulations are described. Results are compared to those obtained by other techniques.},
	number = {20},
	journal = {Physical Review B - Condensed Matter and Materials Physics},
	author = {Segal, Dvira and Millis, Andrew J. and Reichman, David R.},
	year = {2010},
	note = {arXiv: 1008.5200
ISBN: 1098-0121},
	pages = {1--29},
}

@article{adams_bayesian_2007,
	title = {Bayesian {Online} {Changepoint} {Detection}},
	url = {http://arxiv.org/abs/0710.3742},
	doi = {arXiv:0710.3742v1},
	abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
	author = {Adams, Ryan Prescott and MacKay, David J. C.},
	year = {2007},
	note = {arXiv: 0710.3742},
	pages = {7},
}

@article{shalizi_logistic_2012,
	title = {Logistic {Regression}},
	journal = {2012},
	author = {Shalizi, Cosma},
	year = {2012},
	pages = {223--237},
}

@article{dive_quantum_2015,
	title = {Quantum simulations of dissipative dynamics: time-dependence instead of size},
	journal = {arXiv preprint},
	author = {Dive, Benjamin and Mintert, Florian and Burgarth, Daniel},
	year = {2015},
	note = {arXiv: 1503.01010v1},
	pages = {1--7},
}

@article{bacon_sparse_2014,
	title = {Sparse {Quantum} {Codes} from {Quantum} {Circuits}},
	author = {Bacon, Dave and Flammia, Steven T. and Harrow, Aram W. and Shi, Jonathan},
	year = {2014},
	note = {arXiv: 1411.3334},
	pages = {1--23},
}

@article{viola_dynamical_1999,
	title = {Dynamical {Decoupling} of {Open} {Quantum} {Systems}},
	volume = {82},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.82.2417},
	abstract = {We propose a novel dynamical method for beating decoherence and dissipation in open quantum systems. We demonstrate the possibility of filtering out the effects of unwanted (not necessarily known) system-environment interactions and show that the noise-suppression procedure can be combined with the capability of retaining control over the effective dynamical evolution of the open quantum system. Implications for quantum information processing are discussed.},
	number = {March},
	journal = {Physical Review Letters},
	author = {Viola, Lorenza and Knill, Emanuel and Lloyd, Seth},
	year = {1999},
	note = {arXiv: quant-ph/9809071
ISBN: 0031-9007{\textbackslash}n1079-7114},
	pages = {2417--2421},
}

@article{bacon_operator_2006,
	title = {Operator quantum error-correcting subsystems for self-correcting quantum memories},
	volume = {73},
	issn = {10502947},
	doi = {10.1103/PhysRevA.73.012340},
	abstract = {The most general method for encoding quantum information is not to encode the information into a subspace of a Hilbert space, but to encode information into a subsystem of a Hilbert space. Recently this notion has led to a more general notion of quantum error correction known as operator quantum error correction. In standard quantum error correcting codes, one requires the ability to apply a procedure which exactly reverses on the error correcting subspace any correctable error. In contrast, for operator error correcting subsystems, the correction procedure need not undo the error which has occurred, but instead one must perform correction only modulo the subsystem structure. This does not lead to codes which differ from subspace codes, but does lead to recovery routines which explicitly make use of the subsystem structure. Here we present two examples of such operator error correcting subsystems. These examples are motivated by simple spatially local Hamiltonians on square and cubic lattices. In three dimensions we provide evidence, in the form a simple mean field theory, that our Hamiltonian gives rise to a system which is self-correcting. Such a system will be a natural high-temperature quantum memory, robust to noise without external intervening quantum error correction procedures.},
	number = {1},
	journal = {Physical Review A - Atomic, Molecular, and Optical Physics},
	author = {Bacon, Dave},
	year = {2006},
	note = {arXiv: quant-ph/0506023
ISBN: 1050-2947},
	pages = {1--17},
}

@article{keck_quantum_nodate,
	title = {Quantum optimal control within the rotating wave approximation},
	author = {Keck, Maximilian and Matthias, M M and Calarco, Tommaso and Montangero, Simone},
	note = {arXiv: 1502.07739v1},
}

@article{form_chapter_nodate,
	title = {Chapter 2 : {Quantum} {Master} {Equations}},
	number = {0},
	author = {Form, T H E Lindblad},
}

@article{englert_five_2002,
	title = {Five {Lectures} {On} {Dissipative} {Master} {Equations}},
	url = {http://arxiv.org/abs/quant-ph/0206116},
	doi = {10.1007/3-540-45855-7_2},
	abstract = {1 First Lecture: Basics   1.1 Physical Derivation of the Master Equation   1.2 Some Simple Implications   1.3 Steady State   1.4 Action to the Left   2 Second Lecture: Eigenvalues and Eigenvectors of L   2.1 A Simple Case First   2.2 The General Case   3 Third Lecture: Completeness of the Damping Bases   3.1 Phase Space Functions   3.2 Completeness of the Eigenvectors of L   3.3 Positivity Conservation   3.4 Lindblad Form of Liouville Operators   4 Fourth Lecture: Quantum-Optical Applications   4.1 Periodically Driven Damped Oscillator   4.2 Conditional and Unconditional Evolution   4.3 Physical Signicance of Statistical Operators   5 Fifth Lecture: Statistics of Detected Atoms   5.1 Correlation Functions   5.2 Waiting Time Statistics   5.3 Counting Statistics},
	author = {Englert, Berthold-Georg and Morigi, Giovanna},
	year = {2002},
	note = {arXiv: quant-ph/0206116
ISBN: 978-3-540-44354-4},
	pages = {58},
}

@article{banerjee_open_nodate,
	title = {Open {Quantum} {Systems}},
	author = {Banerjee, Subhashish},
	pages = {1--78},
}

@article{viola_random_2005,
	title = {Random decoupling schemes for quantum dynamical control and error suppression},
	volume = {94},
	issn = {00319007},
	doi = {10.1103/PhysRevLett.94.060502},
	abstract = {We present a general control-theoretic framework for constructing and analyzing random decoupling schemes, applicable to quantum dynamical control of arbitrary finite-dimensional composite systems. The basic idea is to design the control propagator according to a random rather than deterministic path on a group. We characterize the performance of random decoupling protocols, and identify control scenarios where they can significantly weaken time scale requirements as compared to cyclic counterparts. Implications for reliable quantum computation are discussed.},
	number = {February},
	journal = {Physical Review Letters},
	author = {Viola, Lorenza and Knill, Emanuel},
	year = {2005},
	pmid = {15783713},
	note = {arXiv: quant-ph/0511120},
	pages = {1--4},
}

@article{lidar_review_2012,
	title = {Review of {Decoherence} {Free} {Subspaces}, {Noiseless} {Subsystems}, and {Dynamical} {Decoupling}},
	issn = {00652385},
	url = {http://arxiv.org/abs/1208.5791},
	doi = {10.1002/9781118742631.ch11},
	abstract = {Quantum information requires protection from the adverse affects of decoherence and noise. This review provides an introduction to the theory of decoherence-free subspaces, noiseless subsystems, and dynamical decoupling. It addresses quantum information preservation as well protected computation.},
	journal = {arXiv preprint arXiv:1208.5791},
	author = {Lidar, Daniel a.},
	year = {2012},
	note = {arXiv: 1208.5791v2},
	pages = {1--34},
}

@article{cui_generalized_nodate,
	title = {Generalized {Graph} {States} {Based} on {Hadamard} {Matrices}},
	volume = {3},
	number = {2},
	author = {Cui, Shawn X and Yu, Nengkun and Zeng, Bei},
	note = {arXiv: 1502.07195v1},
	pages = {1--16},
}

@article{zhou_quantum_2003,
	title = {Quantum computation based on d-level cluster state},
	volume = {68},
	issn = {1050-2947},
	doi = {10.1103/PhysRevA.68.062303},
	abstract = {The concept of a qudit (a d-level system) cluster state is proposed by generalizing the qubit cluster state [Phys. Rev. Lett. 86, 910 (2001)] to higher-dimensional Hilbert space according to the finite-dimensional representations of quantum plane algebra. We demonstrate their quantum correlations and prove a theorem which guarantees the availability of the qudit cluster states in quantum computation. We explicitly construct the network to show the universality of the one-way computer based on the defined qudit cluster states and single-qudit measurement. A protocol of implementing one-way quantum computer is suggested using the high-dimensional “Ising” model which can be found in many magnetic systems.},
	journal = {Physical Review A},
	author = {Zhou, D. and Zeng, B. and Xu, Z. and Sun, C.},
	year = {2003},
	note = {arXiv: quant-ph/0304054},
	pages = {1--15},
}

@article{hall_cluster_2005,
	title = {Cluster state quantum computation for many-level systems},
	issn = {15337146},
	url = {http://arxiv.org/abs/quant-ph/0512130},
	abstract = {The cluster state model for quantum computation [Phys. Rev. Lett. 86, 5188] outlines a scheme that allows one to use measurement on a large set of entangled quantum systems in what is known as a cluster state to undertake quantum computations. The model itself and many works dedicated to it involve using entangled qubits. In this paper we consider the issue of using entangled qudits instead. We present a complete framework for cluster state quantum computation using qudits, which not only contains the features of the original qubit model but also contains the new idea of adaptive computation: via a change in the classical computation that helps to correct the errors that are inherent in the model, the implemented quantum computation can be changed. This feature arises through the extra degrees of freedom that appear when using qudits. Finally, for prime dimensions, we give a very explicit description of the model, making use of mutually unbiased bases.},
	author = {Hall, William},
	year = {2005},
	note = {arXiv: quant-ph/0512130},
	pages = {26},
}

@article{werlang_quantum_2010,
	title = {Quantum correlations in spin chains at finite temperatures and quantum phase transitions},
	volume = {105},
	issn = {00319007},
	doi = {10.1103/PhysRevLett.105.095702},
	abstract = {We compute the quantum correlation [quantum discord (QD)] and the entanglement (EOF) between nearest-neighbor qubits (spin-1/2) in an infinite chain described by the Heisenberg model (XXZ Hamiltonian) at finite temperatures. The chain is in the thermodynamic limit and thermalized with a reservoir at temperature T (canonical ensemble). We show that QD, in contrast to EOF and other thermodynamic quantities, spotlight the critical points associated with quantum phase transitions (QPT) for this model even at finite T. This remarkable property of QD may have important implications for experimental characterization of QPTs when one is unable to reach temperatures below which a QPT can be seen.},
	journal = {Physical Review Letters},
	author = {Werlang, T. and Trippe, C. and Ribeiro, G. a P and Rigolin, Gustavo},
	year = {2010},
	pmid = {20868176},
	note = {arXiv: 1006.3332v2},
	pages = {2--5},
}

@article{roncaglia_hidden_2010,
	title = {Hidden {XY} structure of the bond-charge {Hubbard} model},
	volume = {82},
	issn = {10980121},
	doi = {10.1103/PhysRevB.82.233105},
	abstract = {The repulsive one-dimensional Hubbard model with bond-charge interaction (HBC) in the superconducting regime is mapped onto the spin-1/2 XY model with transverse field. We calculate correlations and phase boundaries, realizing an excellent agreement with numerical results. The critical line for the superconducting transition is shown to coincide with the analytical factorization line identifying the commensurate-incommensurate transition in the XY model.},
	journal = {Physical Review B - Condensed Matter and Materials Physics},
	author = {Roncaglia, Marco and Boschi, Cristian Degli Esposti and Montorsi, Arianna},
	year = {2010},
	note = {arXiv: 1010.0159},
	pages = {2--5},
}

@article{paredes_tonks-girardeau_2004,
	title = {Tonks-{Girardeau} gas of ultracold atoms in an optical lattice},
	volume = {429},
	issn = {1476-4679},
	url = {papers://fe3c0465-e0d0-4e90-a74d-7e674c71995a/Paper/p3832},
	doi = {10.1038/nature02578.},
	abstract = {Strongly correlated quantum systems are among the most intriguing and fundamental systems in physics. One such example is the Tonks-Girardeau gas(1,2), proposed about 40 years ago, but until now lacking experimental realization; in such a gas, the repulsive interactions between bosonic particles confined to one dimension dominate the physics of the system. In order to minimize their mutual repulsion, the bosons are prevented from occupying the same position in space. This mimics the Pauli exclusion principle for fermions, causing the bosonic particles to exhibit fermionic properties(1,2). However, such bosons do not exhibit completely ideal fermionic ( or bosonic) quantum behaviour; for example, this is reflected in their characteristic momentum distribution(3). Here we report the preparation of a Tonks-Girardeau gas of ultracold rubidium atoms held in a two-dimensional optical lattice formed by two orthogonal standing waves. The addition of a third, shallower lattice potential along the long axis of the quantum gases allows us to enter the Tonks-Girardeau regime by increasing the atoms' effective mass and thereby enhancing the role of interactions. We make a theoretical prediction of the momentum distribution based on an approach in which trapped bosons acquire fermionic properties, finding that it agrees closely with the measured distribution.},
	journal = {Nature},
	author = {Paredes, B and Widera, a and Murg, V and Mandel, O and Foelling, S and Cirac, I and Shlyapnikov, G V and Hansch, T W and Bloch, I},
	year = {2004},
	pmid = {15152247},
	note = {ISBN: 0028-0836},
	keywords = {Bose-Einstein Condensation, Ground State, Impenetrable Bosons, Insulator, Molecules, Superfluid, Transition},
	pages = {277--281},
}

@article{article_thermometry_2014,
	title = {Thermometry {Precision} in {Strongly} {Correlated} ultracold lattice gases},
	author = {Article, Special Issue and Barcelona, Universitat Autonoma De and Physics, Optical and Chiara, De},
	year = {2014},
}

@article{noauthor_quantum_nodate,
	title = {Quantum criticality as a resource for quantum estimation {Paolo}},
	issn = {2045-2322},
	url = {http://arxiv.org/abs/1309.2446},
	doi = {10.1038/srep06956},
	pmid = {25378231},
	note = {arXiv: 1309.2446},
}

@article{liu_quantum_2014,
	title = {Quantum {Fisher} information for density matrices with arbitrary ranks},
	volume = {61},
	issn = {02536102},
	doi = {10.1088/0253-6102/61/1/08},
	journal = {Communications in Theoretical Physics},
	author = {Liu, J and Jing, Xx and Zhong, W and Wang, Xg},
	year = {2014},
	note = {arXiv: 1312.6910v1},
	keywords = {density matrix, quantum fisher information, quantum metrology},
	pages = {45--50},
}

@article{correa_individual_2014,
	title = {Individual quantum probes for optimal thermometry},
	author = {Correa, Luis A and Mehboudi, Mohammad and Adesso, Gerardo and Sanpera, Anna},
	year = {2014},
	note = {arXiv: 1411.2437v2},
	pages = {1--5},
}

@article{arenz_distinguishing_2014,
	title = {Distinguishing decoherence from alternative quantum theories by dynamical decoupling},
	url = {http://arxiv.org/abs/1405.7644},
	abstract = {A longstanding challenge in the foundations of quantum mechanics is the verification of alternative collapse theories despite their mathematical similarity to decoherence. To this end, we suggest a novel method based on dynamical decoupling. Experimental observation of nonzero saturation of the decoupling error in the limit of fast decoupling operations can provide evidence for alternative quantum theories.},
	journal = {arXiv},
	author = {Arenz, Christian and Hillier, Robin and Burgarth, Daniel},
	year = {2014},
	note = {arXiv: 1405.7644},
	pages = {4},
}

@article{bermudez_electron-mediated_2011,
	title = {Electron-mediated nuclear-spin interactions between distant nitrogen-vacancy centers},
	volume = {107},
	issn = {00319007},
	doi = {10.1103/PhysRevLett.107.150503},
	abstract = {We propose a scheme enabling controlled quantum coherent interactions between separated nitrogen-vacancy centers in diamond in the presence of strong magnetic fluctuations. The proposed scheme couples nuclear qubits employing the magnetic dipole-dipole interaction between the electron spins and, crucially, benefits from the suppression of the effect of environmental magnetic field fluctuations thanks to a strong microwave driving. This scheme provides a basic building block for a full-scale quantum information processor or quantum simulator based on solid-state technology.},
	journal = {Physical Review Letters},
	author = {Bermudez, a. and Jelezko, F. and Plenio, M. B. and Retzker, a.},
	year = {2011},
	pmid = {22107276},
	note = {arXiv: 1107.2617
ISBN: 0031-9007},
	pages = {1--9},
}

@article{cao_hamiltonian_2015,
	title = {Hamiltonian gadgets with reduced resource requirements},
	volume = {012315},
	doi = {10.1103/PhysRevA.91.012315},
	author = {Cao, Yudong and Babbush, Ryan and Biamonte, Jacob and Kais, Sabre},
	year = {2015},
	pages = {1--25},
}

@article{srinivasa_tunable_2013,
	title = {Tunable {Spin} {Qubit} {Coupling} {Mediated} by a {Multi}-{Electron} {Quantum} {Dot}},
	url = {http://arxiv.org/abs/1312.1711},
	abstract = {We present an approach for entangling electron spin qubits localized on spatially separated impurity atoms or quantum dots via a multi-electron, two-level quantum dot. The effective exchange interaction mediated by the dot can be understood as the simplest manifestation of Ruderman-Kittel-Kasuya-Yosida exchange, and can be manipulated through gate voltage control of level splittings and tunneling amplitudes within the system. This provides both a high degree of tuneability and a means for realizing high-fidelity two-qubit gates between spatially separated spins, yielding an experimentally accessible method of coupling donor electron spins in silicon via a hybrid impurity-dot system.},
	urldate = {2015-01-30},
	author = {Srinivasa, V. and Xu, H. and Taylor, J. M.},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.1711
Genre: Mesoscale and Nanoscale Physics},
	pages = {6},
}

@article{mehl_two-qubit_2014,
	title = {Two-{Qubit} {Couplings} of {Singlet}-{Triplet} {Qubits} {Mediated} by {One} {Quantum} {State}},
	author = {Mehl, Sebastian and Bluhm, Hendrik and Divincenzo, David P},
	year = {2014},
	note = {arXiv: 1403.2910v2},
	pages = {1--12},
}

@article{broadbent_universal_2008,
	title = {Universal blind quantum computation},
	url = {http://arxiv.org/abs/0807.4154},
	doi = {10.1109/FOCS.2009.36},
	abstract = {We present a protocol which allows a client to have a server carry out a quantum computation for her such that the client's inputs, outputs and computation remain perfectly private, and where she does not require any quantum computational power or memory. The client only needs to be able to prepare single qubits randomly chosen from a finite set and send them to the server, who has the balance of the required quantum computational resources. Our protocol is interactive: after the initial preparation of quantum states, the client and server use two-way classical communication which enables the client to drive the computation, giving single-qubit measurement instructions to the server, depending on previous measurement outcomes. Our protocol works for inputs and outputs that are either classical or quantum. We give an authentication protocol that allows the client to detect an interfering server; our scheme can also be made fault-tolerant.   We also generalize our result to the setting of a purely classical client who communicates classically with two non-communicating entangled servers, in order to perform a blind quantum computation. By incorporating the authentication protocol, we show that any problem in BQP has an entangled two-prover interactive proof with a purely classical verifier.   Our protocol is the first universal scheme which detects a cheating server, as well as the first protocol which does not require any quantum computation whatsoever on the client's side. The novelty of our approach is in using the unique features of measurement-based quantum computing which allows us to clearly distinguish between the quantum and classical aspects of a quantum computation.},
	urldate = {2015-01-29},
	author = {Broadbent, Anne and Fitzsimons, Joseph and Kashefi, Elham},
	month = jul,
	year = {2008},
	note = {arXiv: 0807.4154
Genre: Quantum Physics},
	pages = {20},
}

@article{khitun_no_nodate,
	title = {No {Title}},
	author = {Khitun, Alexander and Bao, Mingqiang and Wang, Kang L},
	pages = {1--46},
}

@article{kieferova_power_2014,
	title = {On {The} {Power} {Of} {Coherently} {Controlled} {Quantum} {Adiabatic} {Evolutions}},
	url = {http://arxiv.org/abs/1403.6545},
	abstract = {A major challenge facing adiabatic quantum computing is that algorithm design and error correction can be difficult for adiabatic quantum computing. Recent work has considered addressing his challenge by using coherently controlled adiabatic evolutions in the place of classically controlled evolution. An important question remains: what is the relative power of controlled adiabatic evolution to traditional adiabatic evolutions? We address this by showing that coherent control and measurement provides a way to average different adiabatic evolutions in ways that cause their diabatic errors to cancel, allowing for adiabatic evolutions to combine the best characteristics of existing adiabatic optimizations strategies that are mutually exclusive in conventional adiabatic QIP. This result shows that coherent control and measurement can provide advantages for adiabatic state preparation. We also provide upper bounds on the complexity of simulating such evolutions on a circuit based quantum computer and provide sufficiency conditions for the equivalence of controlled adiabatic evolutions to adiabatic quantum computing.},
	urldate = {2015-01-28},
	author = {Kieferova, Maria and Wiebe, Nathan},
	month = mar,
	year = {2014},
	note = {arXiv: 1403.6545
Genre: Quantum Physics},
	pages = {20},
}

@article{hardy_chapter_nodate,
	title = {Chapter 5 {Characters} and {Character}},
	author = {Hardy, Godfrey H},
}

@article{we_chapter_nodate,
	title = {Chapter 4 {Properties} of {Irreducible}},
	author = {We, Alembert},
}

@article{weinberg_chapter_1979,
	title = {Chapter 6: {Groups} and {Representations} in quantum mechanics},
	author = {Weinberg, Steven and Glashow, Sheldon},
	year = {1979},
	pages = {83--106},
}

@article{zhemchuzhna_coulomb_2015,
	title = {Coulomb {Excitations} for a {Short} {Linear} {Chain} of {Metallic} {Shells}},
	url = {http://arxiv.org/abs/1501.0583},
	abstract = {A self-consistent-field theory is given for the electronic collective modes of a chain containing a finite number, \$N\$, of Coulomb-coupled spherical two-dimensional electron gases (S2DE's) arranged with their centers along a straight line, simulating a narrow micro-ribbon of metallic shells. The separation between nearest-neighbor shells is arbitrary and because of the quantization of the electron energy levels due to their confinement to the spherical surface, all angular momenta \$L\$ of the Coulomb excitations and their projections \$M\$ on the quantization axis are coupled. However, for incoming light with a specific polarization, only one angular momentum quantum number is chosen. We show that when \$N=3\$ the next-nearest-neighbor Coulomb coupling is larger than its value if they are located at opposite ends of a right-angle triangle forming the triad. Additionally, the frequencies of the plasma excitations depend on the orientation of the line joining them with respect to the axis of quantization since the magnetic field generated from the induced oscillating electric dipole moment on one sphere can couple to the induced magnetic dipole moment on another.},
	urldate = {2015-01-27},
	author = {Zhemchuzhna, Liubov and Gumbs, Godfrey and Iurov, Andrii and Huang, Danhong and Gao, Bo},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.0583
Genre: Mesoscale and Nanoscale Physics; Materials Science},
	pages = {13},
}

@article{principle_chapter_2010,
	title = {Chapter 8 {Identical} particles , indistinguishability , and interchange invariance},
	author = {Principle, The and Postulate, The Symmetrization and Pauli, The},
	year = {2010},
}

@article{mintert_ion-trap_2001,
	title = {Ion-trap quantum logic using long-wavelength radiation},
	url = {http://arxiv.org/abs/quant-ph/0104041},
	doi = {10.1103/PhysRevLett.87.257904},
	abstract = {A quantum information processor is proposed that combines experimental techniques and technology successfully demonstrated either in nuclear magnetic resonance experiments or with trapped ions. An additional inhomogenenous magnetic field applied to an ion trap i) shifts individual ionic resonances (qubits), making them distinguishable by frequency, and, ii) mediates the coupling between internal and external degrees of freedom of trapped ions. This scheme permits one to individually address and coherently manipulate ions confined in an electrodynamic trap using radiation in the radiofrequency or microwave regime.},
	urldate = {2015-01-22},
	author = {Mintert, Florian and Wunderlich, Christof},
	month = apr,
	year = {2001},
	note = {arXiv: quant-ph/0104041
Genre: Quantum Physics},
	pages = {4},
}

@article{rebenda_new_2015,
	title = {A new semi-analytical approach for numerical solving of {Cauchy} problem for functional differential equations},
	url = {http://arxiv.org/abs/1501.0411},
	abstract = {One of the major challenges of contemporary mathematics is numerical solving of various problems for functional differential equations (FDE), in particular Cauchy problem for delayed and neutral differential equations. Recently large variety of methods to handle this task appeared. In the paper, we present new semi-analytical approach for FDE's consisting in combination of the method of steps and a technique called differential transformation method (DTM). This approach reduces the original Cauchy problem for delayed or neutral differential equation to Cauchy problem for ordinary differential equation for which DTM is convenient and efficient method. Moreover, there is no need of any symbolic calculations or initial approximation guesstimates in contrast to methods like homotopy analysis method, homotopy perturbation method, variational iteration method or Adomian decomposition method. The efficiency of the proposed method is shown on certain classes of FDE's with multiple constant delays including FDE of neutral type. We also compare it to the current approach of using DTM and the Adomian decomposition method where Cauchy problem is not well posed.},
	urldate = {2015-01-20},
	author = {Rebenda, Josef and Šmarda, Zdeněk and Khan, Yasir},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.0411
Genre: Classical Analysis and ODEs; Numerical Analysis},
}

@article{vedensky_chapter_nodate,
	title = {Chapter 2 {Elements} of {Abstract} {Group} {Groups} : {Definitions} and {Examples}},
	author = {Vedensky, Dimitri},
}

@article{vedensky_chapter_nodate-1,
	title = {Chapter 1},
	author = {Vedensky, Dimitri},
	pages = {1--12},
}

@book{vedensky_chapter_1986,
	title = {Chapter 3},
	urldate = {2015-01-20},
	author = {Vedensky, Dimitri},
	year = {1986},
}

@article{jordan_permutational_2009,
	title = {Permutational {Quantum} {Computing}},
	url = {http://arxiv.org/abs/0906.2508},
	abstract = {In topological quantum computation the geometric details of a particle trajectory are irrelevant; only the topology matters. Taking this one step further, we consider a model of computation that disregards even the topology of the particle trajectory, and computes by permuting particles. Whereas topological quantum computation requires anyons, permutational quantum computation can be performed with ordinary spin-1/2 particles, using a variant of the spin-network scheme of Marzuoli and Rasetti. We do not know whether permutational computation is universal. It may represent a new complexity class within BQP. Nevertheless, permutational quantum computers can in polynomial time approximate matrix elements of certain irreducible representations of the symmetric group and simulate certain processes in the Ponzano-Regge spin foam model of quantum gravity. No polynomial time classical algorithms for these problems are known.},
	urldate = {2015-01-13},
	author = {Jordan, Stephen P.},
	month = jun,
	year = {2009},
	note = {arXiv: 0906.2508
Genre: Quantum Physics},
	pages = {25},
}

@article{gard_introduction_2014,
	title = {An introduction to boson-sampling},
	url = {http://arxiv.org/abs/1406.6767v1},
	urldate = {2014-12-08},
	author = {Gard, Bryan T. and Motes, Keith R. and Olson, Jonathan P. and Rohde, Peter P. and Dowling, Jonathan P.},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.6767},
	pages = {1--13},
}

@article{giovannetti_quantum-enhanced_2003,
	title = {Quantum-{Enhanced} {Measurements} : {Beating} the {Standard} {Quantum} {Limit}},
	volume = {1929},
	number = {1985},
	author = {Giovannetti, Vittorio and Lloyd, Seth and Maccone, Lorenzo},
	year = {2003},
}

@article{abrams_nonlinear_1998,
	title = {Nonlinear {Quantum} {Mechanics} {Implies} {Polynomial}-{Time} {Solution} for {NP} -{Complete} and \# {P} {Problems}},
	author = {Abrams, Daniel S and Lloyd, Seth},
	year = {1998},
}

@article{burgarth_non-abelian_2013,
	title = {Non-{Abelian} phases from quantum {Zeno} dynamics},
	volume = {88},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.88.042107},
	doi = {10.1103/PhysRevA.88.042107},
	number = {4},
	urldate = {2015-01-08},
	journal = {Physical Review A},
	author = {Burgarth, Daniel and Facchi, Paolo and Giovannetti, Vittorio and Nakazato, Hiromichi and Pascazio, Saverio and Yuasa, Kazuya},
	month = oct,
	year = {2013},
	pages = {042107},
}

@article{aaronson_space_2014,
	title = {The space "just above" {BQP}},
	url = {http://arxiv.org/abs/1412.6507},
	abstract = {We explore the space "just above" BQP by defining a complexity class PDQP (Product Dynamical Quantum Polynomial time) which is larger than BQP but does not contain NP relative to an oracle. The class is defined by imagining that quantum computers can perform measurements that do not collapse the wavefunction. This (non-physical) model of computation can efficiently solve problems such as Graph Isomorphism and Approximate Shortest Vector which are believed to be intractable for quantum computers. Furthermore, it can search an unstructured N-element list in \${\textbackslash}tilde O\$(N{\textasciicircum}\{1/3\}) time, but no faster than \{{\textbackslash}Omega\}(N{\textasciicircum}\{1/4\}), and hence cannot solve NP-hard problems in a black box manner. In short, this model of computation is more powerful than standard quantum computation, but only slightly so.   Our work is inspired by previous work of Aaronson on the power of sampling the histories of hidden variables. However Aaronson's work contains an error in its proof of the lower bound for search, and hence it is unclear whether or not his model allows for search in logarithmic time. Our work can be viewed as a conceptual simplification of Aaronson's approach, with a provable polynomial lower bound for search.},
	urldate = {2014-12-23},
	author = {Aaronson, Scott and Bouland, Adam and Fitzsimons, Joseph and Lee, Mitchell},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6507
Genre: Quantum Physics; Computational Complexity},
	pages = {24},
}

@article{biamonte_non-perturbative_2008,
	title = {Non-perturbative k-body to two-body commuting conversion {Hamiltonians} and embedding problem instances into {Ising} spins},
	url = {http://arxiv.org/abs/0801.3800},
	doi = {10.1103/PhysRevA.77.052331},
	abstract = {An algebraic method has been developed which allows one to engineer several energy levels including the low-energy subspace of interacting spin systems. By introducing ancillary qubits, this approach allows k-body interactions to be captured exactly using 2-body Hamiltonians. Our method works when all terms in the Hamiltonian share the same basis and has no dependence on perturbation theory or the associated large spectral gap. Our methods allow problem instance solutions to be embedded into the ground energy state of Ising spin systems. Adiabatic evolution might then be used to place a computational system into it's ground state.},
	urldate = {2014-12-19},
	author = {Biamonte, J. D.},
	month = jan,
	year = {2008},
	note = {arXiv: 0801.3800
Genre: Quantum Physics},
	pages = {1--9},
}

@article{wallman_nonlocality_2014,
	title = {Nonlocality in instantaneous quantum circuits},
	author = {Wallman, Joel J and Adlam, Emily},
	year = {2014},
	note = {arXiv: 1412.4131v1},
	pages = {1--5},
}

@article{wilf_generatingfunctionology_1994,
	title = {generatingfunctionology},
	author = {Wilf, Herbert S},
	year = {1994},
}

@article{mcmahan_multiple_1973,
	title = {Multiple {Exchange} in the {Quantum} {Crystals}},
	volume = {1830},
	url = {http://journals.aps.org/pra/abstract/10.1103/PhysRevA.7.1105},
	number = {1968},
	urldate = {2014-12-10},
	journal = {Physical Review A},
	author = {McMahan, AK and Guyer, RA},
	year = {1973},
}

@article{noauthor_generating_nodate,
	title = {Generating functions},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Generating+Functions#7},
	urldate = {2014-12-15},
}

@article{mcmahan_critique_1972,
	title = {Critique of recent theories of exchange in solid {3He}},
	volume = {8},
	url = {http://link.springer.com/article/10.1007/BF00655553},
	urldate = {2014-12-10},
	journal = {Journal of Low Temperature Physics},
	author = {McMahan, AK},
	year = {1972},
	pages = {115--158},
}

@article{molkculks_no_1962,
	title = {No {Title}},
	author = {Molkculks, O F},
	year = {1962},
}

@article{morimae_hardness_2014,
	title = {Hardness of {Classically} {Simulating} the {One}-{Clean}-{Qubit} {Model}},
	volume = {112},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.112.130502},
	doi = {10.1103/PhysRevLett.112.130502},
	number = {13},
	urldate = {2014-12-02},
	journal = {Physical Review Letters},
	author = {Morimae, Tomoyuki and Fujii, Keisuke and Fitzsimons, Joseph F.},
	month = apr,
	year = {2014},
	pages = {130502},
}

@article{fenner_bounds_2008,
	title = {Bounds on the {Power} of {Constant}-{Depth} {Quantum} {Circuits}},
	author = {Fenner, S},
	year = {2008},
	note = {arXiv: quant-ph/0312209v2},
	pages = {1--16},
}

@article{hoban_measurement-based_2014,
	title = {Measurement-{Based} {Classical} {Computation}},
	volume = {112},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.112.140505},
	doi = {10.1103/PhysRevLett.112.140505},
	number = {14},
	journal = {Physical Review Letters},
	author = {Hoban, Matty J. and Wallman, Joel J. and Anwar, Hussain and Usher, Naïri and Raussendorf, Robert and Browne, Dan E.},
	month = apr,
	year = {2014},
	note = {arXiv: 1304.2667v5},
	pages = {140505},
}

@article{bansal_classical_2013,
	title = {Classical approximation schemes for the ground-state energy of quantum and classical {Ising} spin {Hamiltonians} on planar graphs},
	author = {Bansal, Nikhil and Terhal, Barbara M},
	year = {2013},
	note = {arXiv: 0705.1115v4},
	pages = {1--16},
}

@article{nakata_diagonal_2014,
	title = {Diagonal quantum circuits: {Their} computational power and applications},
	volume = {129},
	issn = {2190-5444},
	url = {http://link.springer.com/10.1140/epjp/i2014-14152-9},
	doi = {10.1140/epjp/i2014-14152-9},
	number = {7},
	urldate = {2014-12-01},
	journal = {The European Physical Journal Plus},
	author = {Nakata, Yoshifumi and Murao, Mio},
	month = jul,
	year = {2014},
	pages = {152},
}

@article{schuch_computational_2007,
	title = {Computational {Complexity} of {Projected} {Entangled} {Pair} {States}},
	volume = {98},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.98.140506},
	doi = {10.1103/PhysRevLett.98.140506},
	number = {14},
	urldate = {2014-12-05},
	journal = {Physical Review Letters},
	author = {Schuch, Norbert and Wolf, Michael and Verstraete, Frank and Cirac, J.},
	month = apr,
	year = {2007},
	pages = {140506},
}

@article{lloyd_closed_2011,
	title = {Closed {Timelike} {Curves} via {Postselection}: {Theory} and {Experimental} {Test} of {Consistency}},
	volume = {106},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.106.040403},
	doi = {10.1103/PhysRevLett.106.040403},
	number = {4},
	urldate = {2014-12-05},
	journal = {Physical Review Letters},
	author = {Lloyd, Seth and Maccone, Lorenzo and Garcia-Patron, Raul and Giovannetti, Vittorio and Shikano, Yutaka and Pirandola, Stefano and Rozema, Lee a. and Darabi, Ardavan and Soudagar, Yasaman and Shalm, Lynden K. and Steinberg, Aephraim M.},
	month = jan,
	year = {2011},
	pages = {040403},
}

@article{lloyd_quantum_2011,
	title = {Quantum mechanics of time travel through post-selected teleportation},
	volume = {84},
	issn = {1550-7998},
	url = {http://link.aps.org/doi/10.1103/PhysRevD.84.025007},
	doi = {10.1103/PhysRevD.84.025007},
	number = {2},
	urldate = {2014-12-05},
	journal = {Physical Review D},
	author = {Lloyd, Seth and Maccone, Lorenzo and Garcia-Patron, Raul and Giovannetti, Vittorio and Shikano, Yutaka},
	month = jul,
	year = {2011},
	pages = {025007},
}

@article{shepherd_temporally_2009,
	title = {Temporally unstructured quantum computation},
	volume = {465},
	issn = {1364-5021},
	url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.2008.0443},
	doi = {10.1098/rspa.2008.0443},
	number = {2105},
	urldate = {2014-12-05},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Shepherd, D. and Bremner, M. J.},
	month = feb,
	year = {2009},
	keywords = {abelian dynamics, binary matroids, quantum algorithm, quantum computational complexity},
	pages = {1413--1439},
}

@article{tripathi_complexity_nodate,
	title = {Complexity {Upper} {Bounds} for {Classical} {Locally} {Random} {Reductions} {Using} a {Quantum} {Computational} {Argument}},
	author = {Tripathi, Rahul},
}

@article{gross_computational_2008,
	title = {Computational power of quantum many-body states and some results on discrete phase spaces},
	author = {Gross, David},
	year = {2008},
}

@article{de_beaudrap_power_2014,
	title = {On the power of "modal quantum" computation},
	url = {http://arxiv.org/abs/1405.7381v1},
	number = {May},
	urldate = {2014-12-05},
	author = {de Beaudrap, Niel},
	month = may,
	year = {2014},
	note = {arXiv: 1405.7381},
	keywords = {()},
	pages = {1--35},
}

@article{devin_postselected_2014,
	title = {Postselected quantum circuits},
	url = {http://arxiv.org/abs/1405.6632v1},
	urldate = {2014-12-05},
	author = {Devin, Michael},
	month = may,
	year = {2014},
	note = {arXiv: 1405.6632},
}

@article{aaronson_are_2005,
	title = {Are {Quantum} {States} {Exponentially} {Long} {Vectors} ?},
	author = {Aaronson, Scott},
	year = {2005},
	note = {arXiv: quant-ph/0507242v1},
	pages = {1--5},
}

@article{shi_both_nodate,
	title = {Both {Toffoli} and {Controlled}-{NOT} need little help to do universal quantum computation 1},
	author = {Shi, Yaoyun},
	note = {arXiv: quant-ph/0205115v2},
	keywords = {quantum circuit, toffoli, universal basis, universal quantum computation},
	pages = {1--11},
}

@article{fujii_computational_2014,
	title = {Computational quantum-classical boundary of commuting quantum circuits},
	author = {Fujii, Keisuke and Tamate, Shuhei},
	year = {2014},
	note = {arXiv: 1406.6932v1},
	pages = {1--8},
}

@article{goldberg_complexity_2014,
	title = {The complexity of approximating complex-valued {Ising} and {Tutte} partition functions with applications to quantum simulation},
	url = {http://arxiv.org/abs/1409.5627v2},
	urldate = {2014-12-02},
	author = {Goldberg, Leslie Ann and Guo, Heng},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.5627},
	pages = {1--36},
}

@article{ni_commuting_2012,
	title = {Commuting quantum circuits: efficient classical simulations versus hardness results},
	url = {http://arxiv.org/abs/1204.4570v1},
	urldate = {2014-12-02},
	author = {Ni, Xiaotong and Nest, Maarten Van Den},
	month = apr,
	year = {2012},
	note = {arXiv: 1204.4570},
	pages = {1--19},
}

@article{lovett_bounded-depth_2012,
	title = {Bounded-{Depth} {Circuits} {Cannot} {Sample} {Good} {Codes}},
	volume = {21},
	issn = {1016-3328},
	url = {http://link.springer.com/10.1007/s00037-012-0039-3},
	doi = {10.1007/s00037-012-0039-3},
	number = {2},
	urldate = {2014-11-28},
	journal = {Computational Complexity},
	author = {Lovett, Shachar and Viola, Emanuele},
	month = mar,
	year = {2012},
	keywords = {68q17, isoperimetric inequalities, noise sensitivity, sampling, small depth circuits, subject classification},
	pages = {245--266},
}

@article{mckague_power_2013,
	title = {On the power quantum computation over real {Hilbert} spaces},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0219749913500019},
	number = {2},
	urldate = {2014-11-28},
	journal = {International Journal of Quantum Information},
	author = {McKague, M},
	year = {2013},
	note = {arXiv: 1109.0795v1},
	pages = {1--14},
}

@article{aaronson_forrelation_2013,
	title = {Forrelation : {A} {Problem} that {Optimally} {Separates} {Quantum} from},
	volume = {600700},
	author = {Aaronson, Scott and Ambainis, Andris},
	year = {2013},
	note = {arXiv: 1411.5729v1},
	pages = {1--60},
}

@article{lea_could_2000,
	title = {Could we {Quantum} {Compute} with {Electrons} on {Helium}?},
	volume = {48},
	issn = {00158208},
	url = {http://doi.wiley.com/10.1002/1521-3978%28200009%2948%3A9%2F11%3C1109%3A%3AAID-PROP1109%3E3.0.CO%3B2-I},
	doi = {10.1002/1521-3978(200009)48:9/11<1109::AID-PROP1109>3.0.CO;2-I},
	number = {9-11},
	journal = {Fortschritte der Physik},
	author = {Lea, M.J. and Frayne, P.G. and Mukharsky, Yu.},
	month = sep,
	year = {2000},
	pages = {1109--1124},
}

@article{zhu_variational_1995,
	title = {Variational quantum {Monte} {Carlo} study of two-dimensional {Wigner} crystals: {Exchange}, correlation and magnetic-field effects.},
	volume = {52},
	url = {http://cds.cern.ch/record/281931},
	number = {8},
	urldate = {2014-11-24},
	author = {Zhu, X and Louie, SG},
	year = {1995},
}

@article{parhami_fault-tolerant_nodate,
	title = {Fault-{Tolerant} {Reversible} {Circuits} {P} = {ACA}},
	number = {2 4},
	author = {Parhami, Behrooz},
	note = {ISBN: 1424407850},
	pages = {1726--1729},
}

@article{hajek_maximum_1991,
	title = {On the maximum tolerable noise for reliable computation by formulas},
	volume = {37},
	issn = {00189448},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=75261},
	doi = {10.1109/18.75261},
	number = {2},
	journal = {IEEE Transactions on Information Theory},
	author = {Hajek, B. and Weller, T.},
	month = mar,
	year = {1991},
	pages = {388--391},
}

@article{noauthor_technion_1999,
	title = {Technion – {Israel} {Institute} of {Technology} {Minerva} {Optimization} {Center}},
	year = {1999},
}

@article{nikolic_architectures_2001,
	title = {Architectures for reliable computing with unreliable nanodevices},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=966429},
	doi = {10.1109/NANO.2001.966429},
	journal = {Proceedings of the 2001 1st IEEE Conference on Nanotechnology. IEEE-NANO 2001 (Cat. No.01EX516)},
	author = {Nikolic, K. and Sadek, a. and Forshaw, M.},
	year = {2001},
	note = {Publisher: Ieee
ISBN: 0-7803-7215-8},
	pages = {254--259},
}

@article{taylor_wigner_2008,
	title = {Wigner crystals of ions as quantum hard drives},
	volume = {78},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.78.062331},
	doi = {10.1103/PhysRevA.78.062331},
	number = {6},
	urldate = {2014-11-15},
	journal = {Physical Review A},
	author = {Taylor, J. and Calarco, T.},
	month = dec,
	year = {2008},
	pages = {062331},
}

@article{langer_long-lived_2005,
	title = {Long-{Lived} {Qubit} {Memory} {Using} {Atomic} {Ions}},
	volume = {95},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.95.060502},
	doi = {10.1103/PhysRevLett.95.060502},
	number = {6},
	urldate = {2014-11-18},
	journal = {Physical Review Letters},
	author = {Langer, C. and Ozeri, R. and Jost, J. and Chiaverini, J. and DeMarco, B. and Ben-Kish, a. and Blakestad, R. and Britton, J. and Hume, D. and Itano, W. and Leibfried, D. and Reichle, R. and Rosenband, T. and Schaetz, T. and Schmidt, P. and Wineland, D.},
	month = aug,
	year = {2005},
	pages = {060502},
}

@article{haffner_robust_2005,
	title = {Robust entanglement},
	volume = {81},
	issn = {0946-2171},
	url = {http://link.springer.com/10.1007/s00340-005-1917-z},
	doi = {10.1007/s00340-005-1917-z},
	number = {2-3},
	urldate = {2014-11-18},
	journal = {Applied Physics B},
	author = {Häffner, H. and Schmidt-Kaler, F. and Hänsel, W. and Roos, C. F. and Körber, T. and Chwalla, M. and Riebe, M. and Benhelm, J. and Rapol, U. D. and Becher, C. and Blatt, R.},
	month = jul,
	year = {2005},
	pages = {151--153},
}

@article{klein_extension_1980,
	title = {Extension of the {Dirac} identity},
	url = {http://iopscience.iop.org/0305-4470/13/10/011},
	urldate = {2014-11-17},
	journal = {Journal of Physics A: Mathematical and General},
	author = {Klein, DJ},
	year = {1980},
	pages = {1--7},
}

@article{baltrusch_fast_2011,
	title = {Fast and robust quantum computation with ionic {Wigner} crystals},
	volume = {83},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.83.042319},
	doi = {10.1103/PhysRevA.83.042319},
	number = {4},
	urldate = {2014-11-15},
	journal = {Physical Review A},
	author = {Baltrusch, J. D. and Negretti, a. and Taylor, J. M. and Calarco, T.},
	month = apr,
	year = {2011},
	pages = {042319},
}

@article{dykman_quantum_2000,
	title = {Quantum {Computing} {Using} {Electrons} {Floating} on {Liquid} {Helium}},
	url = {http://arxiv.org/abs/quant-ph/0007113},
	number = {1},
	urldate = {2014-11-15},
	journal = {arXiv preprint quant-ph/0007113},
	author = {Dykman, MI and Platzman, PM},
	year = {2000},
	pages = {1--22},
}

@article{deshpande_one-dimensional_2008,
	title = {The one-dimensional {Wigner} crystal in carbon nanotubes},
	volume = {4},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys895},
	doi = {10.1038/nphys895},
	number = {4},
	urldate = {2014-11-15},
	journal = {Nature Physics},
	author = {Deshpande, Vikram V. and Bockrath, Marc},
	month = mar,
	year = {2008},
	pages = {314--318},
}

@article{chesi_quantum_nodate,
	title = {Quantum {Computing} with {Electron} {Spins} in {Quantum} {Dots} ˙ ¨},
	author = {Chesi, Stefano and Loss, Daniel and Zak, Robert Andrzej},
	note = {arXiv: 0906.4045v2},
	pages = {1--61},
}

@article{peres_reversible_1985,
	title = {Reversible logic and quantum computers},
	volume = {32},
	url = {http://journals.aps.org/pra/abstract/10.1103/PhysRevA.32.3266},
	number = {6},
	urldate = {2014-11-14},
	journal = {Physical review A},
	author = {Peres, Asher},
	year = {1985},
	pages = {3266--3276},
}

@article{szkopek_physical_2011,
	title = {Physical {Fault} {Tolerance} of {Nanoelectronics}},
	volume = {106},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.106.176801},
	doi = {10.1103/PhysRevLett.106.176801},
	number = {17},
	urldate = {2014-11-10},
	journal = {Physical Review Letters},
	author = {Szkopek, Thomas and Roychowdhury, Vwani P. and Antoniadis, Dimitri a. and Damoulakis, John N.},
	month = apr,
	year = {2011},
	pages = {176801},
}

@misc{noauthor_43483299.pdf_nodate,
	title = {43483299.pdf},
}

@article{wolfowicz_decoherence_2012,
	title = {Decoherence mechanisms of {\textasciicircum}\{209\}{Bi} donor electron spins in isotopically pure {\textasciicircum}\{28\}{Si}},
	volume = {86},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.86.245301},
	doi = {10.1103/PhysRevB.86.245301},
	number = {24},
	urldate = {2014-11-07},
	journal = {Physical Review B},
	author = {Wolfowicz, Gary and Simmons, Stephanie and Tyryshkin, Alexei M. and George, Richard E. and Riemann, Helge and Abrosimov, Nikolai V. and Becker, Peter and Pohl, Hans-Joachim and Lyon, Stephen a. and Thewalt, Mike L. W. and Morton, John J. L.},
	month = dec,
	year = {2012},
	pages = {245301},
}

@article{nickerson_topological_2013,
	title = {Topological quantum computing with a very noisy network and local error rates approaching one percent.},
	volume = {4},
	issn = {2041-1723},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3644110&tool=pmcentrez&rendertype=abstract},
	doi = {10.1038/ncomms2773},
	abstract = {A scalable quantum computer could be built by networking together many simple processor cells, thus avoiding the need to create a single complex structure. The difficulty is that realistic quantum links are very error prone. A solution is for cells to repeatedly communicate with each other and so purify any imperfections; however prior studies suggest that the cells themselves must then have prohibitively low internal error rates. Here we describe a method by which even error-prone cells can perform purification: groups of cells generate shared resource states, which then enable stabilization of topologically encoded data. Given a realistically noisy network (≥10\% error rate) we find that our protocol can succeed provided that intra-cell error rates for initialisation, state manipulation and measurement are below 0.82\%. This level of fidelity is already achievable in several laboratory systems.},
	urldate = {2014-10-28},
	journal = {Nature communications},
	author = {Nickerson, Naomi H and Li, Ying and Benjamin, Simon C},
	month = jan,
	year = {2013},
	pmid = {23612297},
	note = {Publisher: Nature Publishing Group},
	pages = {1756},
}

@article{knill_quantum_2004,
	title = {Quantum {Computing} with {Very} {Noisy} {Devices}},
	url = {http://arxiv.org/abs/quant-ph/0410199},
	doi = {10.1038/nature03350},
	abstract = {In theory, quantum computers can efficiently simulate quantum physics, factor large numbers and estimate integrals, thus solving otherwise intractable computational problems. In practice, quantum computers must operate with noisy devices called ``gates'' that tend to destroy the fragile quantum states needed for computation. The goal of fault-tolerant quantum computing is to compute accurately even when gates have a high probability of error each time they are used. Here we give evidence that accurate quantum computing is possible with error probabilities above 3\% per gate, which is significantly higher than what was previously thought possible. However, the resources required for computing at such high error probabilities are excessive. Fortunately, they decrease rapidly with decreasing error probabilities. If we had quantum resources comparable to the considerable resources available in today's digital computers, we could implement non-trivial quantum computations at error probabilities as high as 1\% per gate.},
	urldate = {2014-11-06},
	author = {Knill, E.},
	month = oct,
	year = {2004},
	note = {arXiv: quant-ph/0410199
Genre: Quantum Physics},
	pages = {47},
}

@article{chien_moores_2013,
	title = {Moore’s {Law}: {The} first ending and a new beginning},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Moore’s+Law:+The+first+ending+and+a+new+beginning#0},
	urldate = {2014-11-05},
	journal = {Citeseer},
	author = {Chien, A.A and Karamcheti, V.},
	year = {2013},
	pages = {0--5},
}

@article{markov_limits_2014,
	title = {Limits on fundamental limits to computation},
	volume = {512},
	issn = {0028-0836},
	url = {http://www.nature.com/doifinder/10.1038/nature13570},
	doi = {10.1038/nature13570},
	number = {7513},
	urldate = {2014-08-13},
	journal = {Nature},
	author = {Markov, Igor L.},
	month = aug,
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	pages = {147--154},
}

@article{deslauriers_scaling_2006,
	title = {Scaling and {Suppression} of {Anomalous} {Heating} in {Ion} {Traps}},
	volume = {97},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.97.103007},
	doi = {10.1103/PhysRevLett.97.103007},
	number = {10},
	urldate = {2014-11-03},
	journal = {Physical Review Letters},
	author = {Deslauriers, L. and Olmschenk, S. and Stick, D. and Hensinger, W. and Sterk, J. and Monroe, C.},
	month = sep,
	year = {2006},
	pages = {103007},
}

@article{wolfowicz_atomic_2013,
	title = {Atomic clock transitions in silicon-based spin qubits.},
	volume = {8},
	issn = {1748-3395},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23793304},
	doi = {10.1038/nnano.2013.117},
	abstract = {A major challenge in using spins in the solid state for quantum technologies is protecting them from sources of decoherence. This is particularly important in nanodevices where the proximity of material interfaces, and their associated defects, can play a limiting role. Spin decoherence can be addressed to varying degrees by improving material purity or isotopic composition, for example, or active error correction methods such as dynamic decoupling (or even combinations of the two). However, a powerful method applied to trapped ions in the context of atomic clocks is the use of particular spin transitions that are inherently robust to external perturbations. Here, we show that such 'clock transitions' can be observed for electron spins in the solid state, in particular using bismuth donors in silicon. This leads to dramatic enhancements in the electron spin coherence time, exceeding seconds. We find that electron spin qubits based on clock transitions become less sensitive to the local magnetic environment, including the presence of (29)Si nuclear spins as found in natural silicon. We expect the use of such clock transitions will be of additional significance for donor spins in nanodevices, mitigating the effects of magnetic or electric field noise arising from nearby interfaces and gates.},
	number = {8},
	urldate = {2014-11-04},
	journal = {Nature nanotechnology},
	author = {Wolfowicz, Gary and Tyryshkin, Alexei M and George, Richard E and Riemann, Helge and Abrosimov, Nikolai V and Becker, Peter and Pohl, Hans-Joachim and Thewalt, Mike L W and Lyon, Stephen a and Morton, John J L},
	month = aug,
	year = {2013},
	pmid = {23793304},
	note = {Publisher: Nature Publishing Group},
	pages = {561--4},
}

@article{hughes_microfabricated_2011,
	title = {Microfabricated ion traps},
	volume = {52},
	issn = {0010-7514},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00107514.2011.601918},
	doi = {10.1080/00107514.2011.601918},
	number = {6},
	urldate = {2014-11-03},
	journal = {Contemporary Physics},
	author = {Hughes, Marcus D. and Lekitsch, Bjoern and Broersma, Jiddu a. and Hensinger, Winfried K.},
	month = nov,
	year = {2011},
	keywords = {anomalous heating, ion traps, laser cooling and, microfabrication, quantum information processing},
	pages = {505--529},
}

@article{deng_effective_2005,
	title = {Effective {Spin} {Quantum} {Phases} in {Systems} of {Trapped} {Ions}},
	url = {http://arxiv.org/abs/quant-ph/0509197},
	doi = {10.1103/PhysRevA.72.063407},
	abstract = {A system of trapped ions under the action of off--resonant standing--waves can be used to simulate a variety of quantum spin models. In this work, we describe theoretically quantum phases that can be observed in the simplest realization of this idea: quantum Ising and XY models. Our numerical calculations with the Density Matrix Renormalization Group method show that experiments with ion traps should allow one to access general properties of quantum critical systems. On the other hand, ion trap quantum spin models show a few novel features due to the peculiarities of induced effective spin--spin interactions which lead to interesting effects like long--range quantum correlations and the coexistence of different spin phases.},
	urldate = {2014-11-03},
	author = {Deng, X. -L. and Porras, D. and Cirac, J. I.},
	month = sep,
	year = {2005},
	note = {arXiv: quant-ph/0509197
Genre: Quantum Physics; Strongly Correlated Electrons},
	pages = {11},
}

@article{divincenzo_dogma_2001,
	title = {Dogma and heresy in quantum computing},
	url = {http://dl.acm.org/citation.cfm?id=2016996},
	urldate = {2014-11-03},
	journal = {Quantum Information \& Computation},
	author = {DiVincenzo, DP},
	year = {2001},
}

@article{cirac_scalable_2000,
	title = {A scalable quantum computer with ions in an array of microtraps},
	author = {Cirac, J I and Zoller, P},
	year = {2000},
	pages = {579--581},
}

@article{seidelin_microfabricated_2006,
	title = {Microfabricated {Surface}-{Electrode} {Ion} {Trap} for {Scalable} {Quantum} {Information} {Processing}},
	volume = {96},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.96.253003},
	doi = {10.1103/PhysRevLett.96.253003},
	number = {25},
	urldate = {2014-11-03},
	journal = {Physical Review Letters},
	author = {Seidelin, S. and Chiaverini, J. and Reichle, R. and Bollinger, J. and Leibfried, D. and Britton, J. and Wesenberg, J. and Blakestad, R. and Epstein, R. and Hume, D. and Itano, W. and Jost, J. and Langer, C. and Ozeri, R. and Shiga, N. and Wineland, D.},
	month = jun,
	year = {2006},
	pages = {253003},
}

@article{lin_large-scale_2009,
	title = {Large-scale quantum computation in an anharmonic linear ion trap},
	volume = {86},
	issn = {0295-5075},
	url = {http://stacks.iop.org/0295-5075/86/i=6/a=60004?key=crossref.fe864bdf0a363006b13a8096d0a484b9},
	doi = {10.1209/0295-5075/86/60004},
	number = {6},
	urldate = {2014-10-22},
	journal = {EPL (Europhysics Letters)},
	author = {Lin, G.-D. and Zhu, S.-L. and Islam, R. and Kim, K. and Chang, M.-S. and Korenblit, S. and Monroe, C. and Duan, L.-M.},
	month = jun,
	year = {2009},
	pages = {60004},
}

@article{chiaverini_surface-electrode_2005,
	title = {Surface-{Electrode} {Architecture} for {Ion}-{Trap} {Quantum} {Information} {Processing}},
	volume = {5},
	url = {http://arxiv.org/abs/quant-ph/0501147},
	abstract = {We investigate a surface-mounted electrode geometry for miniature linear radio frequency Paul ion traps. The electrodes reside in a single plane on a substrate, and the pseudopotential minimum of the trap is located above the substrate at a distance on order of the electrodes' lateral extent or separation. This architecture provides the possibility to apply standard microfabrication principles to the construction of multiplexed ion traps, which may be of particular importance in light of recent proposals for large-scale quantum computation based on individual trapped ions.},
	number = {6},
	urldate = {2014-11-03},
	author = {Chiaverini, J. and Blakestad, R. B. and Britton, J. and Jost, J. D. and Langer, C. and Leibfried, D. and Ozeri, R. and Wineland, D. J.},
	month = jan,
	year = {2005},
	note = {arXiv: quant-ph/0501147
Genre: Quantum Physics},
	keywords = {ion traps, microfabrication, quantum computation, quantum information},
	pages = {21},
}

@misc{noauthor_82369146.pdf_nodate,
	title = {82369146.pdf},
}

@article{haffner_quantum_2008,
	title = {Quantum computing with trapped ions},
	volume = {469},
	issn = {03701573},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0370157308003463},
	doi = {10.1016/j.physrep.2008.09.003},
	number = {4},
	urldate = {2014-08-21},
	journal = {Physics Reports},
	author = {Haffner, H and Roos, C and Blatt, R},
	month = dec,
	year = {2008},
	note = {Publisher: Elsevier B.V.},
	keywords = {quantum computing and information},
	pages = {155--203},
}

@article{brandeho_optimal_2014,
	title = {An optimal adiabatic quantum query algorithm},
	url = {http://arxiv.org/abs/1409.3558},
	abstract = {Quantum query complexity is known to be characterized by the so-called quantum adversary bound. While this result has been proved in the standard discrete-time model of quantum computation, it also holds for continuous-time (or Hamiltonian-based) quantum computation, due to a known equivalence between these two query complexity models. In this work, we revisit this result by providing a direct proof in the continuous-time model. One originality of our proof is that it draws new connections between the adversary bound, a modern theoretical computer science technique, and early theorems of quantum mechanics. Indeed, the proof of the lower bound is based on Ehrenfest's theorem, while the upper bound relies on the Adiabatic theorem, as we construct an optimal adiabatic quantum query algorithm.},
	urldate = {2014-11-03},
	author = {Brandeho, Mathieu and Roland, Jérémie},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.3558
Genre: Quantum Physics; Computational Complexity},
	pages = {1--17},
}

@article{monroe_scaling_2013,
	title = {Scaling the ion trap quantum processor.},
	volume = {339},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23471398},
	doi = {10.1126/science.1231298},
	abstract = {Trapped atomic ions are standards for quantum information processing, serving as quantum memories, hosts of quantum gates in quantum computers and simulators, and nodes of quantum communication networks. Quantum bits based on trapped ions enjoy a rare combination of attributes: They have exquisite coherence properties, they can be prepared and measured with nearly 100\% efficiency, and they are readily entangled with each other through the Coulomb interaction or remote photonic interconnects. The outstanding challenge is the scaling of trapped ions to hundreds or thousands of qubits and beyond, at which scale quantum processors can outperform their classical counterparts in certain applications. We review the latest progress and prospects in that effort, with the promise of advanced architectures and new technologies, such as microfabricated ion traps and integrated photonics.},
	number = {6124},
	urldate = {2014-09-22},
	journal = {Science (New York, N.Y.)},
	author = {Monroe, C and Kim, J},
	month = mar,
	year = {2013},
	pmid = {23471398},
	pages = {1164--9},
}

@article{simmons_silicon-based_2013,
	title = {Silicon-based quantum computation},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6563315},
	doi = {10.1109/ICICDT.2013.6563315},
	journal = {Proceedings of 2013 International Conference on IC Design \& Technology (ICICDT)},
	author = {Simmons, Stephanie},
	month = may,
	year = {2013},
	note = {Publisher: Ieee
ISBN: 978-1-4673-4743-3},
	pages = {109--114},
}

@article{zwanenburg_silicon_2013,
	title = {Silicon quantum electronics},
	volume = {85},
	issn = {0034-6861},
	url = {http://link.aps.org/doi/10.1103/RevModPhys.85.961},
	doi = {10.1103/RevModPhys.85.961},
	number = {3},
	urldate = {2014-07-14},
	journal = {Reviews of Modern Physics},
	author = {Zwanenburg, Floris a. and Dzurak, Andrew S. and Morello, Andrea and Simmons, Michelle Y. and Hollenberg, Lloyd C. L. and Klimeck, Gerhard and Rogge, Sven and Coppersmith, Susan N. and Eriksson, Mark a.},
	month = jul,
	year = {2013},
	pages = {961--1019},
}

@article{hollenberg_two-dimensional_2006,
	title = {Two-dimensional architectures for donor-based quantum computing},
	volume = {74},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.74.045311},
	doi = {10.1103/PhysRevB.74.045311},
	number = {4},
	urldate = {2014-10-30},
	journal = {Physical Review B},
	author = {Hollenberg, L. and Greentree, a. and Fowler, a. and Wellard, C.},
	month = jul,
	year = {2006},
	pages = {045311},
}

@article{sharmin_design_2011,
	title = {Design of a compact reversible random access memory},
	urldate = {2014-10-27},
	journal = {4th IEEE …},
	author = {Sharmin, Farah},
	year = {2011},
	note = {ISBN: 9781612848365},
	keywords = {- reversible logic, flip-flop, garbage output},
	pages = {0--4},
}

@article{havinga_low_1997,
	title = {Low power system design techniques for mobile computers},
	url = {http://doc.utwente.nl/18109/},
	urldate = {2014-10-27},
	author = {Havinga, PJM and Smit, GJM},
	year = {1997},
	pages = {1--17},
}

@article{patra_approaches_1995,
	title = {Approaches to {Design} of {Circuits} for {Low}-power {Computation} {Doctor}},
	urldate = {2014-10-27},
	author = {Patra, P},
	year = {1995},
}

@article{drechsler_truth_2011,
	title = {From {Truth} {Tables} to {Programming} {Languages}: {Progress} in the {Design} of {Reversible} {Circuits}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5954213},
	doi = {10.1109/ISMVL.2011.40},
	urldate = {2014-10-27},
	journal = {2011 41st IEEE International Symposium on Multiple-Valued Logic},
	author = {Drechsler, Rolf and Wille, Robert},
	month = may,
	year = {2011},
	note = {Publisher: Ieee
ISBN: 978-1-4577-0112-2},
	pages = {78--85},
}

@article{kole_optimal_2010,
	title = {Optimal {Reversible} {Logic} {Circuit} {Synthesis} {Based} on a {Hybrid} {DFS}-{BFS} {Technique}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5715177},
	doi = {10.1109/ISED.2010.47},
	urldate = {2014-10-27},
	journal = {2010 International Symposium on Electronic System Design},
	author = {Kole, D K and Rahaman, H and Das, D K and Bhattacharya, B B},
	month = dec,
	year = {2010},
	note = {Publisher: Ieee
ISBN: 978-1-4244-8979-4},
	keywords = {bfs, dfs, permutation, reversible circuit, reversible synthesis, toffoli gate},
	pages = {208--212},
}

@article{lloyd_unconventional_2000,
	title = {Unconventional {Quantum} {Computing} {Devices}},
	url = {http://arxiv.org/abs/quant-ph/0003151},
	abstract = {This paper investigates a variety of unconventional quantum computation devices, including fermionic quantum computers and computers that exploit nonlinear quantum mechanics. It is shown that unconventional quantum computing devices can in principle compute some quantities more rapidly than `conventional' quantum computers.},
	urldate = {2014-10-27},
	author = {Lloyd, Seth},
	month = mar,
	year = {2000},
	note = {arXiv: quant-ph/0003151
Genre: Quantum Physics},
	pages = {9},
}

@article{wille_towards_2010,
	title = {Towards a {Design} {Flow} for {Reversible} {Logic}},
	url = {http://link.springer.com/10.1007/978-90-481-9579-4},
	doi = {10.1007/978-90-481-9579-4},
	author = {Wille, Robert and Drechsler, Rolf},
	year = {2010},
	note = {Publisher: Springer Netherlands
Place: Dordrecht
ISBN: 978-90-481-9578-7},
}

@article{babu_efficient_2013,
	title = {An efficient approach for designing a reversible fault tolerant n-bit carry look-ahead adder},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6749668},
	doi = {10.1109/SOCC.2013.6749668},
	journal = {2013 IEEE International SOC Conference},
	author = {Babu, Hafiz Md. Hasan and Jamal, Lafifa and Saleheen, Nazir},
	month = sep,
	year = {2013},
	note = {Publisher: Ieee
ISBN: 978-1-4799-1166-0},
	pages = {98--103},
}

@article{nayeem_new_2012,
	title = {A new approach to online testing of {TGFSOP}-based ternary {Toffoli} circuits},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6214828},
	doi = {10.1109/ISMVL.2012.57},
	urldate = {2014-10-27},
	journal = {… -Valued Logic (ISMVL), 2012 42nd IEEE …},
	author = {Nayeem, NM and Rice, JE},
	month = may,
	year = {2012},
	note = {Publisher: Ieee
ISBN: 978-1-4673-0908-0},
	pages = {315--321},
}

@article{younis_asymptotically_1994,
	title = {Asymptotically {Zero} {Energy} {Computing} {Using} {Split}-{Level} charge recovery logic},
	volume = {4},
	urldate = {2014-10-27},
	author = {Younis, SG},
	year = {1994},
}

@article{frank_reversibility_1999,
	title = {Reversibility for {Efficient} {Computing} by},
	urldate = {2014-10-27},
	author = {Frank, MP and Jr, TF Knight},
	year = {1999},
}

@article{maslov_techniques_2007,
	title = {Techniques for the synthesis of reversible {Toffoli} networks},
	volume = {12},
	issn = {10844309},
	url = {http://portal.acm.org/citation.cfm?doid=1278349.1278355},
	doi = {10.1145/1278349.1278355},
	number = {4},
	urldate = {2014-10-27},
	journal = {ACM Transactions on Design Automation of Electronic Systems},
	author = {Maslov, D. and Dueck, G. W. and Miller, D. M.},
	month = sep,
	year = {2007},
	pages = {42--es},
}

@article{dickinson_adiabatic_1995,
	title = {Adiabatic dynamic logic},
	volume = {30},
	issn = {00189200},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=364447},
	doi = {10.1109/4.364447},
	number = {3},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Dickinson, a.G. and Denker, J.S.},
	month = mar,
	year = {1995},
	pages = {311--315},
}

@article{sai-halasz_performance_1995,
	title = {Performance trends in high-end processors},
	volume = {83},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=362754},
	doi = {10.1109/5.362754},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Sai-Halasz, G.a.},
	year = {1995},
	pages = {20--36},
}

@article{sadek_parallel_2004,
	title = {Parallel information and computation with restitution for noise-tolerant nanoscale logic networks},
	volume = {15},
	issn = {0957-4484},
	url = {http://stacks.iop.org/0957-4484/15/i=1/a=037?key=crossref.fac54d3909cb0df239df936a0fcda809},
	doi = {10.1088/0957-4484/15/1/037},
	number = {1},
	urldate = {2014-10-27},
	journal = {Nanotechnology},
	author = {Sadek, Akram S and Nikoli, Konstantin and Forshaw, Michael},
	month = jan,
	year = {2004},
	pages = {192--210},
}

@article{li_reversibility_1996,
	title = {Reversibility and {Adiabatic} {Computation}: {Trading} {Time} and {Space} for {Energy}},
	volume = {452},
	issn = {1364-5021},
	url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1996.0039},
	doi = {10.1098/rspa.1996.0039},
	number = {1947},
	urldate = {2014-10-27},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Li, M. and Vitanyi, P.},
	month = apr,
	year = {1996},
	pages = {769--789},
}

@article{li_reversible_1998,
	title = {Reversible simulation of irreversible computation *},
	volume = {120},
	author = {Li, Ming and Tromp, John and Vitfinyi, Paul},
	year = {1998},
	pages = {168--176},
}

@article{goldhaber-gordon_overview_1997,
	title = {Overview of nanoelectronic devices},
	volume = {85},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=573739},
	doi = {10.1109/5.573739},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {Goldhaber-Gordon, D. and Montemerlo, M.S. and Love, J.C. and Opiteck, G.J. and Ellenbogen, J.C.},
	month = apr,
	year = {1997},
	keywords = {dots, molecular electronics, nanoelectronics, quantum, quantum-effect devices, resonant tunneling, single-electron},
	pages = {521--540},
}

@article{merkle_reversible_1993,
	title = {Reversible electronic logic using switches},
	url = {http://iopscience.iop.org/0957-4484/4/1/002},
	urldate = {2014-11-11},
	journal = {Nanotechnology},
	author = {Merkle, RC},
	year = {1993},
}

@book{noauthor_more_2009,
	title = {More than {Moore}},
	urldate = {2014-10-27},
	year = {2009},
}

@article{smit_survey_1997,
	title = {A survey of energy saving techniques for mobile computers},
	url = {http://doc.utwente.nl/56244/},
	urldate = {2014-10-27},
	author = {Smit, GJM and Havinga, PJM},
	year = {1997},
	pages = {1--11},
}

@article{kielpinski_architecture_2002,
	title = {Architecture for a large-scale ion-trap quantum computer},
	volume = {417},
	number = {June},
	author = {Kielpinski, D},
	year = {2002},
	pages = {709--711},
}

@article{marvian_quantum_2014,
	title = {Quantum error suppression with commuting {Hamiltonians}: {Two}-local is too local},
	url = {http://arxiv.org/abs/1410.5487v1},
	urldate = {2014-10-22},
	author = {Marvian, Iman and Lidar, Daniel a.},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.5487},
	pages = {1--11},
}

@article{tans_individual_1997,
	title = {Individual single-wall carbon nanotubes as quantum wires},
	url = {http://repository.tudelft.nl/view/ir/uuid:4e58e2bc-5f69-4dbe-9942-aabcc9eaad35/},
	urldate = {2014-10-21},
	journal = {Nature 386 (6624), …},
	author = {Tans, SJ and Devoret, MH and Dai, H},
	year = {1997},
}

@article{okamoto_spin_1999,
	title = {Spin {Degree} of {Freedom} in a {Two}-{Dimensional} {Electron} {Liquid}},
	volume = {82},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.82.3875},
	doi = {10.1103/PhysRevLett.82.3875},
	number = {19},
	journal = {Physical Review Letters},
	author = {Okamoto, Tohru and Hosoya, Kunio and Kawaji, Shinji and Yagi, Atsuo},
	month = may,
	year = {1999},
	pages = {3875--3878},
}

@article{haft_optical_2000,
	title = {Optical emission from a charge-tunable quantum ring},
	volume = {405},
	number = {June},
	author = {Haft, D and Bickel, F and Lorke, A and Warburton, R J and Scha, C and Karrai, K and Garcia, J M and Schoenfeld, W and Petroff, P M},
	year = {2000},
	pages = {8--11},
}

@article{yoon_wigner_1999,
	title = {Wigner {Crystallization} and {Metal}-{Insulator} {Transition} of {Two}-{Dimensional} {Holes} in {GaAs} at {B}=0},
	volume = {82},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.82.1744},
	doi = {10.1103/PhysRevLett.82.1744},
	number = {8},
	journal = {Physical Review Letters},
	author = {Yoon, Jongsoo and Li, C. and Shahar, D. and Tsui, D. and Shayegan, M.},
	month = feb,
	year = {1999},
	pages = {1744--1747},
}

@article{heterostructures_no_2011,
	title = {No {Title}},
	author = {Heterostructures, Gallium Arsenide-based},
	year = {2011},
}

@article{kato_thesis_1999,
	title = {Thesis {Electron} {Transport} in a {Two}-{Dimensional} {Electron} {Gas} at {GaAs}/{AlGaAs} {Heterointerface} under a spatially modulated magnetic eld},
	number = {December},
	author = {Kato, Mayumi},
	year = {1999},
}

@article{mizel_gapped_2014,
	title = {Gapped spin {Hamiltonian} motivated by quantum teleportation},
	volume = {20740},
	url = {http://arxiv.org/abs/1410.1928v1},
	number = {1},
	urldate = {2014-10-16},
	author = {Mizel, Ari},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.1928},
	pages = {1--7},
}

@article{williamson_symmetry-protected_2014,
	title = {Symmetry-protected adiabatic quantum transistors},
	url = {http://arxiv.org/abs/1408.3415v1},
	urldate = {2014-10-16},
	author = {Williamson, Dominic J. and Bartlett, Stephen D.},
	month = aug,
	year = {2014},
	note = {arXiv: 1408.3415},
	pages = {1--20},
}

@article{meyer_transition_2007,
	title = {Transition from a {One}-{Dimensional} to a {Quasi}-{One}-{Dimensional} {State} in {Interacting} {Quantum} {Wires}},
	volume = {98},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.98.126404},
	doi = {10.1103/PhysRevLett.98.126404},
	number = {12},
	urldate = {2014-10-16},
	journal = {Physical Review Letters},
	author = {Meyer, Julia and Matveev, K. and Larkin, a.},
	month = mar,
	year = {2007},
	pages = {126404},
}

@article{ilani_measurement_2006,
	title = {Measurement of the quantum capacitance of interacting electrons in carbon nanotubes},
	volume = {2},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys412},
	doi = {10.1038/nphys412},
	number = {10},
	urldate = {2014-10-13},
	journal = {Nature Physics},
	author = {Ilani, S. and Donev, L. a. K. and Kindermann, M. and McEuen, P. L.},
	month = sep,
	year = {2006},
	pages = {687--691},
}

@article{wees_spin_2005,
	title = {Spin in the slow lane},
	volume = {437},
	number = {October},
	author = {Wees, Bart Van},
	year = {2005},
	pages = {245308},
}

@article{steinberg_charge_2007,
	title = {Charge fractionalization in quantum wires},
	volume = {4},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys810},
	doi = {10.1038/nphys810},
	number = {2},
	urldate = {2014-10-16},
	journal = {Nature Physics},
	author = {Steinberg, Hadar and Barak, Gilad and Yacoby, Amir and Pfeiffer, Loren N. and West, Ken W. and Halperin, Bertrand I. and Le Hur, Karyn},
	month = dec,
	year = {2007},
	pages = {116--119},
}

@article{tserkovnyak_aharonov-casher_2009,
	title = {Aharonov-{Casher} {Effect} in {Exchange} {Interactions} in a {Wigner} {Crystal}},
	volume = {102},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.102.126801},
	doi = {10.1103/PhysRevLett.102.126801},
	number = {12},
	urldate = {2014-10-16},
	journal = {Physical Review Letters},
	author = {Tserkovnyak, Yaroslav and Kindermann, Markus},
	month = mar,
	year = {2009},
	pages = {126801},
}

@article{schafer_new_2008,
	title = {New {Model} {System} for a {One}-{Dimensional} {Electron} {Liquid}: {Self}-{Organized} {Atomic} {Gold} {Chains} on {Ge}(001)},
	volume = {101},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.101.236802},
	doi = {10.1103/PhysRevLett.101.236802},
	number = {23},
	urldate = {2014-10-16},
	journal = {Physical Review Letters},
	author = {Schäfer, J. and Blumenstein, C. and Meyer, S. and Wisniewski, M. and Claessen, R.},
	month = dec,
	year = {2008},
	pages = {236802},
}

@article{laroche_1d-1d_2014,
	title = {{1D}-{1D} {Coulomb} drag signature of a {Luttinger} liquid.},
	volume = {343},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24457214},
	doi = {10.1126/science.1244152},
	abstract = {One-dimensional (1D) interacting electronic systems exhibit distinct properties when compared to their counterparts in higher dimensions. We report Coulomb drag measurements between vertically integrated quantum wires separated by a barrier only 15 nanometers wide. The temperature dependence of the drag resistance is measured in the true 1D regime where both wires have less than one 1D subband occupied. As a function of temperature, an upturn in the drag resistance is observed below a temperature T* {\textasciitilde} 1.6 kelvin. This crossover in Coulomb drag behavior is consistent with Tomonaga-Luttinger liquid models for the 1D-1D drag between quantum wires.},
	number = {6171},
	urldate = {2014-09-23},
	journal = {Science (New York, N.Y.)},
	author = {Laroche, D and Gervais, G and Lilly, M P and Reno, J L},
	month = feb,
	year = {2014},
	pmid = {24457214},
	pages = {631--4},
}

@article{chang_crystallization_2008,
	title = {Crystallization of strongly interacting photons in a nonlinear optical fibre},
	volume = {4},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys1074},
	doi = {10.1038/nphys1074},
	number = {11},
	urldate = {2014-10-08},
	journal = {Nature Physics},
	author = {Chang, D. E. and Gritsev, V. and Morigi, G. and Vuletić, V. and Lukin, M. D. and Demler, E. a.},
	month = sep,
	year = {2008},
	pages = {884--889},
}

@article{quay_observation_2010,
	title = {Observation of a one-dimensional spin–orbit gap in a quantum wire},
	volume = {6},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys1626},
	doi = {10.1038/nphys1626},
	number = {5},
	urldate = {2014-08-21},
	journal = {Nature Physics},
	author = {Quay, C. H. L. and Hughes, T. L. and Sulpizio, J. a. and Pfeiffer, L. N. and Baldwin, K. W. and West, K. W. and Goldhaber-Gordon, D. and de Picciotto, R.},
	month = mar,
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {336--339},
}

@article{hilbert_worlds_2011,
	title = {The world's technological capacity to store, communicate, and compute information.},
	volume = {332},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21310967},
	doi = {10.1126/science.1200970},
	abstract = {We estimated the world's technological capacity to store, communicate, and compute information, tracking 60 analog and digital technologies during the period from 1986 to 2007. In 2007, humankind was able to store 2.9 × 10(20) optimally compressed bytes, communicate almost 2 × 10(21) bytes, and carry out 6.4 × 10(18) instructions per second on general-purpose computers. General-purpose computing capacity grew at an annual rate of 58\%. The world's capacity for bidirectional telecommunication grew at 28\% per year, closely followed by the increase in globally stored information (23\%). Humankind's capacity for unidirectional information diffusion through broadcasting channels has experienced comparatively modest annual growth (6\%). Telecommunication has been dominated by digital technologies since 1990 (99.9\% in digital format in 2007), and the majority of our technological memory has been in digital format since the early 2000s (94\% digital in 2007).},
	number = {6025},
	urldate = {2014-07-11},
	journal = {Science (New York, N.Y.)},
	author = {Hilbert, Martin and López, Priscila},
	month = apr,
	year = {2011},
	pmid = {21310967},
	pages = {60--5},
}

@article{jompol_probing_2009,
	title = {Probing spin-charge separation in a {Tomonaga}-{Luttinger} liquid.},
	volume = {325},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/19644117},
	doi = {10.1126/science.1171769},
	abstract = {In a one-dimensional (1D) system of interacting electrons, excitations of spin and charge travel at different speeds, according to the theory of a Tomonaga-Luttinger liquid (TLL) at low energies. However, the clear observation of this spin-charge separation is an ongoing challenge experimentally. We have fabricated an electrostatically gated 1D system in which we observe spin-charge separation and also the predicted power-law suppression of tunneling into the 1D system. The spin-charge separation persists even beyond the low-energy regime where the TLL approximation should hold. TLL effects should therefore also be important in similar, but shorter, electrostatically gated wires, where interaction effects are being studied extensively worldwide.},
	number = {5940},
	urldate = {2014-09-18},
	journal = {Science (New York, N.Y.)},
	author = {Jompol, Y and Ford, C J B and Griffiths, J P and Farrer, I and Jones, G a C and Anderson, D and Ritchie, D a and Silk, T W and Schofield, a J},
	month = jul,
	year = {2009},
	pmid = {19644117},
	pages = {597--601},
}

@article{auslaender_spin-charge_2005,
	title = {Spin-charge separation and localization in one dimension.},
	volume = {308},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/15802599},
	doi = {10.1126/science.1107821},
	abstract = {We report on measurements of quantum many-body modes in ballistic wires and their dependence on Coulomb interactions, obtained by tunneling between two parallel wires in an GaAs/AlGaAs heterostructure while varying electron density. We observed two spin modes and one charge mode of the coupled wires and mapped the dispersion velocities of the modes down to a critical density, at which spontaneous localization was observed. Theoretical calculations of the charge velocity agree well with the data, although they also predict an additional charge mode that was not observed. The measured spin velocity was smaller than theoretically predicted.},
	number = {5718},
	urldate = {2014-10-16},
	journal = {Science (New York, N.Y.)},
	author = {Auslaender, O M and Steinberg, H and Yacoby, a and Tserkovnyak, Y and Halperin, B I and Baldwin, K W and Pfeiffer, L N and West, K W},
	month = apr,
	year = {2005},
	pmid = {15802599},
	pages = {88--92},
}

@article{timm_theory_2011,
	title = {Theory of {Magnetism}},
	author = {Timm, Carsten},
	year = {2011},
}

@article{noauthor_chapter_nodate,
	title = {Chapter 3 {Magnetic} {Model}},
}

@article{me__1959,
	title = {' 2i+1},
	volume = {834},
	number = {1956},
	author = {Me, V O I U and Ber, N U M},
	year = {1959},
}

@article{fan_221a_2002,
	title = {{221A} {Lecture} {Notes} {Fine} and {Hyperfine} {Structures} of the {Hydrogen} {Atom}},
	volume = {209},
	author = {Fan, Xiao-ming and Burles, Scott},
	year = {2002},
	pages = {207--209},
}

@article{noauthor_first-principles_nodate,
	title = {First-{Principles} {Calculation} of {Exchange} {Interactions}},
	pages = {16--24},
}

@article{aharonov_time_1961,
	title = {Time in the {Quantum} {Theory} and the {Uncertainty} {Relation} for {Time} and {Energy}},
	volume = {i},
	url = {http://journals.aps.org/pr/abstract/10.1103/PhysRev.122.1649},
	number = {2},
	urldate = {2014-09-11},
	journal = {Physical Review},
	author = {Aharonov, Y and Bohm, D},
	year = {1961},
}

@article{anandan_geometry_1990,
	title = {Geometry of quantum evolution},
	volume = {65},
	url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.65.1697},
	number = {14},
	urldate = {2014-09-11},
	journal = {Physical review letters},
	author = {Anandan, J and Aharonov, Y},
	year = {1990},
	pages = {1697--1700},
}

@article{aharonov_measuring_2002,
	title = {Measuring energy, estimating {Hamiltonians}, and the time-energy uncertainty relation},
	volume = {66},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.66.052107},
	doi = {10.1103/PhysRevA.66.052107},
	number = {5},
	urldate = {2014-09-11},
	journal = {Physical Review A},
	author = {Aharonov, Y. and Massar, S. and Popescu, S.},
	month = nov,
	year = {2002},
	pages = {052107},
}

@article{aharonov_answer_1964,
	title = {Answer to {Fock} {Concerning} the {Time} {Energy} {Indeterminacy}},
	url = {http://journals.aps.org/pr/abstract/10.1103/PhysRev.134.B1417},
	urldate = {2014-09-11},
	journal = {Physical Review},
	author = {Aharonov, Yakir and Bohm, David},
	year = {1964},
}

@article{cesare_adiabatic_2014,
	title = {Adiabatic topological quantum computing},
	url = {http://arxiv.org/abs/1406.2690},
	abstract = {Topological quantum computing promises error-resistant quantum computation without active error correction. However, there is a worry that during the process of executing quantum gates by braiding anyons around each other, extra anyonic excitations will be created that will disorder the encoded quantum information. Here we explore this question in detail by studying adiabatic code deformations on Hamiltonians based on topological codes, notably Kitaev's surface codes and the more recently discovered color codes. We develop protocols that enable universal quantum computing by adiabatic evolution in a way that keeps the energy gap of the system constant with respect to the computation size and introduces only simple local Hamiltonian interactions. This allows one to perform holonomic quantum computing with these topological quantum computing systems. The tools we develop allow one to go beyond numerical simulations and understand these processes analytically.},
	urldate = {2014-09-04},
	author = {Cesare, Chris and Landahl, Andrew J. and Bacon, Dave and Flammia, Steven T. and Neels, Alice},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2690
Genre: Quantum Physics},
	pages = {23},
}

@article{xiao_berry_2010,
	title = {Berry phase effects on electronic properties},
	volume = {82},
	issn = {0034-6861},
	url = {http://link.aps.org/doi/10.1103/RevModPhys.82.1959},
	doi = {10.1103/RevModPhys.82.1959},
	number = {3},
	urldate = {2014-07-14},
	journal = {Reviews of Modern Physics},
	author = {Xiao, Di and Chang, Ming-Che and Niu, Qian},
	month = jul,
	year = {2010},
	pages = {1959--2007},
}

@article{oi_unitary_2014,
	title = {Unitary {Holonomies} by {Direct} {Degenerate} {Projections}},
	author = {Oi, Daniel K L},
	year = {2014},
	note = {arXiv: 1402.1104v3},
	pages = {1--4},
}

@article{brell_generalized_nodate,
	title = {Generalized {Cluster} {States} {Based} on {Finite} {Groups}},
	author = {Brell, Courtney G},
	note = {arXiv: 1408.6237v1},
}

@article{childs_hamiltonian_nodate,
	title = {Hamiltonian {Simulation} {Using} {Linear} {Combinations} of {Unitary} {Operations}},
	number = {1},
	author = {Childs, Andrew M and Wiebe, Nathan},
	note = {arXiv: 1202.5822v1},
	pages = {1--18},
}

@article{solinas_robustness_2004,
	title = {Robustness of non-{Abelian} holonomic quantum gates against parametric noise},
	volume = {70},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.70.042316},
	doi = {10.1103/PhysRevA.70.042316},
	number = {4},
	urldate = {2014-08-26},
	journal = {Physical Review A},
	author = {Solinas, Paolo and Zanardi, Paolo and Zanghì, Nino},
	month = oct,
	year = {2004},
	pages = {042316},
}

@article{thouless_exchange_1965,
	title = {Exchange in solid {3He} and the {Heisenberg} {Hamiltonian}},
	volume = {893},
	url = {http://iopscience.iop.org/0370-1328/86/5/301},
	urldate = {2014-08-15},
	journal = {Proceedings of the Physical Society},
	author = {Thouless, DJ},
	year = {1965},
}

@article{wootters_entanglement_1998,
	title = {Entanglement of {Formation} of an {Arbitrary} {State} of {Two} {Qubits}},
	volume = {80},
	number = {10},
	author = {Wootters, William K},
	year = {1998},
	pages = {2245--2248},
}

@article{massar_optimal_1995,
	title = {Optimal {Extraction} of {Information} from {Finite} {Quantum} {Ensembles}},
	volume = {74},
	url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.74.1259},
	number = {February},
	urldate = {2014-08-14},
	journal = {Physical review letters},
	author = {Massar, S and Popescu, S},
	year = {1995},
	pages = {1259--1263},
}

@article{horodecki_general_1999,
	title = {General teleportation channel, singlet fraction, and quasidistillation},
	volume = {60},
	url = {http://journals.aps.org/pra/abstract/10.1103/PhysRevA.60.1888},
	number = {3},
	urldate = {2014-11-13},
	journal = {Physical Review A},
	author = {Horodecki, M and Horodecki, P and Horodecki, Ryszard},
	year = {1999},
	pages = {1888--1898},
}

@article{bose_quantum_nodate,
	title = {Quantum {Communication} {Through} an {Unmodulated} {Spin} {Chain}},
	author = {Bose, Sougato},
	note = {arXiv: quant-ph/0212041v2},
	pages = {1--4},
}

@article{wineland_quantum_2011,
	title = {Quantum information processing and metrology with trapped ions},
	volume = {8},
	issn = {16122011},
	url = {http://stacks.iop.org/1612-202X/8/175},
	doi = {10.1002/lapl.201010125},
	number = {3},
	urldate = {2014-08-13},
	journal = {Laser Physics Letters},
	author = {Wineland, D.J. and Leibfried, D.},
	month = mar,
	year = {2011},
	keywords = {coherent quantum control, quantum information processing, quantum state control, trapped ions},
	pages = {175--188},
}

@article{haffner_scalable_2005,
	title = {Scalable multiparticle entanglement of trapped ions.},
	volume = {438},
	issn = {1476-4687},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16319886},
	doi = {10.1038/nature04279},
	abstract = {The generation, manipulation and fundamental understanding of entanglement lies at the very heart of quantum mechanics. Entangled particles are non-interacting but are described by a common wavefunction; consequently, individual particles are not independent of each other and their quantum properties are inextricably interwoven. The intriguing features of entanglement become particularly evident if the particles can be individually controlled and physically separated. However, both the experimental realization and characterization of entanglement become exceedingly difficult for systems with many particles. The main difficulty is to manipulate and detect the quantum state of individual particles as well as to control the interaction between them. So far, entanglement of four ions or five photons has been demonstrated experimentally. The creation of scalable multiparticle entanglement demands a non-exponential scaling of resources with particle number. Among the various kinds of entangled states, the 'W state' plays an important role as its entanglement is maximally persistent and robust even under particle loss. Such states are central as a resource in quantum information processing and multiparty quantum communication. Here we report the scalable and deterministic generation of four-, five-, six-, seven- and eight-particle entangled states of the W type with trapped ions. We obtain the maximum possible information on these states by performing full characterization via state tomography, using individual control and detection of the ions. A detailed analysis proves that the entanglement is genuine. The availability of such multiparticle entangled states, together with full information in the form of their density matrices, creates a test-bed for theoretical studies of multiparticle entanglement. Independently, 'Greenberger-Horne-Zeilinger' entangled states with up to six ions have been created and analysed in Boulder.},
	number = {7068},
	urldate = {2014-07-21},
	journal = {Nature},
	author = {Häffner, H and Hänsel, W and Roos, C F and Benhelm, J and Chek-al-Kar, D and Chwalla, M and Körber, T and Rapol, U D and Riebe, M and Schmidt, P O and Becher, C and Gühne, O and Dür, W and Blatt, R},
	month = dec,
	year = {2005},
	pmid = {16319886},
	pages = {643--6},
}

@article{saeedi_room-temperature_2013,
	title = {Room-temperature quantum bit storage exceeding 39 minutes using ionized donors in silicon-28.},
	volume = {342},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24233718},
	doi = {10.1126/science.1239584},
	abstract = {Quantum memories capable of storing and retrieving coherent information for extended times at room temperature would enable a host of new technologies. Electron and nuclear spin qubits using shallow neutral donors in semiconductors have been studied extensively but are limited to low temperatures (≲10 kelvin); however, the nuclear spins of ionized donors have the potential for high-temperature operation. We used optical methods and dynamical decoupling to realize this potential for an ensemble of phosphorous-31 donors in isotopically purified silicon-28 and observed a room-temperature coherence time of over 39 minutes. We further showed that a coherent spin superposition can be cycled from 4.2 kelvin to room temperature and back, and we report a cryogenic coherence time of 3 hours in the same system.},
	number = {6160},
	urldate = {2014-08-07},
	journal = {Science (New York, N.Y.)},
	author = {Saeedi, Kamyar and Simmons, Stephanie and Salvail, Jeff Z and Dluhy, Phillip and Riemann, Helge and Abrosimov, Nikolai V and Becker, Peter and Pohl, Hans-Joachim and Morton, John J L and Thewalt, Mike L W},
	month = nov,
	year = {2013},
	pmid = {24233718},
	pages = {830--3},
}

@article{tyryshkin_electron_2012,
	title = {Electron spin coherence exceeding seconds in high-purity silicon.},
	volume = {11},
	issn = {1476-1122},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22138791},
	doi = {10.1038/nmat3182},
	abstract = {Silicon is one of the most promising semiconductor materials for spin-based information processing devices. Its advanced fabrication technology facilitates the transition from individual devices to large-scale processors, and the availability of a (28)Si form with no magnetic nuclei overcomes a primary source of spin decoherence in many other materials. Nevertheless, the coherence lifetimes of electron spins in the solid state have typically remained several orders of magnitude lower than that achieved in isolated high-vacuum systems such as trapped ions. Here we examine electron spin coherence of donors in pure (28)Si material (residual (29)Si concentration {\textless}50 ppm) with donor densities of 10(14)-10(15) cm(-3). We elucidate three mechanisms for spin decoherence, active at different temperatures, and extract a coherence lifetime T(2) up to 2 s. In this regime, we find the electron spin is sensitive to interactions with other donor electron spins separated by {\textasciitilde}200 nm. A magnetic field gradient suppresses such interactions, producing an extrapolated electron spin T(2) of 10 s at 1.8 K. These coherence lifetimes are without peer in the solid state and comparable to high-vacuum qubits, making electron spins of donors in silicon ideal components of quantum computers, or quantum memories for systems such as superconducting qubits.},
	number = {2},
	urldate = {2014-07-23},
	journal = {Nature materials},
	author = {Tyryshkin, Alexei M and Tojo, Shinichi and Morton, John J L and Riemann, Helge and Abrosimov, Nikolai V and Becker, Peter and Pohl, Hans-Joachim and Schenkel, Thomas and Thewalt, Michael L W and Itoh, Kohei M and Lyon, S a},
	month = feb,
	year = {2012},
	pmid = {22138791},
	note = {Publisher: Nature Publishing Group},
	pages = {143--7},
}

@article{muhonen_storing_nodate,
	title = {Storing quantum information for 30 seconds in a nanoelectronic device},
	author = {Muhonen, Juha T and Dehollain, Juan P and Laucht, Arne and Hudson, Fay E and Sekiguchi, Takeharu and Itoh, M and Jamieson, David N and Mccallum, Jeffrey C and Dzurak, Andrew S and Morello, Andrea},
	note = {arXiv: 1402.7140v1},
	pages = {1--14},
}

@article{steger_quantum_2012,
	title = {Quantum information storage for over 180 s using donor spins in a {28Si} "semiconductor vacuum".},
	volume = {336},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22679091},
	doi = {10.1126/science.1217635},
	abstract = {A quantum computer requires systems that are isolated from their environment, but can be integrated into devices, and whose states can be measured with high accuracy. Nuclear spins in solids promise long coherence lifetimes, but they are difficult to initialize into known states and to detect with high sensitivity. We show how the distinctive optical properties of enriched (28)Si enable the use of hyperfine-resolved optical transitions, as previously applied to great effect for isolated atoms and ions in vacuum. Together with efficient Auger photoionization, these resolved hyperfine transitions permit rapid nuclear hyperpolarization and electrical spin-readout. We combine these techniques to detect nuclear magnetic resonance from dilute (31)P in the purest available sample of (28)Si, at concentrations inaccessible to conventional measurements, measuring a solid-state coherence time of over 180 seconds.},
	number = {6086},
	urldate = {2014-08-05},
	journal = {Science (New York, N.Y.)},
	author = {Steger, M and Saeedi, K and Thewalt, M L W and Morton, J J L and Riemann, H and Abrosimov, N V and Becker, P and Pohl, H-J},
	month = jun,
	year = {2012},
	pmid = {22679091},
	pages = {1280--3},
}

@article{morton_solid-state_2008,
	title = {Solid-state quantum memory using the {31P} nuclear spin},
	volume = {455},
	issn = {0028-0836},
	url = {http://www.nature.com/doifinder/10.1038/nature07295},
	doi = {10.1038/nature07295},
	number = {7216},
	urldate = {2014-08-01},
	journal = {Nature},
	author = {Morton, John J. L. and Tyryshkin, Alexei M. and Brown, Richard M. and Shankar, Shyam and Lovett, Brendon W. and Ardavan, Arzhang and Schenkel, Thomas and Haller, Eugene E. and Ager, Joel W. and Lyon, S. a.},
	month = oct,
	year = {2008},
	pages = {1085--1088},
}

@article{ladd_quantum_2010,
	title = {Quantum computers.},
	volume = {464},
	issn = {1476-4687},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/20203602},
	doi = {10.1038/nature08812},
	abstract = {Over the past several decades, quantum information science has emerged to seek answers to the question: can we gain some advantage by storing, transmitting and processing information encoded in systems that exhibit unique quantum properties? Today it is understood that the answer is yes, and many research groups around the world are working towards the highly ambitious technological goal of building a quantum computer, which would dramatically improve computational power for particular tasks. A number of physical systems, spanning much of modern physics, are being developed for quantum computation. However, it remains unclear which technology, if any, will ultimately prove successful. Here we describe the latest developments for each of the leading approaches and explain the major challenges for the future.},
	number = {7285},
	urldate = {2014-07-14},
	journal = {Nature},
	author = {Ladd, T D and Jelezko, F and Laflamme, R and Nakamura, Y and Monroe, C and O'Brien, J L},
	month = mar,
	year = {2010},
	pmid = {20203602},
	note = {Publisher: Nature Publishing Group},
	pages = {45--53},
}

@article{buluta_natural_2011,
	title = {Natural and artificial atoms for quantum computation},
	volume = {74},
	issn = {0034-4885},
	url = {http://stacks.iop.org/0034-4885/74/i=10/a=104401?key=crossref.df95aa48b98941f9e7da83a8881a175e},
	doi = {10.1088/0034-4885/74/10/104401},
	number = {10},
	urldate = {2014-07-29},
	journal = {Reports on Progress in Physics},
	author = {Buluta, Iulia and Ashhab, Sahel and Nori, Franco},
	month = oct,
	year = {2011},
	pages = {104401},
}

@article{morley_review_nodate,
	title = {Review : {Towards} {Spintronic} {Quantum} {Technologies} with {Dopants} in {Silicon} {Table} of {Contents}},
	author = {Morley, Gavin W},
	pages = {1--13},
}

@article{hornberger_5_2009,
	title = {5 {Introduction} to {Decoherence} {Theory}},
	volume = {768},
	author = {Hornberger, Klaus},
	year = {2009},
	note = {arXiv: quant-ph/0612118v3},
	pages = {223--278},
}

@article{divincenzo_physical_2008,
	title = {The {Physical} {Implementation} of {Quantum} {Computation}},
	author = {Divincenzo, David P},
	year = {2008},
	note = {arXiv: quant-ph/0002077v3},
}

@article{pachos_focus_2014,
	title = {Focus on topological quantum computation},
	volume = {16},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/16/i=6/a=065003?key=crossref.c62a38a89888ee26e5875574752c488f},
	doi = {10.1088/1367-2630/16/6/065003},
	number = {6},
	urldate = {2014-07-24},
	journal = {New Journal of Physics},
	author = {Pachos, Jiannis K and Simon, Steven H},
	month = jun,
	year = {2014},
	note = {Publisher: IOP Publishing},
	pages = {065003},
}

@book{causon_introductory_nodate,
	title = {Introductory {Finite} {Difference} {Methods} for {PDEs}},
	isbn = {978-87-7681-642-1},
	author = {Causon, D M and Mingham, C G},
}

@article{pillai_matrix_2012,
	title = {Matrix {Numerov} {Method} for {Solving} {Schr} ¨ odinger ’ s {Equation}},
	author = {Pillai, Mohandas and Goglio, Joshua and Walker, Thad G},
	year = {2012},
	pages = {1--7},
}

@article{takahashi_half-filed_nodate,
	title = {Half-filed {Hubbard} model at low temperature},
	volume = {1289},
	author = {Takahashi, Minoru},
}

@article{ledoux_numerical_2007,
	title = {The numerical solution of {Sturm}-{Liouville} and {Schrödinger} eigenvalue problems {Matrix} methods {Discretization} methods {Pros} and {Cons}},
	volume = {2007},
	author = {Ledoux, V},
	year = {2007},
}

@misc{noauthor_schrod7.pdf_nodate,
	title = {schrod7.pdf},
}

@article{mckague_simulating_2009,
	title = {Simulating {Quantum} {Systems} {Using} {Real} {Hilbert} {Spaces}},
	volume = {102},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.102.020505},
	doi = {10.1103/PhysRevLett.102.020505},
	number = {2},
	urldate = {2014-06-11},
	journal = {Physical Review Letters},
	author = {McKague, Matthew and Mosca, Michele and Gisin, Nicolas},
	month = jan,
	year = {2009},
	pages = {020505},
}

@book{unger_noise_nodate,
	title = {Noise in {Quantum} and {Classical} {Computation} \& {Non}-locality},
	isbn = {90-6196-547-0},
	author = {Unger, Falk},
}

@article{noauthor_propagator_2010,
	title = {The {Propagator} and the {Path} {Integral}},
	year = {2010},
	pages = {1--31},
}

@article{bernstein_quantum_1997,
	title = {Quantum {Complexity} {Theory}},
	volume = {26},
	issn = {0097-5397},
	url = {http://epubs.siam.org/doi/abs/10.1137/S0097539796300921},
	doi = {10.1137/S0097539796300921},
	number = {5},
	journal = {SIAM Journal on Computing},
	author = {Bernstein, Ethan and Vazirani, Umesh},
	month = oct,
	year = {1997},
	keywords = {03d10, 03d15, 68q05, 68q15, ams subject classifications, fourier sampling, mial time, pii, quantum computation, quantum polyno-, quantum turing machines, reversibility, s0097539796300921, universal quantum turing machine},
	pages = {1411--1473},
}

@article{ambrose_path_nodate,
	title = {Path {Integral} {Formulation} of {Quantum} {Tunneling}: {Numerical} {Approximation} and {Application} to {Coupled} {Domain} {Wall} {Pinning}},
	urldate = {2014-07-15},
	author = {Ambrose, Matthew},
}

@article{vainshtein_abc_1982,
	title = {{ABC} of instantons},
	volume = {195},
	url = {http://iopscience.iop.org/0038-5670/25/4/R01},
	urldate = {2014-06-25},
	journal = {Soviet Physics …},
	author = {Vaĭnshteĭn, AI and Zakharov, VI},
	year = {1982},
}

@article{kleinert_path_2013,
	title = {Path {Integrals}—{Elementary} {Properties} and {Simple} {Solutions}},
	volume = {2013},
	urldate = {2014-06-25},
	journal = {users.physik.fu-berlin.de},
	author = {Kleinert, H},
	year = {2013},
}

@article{zinn-justin_quantum_2002,
	title = {Quantum {Field} theory and {Critical} {Phenomena}},
	url = {http://cds.cern.ch/record/782802},
	urldate = {2014-06-25},
	author = {Zinn-Justin, J},
	year = {2002},
}

@article{mackenzie_path_nodate,
	title = {Path {Integral} {Methods} and {Applications} ∗},
	number = {January 2000},
	author = {Mackenzie, Richard and Ren, Laboratoire and Canada, Q C H C},
	note = {arXiv: quant-ph/0004090v1},
}

@article{asadzadeh_introduction_2010,
	title = {An {Introduction} to the {Finite} {Element} {Method} ({FEM}) for {Differential} {Equations}},
	author = {Asadzadeh, Mohammad},
	year = {2010},
}

@article{coleman_instantons_nodate,
	title = {Instantons},
	volume = {2},
	author = {Coleman, S and Rajaraman, R},
}

@article{morley_initialization_2010,
	title = {The initialization and manipulation of quantum information stored in silicon by bismuth dopants.},
	volume = {9},
	issn = {1476-1122},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/20711180},
	doi = {10.1038/nmat2828},
	abstract = {A prerequisite for exploiting spins for quantum data storage and processing is long spin coherence times. Phosphorus dopants in silicon (Si:P) have been favoured as hosts for such spins because of measured electron spin coherence times (T2) longer than any other electron spin in the solid state: 14 ms at 7 K with isotopically purified silicon. Heavier impurities such as bismuth in silicon (Si:Bi) could be used in conjunction with Si:P for quantum information proposals that require two separately addressable spin species. However, the question of whether the incorporation of the much less soluble Bi into Si leads to defect species that destroy coherence has not been addressed. Here we show that schemes involving Si:Bi are indeed feasible as the electron spin coherence time T2 is at least as long as for Si:P with non-isotopically purified silicon. We polarized the Si:Bi electrons and hyperpolarized the I=9/2 nuclear spin of (209)Bi, manipulating both with pulsed magnetic resonance. The larger nuclear spin means that a Si:Bi dopant provides a 20-dimensional Hilbert space rather than the four-dimensional Hilbert space of an I=1/2 Si:P dopant.},
	number = {9},
	urldate = {2014-05-24},
	journal = {Nature materials},
	author = {Morley, Gavin W and Warner, Marc and Stoneham, a Marshall and Greenland, P Thornton and van Tol, Johan and Kay, Christopher W M and Aeppli, Gabriel},
	month = sep,
	year = {2010},
	pmid = {20711180},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bismuth, Bismuth: chemistry, Electrons, Magnetic Resonance Spectroscopy, Phosphorus, Phosphorus: chemistry, Silicon, Silicon: chemistry},
	pages = {725--9},
}

@article{james_quantum_nodate,
	title = {Quantum dynamics of cold trapped ions , with application to quantum computation},
	author = {James, Daniel F V},
	note = {arXiv: quant-ph/9702053v1},
}

@article{wunderlich_spin_2003,
	title = {Spin resonance with trapped ions},
	volume = {4075},
	url = {http://iopscience.iop.org/0953-4075/36/5/325},
	number = {03},
	urldate = {2014-05-26},
	journal = {Journal of Physics B: …},
	author = {Wunderlich, C and Balzer, C},
	year = {2003},
	pages = {1063--1072},
}

@article{browne_one-way_nodate,
	title = {One-way {Quantum} {Computation}},
	author = {Browne, Dan and Briegel, Hans},
	note = {arXiv: quant-ph/0603226v2},
	pages = {1--23},
}

@article{sorensen_quantum_1999,
	title = {Quantum {Computation} with {Ions} in {Thermal} {Motion}},
	volume = {82},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.82.1971},
	doi = {10.1103/PhysRevLett.82.1971},
	number = {9},
	journal = {Physical Review Letters},
	author = {Sørensen, Anders and Mølmer, Klaus},
	month = mar,
	year = {1999},
	pages = {1971--1974},
}

@article{chakravarty_wigner_2008,
	title = {Wigner {Glass}, {Spin}-liquids, and the {Metal}-{Insulator} {Transition}},
	author = {Chakravarty, Sudip and Kivelson, Steven and Nayak, Chetan and Voelker, Klaus},
	year = {2008},
	note = {arXiv: cond-mat/9805383v2},
}

@article{so_entanglement_2000,
	title = {Entanglement and quantum computation with ions in thermal motion},
	volume = {62},
	number = {February},
	author = {So, Anders and Mo, Klaus},
	year = {2000},
	pages = {1--11},
}

@article{meyer_transition_2007,
	title = {Transition from a one-dimensional to a quasi-one-dimensional state in interacting quantum wires},
	url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.98.126404},
	urldate = {2014-06-25},
	journal = {Physical review letters},
	author = {Meyer, JS and Matveev, KA and Larkin, AI},
	year = {2007},
	note = {arXiv: cond-mat/0612101v1},
	pages = {1--4},
}

@article{tserkovnyak_aharonov-casher_2008,
	title = {Aharonov-{Casher} {Effect} in {Wigner} {Crystal} {Exchange} {Interactions}},
	url = {http://arxiv.org/abs/0809.0598},
	number = {2},
	urldate = {2014-05-07},
	journal = {arXiv preprint arXiv:0809.0598},
	author = {Tserkovnyak, Yaroslav and Kindermann, Markus},
	year = {2008},
	note = {arXiv: 0809.0598v2},
	pages = {1--4},
}

@article{matveev_conductance_2003,
	title = {Conductance of a quantum wire in the {Wigner} crystal regime},
	volume = {27708},
	author = {Matveev, K A},
	year = {2003},
	note = {arXiv: cond-mat/0310261v1},
}

@article{fogler_exchange_2005,
	title = {Exchange interaction in quantum rings and wires in the {Wigner}-crystal limit},
	volume = {72},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.72.195344},
	doi = {10.1103/PhysRevB.72.195344},
	number = {19},
	urldate = {2014-05-07},
	journal = {Physical Review B},
	author = {Fogler, Michael and Pivovarov, Eugene},
	month = nov,
	year = {2005},
	pages = {195344},
}

@article{Osborne1961,
	title = {A {Note} on {Finite} {Difference} {Methods} for {Solving} the {Eigenvalue} {Problems} of {Second}-{Order} {Differential} {Equations}},
	author = {Osborne, By M R and Xg, J Vi-i Í},
	year = {1961},
}

@article{cleve_quantum_1996,
	title = {Quantum {Algorithms} {Revisited}},
	author = {Cleve, B R and Ekert, A and Macchiavello, C and Mosca, M},
	year = {1996},
	note = {arXiv: quant-ph/9708016v1},
}

@article{coleman_uses_1979,
	title = {The uses of instantons},
	url = {http://link.springer.com/chapter/10.1007/978-1-4684-0991-8_16},
	urldate = {2014-05-06},
	journal = {The Whys of Subnuclear Physics},
	author = {Coleman, S},
	year = {1979},
	pages = {805--941},
}

@article{Leveque2006,
	title = {Finite {Difference} {Methods} for {Differential} {Equations}},
	author = {Leveque, Randall J},
	year = {2006},
	pages = {1998--2006},
}

@article{abbott_understanding_2010,
	title = {Understanding the {Quantum} {Computational} {Speed}-up via {De}-quantisation},
	volume = {26},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/1006.1419v1},
	doi = {10.4204/EPTCS.26.1},
	number = {Dcm},
	urldate = {2014-05-02},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Abbott, Alastair a. and Calude, Cristian S.},
	month = jun,
	year = {2010},
	pages = {1--12},
}

@article{nest_simulating_2010,
	title = {Simulating quantum computers with probabilistic methods},
	author = {Nest, Maarten Van Den},
	year = {2010},
	note = {arXiv: 0911.1624v3},
	pages = {1--26},
}

@article{nest_classical_2009,
	title = {Classical simulation of quantum computation , the {Gottesman}-{Knill} theorem , and slightly beyond},
	author = {Nest, Maarten Van Den},
	year = {2009},
	note = {arXiv: 0811.0898v2},
	pages = {1--14},
}

@article{Zhou2006,
	title = {Exchange frequencies and magnetic ground state of {Wigner} crystal in anisotropic two-dimensional electron systems},
	volume = {74},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.74.205325},
	doi = {10.1103/PhysRevB.74.205325},
	number = {20},
	urldate = {2014-05-02},
	journal = {Physical Review B},
	author = {Zhou, Chenggang and Bhatt, R.},
	month = nov,
	year = {2006},
	pages = {205325},
}

@article{tan_towards_nodate,
	title = {Towards {Quantifying} {Complexity} with {Quantum} {Mechanics}},
	author = {Tan, Ryan and Terno, Daniel R and Thompson, Jayne and Vedral, Vlatko and Gu, Mile},
	note = {arXiv: 1404.6255v1},
	pages = {1--10},
}

@article{Bolton2008,
	title = {Eigenvalues of differential equations by finite-difference methods},
	volume = {52},
	issn = {0305-0041},
	url = {http://www.journals.cambridge.org/abstract_S0305004100031200},
	doi = {10.1017/S0305004100031200},
	number = {02},
	urldate = {2014-05-02},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Bolton, H. C. and Scoins, H. I. and Rushbrooke, G. S.},
	month = oct,
	year = {2008},
	pages = {215},
}

@article{Roger2010,
	title = {Multiple-{Spin} {Exchange} in {Strongly} {Correlated} {Fermion} {Systems}},
	volume = {162},
	issn = {0022-2291},
	url = {http://link.springer.com/10.1007/s10909-010-0293-1},
	doi = {10.1007/s10909-010-0293-1},
	number = {5-6},
	urldate = {2014-05-02},
	journal = {Journal of Low Temperature Physics},
	author = {Roger, M.},
	month = dec,
	year = {2010},
	keywords = {1 a brief historical, beginning of the 20th, century in a, correlated fermions, exchange appeared at the, introduction, magnetism, mott-hubbard transition, quantum solids, spin liquids, the concept of spin},
	pages = {625--637},
}

@article{fuechsle_single-atom_2012,
	title = {A single-atom transistor.},
	volume = {7},
	issn = {1748-3395},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22343383},
	doi = {10.1038/nnano.2012.21},
	abstract = {The ability to control matter at the atomic scale and build devices with atomic precision is central to nanotechnology. The scanning tunnelling microscope can manipulate individual atoms and molecules on surfaces, but the manipulation of silicon to make atomic-scale logic circuits has been hampered by the covalent nature of its bonds. Resist-based strategies have allowed the formation of atomic-scale structures on silicon surfaces, but the fabrication of working devices-such as transistors with extremely short gate lengths, spin-based quantum computers and solitary dopant optoelectronic devices-requires the ability to position individual atoms in a silicon crystal with atomic precision. Here, we use a combination of scanning tunnelling microscopy and hydrogen-resist lithography to demonstrate a single-atom transistor in which an individual phosphorus dopant atom has been deterministically placed within an epitaxial silicon device architecture with a spatial accuracy of one lattice site. The transistor operates at liquid helium temperatures, and millikelvin electron transport measurements confirm the presence of discrete quantum levels in the energy spectrum of the phosphorus atom. We find a charging energy that is close to the bulk value, previously only observed by optical spectroscopy.},
	number = {4},
	urldate = {2014-04-29},
	journal = {Nature nanotechnology},
	author = {Fuechsle, Martin and Miwa, Jill a and Mahapatra, Suddhasatta and Ryu, Hoon and Lee, Sunhee and Warschkow, Oliver and Hollenberg, Lloyd C L and Klimeck, Gerhard and Simmons, Michelle Y},
	month = apr,
	year = {2012},
	pmid = {22343383},
	note = {Publisher: Nature Publishing Group},
	keywords = {Equipment Design, Equipment Failure Analysis, Microelectrodes, Nanotechnology, Nanotechnology: instrumentation, Transistors, Electronic},
	pages = {242--6},
}

@article{aolita_noisy_2010,
	title = {Noisy evolution of graph-state entanglement},
	volume = {82},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.82.032317},
	doi = {10.1103/PhysRevA.82.032317},
	number = {3},
	urldate = {2014-04-23},
	journal = {Physical Review A},
	author = {Aolita, L. and Cavalcanti, D. and Chaves, R. and Dhara, C. and Davidovich, L. and Acín, a.},
	month = sep,
	year = {2010},
	pages = {032317},
}

@article{Arbenz2010,
	title = {Lecture {Notes} on {Solving} {Large} {Scale} {Eigenvalue} {Problems}},
	author = {Arbenz, Prof Peter},
	year = {2010},
}

@article{Barzilai1988,
	title = {Two-{Point} {Step} {Size} {Gradient} {Methods}},
	volume = {8},
	issn = {0272-4979},
	url = {http://imanum.oxfordjournals.org/cgi/doi/10.1093/imanum/8.1.141},
	doi = {10.1093/imanum/8.1.141},
	number = {1},
	journal = {IMA Journal of Numerical Analysis},
	author = {Barzilai, Jonathan and Borwein, Jonathan M.},
	year = {1988},
	pages = {141--148},
}

@article{hein_multiparty_2004,
	title = {Multiparty entanglement in graph states},
	volume = {69},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.69.062311},
	doi = {10.1103/PhysRevA.69.062311},
	number = {6},
	urldate = {2014-03-19},
	journal = {Physical Review A},
	author = {Hein, M. and Eisert, J. and Briegel, H.},
	month = jun,
	year = {2004},
	pages = {062311},
}

@article{suchara_constructions_2011,
	title = {Constructions and noise threshold of topological subsystem codes},
	url = {http://iopscience.iop.org/1751-8121/44/15/155301},
	urldate = {2014-04-23},
	journal = {Journal of Physics A: …},
	author = {Suchara, Martin and Bravyi, Sergey and Terhal, Barbara},
	year = {2011},
	note = {arXiv: 1012.0425v2},
	pages = {1--30},
}

@article{crowley_quantum_2014,
	title = {Quantum and {Classical} in {Adiabatic} {Computation}},
	author = {Crowley, P and Ðurić, T and Vinci, W and Warburton, P and Green, A G},
	year = {2014},
	pages = {1--9},
}

@article{holmes_energy-efficient_2013,
	title = {Energy-{Efficient} {Superconducting} {Computing}-{Power} {Budgets} and {Requirements}},
	volume = {23},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6449287},
	number = {3},
	urldate = {2014-04-29},
	author = {Holmes, D and Ripple, A and Manheimer, M},
	year = {2013},
}

@article{islam_onset_2011,
	title = {Onset of a {Quantum} {Phase} {Transition} with a {Trapped} {Ion} {Quantum} {Simulator}},
	number = {2},
	author = {Islam, R and Edwards, E E and Kim, K and Korenblit, S and Noh, C and Carmichael, H and Freericks, J K and Monroe, C},
	year = {2011},
	note = {arXiv: 1103.2400v1},
	pages = {1--7},
}

@article{sterling_fabrication_2014,
	title = {Fabrication and operation of a two-dimensional ion-trap lattice on a high-voltage microchip.},
	volume = {5},
	issn = {2041-1723},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24704758},
	doi = {10.1038/ncomms4637},
	abstract = {Microfabricated ion traps are a major advancement towards scalable quantum computing with trapped ions. The development of more versatile ion-trap designs, in which tailored arrays of ions are positioned in two dimensions above a microfabricated surface, will lead to applications in fields as varied as quantum simulation, metrology and atom-ion interactions. Current surface ion traps often have low trap depths and high heating rates, because of the size of the voltages that can be applied to them, limiting the fidelity of quantum gates. Here we report on a fabrication process that allows for the application of very high voltages to microfabricated devices in general and use this advance to fabricate a two-dimensional ion-trap lattice on a microchip. Our microfabricated architecture allows for reliable trapping of two-dimensional ion lattices, long ion lifetimes, rudimentary shuttling between lattice sites and the ability to deterministically introduce defects into the ion lattice.},
	urldate = {2014-04-10},
	journal = {Nature communications},
	author = {Sterling, R C and Rattanasonti, H and Weidt, S and Lake, K and Srinivasan, P and Webster, S C and Kraft, M and Hensinger, W K},
	month = jan,
	year = {2014},
	pmid = {24704758},
	note = {Publisher: Nature Publishing Group},
	pages = {3637},
}

@article{ivanov_scalable_2011,
	title = {Scalable uniform construction of highly-conditional quantum gates},
	volume = {3},
	author = {Ivanov, Svetoslav S and Vitanov, Nikolay V},
	year = {2011},
	note = {arXiv: 1106.0270v2},
	pages = {1--5},
}

@article{mizel_fault-tolerant_2014,
	title = {Fault-tolerant, {Universal} {Adiabatic} {Quantum} {Computation}},
	url = {http://arxiv.org/abs/1403.7694},
	number = {c},
	urldate = {2014-11-28},
	journal = {arXiv preprint arXiv:1403.7694},
	author = {Mizel, Ari},
	year = {2014},
	note = {arXiv: 1403.7694v1},
	pages = {1--27},
}

@article{beaudrap_quantum_2014,
	title = {Quantum linear network coding as one-way quantum computation},
	author = {Beaudrap, Niel De and Roetteler, Martin},
	year = {2014},
	note = {arXiv: 1403.3533v1},
	keywords = {()},
	pages = {1--17},
}

@article{robertson_uncertainty_2006,
	title = {The {Uncertainty} {Principle}},
	url = {http://www.illc.uva.nl/~seop/entries/qt-uncertainty/},
	urldate = {2014-03-21},
	journal = {Phys. Rev.},
	author = {Robertson, H.P.},
	year = {2006},
}

@article{Matera1900,
	title = {Phase diagram study of a dimerized spin-{S} zig-zag ladder},
	url = {http://arxiv.org/abs/1403.3737},
	number = {1},
	urldate = {2014-06-25},
	journal = {arXiv preprint arXiv:1403.3737},
	author = {Matera, JM and Lamas, CA},
	year = {2014},
	note = {arXiv: 1403.3737v1},
	pages = {1--14},
}

@article{andersson_comparison_2009,
	title = {Comparison of quantum dynamics and quantum transition state theory estimates of the {H} + {CH4} reaction rate.},
	volume = {113},
	issn = {1520-5215},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/19275158},
	doi = {10.1021/jp811070w},
	abstract = {Thermal rate constants are calculated for the H + CH(4) --{\textgreater} CH(3) + H(2) reaction employing the potential energy surface of Espinosa-Garcia (Espinosa-Garcia, J. J. Chem. Phys. 2002, 116, 10664). Two theoretical approaches are used. First, we employ the multiconfigurational time-dependent Hartree method combined with flux correlation functions. In this way rate constants in the range 225-400 K are obtained and compared with previous results using the same theoretical method but the potential energy surface of Wu et al. (Wu, T.; Werner, H.-J.; Manthe, U. Science 2004, 306, 2227). It is found that the Espinosa-Garcia surface results in larger rate constants. Second, a harmonic quantum transition state theory (HQTST) implementation of instanton theory is used to obtain rate constants in a temperature interval from 20 K up to the crossover temperature at 296 K. The HQTST estimates are larger than MCTDH ones by a factor of about three in the common temperature range. Comparison is also made with various tunneling corrections to transition state theory and quantum instanton theory.},
	number = {16},
	journal = {The journal of physical chemistry. A},
	author = {Andersson, Stefan and Nyman, Gunnar and Arnaldsson, Andri and Manthe, Uwe and Jónsson, Hannes},
	month = apr,
	year = {2009},
	pmid = {19275158},
	pages = {4468--78},
}

@article{mandelstam_uncertainty_1991,
	title = {the uncertainty relation between energy and time in non-relativistic quantum mechanics},
	urldate = {2014-03-21},
	journal = {Selected Papers},
	author = {Mandelstam, L and Tamm, I},
	year = {1991},
}

@article{Chandler1981,
	title = {Exploiting the isomorphism between quantum theory and classical statistical mechanics of polyatomic fluids},
	volume = {74},
	issn = {00219606},
	url = {http://link.aip.org/link/?JCP/74/4078/1&Agg=doi},
	doi = {10.1063/1.441588},
	number = {7},
	urldate = {2014-02-26},
	journal = {The Journal of Chemical Physics},
	author = {Chandler, David},
	year = {1981},
	pages = {4078},
}

@article{koiller_shallow_2008,
	title = {Shallow donor wavefunctions and donor-pair exchange in silicon: {Ab} initio theory and floating-phase {Heitler}-{London} approach},
	author = {Koiller, Belita and Capaz, R B},
	year = {2008},
	note = {arXiv: cond-mat/0402266v3},
}

@article{Kawatsu2013,
	title = {An efficient computational method for the implementation of a semi-classical instanton approach using discretized path integrals},
	volume = {454},
	issn = {1742-6596},
	url = {http://stacks.iop.org/1742-6596/454/i=1/a=012030?key=crossref.d44c1732377be4e4b8401f774f841684},
	doi = {10.1088/1742-6596/454/1/012030},
	urldate = {2014-03-14},
	journal = {Journal of Physics: Conference Series},
	author = {Kawatsu, T and Miura, S},
	month = aug,
	year = {2013},
	pages = {012030},
}

@article{Richardson2011,
	title = {Ring-polymer instanton method for calculating tunneling splittings.},
	volume = {134},
	issn = {1089-7690},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21303094},
	doi = {10.1063/1.3530589},
	abstract = {The semiclassical instanton expression for the tunneling splitting between two symmetric wells is rederived, starting from the ring-polymer representation of the quantum partition function. This leads to simpler mathematics by replacing functional determinants with matrix determinants. By exploiting the simple Hückel-like structure of the matrices, we derive an expression for the instanton tunneling splitting in terms of a minimum on the potential surface of a linear polymer. The latter is a section cut out of a ring polymer, consisting of an infinite number of beads, which describes a periodic orbit on the inverted potential surface. The approach is straightforward to generalize to multiple dimensions, and we demonstrate that it is computationally practical by carrying out instanton calculations of tunneling splittings in HO(2) and malonaldehyde in full dimensionality.},
	number = {5},
	urldate = {2014-03-18},
	journal = {The Journal of chemical physics},
	author = {Richardson, Jeremy O and Althorpe, Stuart C},
	month = feb,
	year = {2011},
	pmid = {21303094},
	pages = {054109},
}

@article{chakravarty_wigner_1999,
	title = {Wigner glass, spin liquids and the metal-insulator transition},
	volume = {79},
	issn = {1364-2812},
	url = {http://www.tandfonline.com/doi/abs/10.1080/13642819908214845},
	doi = {10.1080/13642819908214845},
	number = {6},
	urldate = {2014-03-12},
	journal = {Philosophical Magazine Part B},
	author = {Chakravarty, Sudip and Kivelson, Steven and Nayak, Cheta and Voelker, Klaus},
	month = jun,
	year = {1999},
	pages = {859--868},
}

@article{koiller_exchange_2008,
	title = {Exchange coupling in semiconductor nanostructures: {Validity} and limitations of the {Heitler}-{London} approach},
	author = {Koiller, Belita and Sarma, S Das},
	year = {2008},
	note = {arXiv: cond-mat/0512321v2},
	pages = {1--6},
}

@article{Kawatsu2013a,
	title = {An efficient computational method for the implementation of a semi-classical instanton approach using discretized path integrals},
	volume = {454},
	issn = {1742-6596},
	url = {http://stacks.iop.org/1742-6596/454/i=1/a=012030?key=crossref.d44c1732377be4e4b8401f774f841684},
	doi = {10.1088/1742-6596/454/1/012030},
	urldate = {2014-03-14},
	journal = {Journal of Physics: Conference Series},
	author = {Kawatsu, T and Miura, S},
	month = aug,
	year = {2013},
	pages = {012030},
}

@article{Rasanen2003,
	title = {Wigner molecules in polygonal quantum dots: {A} density-functional study},
	url = {http://prb.aps.org/abstract/PRB/v67/i3/e035326},
	urldate = {2014-03-11},
	journal = {Physical Review B},
	author = {Räsänen, E and Saarikoski, H and Puska, MJ and Nieminen, RM},
	year = {2003},
	note = {arXiv: cond-mat/0208244v1},
}

@article{Katano2000,
	title = {Multiple-spin exchange in a two-dimensional {Wigner} crystal},
	volume = {62},
	issn = {0163-1829},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.62.2573},
	doi = {10.1103/PhysRevB.62.2573},
	number = {4},
	urldate = {2014-03-12},
	journal = {Physical Review B},
	author = {Katano, Masafumi and Hirashima, DS},
	month = jul,
	year = {2000},
	pages = {2573--2580},
}

@article{Hirashima2001b,
	title = {Multiple-spin exchange in a two-dimensional {Wigner} crystal},
	volume = {10},
	issn = {13869477},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1386947701000650},
	doi = {10.1016/S1386-9477(01)00065-0},
	number = {1-3},
	journal = {Physica E: Low-dimensional Systems and Nanostructures},
	author = {Hirashima, Dai and Kubo, Katsunori and Katano, Masafumi},
	month = may,
	year = {2001},
	keywords = {aharanov, bohm e ect, multiple-spin exchange, wigner crystal},
	pages = {117--119},
}

@article{Bernu2001,
	title = {Exchange {Frequencies} in the {2D} {Wigner} {Crystal}},
	volume = {86},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.86.870},
	doi = {10.1103/PhysRevLett.86.870},
	number = {5},
	urldate = {2014-03-11},
	journal = {Physical Review Letters},
	author = {Bernu, B. and Cândido, Ladir and Ceperley, D.},
	month = jan,
	year = {2001},
	pages = {870--873},
}

@article{Hirashima2002,
	title = {Note on {Multiple}-{Spin} {Exchange} in the {Two}-{Dimensional} {Wigner} {Crystal}},
	volume = {71},
	issn = {0031-9015},
	url = {http://journals.jps.jp/doi/abs/10.1143/JPSJ.71.1407},
	doi = {10.1143/JPSJ.71.1407},
	number = {5},
	urldate = {2014-03-11},
	journal = {Journal of the Physical Society of Japan},
	author = {Hirashima, Dai S.},
	month = may,
	year = {2002},
	keywords = {exchange interaction, wigner crystal, wkb approx-},
	pages = {1407--1408},
}

@article{Klironomos2005,
	title = {Exchange coupling in a one-dimensional {Wigner} crystal},
	volume = {72},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.72.195343},
	doi = {10.1103/PhysRevB.72.195343},
	number = {19},
	urldate = {2014-03-11},
	journal = {Physical Review B},
	author = {Klironomos, a. and Ramazashvili, R. and Matveev, K.},
	month = nov,
	year = {2005},
	pages = {195343},
}

@article{Hikihara2008,
	title = {Magnetic {Phase} {Diagram} of {Spin}-1/2 {Two}-{Leg} {Ladder} with {Four}-{Spin} {Ring} {Exchanges}},
	volume = {77},
	issn = {0031-9015},
	url = {http://journals.jps.jp/doi/abs/10.1143/JPSJ.77.014709},
	doi = {10.1143/JPSJ.77.014709},
	number = {1},
	urldate = {2014-03-11},
	journal = {Journal of the Physical Society of Japan},
	author = {Hikihara, Toshiya and Yamamoto, Shoji},
	month = jan,
	year = {2008},
	keywords = {density-matrix renormaliza-, frustration, nematic phase, ring exchange, spin ladder, vector chirality},
	pages = {014709},
}

@article{Zhou2008,
	title = {Zero temperature magnetic phase diagram of {Wigner} crystal in anisotropic two-dimensional electron systems},
	volume = {403},
	issn = {09214526},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0921452607012185},
	doi = {10.1016/j.physb.2007.10.319},
	number = {5-9},
	urldate = {2014-03-11},
	journal = {Physica B: Condensed Matter},
	author = {Zhou, Chenggang and Bhatt, Ravin N.},
	month = apr,
	year = {2008},
	keywords = {1, 2, coulomb repulsion, electron systems form a, have predicted, mass anisotropy, phase due to the, quantum monte carlo simulations, ring exchange, that low density two-dimensional, two-dimensional electron system, wigner crystal},
	pages = {1547--1549},
}

@article{Berg2012,
	title = {Electronic liquid crystalline phases in a spin-orbit coupled two-dimensional electron gas},
	volume = {85},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.85.035116},
	doi = {10.1103/PhysRevB.85.035116},
	number = {3},
	urldate = {2014-03-11},
	journal = {Physical Review B},
	author = {Berg, Erez and Rudner, Mark S. and Kivelson, Steven a.},
	month = jan,
	year = {2012},
	pages = {035116},
}

@article{wiebe_higher_2010,
	title = {Higher order decompositions of ordered operator exponentials},
	volume = {1},
	url = {http://iopscience.iop.org/1751-8121/43/6/065203},
	number = {4},
	urldate = {2014-03-09},
	journal = {Journal of Physics A: …},
	author = {Wiebe, Nathan and Berry, Dominic},
	year = {2010},
	note = {arXiv: 0812.0562v3},
	pages = {1--16},
}

@article{huyghebaert_product_1990,
	title = {Product formula methods for time-dependent {Schrodinger} problems},
	volume = {5777},
	url = {http://iopscience.iop.org/0305-4470/23/24/019},
	urldate = {2014-03-09},
	journal = {Journal of Physics A: Mathematical …},
	author = {Huyghebaert, J and Raedt, H De},
	year = {1990},
}

@article{poulin_quantum_2011,
	title = {Quantum {Simulation} of {Time}-{Dependent} {Hamiltonians} and the {Convenient} {Illusion} of {Hilbert} {Space}},
	volume = {106},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.106.170501},
	doi = {10.1103/PhysRevLett.106.170501},
	number = {17},
	urldate = {2014-03-09},
	journal = {Physical Review Letters},
	author = {Poulin, David and Qarry, Angie and Somma, Rolando and Verstraete, Frank},
	month = apr,
	year = {2011},
	pages = {170501},
}

@article{bapst_quantum_2013,
	title = {The quantum adiabatic algorithm applied to random optimization problems: {The} quantum spin glass perspective},
	volume = {523},
	issn = {03701573},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S037015731200347X},
	doi = {10.1016/j.physrep.2012.10.002},
	number = {3},
	urldate = {2014-03-01},
	journal = {Physics Reports},
	author = {Bapst, V. and Foini, L. and Krzakala, F. and Semerjian, G. and Zamponi, F.},
	month = feb,
	year = {2013},
	note = {Publisher: Elsevier B.V.},
	keywords = {quantum spin glasses},
	pages = {127--205},
}

@article{piltz_protecting_2013,
	title = {Protecting {Conditional} {Quantum} {Gates} by {Robust} {Dynamical} {Decoupling}},
	volume = {110},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.110.200501},
	doi = {10.1103/PhysRevLett.110.200501},
	number = {20},
	urldate = {2014-02-21},
	journal = {Physical Review Letters},
	author = {Piltz, Ch. and Scharfenberger, B. and Khromova, a. and Varón, a. F. and Wunderlich, Ch.},
	month = may,
	year = {2013},
	pages = {200501},
}

@article{Cheng1997,
	title = {Variational {Hartree}-{Fock} {Ground}-{State} {Energy} for a {Two}-{Dimensional} {Wigner} {Crystal} {Induced} by a {Strong} {Magnetic} {Field}},
	volume = {35},
	url = {http://psroc.phys.ntu.edu.tw/cjp/download.php?type=full&vol=35&num=6-I&page=718},
	number = {6},
	urldate = {2014-03-11},
	journal = {Chinese Journal of Physics},
	author = {Cheng, SC},
	year = {1997},
}

@article{Misguich1999,
	title = {Spin-liquid phase of the multiple-spin exchange {Hamiltonian} on the triangular lattice},
	volume = {60},
	url = {http://prb.aps.org/abstract/PRB/v60/i2/p1064_1},
	number = {2},
	urldate = {2014-03-07},
	journal = {Physical Review B},
	author = {Misguich, G and Lhuillier, C and Bernu, B and Waldtmann, C},
	year = {1999},
	pages = {1064--1074},
}

@article{Hirashima2001a,
	title = {Magnetism of a {Bilayer} {Wigner} {Crystal}},
	volume = {70},
	issn = {0031-9015},
	url = {http://journals.jps.jp/doi/abs/10.1143/JPSJ.70.931},
	doi = {10.1143/JPSJ.70.931},
	number = {4},
	journal = {Journal of the Physical Society of Japan},
	author = {Hirashima, Dai S.},
	month = apr,
	year = {2001},
	keywords = {1, 2d, a 2d, a long period, a surface of liquid, exchange interaction, have been studied over, one-dimensional heisenberg model, tal, the properties of a, to be realized on, two-dimensional, wc, wc was first found, wigner crys-, wigner crystal},
	pages = {931--934},
}

@article{Viefers2004,
	title = {Quantum rings for beginners: energy spectra and persistent currents},
	url = {http://www.sciencedirect.com/science/article/pii/S1386947703005186},
	urldate = {2014-03-11},
	journal = {Physica E: Low- …},
	author = {Viefers, S and Koskinen, P},
	year = {2004},
	note = {arXiv: cond-mat/0310064v1},
	pages = {1--27},
}

@article{Reimann2002,
	title = {Electronic structure of quantum dots},
	volume = {74},
	url = {http://rmp.aps.org/abstract/RMP/v74/i4/p1283_1},
	number = {October},
	urldate = {2014-03-11},
	journal = {Reviews of Modern Physics},
	author = {Reimann, SM and Manninen, Matti},
	year = {2002},
	pages = {1283--1342},
}

@article{Fogler2006,
	title = {Spin exchange in quantum rings and wires in the {Wigner}-crystal limit},
	volume = {18},
	issn = {0953-8984},
	url = {http://stacks.iop.org/0953-8984/18/i=1/a=L02?key=crossref.2e9a41fb1e423e8e4c7b48974056ba36},
	doi = {10.1088/0953-8984/18/1/L02},
	number = {1},
	urldate = {2014-03-06},
	journal = {Journal of Physics: Condensed Matter},
	author = {Fogler, Michael M and Pivovarov, Eugene},
	month = jan,
	year = {2006},
	pages = {L7--L13},
}

@article{khromova_designer_2011,
	title = {Designer {Spin} {Pseudomolecule} {Implemented} with {Trapped} {Ions} in a {Magnetic} {Gradient}},
	author = {Khromova, A and Scharfenberger, B and Gloger, T F and Johanning, M},
	year = {2011},
	note = {arXiv: 1112.5302v2},
}

@article{wunderlich_conditional_2001,
	title = {Conditional {Spin} {Resonance} with {Trapped} {Ions} ∗},
	author = {Wunderlich, Christof},
	year = {2001},
	note = {arXiv: quant-ph/0111158v1},
	pages = {1--8},
}

@article{brouwer_spectra_2011,
	title = {Spectra of graphs},
	author = {Brouwer, Andries E and Haemers, Willem H},
	year = {2011},
}

@article{stone_using_2012,
	title = {Using {VMD}},
	author = {Stone, John},
	year = {2012},
}

@article{aldous_reversible_2008,
	title = {Reversible {Markov} {Chains}},
	urldate = {2014-02-26},
	author = {Aldous, David and Fill, James Allen},
	year = {2008},
}

@article{wells_user-defined_2006,
	title = {User-{Defined} {Forces} in {NAMD}},
	number = {November},
	author = {Wells, David},
	year = {2006},
}

@article{wilmer_markov_nodate,
	title = {Markov {Chains} and {Mixing} {Times} {David} {A} . {Levin} {Yuval} {Peres}},
	author = {Wilmer, Elizabeth L},
}

@techreport{phillips_namd_2012,
	title = {{NAMD} {Tutorial}},
	author = {Phillips, James and Isgro, Tim and Villa, Elizabeth and Yu, Hang and Tanner, David and Liu, Yanxin},
	year = {2012},
	note = {Issue: February},
}

@article{breuckmann_quantum_2011,
	title = {Quantum {Subsystem} {Codes} {Their} {Theory} and {Use}},
	author = {Breuckmann, Nikolas P},
	year = {2011},
	pages = {1--40},
}

@article{zanardi_quantum_2003,
	title = {Quantum tensor product structures are observable-induced},
	url = {http://arxiv.org/abs/quant-ph/0308043},
	urldate = {2014-02-18},
	journal = {arXiv preprint quant-ph/0308043},
	author = {Zanardi, Paolo and Lidar, D and Lloyd, Seth},
	year = {2003},
	note = {arXiv: quant-ph/0308043v1},
	pages = {1--5},
}

@article{ozaydin_fusing_2014,
	title = {Fusing multiple {W} states simultaneously with a {Fredkin} gate 6, ‡ ¨},
	author = {Ozaydin, Fatih and Bugu, Sinan and Yesilyurt, Can and Altintas, Azmi Ali and Tame, Mark},
	year = {2014},
	note = {arXiv: 1402.3152v2},
	pages = {1--8},
}

@article{wang_-phase_1991,
	title = {In-phase flux state of the two-dimensional antiferromagnetic {Heisenberg} model and the {Raman} spectrum of {La2Cu04}},
	volume = {43},
	url = {http://prb.aps.org/abstract/PRB/v43/i16/p13774_1},
	number = {16},
	urldate = {2014-02-12},
	journal = {Physical Review B},
	author = {Wang, YR},
	year = {1991},
}

@article{protomol_protomol_nodate,
	title = {Protomol {Version} 2.0.3},
	issn = {1467-0100},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/18970472},
	doi = {10.1179/cim.2005.6.3.104},
	author = {{Protomol}},
	pmid = {18792328},
}

@article{bednarek_quantum_2008,
	title = {Quantum dot defined in a two-dimensional electron gas at a n-{AlGaAs}∕{GaAs} heterojunction: {Simulation} of electrostatic potential and charging properties},
	volume = {77},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.77.115320},
	doi = {10.1103/PhysRevB.77.115320},
	number = {11},
	urldate = {2014-02-13},
	journal = {Physical Review B},
	author = {Bednarek, S. and Lis, K. and Szafran, B.},
	month = mar,
	year = {2008},
	pages = {115320},
}

@article{wang_excitation_1991,
	title = {Excitation spectrum of the two-dimensional quantum antiferromagnetic {Heisenberg} model: {Wigner}-{Jordan} fermions or spin waves},
	volume = {44},
	url = {http://prb.aps.org/abstract/PRB/v44/i17/p9743_1},
	number = {17},
	urldate = {2014-02-12},
	journal = {Physical Review B},
	author = {Wang, YR and Rice, MJ and Choi, HY},
	year = {1991},
	pages = {9743--9745},
}

@article{fujii_introduction_nodate,
	title = {Introduction to the {Rotating} {Wave} {Approximation} ( {RWA} ) : {Two} {Coherent} {Oscillations}},
	author = {Fujii, Kazuyuki},
	note = {arXiv: 1301.3585v2},
	keywords = {coherent oscillation, quantum optics, rabi model, rotating wave approximation},
	pages = {1--20},
}

@article{ortigoso_quantum_2012,
	title = {Quantum adiabatic theorem in light of the {Marzlin}-{Sanders} inconsistency},
	volume = {86},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.86.032121},
	doi = {10.1103/PhysRevA.86.032121},
	number = {3},
	urldate = {2014-02-10},
	journal = {Physical Review A},
	author = {Ortigoso, Juan},
	month = sep,
	year = {2012},
	pages = {032121},
}

@article{kato_adiabatic_1950,
	title = {On the adiabatic theorem of quantum mechanics},
	url = {http://jpsj.ipap.jp/link?JPSJ/5/435/},
	urldate = {2014-02-10},
	journal = {J. Phys. Soc. Japan},
	author = {Kato, Tosio},
	year = {1950},
}

@article{lopes_interacting_2011,
	title = {Interacting spinless fermions in a diamond chain},
	volume = {84},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.84.085124},
	doi = {10.1103/PhysRevB.84.085124},
	number = {8},
	urldate = {2014-02-12},
	journal = {Physical Review B},
	author = {Lopes, a. a. and Dias, R. G.},
	month = aug,
	year = {2011},
	pages = {085124},
}

@article{tong_sufficiency_2007,
	title = {Sufficiency {Criterion} for the {Validity} of the {Adiabatic} {Approximation}},
	volume = {98},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.98.150402},
	doi = {10.1103/PhysRevLett.98.150402},
	number = {15},
	urldate = {2014-02-10},
	journal = {Physical Review Letters},
	author = {Tong, D. and Singh, K. and Kwek, L. and Oh, C.},
	month = apr,
	year = {2007},
	pages = {150402},
}

@article{sarandy_consistency_2004,
	title = {Consistency of the adiabatic theorem},
	url = {http://link.springer.com/article/10.1007/s11128-004-7712-7},
	number = {1},
	urldate = {2014-02-10},
	journal = {Quantum Information Processing},
	author = {Sarandy, MS and Wu, LA and Lidar, DA},
	year = {2004},
	note = {arXiv: quant-ph/0405059v3},
	keywords = {adiabatic theorem, berry, open quantum systems, quantum computation, s phases},
	pages = {1--11},
}

@article{yao_projection_2000,
	title = {Projection operator approach to time-independent perturbation theory in quantum mechanics},
	volume = {68},
	issn = {00029505},
	url = {http://link.aip.org/link/AJPIAS/v68/i3/p278/s1&Agg=doi},
	doi = {10.1119/1.19419},
	number = {3},
	urldate = {2014-01-30},
	journal = {American Journal of Physics},
	author = {Yao, Demin and Shi, Jicong},
	year = {2000},
	pages = {278},
}

@article{webster_simple_2013,
	title = {Simple {Manipulation} of a {Microwave} {Dressed}-{State} {Ion} {Qubit}},
	volume = {111},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.111.140501},
	doi = {10.1103/PhysRevLett.111.140501},
	number = {14},
	urldate = {2014-01-29},
	journal = {Physical Review Letters},
	author = {Webster, S. C. and Weidt, S. and Lake, K. and McLoughlin, J. J. and Hensinger, W. K.},
	month = oct,
	year = {2013},
	pages = {140501},
}

@article{chailloux_complexity_2011,
	title = {The {Complexity} of the {Separable} {Hamiltonian} {Problem}},
	author = {Chailloux, André},
	year = {2011},
	note = {arXiv: 1111.5247v1},
	pages = {1--16},
}

@article{eisert_quantum_nodate,
	title = {Quantum measurement occurrence is undecidable},
	number = {1},
	author = {Eisert, J and Gogolin, C},
	note = {arXiv: 1111.3965v3},
	pages = {1--5},
}

@article{lowdin_studies_1962,
	title = {Studies in {Perturbation} {Theory}. {IV}. {Solution} of {Eigenvalue} {Problem} by {Projection} {Operator} {Formalism}},
	volume = {3},
	issn = {00222488},
	url = {http://link.aip.org/link/JMAPAQ/v3/i5/p969/s1&Agg=doi},
	doi = {10.1063/1.1724312},
	number = {5},
	urldate = {2014-01-30},
	journal = {Journal of Mathematical Physics},
	author = {Löwdin, Per-Olov},
	year = {1962},
	pages = {969},
}

@article{shin_how_nodate,
	title = {How “ {Quantum} ” is the {D}-{Wave} {Machine} ?},
	author = {Shin, Seung Woo and Smith, Graeme and Smolin, John A and Vazirani, Umesh},
	note = {arXiv: 1401.7087v1},
}

@article{mohammady_nuclear-electronic_2013,
	title = {{NUCLEAR}-{ELECTRONIC} {SPIN} {SYSTEMS}, {MAGNETIC} {RESONANCE}, {AND} {QUANTUM} {INFORMATION} {PROCESSING}},
	url = {http://arxiv.org/abs/1305.0039},
	doi = {10.1038/nmat3499.ii},
	urldate = {2014-01-29},
	journal = {arXiv preprint arXiv:1305.0039},
	author = {Mohammady, MH},
	year = {2013},
	note = {arXiv: 1305.0039v2},
}

@article{lloyd_ultimate_2000,
	title = {Ultimate physical limits to computation},
	volume = {406},
	issn = {1476-4687},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10984064},
	doi = {10.1038/35023282},
	abstract = {Computers are physical systems: the laws of physics dictate what they can and cannot do. In particular, the speed with which a physical device can process information is limited by its energy and the amount of information that it can process is limited by the number of degrees of freedom it possesses. Here I explore the physical limits of computation as determined by the speed of light c, the quantum scale h and the gravitational constant G. As an example, I put quantitative bounds to the computational power of an 'ultimate laptop' with a mass of one kilogram confined to a volume of one litre.},
	number = {6799},
	journal = {Nature},
	author = {Lloyd, S},
	month = aug,
	year = {2000},
	pmid = {10984064},
	pages = {1047--54},
}

@article{spiller_introduction_2005,
	title = {An introduction to quantum information processing: applications and realizations},
	volume = {46},
	issn = {0010-7514},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00107510500293261},
	doi = {10.1080/00107510500293261},
	number = {6},
	urldate = {2014-01-29},
	journal = {Contemporary Physics},
	author = {Spiller, Timothy P. and Munro, William J. and Barrett, Sean D. and Kok, Pieter},
	month = nov,
	year = {2005},
	pages = {407--436},
}

@article{auer_entanglement_nodate,
	title = {Entanglement {Purification} with the {Exchange} {Interaction}},
	author = {Auer, Adrian and Burkard, Guido},
	note = {arXiv: 1401.5670v1},
	pages = {1--5},
}

@article{pla_single-atom_2012,
	title = {A single-atom electron spin qubit in silicon.},
	volume = {489},
	issn = {1476-4687},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22992519},
	doi = {10.1038/nature11449},
	abstract = {A single atom is the prototypical quantum system, and a natural candidate for a quantum bit, or qubit--the elementary unit of a quantum computer. Atoms have been successfully used to store and process quantum information in electromagnetic traps, as well as in diamond through the use of the nitrogen-vacancy-centre point defect. Solid-state electrical devices possess great potential to scale up such demonstrations from few-qubit control to larger-scale quantum processors. Coherent control of spin qubits has been achieved in lithographically defined double quantum dots in both GaAs (refs 3-5) and Si (ref. 6). However, it is a formidable challenge to combine the electrical measurement capabilities of engineered nanostructures with the benefits inherent in atomic spin qubits. Here we demonstrate the coherent manipulation of an individual electron spin qubit bound to a phosphorus donor atom in natural silicon, measured electrically via single-shot read-out. We use electron spin resonance to drive Rabi oscillations, and a Hahn echo pulse sequence reveals a spin coherence time exceeding 200 µs. This time should be even longer in isotopically enriched (28)Si samples. Combined with a device architecture that is compatible with modern integrated circuit technology, the electron spin of a single phosphorus atom in silicon should be an excellent platform on which to build a scalable quantum computer.},
	number = {7417},
	urldate = {2014-01-28},
	journal = {Nature},
	author = {Pla, Jarryd J and Tan, Kuan Y and Dehollain, Juan P and Lim, Wee H and Morton, John J L and Jamieson, David N and Dzurak, Andrew S and Morello, Andrea},
	month = sep,
	year = {2012},
	pmid = {22992519},
	note = {Publisher: Nature Publishing Group},
	pages = {541--5},
}

@article{morello_single-shot_2010,
	title = {Single-shot readout of an electron spin in silicon.},
	volume = {467},
	issn = {1476-4687},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/20877281},
	doi = {10.1038/nature09392},
	abstract = {The size of silicon transistors used in microelectronic devices is shrinking to the level at which quantum effects become important. Although this presents a significant challenge for the further scaling of microprocessors, it provides the potential for radical innovations in the form of spin-based quantum computers and spintronic devices. An electron spin in silicon can represent a well-isolated quantum bit with long coherence times because of the weak spin-orbit coupling and the possibility of eliminating nuclear spins from the bulk crystal. However, the control of single electrons in silicon has proved challenging, and so far the observation and manipulation of a single spin has been impossible. Here we report the demonstration of single-shot, time-resolved readout of an electron spin in silicon. This has been performed in a device consisting of implanted phosphorus donors coupled to a metal-oxide-semiconductor single-electron transistor-compatible with current microelectronic technology. We observed a spin lifetime of ∼6 seconds at a magnetic field of 1.5 tesla, and achieved a spin readout fidelity better than 90 per cent. High-fidelity single-shot spin readout in silicon opens the way to the development of a new generation of quantum computing and spintronic devices, built using the most important material in the semiconductor industry.},
	number = {7316},
	urldate = {2014-01-25},
	journal = {Nature},
	author = {Morello, Andrea and Pla, Jarryd J and Zwanenburg, Floris a and Chan, Kok W and Tan, Kuan Y and Huebl, Hans and Möttönen, Mikko and Nugroho, Christopher D and Yang, Changyi and van Donkelaar, Jessica a and Alves, Andrew D C and Jamieson, David N and Escott, Christopher C and Hollenberg, Lloyd C L and Clark, Robert G and Dzurak, Andrew S},
	month = oct,
	year = {2010},
	pmid = {20877281},
	pages = {687--91},
}

@article{topic_markov_2004,
	title = {Markov ’ s {Inequality} {Chebyshev} {Bounds} {Chernoff} {Bounds}},
	volume = {859},
	number = {M},
	author = {Topic, Randomized Algorithms and Scribe, Chernoff Bounds and Lecturer, Mugizi Rwebangira and Date, Shuchi Chawla},
	year = {2004},
	pages = {1--6},
}

@book{mackay_information_2003,
	title = {Information {Theory} , {Inference} , and {Learning} {Algorithms}},
	isbn = {0-521-64298-1},
	author = {Mackay, David J C},
	year = {2003},
}

@misc{messiah_quantum_nodate,
	title = {Quantum {Mechanics}},
	author = {Messiah, Albert},
}

@book{hubac_brillouin-wigner_nodate,
	title = {Brillouin-{Wigner} methods for many-body systems {Brillouin}-{Wigner} methods for many-body systems},
	isbn = {4-7813-0590-3},
	author = {Hubač, Ivan and Wilson, Stephen},
}

@misc{noauthor_gogolin_bosonization_nodate,
	title = {Gogolin\_Bosonization},
}

@book{dalessandro_introduction_2007,
	title = {Introduction to {Quantum} {Control} and {Dynamics}},
	isbn = {978-1-58488-884-0},
	url = {http://books.google.com/books?hl=en&lr=&id=e5M0id5enzQC&oi=fnd&pg=PP1&dq=Introduction+to+Quantum+Control+and+Dynamics&ots=Eqk6GV8ONk&sig=AcUTUDT3krX7G7aKtIFfqyS4qLI},
	urldate = {2014-01-29},
	author = {d'Alessandro, D},
	year = {2007},
}

@article{chang-hua_heralded_nodate,
	title = {Heralded linear optical quantum {Fredkin} gate based on one auxiliary qubit and one single photon detector},
	issn = {0065-2598},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24445827},
	doi = {10.1007/978-94-007-7893-1_1},
	author = {Chang-Hua, Zhu},
	pmid = {24443017},
	keywords = {03, 42, 50, 67, ex, hk, linear optics, pacs, quantum fredkin gate, quantum switching network},
}

@article{morley_quantum_2013,
	title = {Quantum control of hybrid nuclear-electronic qubits.},
	volume = {12},
	issn = {1476-1122},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23202370},
	doi = {10.1038/nmat3499},
	abstract = {Pulsed magnetic resonance allows the quantum state of electronic and nuclear spins to be controlled on the timescale of nanoseconds and microseconds respectively. The time required to flip dilute spins is orders of magnitude shorter than their coherence times, leading to several schemes for quantum information processing with spin qubits. Instead, we investigate 'hybrid nuclear-electronic' qubits consisting of near 50:50 superpositions of the electronic and nuclear spin states. Using bismuth-doped silicon, we demonstrate quantum control over these states in 32 ns, which is orders of magnitude faster than previous experiments using pure nuclear states. The coherence times of up to 4 ms are five orders of magnitude longer than the manipulation times, and are limited only by naturally occurring (29)Si nuclear spin impurities. We find a quantitative agreement between our experiments and an analytical theory for the resonance positions, as well as their relative intensities and Rabi oscillation frequencies. These results bring spins in a solid material a step closer to research on ion-trap qubits.},
	number = {2},
	urldate = {2014-01-16},
	journal = {Nature materials},
	author = {Morley, Gavin W and Lueders, Petra and Mohammady, M Hamed and Balian, Setrak J and Aeppli, Gabriel and Kay, Christopher W M and Witzel, Wayne M and Jeschke, Gunnar and Monteiro, Tania S},
	month = feb,
	year = {2013},
	pmid = {23202370},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bismuth, Bismuth: chemistry, Electromagnetic Fields, Electron Transport, Electrons, Magnetic Resonance Spectroscopy, Magnetic Resonance Spectroscopy: methods, Materials Testing, Nanoparticles, Quantum Theory, Signal Processing, Computer-Assisted, Silicon, Silicon: chemistry},
	pages = {103--7},
}

@article{katzgraber_designing_2014,
	title = {Designing better benchmarks for quantum annealing machines},
	author = {Katzgraber, Helmut G and Hamze, Firas and Andrist, Ruben S and One, D-wave},
	year = {2014},
	note = {arXiv: 1401.1546v1},
	pages = {1--7},
}

@article{rees_cooling_1988,
	title = {On cooling tea and coffee},
	volume = {56},
	issn = {00029505},
	url = {http://link.aip.org/link/?AJP/56/434/1&Agg=doi},
	doi = {10.1119/1.15572},
	number = {5},
	urldate = {2014-01-15},
	journal = {American Journal of Physics},
	author = {Rees, W. G.},
	year = {1988},
	pages = {434},
}

@article{ronnow_defining_2014,
	title = {Defining and detecting quantum speedup},
	url = {http://arxiv.org/abs/1401.2910},
	urldate = {2014-01-29},
	journal = {arXiv preprint arXiv: …},
	author = {Rønnow, TF and Wang, Zhihui and Job, Joshua and Boixo, Sergio},
	year = {2014},
	note = {arXiv: 1401.2910v1},
	pages = {1--15},
}

@article{donald_reversible_2008,
	title = {Reversible logic synthesis with {Fredkin} and {Peres} gates},
	volume = {4},
	issn = {15504832},
	url = {http://portal.acm.org/citation.cfm?doid=1330521.1330523},
	doi = {10.1145/1330521.1330523},
	number = {1},
	urldate = {2014-01-07},
	journal = {ACM Journal on Emerging Technologies in Computing Systems},
	author = {Donald, James and Jha, Niraj K.},
	month = mar,
	year = {2008},
	pages = {1--19},
}

@article{likharev_single-electron_2013,
	title = {Single-{Electron} {Parametron}: {Reversible} {Computation} in a {Discrete} {State} {System}},
	author = {Likharev, Konstantin K and Korotkov, Alexander N},
	year = {2013},
	note = {arXiv: cond-mat/9602140v1},
}

@article{lanyon_measurement-based_2013,
	title = {Measurement-{Based} {Quantum} {Computation} with {Trapped} {Ions}},
	volume = {111},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.111.210501},
	doi = {10.1103/PhysRevLett.111.210501},
	number = {21},
	urldate = {2013-12-12},
	journal = {Physical Review Letters},
	author = {Lanyon, B. P. and Jurcevic, P. and Zwerger, M. and Hempel, C. and Martinez, E. a. and Dür, W. and Briegel, H. J. and Blatt, R. and Roos, C. F.},
	month = nov,
	year = {2013},
	pages = {210501},
}

@article{kumar_large_nodate,
	title = {Do {Large} {Number} of {Parties} {Enforce} {Monogamy} in {All} {Quantum} {Correlations}?},
	author = {Kumar, Asutosh and Prabhu, R and Sen, Ujjwal},
	note = {arXiv: 1312.6640v1},
}

@article{sarovar_error_nodate,
	title = {Error suppression and error correction in adiabatic quantum computation : non-equilibrium dynamics},
	volume = {041013},
	number = {2013},
	author = {Sarovar, Mohan and Young, Kevin C},
	note = {arXiv: 1307.5892v4},
}

@article{lloyd_no_1937,
	title = {No {Title}},
	author = {Lloyd, Seth},
	year = {1937},
	note = {arXiv: 1312.4456v1},
	pages = {1--12},
}

@article{childs_bose-hubbard_nodate,
	title = {The bose-hubbard model is qma-complete},
	author = {Childs, Andrew M and Gosset, David and Webb, Z A K},
	note = {arXiv: 1311.3297v1},
	pages = {1--83},
}

@article{lloyd_universe_2013,
	title = {The {Universe} as {Quantum} {Computer}},
	author = {Lloyd, Seth},
	year = {2013},
	note = {arXiv: 1312.4455v1},
	pages = {1--16},
}

@article{hallgren_local_2013,
	title = {The {Local} {Hamiltonian} problem on a line with eight states is {QMA}-complete},
	number = {239937},
	author = {Hallgren, Sean and Nagaj, Daniel and Narayanaswami, Sandeep},
	year = {2013},
	note = {arXiv: 1312.1469v1},
	pages = {1--28},
}

@article{aharonov_commuting_2013,
	title = {The commuting local {Hamiltonian} on locally-expanding graphs is in {NP} .},
	author = {Aharonov, Dorit},
	year = {2013},
	note = {arXiv: 1311.7378v1},
	pages = {1--16},
}

@article{kay_comment_2013,
	title = {Comment on {Partial} {Adiabatic} {Quantum} {Search}},
	author = {Kay, Alastair},
	year = {2013},
	note = {arXiv: 1311.5752v1},
	pages = {1--3},
}

@article{hallgren_local_2013-1,
	title = {{THE} {LOCAL} {HAMILTONIAN} {PROBLEM} {ON} {A} {LINE} {WITH} {EIGHT} {STATES}},
	volume = {13},
	number = {9},
	author = {Hallgren, Sean and Nagaj, Daniel and Narayanaswami, Sandeep and Mosca, M},
	year = {2013},
	keywords = {b kane, communicated by, local hamiltonian, m mosca, qma-complete},
	pages = {721--750},
}

@article{brandao_product-state_2013,
	title = {Product-state approximations to quantum ground states},
	url = {http://dl.acm.org/citation.cfm?id=2488719},
	urldate = {2014-01-29},
	journal = {Proceedings of the 45th annual ACM …},
	author = {Brandão, FGSL and Harrow, AW},
	year = {2013},
	note = {arXiv: 1310.0017v1},
	pages = {1--44},
}

@article{nagaj_local_2008,
	title = {Local {Hamiltonians} in {Quantum} {Computation}},
	author = {Nagaj, Daniel},
	year = {2008},
	note = {arXiv: 0808.2117v1},
}

@article{aharonov_power_2009,
	title = {The power of quantum systems on a line},
	url = {http://link.springer.com/article/10.1007/s00220-008-0710-3},
	number = {015848},
	urldate = {2013-11-15},
	journal = {… in Mathematical Physics},
	author = {Aharonov, Dorit and Gottesman, D and Irani, S and Kempe, J},
	year = {2009},
	note = {arXiv: 0705.4077v3},
}

@article{bremner_classical_2011,
	title = {Classical simulation of commuting quantum computations implies collapse of the polynomial hierarchy},
	url = {http://rspa.royalsocietypublishing.org/content/467/2126/459.short},
	urldate = {2014-07-07},
	journal = {… of the Royal …},
	author = {Bremner, MJ and Jozsa, R and Shepherd, DJ},
	year = {2011},
	note = {arXiv: 1005.1407v1},
	pages = {1--13},
}

@article{steane_ion_1997,
	title = {The ion trap quantum information processor},
	urldate = {2014-02-25},
	journal = {Applied Physics B: Lasers and Optics},
	author = {Steane, Andrew},
	year = {1997},
	note = {arXiv: quant-ph/9608011v2},
}

@article{zhang_criterion_2013,
	title = {Criterion of local unitary equivalence for multipartite states},
	volume = {88},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.88.042304},
	doi = {10.1103/PhysRevA.88.042304},
	number = {4},
	urldate = {2013-11-11},
	journal = {Physical Review A},
	author = {Zhang, Ting-Gui and Zhao, Ming-Jing and Li, Ming and Fei, Shao-Ming and Li-Jost, Xianqing},
	month = oct,
	year = {2013},
	pages = {042304},
}

@article{mckague_interactive_nodate,
	title = {Interactive proofs for {BQP} via self-tested graph states},
	author = {Mckague, Matthew},
	note = {arXiv: 1311.1534v1},
}

@article{pepper_adiabatic_2013,
	title = {Adiabatic computing using 2-local {Hamiltonians} on a line},
	author = {Pepper, Brian},
	year = {2013},
	note = {arXiv: 1310.3562v1},
}

@article{nakago_parallelized_2013,
	title = {Parallelized adiabatic gate teleportation},
	author = {Nakago, Kosuke and Hajduˇ, Michal and Nakayama, Shojun and Murao, Mio},
	year = {2013},
	note = {arXiv: 1310.4061v1},
	pages = {1--15},
}

@article{aaronson_quantum_2005,
	title = {Quantum computing and hidden variables},
	volume = {71},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.71.032325},
	doi = {10.1103/PhysRevA.71.032325},
	number = {3},
	journal = {Physical Review A},
	author = {Aaronson, Scott},
	month = mar,
	year = {2005},
	pages = {032325},
}

@article{hemaspaandra_sigact_nodate,
	title = {{SIGACT} {News} {Complexity} {Theory} {Column} 46 {Introduction} to {Complexity} {Theory} {Column} 46 {Guest} {Column} : {NP}-complete {Problems} and {Physical} {Reality}},
	volume = {36},
	number = {1},
	author = {Hemaspaandra, Lane A and Aaronson, Scott},
	pages = {30--52},
}

@article{boixo_necessary_2010,
	title = {Necessary condition for the quantum adiabatic approximation},
	volume = {81},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.81.032308},
	doi = {10.1103/PhysRevA.81.032308},
	number = {3},
	urldate = {2013-10-07},
	journal = {Physical Review A},
	author = {Boixo, S. and Somma, R. D.},
	month = mar,
	year = {2010},
	pages = {032308},
}

@article{micuda_efficient_2013,
	title = {Efficient {Experimental} {Estimation} of {Fidelity} of {Linear} {Optical} {Quantum} {Toffoli} {Gate}},
	volume = {111},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.111.160407},
	doi = {10.1103/PhysRevLett.111.160407},
	number = {16},
	urldate = {2013-10-21},
	journal = {Physical Review Letters},
	author = {Mičuda, M. and Sedlák, M. and Straka, I. and Miková, M. and Dušek, M. and Ježek, M. and Fiurášek, J.},
	month = oct,
	year = {2013},
	pages = {160407},
}

@article{blume-kohout_adiabatic_nodate,
	title = {Adiabatic quantum optimization with the wrong {Hamiltonian}},
	number = {08961},
	author = {Blume-kohout, Robin and Lidar, Daniel A},
	note = {arXiv: 1310.0529v1},
}

@article{benjamin_quantum_2004,
	title = {Quantum computing in arrays coupled by “always-on” interactions},
	volume = {70},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.70.032314},
	doi = {10.1103/PhysRevA.70.032314},
	number = {3},
	urldate = {2013-09-24},
	journal = {Physical Review A},
	author = {Benjamin, S. and Bose, S.},
	month = sep,
	year = {2004},
	pages = {032314},
}

@article{burkard_physical_1999,
	title = {Physical optimization of quantum error correction circuits},
	volume = {60},
	issn = {0163-1829},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.60.11404},
	doi = {10.1103/PhysRevB.60.11404},
	number = {16},
	journal = {Physical Review B},
	author = {Burkard, Guido and Loss, Daniel and DiVincenzo, David and Smolin, John},
	month = oct,
	year = {1999},
	pages = {11404--11416},
}

@article{monz_realization_2009,
	title = {Realization of the quantum {Toffoli} gate with trapped ions},
	url = {http://prl.aps.org/abstract/PRL/v102/i4/e040501},
	urldate = {2013-09-12},
	journal = {Physical review …},
	author = {Monz, T and Kim, K and Hänsel, W and Riebe, M},
	year = {2009},
	note = {arXiv: 0804.0082v1},
	pages = {1--11},
}

@article{benjamin_quantum_2003,
	title = {Quantum {Computing} with an {Always}-{On} {Heisenberg} {Interaction}},
	volume = {90},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.90.247901},
	doi = {10.1103/PhysRevLett.90.247901},
	number = {24},
	urldate = {2013-09-24},
	journal = {Physical Review Letters},
	author = {Benjamin, Simon and Bose, Sougato},
	month = jun,
	year = {2003},
	pages = {247901},
}

@article{zheng_fault-tolerant_2013,
	title = {A {Fault}-{Tolerant} {Scheme} of {Holonomic} {Quantum} {Computation} on {Stabilizer} {Codes} with {Robustness} to {Thermal} {Noise}},
	author = {Zheng, Yi-cong and Brun, Todd A},
	year = {2013},
	note = {arXiv: 1309.1534v1},
}

@article{shao_implementing_nodate,
	title = {Implementing the genuine {Fredkin} gate in an array of coupled cavities},
	author = {Shao, Xiao-qiang and Zheng, Tai-yu and Zhang, Shou and Oh, C H},
	note = {arXiv: 1309.0937v2},
	pages = {1--8},
}

@article{barahona_computational_1982,
	title = {On the computational complexity of {Ising} spin glass models},
	volume = {15},
	issn = {0305-4470},
	url = {http://stacks.iop.org/0305-4470/15/i=10/a=028?key=crossref.1fe57df6674a7c759374b69321415b44},
	doi = {10.1088/0305-4470/15/10/028},
	number = {10},
	journal = {Journal of Physics A: Mathematical and General},
	author = {Barahona, F},
	month = oct,
	year = {1982},
	pages = {3241--3253},
}

@article{kempe_complexity_2006,
	title = {The complexity of the local {Hamiltonian} problem},
	urldate = {2013-09-12},
	journal = {SIAM Journal on Computing},
	author = {Kempe, Julia and Kitaev, A and Regev, O},
	year = {2006},
	note = {arXiv: quant-ph/0406180v2},
	pages = {1--30},
}

@article{chen_multipath_2013,
	title = {Multipath adiabatic quantum state transfer},
	volume = {88},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.88.022323},
	doi = {10.1103/PhysRevA.88.022323},
	number = {2},
	urldate = {2013-09-02},
	journal = {Physical Review A},
	author = {Chen, Bing and Fan, Wei and Xu, Yan and Peng, Yan-Dong and Zhang, Hui-Yun},
	month = aug,
	year = {2013},
	pages = {022323},
}

@article{bremner_fungible_2004,
	title = {Fungible dynamics: {There} are only two types of entangling multiple-qubit interactions},
	volume = {69},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.69.012313},
	doi = {10.1103/PhysRevA.69.012313},
	number = {1},
	urldate = {2013-09-02},
	journal = {Physical Review A},
	author = {Bremner, Michael and Dodd, Jennifer and Nielsen, Michael and Bacon, Dave},
	month = jan,
	year = {2004},
	pages = {012313},
}

@article{brod_computational_2013,
	title = {The computational power of matchgates and the {XY} interaction on arbitrary graphs},
	author = {Brod, Daniel J and Childs, Andrew M},
	year = {2013},
	note = {arXiv: 1308.1463v1},
	pages = {1--10},
}

@article{wang_operator_2013,
	title = {Operator {Quantum} {Zeno} {Effect}: {Protecting} {Quantum} {Information} with {Noisy} {Two}-{Qubit} {Interactions}},
	number = {0},
	author = {Wang, Shu-chao and Li, Ying and Wang, Xiang-bin and Kwek, Leong Chuan},
	year = {2013},
	note = {arXiv: 1303.0055v1},
	pages = {1--7},
}

@article{paetznick_universal_2013,
	title = {Universal {Fault}-{Tolerant} {Quantum} {Computation} with {Only} {Transversal} {Gates} and {Error} {Correction}},
	volume = {111},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.111.090505},
	doi = {10.1103/PhysRevLett.111.090505},
	number = {9},
	urldate = {2013-08-31},
	journal = {Physical Review Letters},
	author = {Paetznick, Adam and Reichardt, Ben W.},
	month = aug,
	year = {2013},
	pages = {090505},
}

@article{karimipour_algorithmic_2012,
	title = {Algorithmic proof for the completeness of the two-dimensional {Ising} model},
	volume = {86},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.86.052303},
	doi = {10.1103/PhysRevA.86.052303},
	number = {5},
	urldate = {2013-08-07},
	journal = {Physical Review A},
	author = {Karimipour, Vahid and Zarei, Mohammad Hossein},
	month = nov,
	year = {2012},
	pages = {052303},
}

@article{warren_adapting_2012,
	title = {Adapting the traveling salesman problem to an adiabatic quantum computer},
	volume = {12},
	issn = {1570-0755},
	url = {http://link.springer.com/10.1007/s11128-012-0490-8},
	doi = {10.1007/s11128-012-0490-8},
	number = {4},
	urldate = {2013-09-02},
	journal = {Quantum Information Processing},
	author = {Warren, Richard H.},
	month = sep,
	year = {2012},
	pages = {1781--1785},
}

@article{kieu_quantum_nodate,
	title = {Quantum {Adiabatic} {Computation} and the {Travelling} {Salesman} {Problem}},
	number = {2},
	author = {Kieu, Tien D},
	note = {arXiv: quant-ph/0601151v2},
}

@article{haegeman_elementary_2013,
	title = {Elementary {Excitations} in {Gapped} {Quantum} {Spin} {Systems}},
	volume = {111},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.111.080401},
	doi = {10.1103/PhysRevLett.111.080401},
	number = {8},
	urldate = {2013-08-20},
	journal = {Physical Review Letters},
	author = {Haegeman, Jutho and Michalakis, Spyridon and Nachtergaele, Bruno and Osborne, Tobias J. and Schuch, Norbert and Verstraete, Frank},
	month = aug,
	year = {2013},
	pages = {080401},
}

@article{van_dam_how_2001,
	title = {How powerful is adiabatic quantum computation?},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=959902},
	urldate = {2013-08-22},
	journal = {Foundations of Computer …},
	author = {van Dam, W and Mosca, Michele and Vazirani, Umesh},
	year = {2001},
	note = {arXiv: quant-ph/0206003v1},
}

@article{rothenstein_relativistic_2003,
	title = {Relativistic {Thermodynamics} for the {Introductory} {Physics} {Course}},
	volume = {5},
	number = {3},
	urldate = {2013-08-29},
	journal = {Journal of Theoretics},
	author = {Rothenstein, B and Zaharie, I},
	year = {2003},
	keywords = {special relativity, thermodynamics},
}

@article{doherty_two-qubit_2013,
	title = {Two-{Qubit} {Gates} for {Resonant} {Exchange} {Qubits}},
	volume = {111},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.111.050503},
	doi = {10.1103/PhysRevLett.111.050503},
	number = {5},
	urldate = {2013-08-11},
	journal = {Physical Review Letters},
	author = {Doherty, Andrew C. and Wardrop, Matthew P.},
	month = jul,
	year = {2013},
	pages = {050503},
}

@article{epstein_adiabatic_2012,
	title = {Adiabatic {Quantum} {Computing} : {An} {Overview}},
	number = {1},
	author = {Epstein, Charles},
	year = {2012},
	pages = {1--7},
}

@article{proctor_minimal_2013,
	title = {Minimal ancilla-controlled quantum computation},
	author = {Proctor, T J and Andersson, E and Kendon, V M},
	year = {2013},
	note = {arXiv: 1307.6095v2},
	pages = {1--10},
}

@article{pudenz_error_nodate,
	title = {Error corrected quantum annealing with hundreds of qubits},
	volume = {5},
	number = {1},
	author = {Pudenz, Kristen L and Albash, Tameem and Lidar, Daniel A},
	note = {arXiv: 1307.8190v1},
	pages = {1--18},
}

@article{pecker_observation_2013,
	title = {Observation and spectroscopy of a two-electron {Wigner} molecule in an ultraclean carbon nanotube},
	volume = {9},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys2692},
	doi = {10.1038/nphys2692},
	number = {8},
	urldate = {2013-08-11},
	journal = {Nature Physics},
	author = {Pecker, S. and Kuemmeth, F. and Secchi, a. and Rontani, M. and Ralph, D. C. and McEuen, P. L. and Ilani, S.},
	month = jul,
	year = {2013},
	note = {Publisher: Nature Publishing Group},
	pages = {1--6},
}

@article{young_error_2013,
	title = {Error suppression and error correction in adiabatic quantum computation {I}: techniques and challenges {Kevin}},
	url = {http://arxiv.org/abs/1307.5893},
	number = {08961},
	urldate = {2013-08-29},
	journal = {arXiv preprint arXiv:1307.5893},
	author = {Young, KC and Sarovar, M and Blume-Kohout, R},
	year = {2013},
	note = {arXiv: 1307.5893v2},
}

@article{koh_high_nodate,
	title = {High fidelity gates in quantum dot spin qubits},
	number = {i},
	author = {Koh, Teck Seng and Coppersmith, S N and Friesen, Mark},
	note = {arXiv: 1307.8406v1},
	pages = {1--20},
}

@article{albanese_mirror_2004,
	title = {Mirror inversion of quantum states in linear registers},
	url = {http://prl.aps.org/abstract/PRL/v93/i23/e230502},
	urldate = {2013-07-31},
	journal = {Physical review letters},
	author = {Albanese, Claudio and Christandl, Matthias and Datta, Nilanjana and Ekert, Artur},
	year = {2004},
	note = {arXiv: quant-ph/0405029v2},
	pages = {1--4},
}

@article{backens_zx_nodate,
	title = {The {ZX} -calculus is complete for stabilizer quantum mechanics},
	author = {Backens, Miriam},
	note = {arXiv: 1307.7025v1},
}

@article{kay_review_2010,
	title = {A {Review} of {Perfect}, {Efficient}, {State} {Transfer} and its {Application} as a {Constructive} {Tool}},
	volume = {1},
	author = {Kay, Alastair},
	year = {2010},
	note = {arXiv: 0903.4274v3},
	pages = {1--18},
}

@article{reeb_im-proving_nodate,
	title = {({Im}-){Proving} {Landauer}’s {Principle}},
	author = {Reeb, David and Wolf, Michael M},
	note = {arXiv: 1306.4352v1},
	pages = {1--32},
}

@article{rao_simulation_2013,
	title = {Simulation of mirror inversion of quantum states in an {XY} spin chain using {NMR}},
	url = {http://arxiv.org/abs/1307.5220},
	urldate = {2013-07-31},
	journal = {arXiv preprint arXiv:1307.5220},
	author = {Rao, K and Mahesh, TS and Kumar, Anil},
	year = {2013},
	note = {arXiv: 1307.5220v1},
	pages = {1--9},
}

@article{vaccaro_information_2011,
	title = {Information erasure without an energy cost},
	volume = {467},
	issn = {1364-5021},
	url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.2010.0577},
	doi = {10.1098/rspa.2010.0577},
	number = {2130},
	urldate = {2013-07-29},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Vaccaro, J. a. and Barnett, S. M.},
	month = jan,
	year = {2011},
	note = {arXiv: 1004.5330v2},
	keywords = {canonical ensemble, information erasure, spin, thermodynamics},
	pages = {1770--1778},
}

@article{kane_silicon-based_1998,
	title = {A silicon-based nuclear spin quantum computer},
	author = {Kane, B E},
	year = {1998},
	pages = {133--137},
}

@article{McCamey2010,
	title = {Electronic spin storage in an electrically readable nuclear spin memory with a lifetime {\textgreater}100 seconds.},
	volume = {330},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21164011},
	doi = {10.1126/science.1197931},
	abstract = {Electron spins are strong candidates with which to implement spintronics because they are both mobile and able to be manipulated. The relatively short lifetimes of electron spins, however, present a problem for the long-term storage of spin information. We demonstrated an ensemble nuclear spin memory in phosphorous-doped silicon, which can be read out electrically and has a lifetime exceeding 100 seconds. The electronic spin information can be mapped onto and stored in the nuclear spin of the phosphorus donors, and the nuclear spins can then be repetitively read out electrically for time periods that exceed the electron spin lifetime. We discuss how this memory can be used in conjunction with other silicon spintronic devices.},
	number = {6011},
	urldate = {2013-06-16},
	journal = {Science (New York, N.Y.)},
	author = {McCamey, D R and Van Tol, J and Morley, G W and Boehme, C},
	month = dec,
	year = {2010},
	pmid = {21164011},
	pages = {1652--6},
}

@article{search_effective_1969,
	title = {An effective {Hamiltonian} and time-independent perturbation theory},
	volume = {2161},
	author = {Search, Home and Journals, Collections and Contact, About and Iopscience, My},
	year = {1969},
}

@article{Andrianov2008,
	title = {Three-qubit pure-state canonical forms},
	url = {http://iopscience.iop.org/0305-4470/34/35/301},
	number = {2},
	urldate = {2013-07-29},
	journal = {Journal of Physics A: …},
	author = {Acin, A and Andrianov, A},
	year = {2001},
	note = {arXiv: quant-ph/0009107v1},
}

@article{ganti_gap_2013,
	title = {On the gap of {Hamiltonians} for the adiabatic simulation of quantum circuits},
	url = {http://arxiv.org/abs/1307.4993},
	urldate = {2013-07-23},
	journal = {arXiv preprint arXiv:1307.4993},
	author = {Ganti, Anand and Somma, R},
	year = {2013},
	note = {arXiv: 1307.4993v1},
}

@article{Sudbery2000,
	title = {On local invariants of pure three-qubit states},
	url = {http://iopscience.iop.org/0305-4470/34/3/323},
	urldate = {2013-07-29},
	journal = {Journal of Physics A: Mathematical and General},
	author = {Sudbery, Anthony},
	year = {2001},
	note = {arXiv: quant-ph/0001116v4},
	pages = {1--13},
}

@article{Schuch,
	title = {Programmable networks for quantum algorithms},
	author = {Schuch, Norbert and Siewert, Jens},
	note = {arXiv: quant-ph/0303063v2},
	pages = {1--4},
}

@article{Groszkowski2011,
	title = {Tunable coupling between three qubits as a building block for a superconducting quantum computer},
	volume = {84},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.84.144516},
	doi = {10.1103/PhysRevB.84.144516},
	number = {14},
	urldate = {2013-07-17},
	journal = {Physical Review B},
	author = {Groszkowski, Peter and Fowler, Austin G. and Motzoi, Felix and Wilhelm, Frank K.},
	month = oct,
	year = {2011},
	pages = {144516},
}

@article{Ashhab2012,
	title = {Speed limits for quantum gates in multiqubit systems},
	volume = {85},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.85.052327},
	doi = {10.1103/PhysRevA.85.052327},
	number = {5},
	urldate = {2013-07-17},
	journal = {Physical Review A},
	author = {Ashhab, S. and de Groot, P. C. and Nori, Franco},
	month = may,
	year = {2012},
	pages = {052327},
}

@article{bishop_heralding_2013,
	title = {Heralding an {Arbitrary} {Decoherence}-{Free} {Qubit} {State}},
	author = {Bishop, C Allen and Bennink, Ryan S and Humble, Travis S and Evans, Philip G and Byrd, Mark S},
	year = {2013},
	note = {arXiv: 1306.6098v1},
	pages = {1--5},
}

@article{briggs_oxford_2013,
	title = {The {Oxford} {Questions} on the foundations of quantum physics {Subject} {Areas} :},
	author = {Briggs, G A D and Butterfield, J N and Zeilinger, A},
	year = {2013},
}

@article{july_practical_2013,
	title = {A {Practical} {Introduction} to {Tensor} {Networks} : {Matrix} {Product} {States} and {Projected} {Entangled} {Pair} {States}},
	author = {July, Germany},
	year = {2013},
	note = {arXiv: 1306.2164v2},
	pages = {1--51},
}

@article{Sterling,
	title = {Two-dimensional ion trap lattice on a microchip for quantum simulation},
	url = {http://arxiv.org/abs/1302.3781},
	urldate = {2013-07-29},
	journal = {arXiv preprint arXiv: …},
	author = {Sterling, RC and Rattanasonti, H and Weidt, S},
	year = {2013},
	note = {arXiv: 1302.3781v4},
	pages = {1--6},
}

@article{brunner_connection_2013,
	title = {Connection between {Bell} nonlocality and {Bayesian} game theory.},
	volume = {4},
	issn = {2041-1723},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23820748},
	doi = {10.1038/ncomms3057},
	abstract = {In 1964, Bell discovered that quantum mechanics is a nonlocal theory. Three years later, in a seemingly unconnected development, Harsanyi introduced the concept of Bayesian games. Here we show that, in fact, there is a deep connection between Bell nonlocality and Bayesian games, and that the same concepts appear in both fields. This link offers interesting possibilities for Bayesian games, namely of allowing the players to receive advice in the form of nonlocal correlations, for instance using entangled quantum particles or more general no-signalling boxes. This will lead to novel joint strategies, impossible to achieve classically. We characterize games for which nonlocal resources offer a genuine advantage over classical ones. Moreover, some of these strategies represent equilibrium points, leading to the notion of quantum/no-signalling Nash equilibrium. Finally, we describe new types of question in the study of nonlocality, namely the consideration of nonlocal advantage given a set of Bell expressions.},
	number = {May},
	urldate = {2013-07-10},
	journal = {Nature communications},
	author = {Brunner, Nicolas and Linden, Noah},
	month = jul,
	year = {2013},
	pmid = {23820748},
	note = {Publisher: Nature Publishing Group},
	pages = {2057},
}

@article{kapit_3-_nodate,
	title = {3- and 4-body {Interactions} from 2-body interactions in {Spin} {Models}: {A} {Route} to {Abelian} and {Non}-{Abelian} {Fractional} {Chern} {Insulators}},
	number = {1},
	author = {Kapit, Eliot and Simon, Steven H},
	note = {arXiv: 1307.3485v1},
	pages = {1--5},
}

@article{laudisa_against_nodate,
	title = {Against the ‘ {No}-{Go} ’ {Philosophy} of {Quantum} {Mechanics}},
	author = {Laudisa, Federico},
	pages = {1--21},
}

@article{Jones2013a,
	title = {Nested composite {NOT} gates for quantum computation},
	url = {http://arxiv.org/abs/1307.3114},
	urldate = {2013-07-18},
	journal = {arXiv preprint arXiv:1307.3114},
	author = {Jones, JA},
	year = {2013},
	note = {arXiv: 1307.3114v1},
	pages = {1--3},
}

@article{Welzel2011,
	title = {Designing spin-spin interactions with one and two dimensional ion crystals in planar micro traps},
	volume = {65},
	issn = {1434-6060},
	url = {http://www.springerlink.com/index/10.1140/epjd/e2011-20098-y},
	doi = {10.1140/epjd/e2011-20098-y},
	number = {1-2},
	urldate = {2013-07-13},
	journal = {The European Physical Journal D},
	author = {Welzel, J. and Bautista-Salvador, a. and Abarbanel, C. and Wineman-Fisher, V. and Wunderlich, C. and Folman, R. and Schmidt-Kaler, F.},
	month = jul,
	year = {2011},
	pages = {285--297},
}

@article{Porras2004,
	title = {Effective {Quantum} {Spin} {Systems} with {Trapped} {Ions}},
	volume = {92},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.92.207901},
	doi = {10.1103/PhysRevLett.92.207901},
	number = {20},
	urldate = {2013-06-08},
	journal = {Physical Review Letters},
	author = {Porras, D. and Cirac, J. I.},
	month = may,
	year = {2004},
	pages = {207901},
}

@article{Wunderlich2009,
	title = {Two-dimensional cluster-state preparation with linear ion traps},
	url = {http://pra.aps.org/abstract/PRA/v79/i5/e052324},
	urldate = {2013-07-13},
	journal = {Physical Review A},
	author = {Wunderlich, H and Wunderlich, C and Singer, Kilian and Schmidt-Kaler, F},
	year = {2009},
	note = {arXiv: 0901.0881v2},
}

@article{sanders_universal_nodate,
	title = {Universal {Quantum} {Simulation}},
	author = {Sanders, Barry C},
	note = {arXiv: 1307.1498v1},
	keywords = {quantum algorithms, quantum computing, quantum sim-},
	pages = {1--11},
}

@article{Pasquale2009,
	title = {{XY} model on the circle: diagonalization, spectrum, and forerunners of the quantum phase transition},
	url = {http://pra.aps.org/abstract/PRA/v80/i3/e032102},
	urldate = {2013-07-13},
	journal = {Physical Review A},
	author = {Pasquale, A De and Facchi, P},
	year = {2009},
	note = {arXiv: 0808.1478v1},
}

@article{Paganelli2006,
	title = {Faithful state transfer through a quantum channel},
	volume = {74},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.74.012316},
	doi = {10.1103/PhysRevA.74.012316},
	number = {1},
	urldate = {2013-07-09},
	journal = {Physical Review A},
	author = {Paganelli, Simone and de Pasquale, Ferdinando and Giorgi, Gian},
	month = jul,
	year = {2006},
	pages = {012316},
}

@article{low_optimal_2013,
	title = {Optimal arbitrarily accurate composite pulse sequences},
	author = {Low, Guang Hao and Yoder, Theodore J and Chuang, Isaac L},
	year = {2013},
	note = {arXiv: 1307.2211v1},
	pages = {1--5},
}

@article{augusiak_many-body_nodate,
	title = {Many-body physics from a quantum information perspective},
	author = {Augusiak, R and Cucchietti, F M and Lewenstein, M},
	note = {arXiv: 1003.3153v3},
	pages = {1--50},
}

@article{de_pasquale_xx_2008,
	title = {{XX} model on the circle},
	volume = {160},
	issn = {1951-6355},
	url = {http://www.springerlink.com/index/10.1140/epjst/e2008-00716-9},
	doi = {10.1140/epjst/e2008-00716-9},
	number = {1},
	urldate = {2013-07-08},
	journal = {The European Physical Journal Special Topics},
	author = {De Pasquale, a. and Costantini, G. and Facchi, P. and Florio, G. and Pascazio, S. and Yuasa, K.},
	month = jul,
	year = {2008},
	pages = {127--138},
}

@article{deffner_quantum_2013,
	title = {Quantum {Speed} {Limit} for {Non}-{Markovian} {Dynamics}},
	volume = {111},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.111.010402},
	doi = {10.1103/PhysRevLett.111.010402},
	number = {1},
	urldate = {2013-07-08},
	journal = {Physical Review Letters},
	author = {Deffner, Sebastian and Lutz, Eric},
	month = jul,
	year = {2013},
	pages = {010402},
}

@article{Frohlich2007,
	title = {Semi-{Classical} {Dynamics} in {Quantum} {Spin} {Systems}},
	volume = {82},
	issn = {0377-9017},
	url = {http://link.springer.com/10.1007/s11005-007-0202-y},
	doi = {10.1007/s11005-007-0202-y},
	number = {2-3},
	urldate = {2013-07-08},
	journal = {Letters in Mathematical Physics},
	author = {Fröhlich, Jürg and Knowles, Antti and Lenzmann, Enno},
	month = nov,
	year = {2007},
	keywords = {egorov, landau-lifschitz equation, mean-field limit, quantum spin systems},
	pages = {275--296},
}

@article{salih_protocol_2013,
	title = {Protocol for {Direct} {Counterfactual} {Quantum} {Communication}},
	volume = {110},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.110.170502},
	doi = {10.1103/PhysRevLett.110.170502},
	number = {17},
	urldate = {2013-06-08},
	journal = {Physical Review Letters},
	author = {Salih, Hatim and Li, Zheng-Hong and Al-Amri, M. and Zubairy, M. Suhail},
	month = apr,
	year = {2013},
	pages = {170502},
}

@article{paganelli_routing_2013,
	title = {Routing quantum information in spin chains},
	volume = {87},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.87.062309},
	doi = {10.1103/PhysRevA.87.062309},
	number = {6},
	urldate = {2013-06-10},
	journal = {Physical Review A},
	author = {Paganelli, Simone and Lorenzo, Salvatore and Apollaro, Tony J. G. and Plastina, Francesco and Giorgi, Gian Luca},
	month = jun,
	year = {2013},
	pages = {062309},
}

@article{Szankowski2013,
	title = {Spin decoherence due to fluctuating fields},
	url = {http://pre.aps.org/abstract/PRE/v87/i5/e052112},
	urldate = {2013-07-08},
	journal = {Physical Review E},
	author = {Szańkowski, P and Trippenbach, M and Band, YB},
	year = {2013},
	note = {arXiv: 1211.3032v2},
	pages = {1--13},
}

@article{kent_one_2013,
	title = {One world versus many: the inadequacy of {Everettian} accounts of evolution, probability, and scientific confirmation},
	author = {Kent, Adrian},
	year = {2013},
	note = {arXiv: 0905.0624v3},
}

@article{blanter_deterministic_2013,
	title = {Deterministic entanglement of superconducting qubits by parity measurement and feedback},
	author = {Blanter, Ya M and Lehnert, K W and Schouten, R N and Dicarlo, L},
	year = {2013},
	note = {arXiv: 1306.4002v1},
}

@article{Shao2012,
	title = {Robust {Toffoli} gate originating from {Stark} shifts},
	volume = {29},
	issn = {0740-3224},
	url = {http://www.opticsinfobase.org/abstract.cfm?URI=josab-29-6-1203},
	doi = {10.1364/JOSAB.29.001203},
	number = {6},
	journal = {Journal of the Optical Society of America B},
	author = {Shao, Xiao-Qiang and Zheng, Tai-Yu and Zhang, Shou},
	month = may,
	year = {2012},
	pages = {1203},
}

@article{Sleator1995,
	title = {Realizable {Universal} quantum logic gates},
	volume = {74},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.74.4087},
	number = {20},
	urldate = {2013-06-19},
	journal = {Physical Review Letters},
	author = {Sleator, T and Weinfurter, H},
	year = {1995},
	pages = {4087--4090},
}

@article{wojcik_multiuser_2007,
	title = {Multiuser quantum communication networks},
	volume = {75},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.75.022330},
	doi = {10.1103/PhysRevA.75.022330},
	number = {2},
	urldate = {2013-06-19},
	journal = {Physical Review A},
	author = {Wójcik, Antoni and Łuczak, Tomasz and Kurzyński, Paweł and Grudka, Andrzej and Gdala, Tomasz and Bednarska, Małgorzata},
	month = feb,
	year = {2007},
	pages = {022330},
}

@article{landahl_quantum_nodate,
	title = {Quantum computing with spin networks},
	author = {Landahl, Andrew and Christandl, Matthias and Datta, Nilanjana and Ekert, Artur},
}

@article{Milburn1989,
	title = {Quantum optical {Fredkin} gate},
	volume = {62},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.62.2124},
	number = {18},
	urldate = {2013-06-19},
	journal = {Physical Review Letters},
	author = {Milburn, GJ},
	year = {1989},
	pages = {2124--2127},
}

@article{Fan2010,
	title = {Implementation of the {Fredkin} gate with a three-qubit mixed-spin {Heisenberg} model},
	volume = {53},
	issn = {1674-7348},
	url = {http://link.springer.com/10.1007/s11433-010-3209-9},
	doi = {10.1007/s11433-010-3209-9},
	number = {7},
	urldate = {2013-06-19},
	journal = {Science China Physics, Mechanics and Astronomy},
	author = {Fan, QiuBo},
	month = jul,
	year = {2010},
	pages = {1276--1280},
}

@article{banchi_long_2011,
	title = {Long quantum channels for high-quality entanglement transfer},
	volume = {13},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/13/i=12/a=123006?key=crossref.57b551843a2ded2fda892c8412520260},
	doi = {10.1088/1367-2630/13/12/123006},
	number = {12},
	urldate = {2013-06-18},
	journal = {New Journal of Physics},
	author = {Banchi, L and Apollaro, T J G and Cuccoli, a and Vaia, R and Verrucchi, P},
	month = dec,
	year = {2011},
	pages = {123006},
}

@article{lalumiere_tunable_2010,
	title = {Tunable joint measurements in the dispersive regime of cavity {QED}},
	volume = {81},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.81.040301},
	doi = {10.1103/PhysRevA.81.040301},
	number = {4},
	urldate = {2013-06-17},
	journal = {Physical Review A},
	author = {Lalumière, Kevin and Gambetta, J. M. and Blais, Alexandre},
	month = apr,
	year = {2010},
	pages = {040301},
}

@article{apollaro_99-fidelity_2012,
	title = {99\%-{Fidelity} {Ballistic} {Quantum}-{State} {Transfer} {Through} {Long} {Uniform} {Channels}},
	volume = {85},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.85.052319},
	doi = {10.1103/PhysRevA.85.052319},
	number = {5},
	urldate = {2013-06-18},
	journal = {Physical Review A},
	author = {Apollaro, T. J. G. and Banchi, L. and Cuccoli, a. and Vaia, R. and Verrucchi, P.},
	month = may,
	year = {2012},
	pages = {052319},
}

@article{hnilo_time_2013,
	title = {Time weakens the {Bell} ’ s inequalities .},
	number = {1603},
	author = {Hnilo, Alejandro A},
	year = {2013},
}

@article{divincenzo_multi-qubit_2013,
	title = {Multi-qubit parity measurement in circuit quantum electrodynamics {arXiv} : 1205 . 1910v2 [ quant-ph ] 22 {Mar} 2013},
	author = {Divincenzo, David P and Solgun, Firat},
	year = {2013},
	note = {arXiv: 1205.1910v2},
	pages = {1--21},
}

@article{tanamoto_strategy_2013,
	title = {Strategy for implementing stabilizer-based codes on solid-state qubits},
	volume = {87},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.87.052305},
	doi = {10.1103/PhysRevA.87.052305},
	number = {5},
	urldate = {2013-06-17},
	journal = {Physical Review A},
	author = {Tanamoto, Tetsufumi and Stojanović, Vladimir M. and Bruder, Christoph and Becker, Daniel},
	month = may,
	year = {2013},
	pages = {052305},
}

@article{nigg_stabilizer_2013,
	title = {Stabilizer {Quantum} {Error} {Correction} {Toolbox} for {Superconducting} {Qubits}},
	volume = {110},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.110.243604},
	doi = {10.1103/PhysRevLett.110.243604},
	number = {24},
	urldate = {2013-06-16},
	journal = {Physical Review Letters},
	author = {Nigg, Simon E. and Girvin, S. M.},
	month = jun,
	year = {2013},
	pages = {243604},
}

@article{giovannetti_efficient_nodate,
	title = {Efficient universal blind computation},
	author = {Giovannetti, Vittorio and Maccone, Lorenzo and Morimae, Tomoyuki and Rudolph, Terry G},
	note = {arXiv: 1306.2724v1},
	pages = {1--4},
}

@article{aaronson_improved_2004,
	title = {Improved simulation of stabilizer circuits},
	volume = {70},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.70.052328},
	doi = {10.1103/PhysRevA.70.052328},
	number = {5},
	urldate = {2013-06-13},
	journal = {Physical Review A},
	author = {Aaronson, Scott and Gottesman, Daniel},
	month = nov,
	year = {2004},
	pages = {052328},
}

@article{audenaert_symmetric_2007,
	title = {Symmetric {Squares} of {Graphs}},
	url = {http://www.sciencedirect.com/science/article/pii/S0095895606000451},
	urldate = {2013-06-18},
	journal = {Journal of Combinatorial …},
	author = {Audenaert, Koenraad and Godsil, Chris and Royle, Gordon and Rudolph, T},
	year = {2007},
	note = {arXiv: math/0507251v1},
	pages = {1--19},
}

@article{paganelli_routing_nodate,
	title = {Routing quantum information in spin chains},
	author = {Paganelli, Simone and Lorenzo, Salvatore and Apollaro, Tony J G and Plastina, Francesco and Giorgi, Gian Luca},
	note = {arXiv: 1301.5610v3},
	pages = {1--8},
}

@article{Grigoryan2013,
	title = {All-optical reversible logic gate via adiabatic population transfer},
	url = {http://arxiv.org/abs/1306.2132},
	urldate = {2013-07-18},
	journal = {arXiv preprint arXiv: …},
	author = {Grigoryan, G and Chaltykyan, V},
	year = {2013},
	note = {arXiv: 1306.2132v1},
	pages = {1--8},
}

@article{ionicioiu_entangling_2007,
	title = {Entangling spins by measuring charge: {A} parity-gate toolbox},
	volume = {75},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.75.032339},
	doi = {10.1103/PhysRevA.75.032339},
	number = {3},
	urldate = {2013-06-10},
	journal = {Physical Review A},
	author = {Ionicioiu, Radu},
	month = mar,
	year = {2007},
	pages = {032339},
}

@article{bachman_perfect_2011,
	title = {Perfect state transfer on quotient graphs},
	author = {Bachman, Rachel and Fredette, Eric and Opperman, Michael and Fuller, Jessica and Landry, Michael},
	year = {2011},
	note = {arXiv: 1108.0339v2},
	keywords = {equitable partition, perfect state transfer, quantum walk, quotient graph},
	pages = {1--20},
}

@article{browne_resource-efficient_nodate,
	title = {Resource-efficient linear optical quantum computation},
	author = {Browne, Daniel E and Rudolph, Terry},
	note = {arXiv: quant-ph/0405157v2},
	pages = {2--6},
}

@article{ionicioiu_generalized_2008,
	title = {Generalized parity measurements},
	volume = {78},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.78.052326},
	doi = {10.1103/PhysRevA.78.052326},
	number = {5},
	urldate = {2013-06-10},
	journal = {Physical Review A},
	author = {Ionicioiu, Radu and Popescu, Anca and Munro, William and Spiller, Timothy},
	month = nov,
	year = {2008},
	pages = {052326},
}

@article{beenakker_charge_2004,
	title = {Charge {Detection} {Enables} {Free}-{Electron} {Quantum} {Computation}},
	volume = {93},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.93.020501},
	doi = {10.1103/PhysRevLett.93.020501},
	number = {2},
	urldate = {2013-06-10},
	journal = {Physical Review Letters},
	author = {Beenakker, C. and DiVincenzo, D. and Emary, C. and Kindermann, M.},
	month = jul,
	year = {2004},
	pages = {020501},
}

@article{gross_measurement-based_2007,
	title = {Measurement-based quantum computation beyond the one-way model},
	volume = {76},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.76.052315},
	doi = {10.1103/PhysRevA.76.052315},
	number = {5},
	urldate = {2013-06-10},
	journal = {Physical Review A},
	author = {Gross, D. and Eisert, J. and Schuch, N. and Perez-Garcia, D.},
	month = nov,
	year = {2007},
	pages = {052315},
}

@article{bancal_quantum_2012,
	title = {Quantum nonlocality based on finite-speed causal influences leads to superluminal signalling},
	author = {Bancal, Jean-daniel and Pironio, Stefano and Ac, Antonio and Liang, Yeong-cherng and Scarani, Valerio and Gisin, Nicolas},
	year = {2012},
	note = {arXiv: 1110.3795v2},
	pages = {1--13},
}

@article{kliesch_lieb-robinson_nodate,
	title = {Lieb-{Robinson} bounds and the simulation of time evolution of local observables in lattice systems},
	author = {Kliesch, Martin and Gogolin, Christian and Eisert, Jens},
	note = {arXiv: 1306.0716v1},
	pages = {1--17},
}

@article{reichardt_classical_2013,
	title = {Classical command of quantum systems.},
	volume = {496},
	issn = {1476-4687},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23619692},
	doi = {10.1038/nature12035},
	abstract = {Quantum computation and cryptography both involve scenarios in which a user interacts with an imperfectly modelled or 'untrusted' system. It is therefore of fundamental and practical interest to devise tests that reveal whether the system is behaving as instructed. In 1969, Clauser, Horne, Shimony and Holt proposed an experimental test that can be passed by a quantum-mechanical system but not by a system restricted to classical physics. Here we extend this test to enable the characterization of a large quantum system. We describe a scheme that can be used to determine the initial state and to classically command the system to evolve according to desired dynamics. The bipartite system is treated as two black boxes, with no assumptions about their inner workings except that they obey quantum physics. The scheme works even if the system is explicitly designed to undermine it; any misbehaviour is detected. Among its applications, our scheme makes it possible to test whether a claimed quantum computer is truly quantum. It also advances towards a goal of quantum cryptography: namely, the use of 'untrusted' devices to establish a shared random key, with security based on the validity of quantum physics.},
	number = {7446},
	urldate = {2013-05-26},
	journal = {Nature},
	author = {Reichardt, Ben W and Unger, Falk and Vazirani, Umesh},
	month = apr,
	year = {2013},
	pmid = {23619692},
	note = {Publisher: Nature Publishing Group},
	pages = {456--60},
}

@article{berry_transitionless_2009,
	title = {Transitionless quantum driving},
	volume = {42},
	issn = {1751-8113},
	url = {http://stacks.iop.org/1751-8121/42/i=36/a=365303?key=crossref.8565bc95dce72711b5ffe1a34b22dc71},
	doi = {10.1088/1751-8113/42/36/365303},
	number = {36},
	urldate = {2013-06-06},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Berry, M V},
	month = sep,
	year = {2009},
	pages = {365303},
}

@article{aaronson_ghost_nodate,
	title = {The {Ghost} in the {Quantum} {Turing} {Machine}},
	number = {0844626},
	author = {Aaronson, Scott},
	note = {arXiv: 1306.0159v1},
	pages = {1--85},
}

@article{berry_adiabatic_1987,
	title = {The {Adiabatic} phase and {Pancharatnam}'s phase for polarized light},
	url = {http://www.tandfonline.com/doi/full/10.1080/09500348714551321},
	urldate = {2013-06-06},
	journal = {Journal of Modern Optics},
	author = {Berry, MV},
	year = {1987},
}

@article{knill_scheme_2001,
	title = {A scheme for efficient quantum computation with linear optics.},
	volume = {409},
	issn = {0028-0836},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/11343107},
	doi = {10.1038/35051009},
	abstract = {Quantum computers promise to increase greatly the efficiency of solving problems such as factoring large integers, combinatorial optimization and quantum physics simulation. One of the greatest challenges now is to implement the basic quantum-computational elements in a physical system and to demonstrate that they can be reliably and scalably controlled. One of the earliest proposals for quantum computation is based on implementing a quantum bit with two optical modes containing one photon. The proposal is appealing because of the ease with which photon interference can be observed. Until now, it suffered from the requirement for non-linear couplings between optical modes containing few photons. Here we show that efficient quantum computation is possible using only beam splitters, phase shifters, single photon sources and photo-detectors. Our methods exploit feedback from photo-detectors and are robust against errors from photon loss and detector inefficiency. The basic elements are accessible to experimental investigation with current technology.},
	number = {6816},
	journal = {Nature},
	author = {Knill, E and Laflamme, R and Milburn, G J},
	month = jan,
	year = {2001},
	pmid = {11343107},
	pages = {46--52},
}

@article{terhal_classical_2002,
	title = {Classical simulation of noninteracting-fermion quantum circuits},
	volume = {65},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.65.032325},
	doi = {10.1103/PhysRevA.65.032325},
	number = {3},
	urldate = {2013-06-05},
	journal = {Physical Review A},
	author = {Terhal, Barbara and DiVincenzo, David},
	month = mar,
	year = {2002},
	pages = {032325},
}

@article{ozols_clifford_2008,
	title = {Clifford group {Pauli} matrices {Clifford} group},
	author = {Ozols, Maris},
	year = {2008},
	pages = {6--9},
}

@article{verstraete_classical_2010,
	title = {Classical {Simulation} of {Quantum} {Systems}},
	author = {Verstraete, Frank},
	year = {2010},
}

@article{verstraete_classical_2010-1,
	title = {Classical {Simulation} of {Quantum} {Systems}},
	author = {Verstraete, Frank},
	year = {2010},
}

@misc{noauthor_classical_simulation.pdf_nodate,
	title = {classical\_simulation.pdf},
}

@article{Kay2005,
	title = {Geometric effects and computation in spin networks},
	volume = {7},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/7/i=1/a=143?key=crossref.047e6709f39344b4423ff7fa5c94fd18},
	doi = {10.1088/1367-2630/7/1/143},
	urldate = {2013-05-29},
	journal = {New Journal of Physics},
	author = {Kay, Alastair and Ericsson, Marie},
	month = jun,
	year = {2005},
	pages = {143--143},
}

@article{zela_pancharatnam-berry_1986,
	title = {The {Pancharatnam}-{Berry} {Phase} : {Theoretical} and {Experimental} {Aspects}},
	author = {Zela, Francisco De},
	year = {1986},
}

@article{Kawano2005,
	title = {Explicit implementation of quantum circuits on a quantum-cellular-automata-like architecture},
	volume = {72},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.72.012301},
	doi = {10.1103/PhysRevA.72.012301},
	number = {1},
	urldate = {2013-05-29},
	journal = {Physical Review A},
	author = {Kawano, Y. and Yamashita, S. and Kitagawa, M.},
	month = jul,
	year = {2005},
	pages = {012301},
}

@article{smolin_classical_nodate,
	title = {Classical signature of quantum annealing},
	author = {Smolin, John A and Smith, Graeme},
	note = {arXiv: 1305.4904v1},
	pages = {1--8},
}

@article{jozsa_classical_nodate,
	title = {Classical simulation complexity of extended {Clifford} circuits},
	author = {Jozsa, Richard and Nest, Maarten Van Den},
	note = {arXiv: 1305.6190v1},
	pages = {1--17},
}

@article{wang_comment_nodate,
	title = {Comment on: “{Classical} signature of quantum annealing”},
	author = {Wang, Lei and Rønnow, Troels F and Boixo, Sergio and Isakov, Sergei V and Wang, Zhihui and Wecker, David and Lidar, Daniel A and Martinis, John M and Troyer, Matthias},
	note = {arXiv: 1305.5837v1},
	pages = {1--4},
}

@article{brandao_second_2013,
	title = {The second laws of quantum thermodynamics},
	url = {http://arxiv.org/abs/1305.5278},
	urldate = {2013-06-14},
	journal = {arXiv preprint arXiv: …},
	author = {Brandao, FGSL and Horodecki, Michal},
	year = {2013},
	note = {arXiv: 1305.5278v1},
	pages = {1--27},
}

@article{fogler_exchange_2005,
	title = {Exchange interaction in quantum rings and wires in the {Wigner}-crystal limit},
	volume = {72},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.72.195344},
	doi = {10.1103/PhysRevB.72.195344},
	number = {19},
	urldate = {2013-05-23},
	journal = {Physical Review B},
	author = {Fogler, Michael and Pivovarov, Eugene},
	month = nov,
	year = {2005},
	pages = {195344},
}

@article{joel_introduction_nodate,
	title = {An introduction to the spectrum, symmetries, and dynamics of spin-1/2 {Heisenberg} chains},
	author = {Joel, Kira and Kollmar, Davida and Santos, Lea F},
	note = {arXiv: 1209.0115v2},
	pages = {13--15},
}

@article{hellberg_robust_2008,
	title = {Robust {Quantum} {Computation} with {Quantum} {Dots}},
	volume = {20375},
	author = {Hellberg, C Stephen},
	year = {2008},
	note = {arXiv: quant-ph/0304150v1},
	pages = {1--4},
}

@article{haegeman_elementary_2013,
	title = {Elementary excitations in gapped quantum spin systems},
	author = {Haegeman, Jutho and Michalakis, Spyridon and Nachtergaele, Bruno and Osborne, Tobias J and Schuch, Norbert and Verstraete, Frank},
	year = {2013},
	note = {arXiv: 1305.2176v1},
	pages = {1--5},
}

@article{memisevic_data_2013,
	title = {Data processing in python {Python} {Python} according to {Wikipedia} :},
	author = {Memisevic, Roland},
	year = {2013},
}

@article{oppenheim_uncertainty_2010,
	title = {The uncertainty principle determines the nonlocality of quantum mechanics.},
	volume = {330},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21097930},
	doi = {10.1126/science.1192065},
	abstract = {Two central concepts of quantum mechanics are Heisenberg's uncertainty principle and a subtle form of nonlocality that Einstein famously called "spooky action at a distance." These two fundamental features have thus far been distinct concepts. We show that they are inextricably and quantitatively linked: Quantum mechanics cannot be more nonlocal with measurements that respect the uncertainty principle. In fact, the link between uncertainty and nonlocality holds for all physical theories. More specifically, the degree of nonlocality of any theory is determined by two factors: the strength of the uncertainty principle and the strength of a property called "steering," which determines which states can be prepared at one location given a measurement at another.},
	number = {6007},
	urldate = {2013-03-05},
	journal = {Science (New York, N.Y.)},
	author = {Oppenheim, Jonathan and Wehner, Stephanie},
	month = nov,
	year = {2010},
	pmid = {21097930},
	pages = {1072--4},
}

@article{pfister_information-theoretic_2013,
	title = {An information-theoretic principle implies that any discrete physical theory is classical},
	volume = {4},
	issn = {2041-1723},
	url = {http://www.nature.com/doifinder/10.1038/ncomms2821},
	doi = {10.1038/ncomms2821},
	number = {May},
	urldate = {2013-05-14},
	journal = {Nature Communications},
	author = {Pfister, Corsin and Wehner, Stephanie},
	month = may,
	year = {2013},
	note = {Publisher: Nature Publishing Group},
	pages = {1851},
}

@article{kryszewski_master_2008,
	title = {Master equation – tutorial approach},
	url = {http://arxiv.org/abs/0801.1757},
	urldate = {2013-05-14},
	journal = {arXiv preprint arXiv:0801.1757},
	author = {Kryszewski, Stanislaw and Czechowska-Kryszk, J},
	year = {2008},
	note = {arXiv: 0801.1757v1},
}

@article{sjoqvist_non-adiabatic_2012,
	title = {Non-adiabatic holonomic quantum computation},
	volume = {14},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/14/i=10/a=103035?key=crossref.f0653db5713a986bf618c06d170a9f6f},
	doi = {10.1088/1367-2630/14/10/103035},
	number = {10},
	urldate = {2013-03-15},
	journal = {New Journal of Physics},
	author = {Sjöqvist, Erik and Tong, D M and Mauritz Andersson, L and Hessmo, Björn and Johansson, Markus and Singh, Kuldip},
	month = oct,
	year = {2012},
	pages = {103035},
}

@article{Hasan2003,
	title = {Reversible logic synthesis for minimization of full-adder circuit},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1231899},
	doi = {10.1109/DSD.2003.1231899},
	journal = {Euromicro Symposium on Digital System Design, 2003. Proceedings.},
	author = {Hasan, Hafiz and Rafiqul, Babu and Ahsan, Islam and Chowdhury, Raja and Mostahed, Syed and Chowdhury, Ali},
	year = {2003},
	note = {Publisher: Ieee
ISBN: 0-7695-2003-0},
	pages = {50--54},
}

@article{feng_experimental_2013,
	title = {Experimental {Realization} of {Nonadiabatic} {Holonomic} {Quantum} {Computation}},
	volume = {110},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.110.190501},
	doi = {10.1103/PhysRevLett.110.190501},
	number = {19},
	urldate = {2013-05-13},
	journal = {Physical Review Letters},
	author = {Feng, Guanru and Xu, Guofu and Long, Guilu},
	month = may,
	year = {2013},
	pages = {190501},
}

@article{wang_minimal_2013,
	title = {Minimal noise subsystems},
	url = {http://arxiv.org/abs/1305.1978},
	number = {1},
	urldate = {2013-06-11},
	journal = {arXiv preprint arXiv:1305.1978},
	author = {Wang, Xiaoting and Byrd, Mark and Jacobs, Kurt},
	year = {2013},
	note = {arXiv: 1305.1978v1},
	pages = {1--5},
}

@article{markiewicz_perfect_2009,
	title = {Perfect {State} {Transfer} without {State} {Initialization} and {Remote} {Collaboration}},
	number = {1},
	author = {Markiewicz, Marcin},
	year = {2009},
	note = {arXiv: 0902.1095v4},
	pages = {1--4},
}

@article{micadei_thermodynamic_nodate,
	title = {Thermodynamic cost of acquiring information},
	author = {Micadei, Kaonan and Serra, Roberto M},
	note = {arXiv: 1211.0506v2},
}

@article{aharonov_each_nodate,
	title = {Each instant of time a new {Universe}},
	author = {Aharonov, Yakir and Popescu, Sandu and Tollaksen, Jeff},
	note = {arXiv: 1305.1615v1},
	pages = {1--8},
}

@article{effect_covariant_nodate,
	title = {The covariant, time-dependent {Aharonov}-{Bohm} {Effect}},
	author = {Effect, Aharonov-bohm and Singleton, Douglas and Vagenas, Elias C},
	note = {arXiv: 1305.1498v1},
}

@article{west_exchange-only_2012,
	title = {Exchange-only dynamical decoupling in the three-qubit decoherence free subsystem},
	volume = {14},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/14/i=8/a=083002?key=crossref.8067350cae85a095c79c31043555ac10},
	doi = {10.1088/1367-2630/14/8/083002},
	number = {8},
	urldate = {2013-05-08},
	journal = {New Journal of Physics},
	author = {West, J R and Fong, B H},
	month = aug,
	year = {2012},
	pages = {083002},
}

@article{garner_general_2013,
	title = {A general framework for phase and interference},
	author = {Garner, Andrew J P and Dahlsten, Oscar C O and Nakata, Yoshifumi and Murao, Mio and Vedral, Vlatko},
	year = {2013},
	note = {arXiv: 1304.5977v1},
	pages = {1--15},
}

@article{Younes2013,
	title = {Tight {Bounds} on the {Synthesis} of 3-bit {Reversible} {Circuits}: {NFT} {Library}},
	url = {http://arxiv.org/abs/1304.5804},
	urldate = {2013-05-02},
	journal = {arXiv preprint arXiv:1304.5804},
	author = {Younes, Ahmed},
	year = {2013},
	note = {arXiv: 1304.5804v2},
	keywords = {circuit optimization, group theory, quantum cost, reversible circuit},
	pages = {1--18},
}

@article{raussendorf_contextuality_2013,
	title = {Contextuality in {Measurement}-based {Quantum} {Computation}},
	author = {Raussendorf, Robert},
	year = {2013},
	note = {arXiv: 0907.5449v3},
	pages = {1--8},
}

@article{Selinger2013,
	title = {Quantum circuits of {T}-depth one},
	volume = {87},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.87.042302},
	doi = {10.1103/PhysRevA.87.042302},
	number = {4},
	urldate = {2013-04-06},
	journal = {Physical Review A},
	author = {Selinger, Peter},
	month = apr,
	year = {2013},
	pages = {042302},
}

@article{Moqadam2013,
	title = {Analyzing the {Toffoli} gate in disordered circuit {QED}},
	volume = {87},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.87.042324},
	doi = {10.1103/PhysRevA.87.042324},
	number = {4},
	urldate = {2013-05-02},
	journal = {Physical Review A},
	author = {Moqadam, Jalil Khatibi and Portugal, Renato and Svaiter, Nami Fux and Corrêa, Gilberto De Oliveira},
	month = apr,
	year = {2013},
	pages = {042324},
}

@article{Zheng2013,
	title = {Implementation of {Toffoli} gates with a single asymmetric {Heisenberg} {XY} interaction},
	volume = {87},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.87.042318},
	doi = {10.1103/PhysRevA.87.042318},
	number = {4},
	urldate = {2013-05-02},
	journal = {Physical Review A},
	author = {Zheng, Shi-Biao},
	month = apr,
	year = {2013},
	pages = {042318},
}

@article{Bruce2002,
	title = {Efficient adder circuits based on a conservative reversible logic gate},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1016879},
	doi = {10.1109/ISVLSI.2002.1016879},
	journal = {Proceedings IEEE Computer Society Annual Symposium on VLSI. New Paradigms for VLSI Systems Design. ISVLSI 2002},
	author = {Bruce, J.W. and Thornton, M.a. and Shivakumaraiah, L. and Kokate, P.S. and Li, X.},
	year = {2002},
	note = {Publisher: IEEE Comput. Soc
ISBN: 0-7695-1486-3},
	pages = {83--88},
}

@article{Troiani2013,
	title = {On the size of linear superpositions in molecular nanomagnets},
	url = {http://arxiv.org/abs/1304.7618},
	urldate = {2013-07-29},
	journal = {arXiv preprint arXiv:1304.7618},
	author = {Troiani, F. and Zanardi, P.},
	year = {2013},
	note = {arXiv: 1304.7618v1},
	pages = {1--10},
}

@article{Brown2013,
	title = {Reducing the number of ancilla qubits and the gate count required for creating large controlled operations},
	url = {http://arxiv.org/abs/1304.5548},
	urldate = {2013-05-02},
	journal = {arXiv preprint arXiv:1304.5548},
	author = {Brown, KL and Daskin, Anmer and Kais, Sabre and Dowling, JP},
	year = {2013},
	note = {arXiv: 1304.5548v1},
	pages = {1--5},
}

@article{Koiller2001,
	title = {Exchange in {Silicon}-{Based} {Quantum} {Computer} {Architecture}},
	volume = {88},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.88.027903},
	doi = {10.1103/PhysRevLett.88.027903},
	number = {2},
	urldate = {2013-04-23},
	journal = {Physical Review Letters},
	author = {Koiller, Belita and Hu, Xuedong and Das Sarma, S.},
	month = dec,
	year = {2001},
	pages = {027903},
}

@article{Wedge2012,
	title = {Chemical {Engineering} of {Molecular} {Qubits}},
	volume = {108},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.108.107204},
	doi = {10.1103/PhysRevLett.108.107204},
	number = {10},
	urldate = {2013-05-01},
	journal = {Physical Review Letters},
	author = {Wedge, C. J. and Timco, G. a. and Spielberg, E. T. and George, R. E. and Tuna, F. and Rigby, S. and McInnes, E. J. L. and Winpenny, R. E. P. and Blundell, S. J. and Ardavan, a.},
	month = mar,
	year = {2012},
	pages = {107204},
}

@article{Takahashi2011,
	title = {Decoherence in crystals of quantum molecular magnets.},
	volume = {476},
	issn = {1476-4687},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21775988},
	doi = {10.1038/nature10314},
	abstract = {Quantum decoherence is a central concept in physics. Applications such as quantum information processing depend on understanding it; there are even fundamental theories proposed that go beyond quantum mechanics, in which the breakdown of quantum theory would appear as an 'intrinsic' decoherence, mimicking the more familiar environmental decoherence processes. Such applications cannot be optimized, and such theories cannot be tested, until we have a firm handle on ordinary environmental decoherence processes. Here we show that the theory for insulating electronic spin systems can make accurate and testable predictions for environmental decoherence in molecular-based quantum magnets. Experiments on molecular magnets have successfully demonstrated quantum-coherent phenomena but the decoherence processes that ultimately limit such behaviour were not well constrained. For molecular magnets, theory predicts three principal contributions to environmental decoherence: from phonons, from nuclear spins and from intermolecular dipolar interactions. We use high magnetic fields on single crystals of Fe(8) molecular magnets (in which the Fe ions are surrounded by organic ligands) to suppress dipolar and nuclear-spin decoherence. In these high-field experiments, we find that the decoherence time varies strongly as a function of temperature and magnetic field. The theoretical predictions are fully verified experimentally, and there are no other visible decoherence sources. In these high fields, we obtain a maximum decoherence quality-factor of 1.49 × 10(6); our investigation suggests that the environmental decoherence time can be extended up to about 500 microseconds, with a decoherence quality factor of ∼6 × 10(7), by optimizing the temperature, magnetic field and nuclear isotopic concentrations.},
	number = {7358},
	urldate = {2013-03-01},
	journal = {Nature},
	author = {Takahashi, S and Tupitsyn, I S and van Tol, J and Beedle, C C and Hendrickson, D N and Stamp, P C E},
	month = aug,
	year = {2011},
	pmid = {21775988},
	note = {Publisher: Nature Publishing Group},
	pages = {76--9},
}

@article{fulton_arxiv_2000,
	title = {{arXiv} : math / 9908012v3 [ math . {AG} ] 27 {Mar} 2000},
	author = {Fulton, William},
	year = {2000},
	note = {arXiv: math/9908012v3},
	pages = {1--42},
}

@article{hanggi_violation_2013,
	title = {A violation of the uncertainty principle implies a violation of the second law of thermodynamics},
	volume = {4},
	issn = {2041-1723},
	url = {http://www.nature.com/doifinder/10.1038/ncomms2665},
	doi = {10.1038/ncomms2665},
	urldate = {2013-04-09},
	journal = {Nature Communications},
	author = {Hänggi, Esther and Wehner, Stephanie},
	month = apr,
	year = {2013},
	pages = {1670},
}

@article{medford_resonant_2013,
	title = {The {Resonant} {Exchange} {Qubit}},
	volume = {0},
	author = {Medford, J and Beil, J and Taylor, J M and Rashba, E I and Lu, H and Gossard, A C and Marcus, C M},
	year = {2013},
	note = {arXiv: 1304.3413v2},
	pages = {1--12},
}

@article{Owerre2013,
	title = {Macroscopic quantum spin tunnelling with two interacting spins},
	url = {http://arxiv.org/abs/1304.3734},
	urldate = {2013-05-02},
	journal = {arXiv preprint arXiv:1304.3734},
	author = {Owerre, SA and Paranjape, MB},
	year = {2013},
	note = {arXiv: 1304.3734v1},
}

@article{doherty_two-qubit_2013,
	title = {Two-{Qubit} {Gates} for {Resonant} {Exchange} {Qubits}},
	url = {http://arxiv.org/abs/1304.3416},
	urldate = {2013-06-11},
	journal = {arXiv preprint arXiv:1304.3416},
	author = {Doherty, AC and Wardrop, MP},
	year = {2013},
	note = {arXiv: 1304.3416v2},
	pages = {1--8},
}

@article{taylor_electrically-protected_nodate,
	title = {Electrically-protected resonant exchange qubits in triple quantum dots},
	author = {Taylor, J M and Srinivasa, V and Medford, J},
	note = {arXiv: 1304.3407v2},
	pages = {1--5},
}

@article{Kumar2007,
	title = {Simplified {Approach} to {Implementing} {Controlled} {Unitary} {Operations} in a {Two}-{Qubit} {System}},
	url = {http://pra.aps.org/abstract/PRA/v76/i2/e022335},
	number = {2},
	urldate = {2013-05-02},
	journal = {Physical Review A},
	author = {Kumar, Preethika and Skinner, SR},
	year = {2007},
}

@article{Cohn1993,
	title = {The diophantine equation},
	volume = {4},
	urldate = {2013-07-29},
	author = {{J. Cohn}},
	year = {1998},
}

@article{avron_adiabatic_1987,
	title = {Adiabatic {Theorems} and {Applications} to the {Quantum} {Hall} {Effect}},
	volume = {49},
	author = {Avron, J E and Seiler, R and Yaffe, L G},
	year = {1987},
	pages = {33--49},
}

@article{AbuMuriefah2008,
	title = {On the diophantine equation},
	volume = {128},
	issn = {0022314X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0022314X08000541},
	doi = {10.1016/j.jnt.2008.01.005},
	number = {6},
	journal = {Journal of Number Theory},
	author = {Abu Muriefah, Fadwa S.},
	month = jun,
	year = {2008},
	pages = {1670--1675},
}

@article{Matthews,
	title = {The {Diophantine} {Equation} x 2 − {Dy} 2 = {N} , {D} {\textgreater} 0 {Keith} {Matthews}},
	author = {Matthews, Keith},
	pages = {333--340},
}

@article{childs_universal_2013,
	title = {Universal computation by multiparticle quantum walk.},
	volume = {339},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23413349},
	doi = {10.1126/science.1229957},
	abstract = {A quantum walk is a time-homogeneous quantum-mechanical process on a graph defined by analogy to classical random walk. The quantum walker is a particle that moves from a given vertex to adjacent vertices in quantum superposition. We consider a generalization to interacting systems with more than one walker, such as the Bose-Hubbard model and systems of fermions or distinguishable particles with nearest-neighbor interactions, and show that multiparticle quantum walk is capable of universal quantum computation. Our construction could, in principle, be used as an architecture for building a scalable quantum computer with no need for time-dependent control.},
	number = {6121},
	urldate = {2013-02-27},
	journal = {Science (New York, N.Y.)},
	author = {Childs, Andrew M and Gosset, David and Webb, Zak},
	month = feb,
	year = {2013},
	pmid = {23413349},
	pages = {791--4},
}

@article{berry_quantal_1984,
	title = {Quantal {Phase} {Factors} {Accompanying} {Adiabatic} {Changes}},
	volume = {392},
	issn = {1364-5021},
	url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1984.0023},
	doi = {10.1098/rspa.1984.0023},
	number = {1802},
	urldate = {2013-02-28},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Berry, M. V.},
	month = mar,
	year = {1984},
	pages = {45--57},
}

@article{Zheng2012,
	title = {Simplified construction and physical realization of n-qubit controlled phase gates},
	volume = {86},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.86.012326},
	doi = {10.1103/PhysRevA.86.012326},
	number = {1},
	urldate = {2013-04-10},
	journal = {Physical Review A},
	author = {Zheng, Shi-Biao},
	month = jul,
	year = {2012},
	pages = {012326},
}

@article{tong_quantitative_2005,
	title = {Quantitative {Conditions} {Do} {Not} {Guarantee} the {Validity} of the {Adiabatic} {Approximation}},
	volume = {95},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.95.110407},
	doi = {10.1103/PhysRevLett.95.110407},
	number = {11},
	urldate = {2013-04-10},
	journal = {Physical Review Letters},
	author = {Tong, D. and Singh, K. and Kwek, L. and Oh, C.},
	month = sep,
	year = {2005},
	pages = {110407},
}

@article{tong_quantitative_2010,
	title = {Quantitative {Condition} is {Necessary} in {Guaranteeing} the {Validity} of the {Adiabatic} {Approximation}},
	volume = {104},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.104.120401},
	doi = {10.1103/PhysRevLett.104.120401},
	number = {12},
	urldate = {2013-03-19},
	journal = {Physical Review Letters},
	author = {Tong, D. M.},
	month = mar,
	year = {2010},
	pages = {120401},
}

@article{reichardt_quantum_2004,
	title = {The quantum adiabatic optimization algorithm and local minima},
	url = {http://portal.acm.org/citation.cfm?doid=1007352.1007428},
	doi = {10.1145/1007352.1007428},
	journal = {Proceedings of the thirty-sixth annual ACM symposium on Theory of computing - STOC '04},
	author = {Reichardt, Ben W.},
	year = {2004},
	note = {Publisher: ACM Press
Place: New York, New York, USA
ISBN: 1581138520},
	keywords = {ising quantum chain, quantum adiabatic optimization},
	pages = {502},
}

@article{di_ventra_parallel_2013,
	title = {The parallel approach},
	volume = {9},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys2566},
	doi = {10.1038/nphys2566},
	number = {4},
	urldate = {2013-04-02},
	journal = {Nature Physics},
	author = {Di Ventra, Massimiliano and Pershin, Yuriy V.},
	month = apr,
	year = {2013},
	note = {Publisher: Nature Publishing Group},
	pages = {200--202},
}

@article{durstberger_geometric_2002,
	title = {Geometric phases in quantum theory},
	author = {Durstberger, Katharina},
	year = {2002},
}

@article{Jones2013,
	title = {Composite {Toffoli} gate with two-round error detection},
	url = {http://pra.aps.org/abstract/PRA/v87/i5/e052334},
	urldate = {2013-07-18},
	journal = {Physical Review A},
	author = {Jones, Cody},
	year = {2013},
	note = {arXiv: 1303.6971v1},
}

@article{lee_can_2013,
	title = {Can {PT}-{Symmetric} {Quantum} {Mechanics} be a {Viable} {Alternative} {Quantum} {Theory}?},
	url = {http://arxiv.org/abs/1303.6339},
	urldate = {2013-06-14},
	journal = {arXiv preprint arXiv:1303.6339},
	author = {Lee, Sungwook and Mead, LR},
	year = {2013},
	note = {arXiv: 1303.6339v1},
	pages = {1--21},
}

@article{Fedorov2012,
	title = {Implementation of a {Toffoli} gate with superconducting circuits.},
	volume = {481},
	issn = {1476-4687},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22170609},
	doi = {10.1038/nature10713},
	abstract = {The Toffoli gate is a three-quantum-bit (three-qubit) operation that inverts the state of a target qubit conditioned on the state of two control qubits. It makes universal reversible classical computation possible and, together with a Hadamard gate, forms a universal set of gates in quantum computation. It is also a key element in quantum error correction schemes. The Toffoli gate has been implemented in nuclear magnetic resonance, linear optics and ion trap systems. Experiments with superconducting qubits have also shown significant progress recently: two-qubit algorithms and two-qubit process tomography have been implemented, three-qubit entangled states have been prepared, first steps towards quantum teleportation have been taken and work on quantum computing architectures has been done. Implementation of the Toffoli gate with only single- and two-qubit gates requires six controlled-NOT gates and ten single-qubit operations, and has not been realized in any system owing to current limits on coherence. Here we implement a Toffoli gate with three superconducting transmon qubits coupled to a microwave resonator. By exploiting the third energy level of the transmon qubits, we have significantly reduced the number of elementary gates needed for the implementation of the Toffoli gate, relative to that required in theoretical proposals using only two-level systems. Using full process tomography and Monte Carlo process certification, we completely characterize the Toffoli gate acting on three independent qubits, measuring a fidelity of 68.5 ± 0.5 per cent. A similar approach to realizing characteristic features of a Toffoli-class gate has been demonstrated with two qubits and a resonator and achieved a limited characterization considering only the phase fidelity. Our results reinforce the potential of macroscopic superconducting qubits for the implementation of complex quantum operations with the possibility of quantum error correction.},
	number = {7380},
	urldate = {2013-03-07},
	journal = {Nature},
	author = {Fedorov, a and Steffen, L and Baur, M and da Silva, M P and Wallraff, a},
	month = jan,
	year = {2012},
	pmid = {22170609},
	note = {Publisher: Nature Publishing Group},
	pages = {170--2},
}

@article{Imre2006,
	title = {Majority logic gate for magnetic quantum-dot cellular automata.},
	volume = {311},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16410520},
	doi = {10.1126/science.1120506},
	abstract = {We describe the operation of, and demonstrate logic functionality in, networks of physically coupled, nanometer-scale magnets designed for digital computation in magnetic quantum-dot cellular automata (MQCA) systems. MQCA offer low power dissipation and high integration density of functional elements and operate at room temperature. The basic MQCA logic gate, that is, the three-input majority logic gate, is demonstrated.},
	number = {5758},
	urldate = {2013-03-05},
	journal = {Science (New York, N.Y.)},
	author = {Imre, a and Csaba, G and Ji, L and Orlov, a and Bernstein, G H and Porod, W},
	month = jan,
	year = {2006},
	pmid = {16410520},
	pages = {205--8},
}

@article{wu_full_2013,
	title = {Full {Electric} {Control} of {Exchange} {Bias}},
	volume = {110},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.110.067202},
	doi = {10.1103/PhysRevLett.110.067202},
	number = {6},
	urldate = {2013-03-01},
	journal = {Physical Review Letters},
	author = {Wu, S. M. and Cybart, Shane a. and Yi, D. and Parker, James M. and Ramesh, R. and Dynes, R. C.},
	month = feb,
	year = {2013},
	pages = {067202},
}

@article{boixo_eigenpath_2009,
	title = {Eigenpath traversal by phase randomization},
	volume = {9},
	number = {9},
	author = {Boixo, Sergio and Knill, Emanuel and Somma, Rolando},
	year = {2009},
	keywords = {adiabatic theorem, b terhal, communicated by, dynamical decoupling, r jozsa, randomization, zeno effect},
	pages = {833--855},
}

@article{somma_spectral_2012,
	title = {Spectral {Gap} {Amplification}},
	author = {Somma, R D and Boixo, S and Computing, Adiabatic Quantum and Monte-carlo, Quantum},
	year = {2012},
	note = {arXiv: 1110.2494v3},
	keywords = {adiabatic quantum computing, quantum algorithms, quantum monte-carlo},
}

@article{Bugu2011,
	title = {Enhancing the {W}-state quantum-network-fusion process with a single {Fredkin} gate},
	url = {http://pra.aps.org/abstract/PRA/v87/i3/e032331},
	urldate = {2013-07-29},
	journal = {Physical Review A},
	author = {Bugu, Sinan and Yesilyurt, Can and Ozaydin, Fatih},
	year = {2013},
	note = {arXiv: 1303.4008v1},
	pages = {1--4},
}

@article{chancellor_scalable_2013,
	title = {Scalable universal holonomic quantum computation realized with an adiabatic quantum data bus and potential implementation using superconducting flux qubits},
	url = {http://arxiv.org/abs/1301.7100},
	urldate = {2013-04-16},
	journal = {arXiv preprint arXiv:1301.7100},
	author = {Chancellor, Nicholas and Haas, Stephan},
	year = {2013},
	note = {arXiv: 1301.7100v2},
}

@article{Mironov2003,
	title = {Mechanism of a strongly anisotropic {MoIII}-{CN}-{MnII} spin-spin coupling in molecular magnets based on the [{Mo}({CN})(7)](4-) heptacyanometalate: a new strategy for single-molecule magnets with high blocking temperatures.},
	volume = {125},
	issn = {0002-7863},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12904041},
	doi = {10.1021/ja029518o},
	abstract = {Unusual spin coupling between Mo(III) and Mn(II) cyano-bridged ions in bimetallic molecular magnets based on the [Mo(III)(CN)(7)](4-) heptacyanometalate is analyzed in terms of the superexchange theory. Due to the orbital degeneracy and strong spin-orbit coupling on Mo(III), the ground state of the pentagonal-bipyramidal [Mo(III)(CN)(7)](4-) complex corresponds to an anisotropic Kramers doublet. Using a specially adapted kinetic exchange model we have shown that the Mo(III)-CN-Mn(II) superexchange interaction is extremely anisotropic: it is described by an Ising-like spin Hamiltonian JS(z)(Mo) S(z)(Mn) for the apical pairs and by the J(z)S(z)(Mo) S(z)(Mn) + J(xy)(Sx(Mo) Sx(Mn) + Sy(Mo) Sy(Mn)) spin Hamiltonian for the equatorial pairs (in the latter case J(z) and J(xy) can have opposite signs). This anisotropy resulted from an interplay of several Ising-like (Sz(Mo) Sz(Mn)) and isotropic (S(Mo)S(Mn)) ferro- and antiferromagnetic contributions originating from metal-to-metal electron transfers through the pi and sigma orbitals of the cyano bridges. The Mo(III)-CN-Mn(II) exchange anisotropy is distinct from the anisotropy of the g-tensor of [Mo(III)(CN)(7)](4-); moreover, there is no correlation between the exchange anisotropy and g-tensor anisotropy. We indicate that highly anisotropic spin-spin couplings (such as the Ising-like JS(z)(Mo) S(z)(Mn)) combined with large exchange parameters represent a very important source of the global magnetic anisotropy of polyatomic molecular magnetic clusters. Since the total spin of such clusters is no longer a good quantum number, the spin spectrum pattern can differ considerably from the conventional scheme described by the zero-field splitting of the isotropic spin of the ground state. As a result, the spin reorientation barrier of the magnetic cluster may be considerably larger. This finding opens a new way in the strategy of designing single-molecule magnets (SMM) with unusually high blocking temperatures. The use of orbitally degenerate complexes with a strong spin-orbit coupling (such as [Mo(III)(CN)(7)](4-) or its 5d analogues) as building blocks is therefore very promising for these purposes.},
	number = {32},
	journal = {Journal of the American Chemical Society},
	author = {Mironov, Vladimir S and Chibotaru, Liviu F and Ceulemans, Arnout},
	month = aug,
	year = {2003},
	pmid = {12904041},
	pages = {9750--60},
}

@article{ohshima_robust_2008,
	title = {Robust state transfer and rotation through a spin chain via dark passage},
	author = {Ohshima, Toshio and Ekert, Artur and Oi, Daniel K L and Kaslizowski, Dagomir and Kwek, L C},
	year = {2008},
	note = {arXiv: quant-ph/0702019v3},
	pages = {1--4},
}

@article{Kay2007,
	title = {Quantum-{Merlin}-{Arthur}-complete translationally invariant {Hamiltonian} problem and the complexity of finding ground-state energies in physical systems},
	volume = {76},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.76.030307},
	doi = {10.1103/PhysRevA.76.030307},
	number = {3},
	urldate = {2013-03-15},
	journal = {Physical Review A},
	author = {Kay, Alastair},
	month = sep,
	year = {2007},
	pages = {030307},
}

@article{Kay2007a,
	title = {Unifying {Quantum} {State} {Transfer} and {State} {Amplification}},
	volume = {98},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.98.010501},
	doi = {10.1103/PhysRevLett.98.010501},
	number = {1},
	urldate = {2013-03-15},
	journal = {Physical Review Letters},
	author = {Kay, Alastair},
	month = jan,
	year = {2007},
	pages = {010501},
}

@article{Pemberton-Ross2011,
	title = {Perfect {Quantum} {Routing} in {Regular} {Spin} {Networks}},
	volume = {106},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.106.020503},
	doi = {10.1103/PhysRevLett.106.020503},
	number = {2},
	urldate = {2013-03-15},
	journal = {Physical Review Letters},
	author = {Pemberton-Ross, Peter J. and Kay, Alastair},
	month = jan,
	year = {2011},
	pages = {020503},
}

@article{Kay2009,
	title = {Interfacing with {Hamiltonian} dynamics},
	volume = {79},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.79.042330},
	doi = {10.1103/PhysRevA.79.042330},
	number = {4},
	urldate = {2013-03-15},
	journal = {Physical Review A},
	author = {Kay, Alastair},
	month = apr,
	year = {2009},
	pages = {042330},
}

@article{Yung2011,
	title = {Spin star as a switch for quantum networks},
	volume = {44},
	issn = {0953-4075},
	url = {http://stacks.iop.org/0953-4075/44/i=13/a=135504?key=crossref.1bdb527a2a11cda4d8d3406c50832773},
	doi = {10.1088/0953-4075/44/13/135504},
	number = {13},
	urldate = {2013-03-15},
	journal = {Journal of Physics B: Atomic, Molecular and Optical Physics},
	author = {Yung, Man-Hong},
	month = jul,
	year = {2011},
	pages = {135504},
}

@article{Christandl2005,
	title = {Perfect transfer of arbitrary states in quantum spin networks},
	volume = {71},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.71.032312},
	doi = {10.1103/PhysRevA.71.032312},
	number = {3},
	urldate = {2013-03-15},
	journal = {Physical Review A},
	author = {Christandl, Matthias and Datta, Nilanjana and Dorlas, Tony and Ekert, Artur and Kay, Alastair and Landahl, Andrew},
	month = mar,
	year = {2005},
	pages = {032312},
}

@article{fr_n-person_2013,
	title = {N-person quantum {Russian} roulette {arXiv} : 1303 . 0155v1 [ quant-ph ] 1 {Mar} 2013},
	volume = {1},
	number = {1},
	author = {Fr, Piotr},
	year = {2013},
	note = {arXiv: 1303.0155v1},
	pages = {4--7},
}

@article{moqadam_analyzing_2013,
	title = {Analyzing {Toffoli} {Gate} in {Disordered} {Circuit} {QED}},
	author = {Moqadam, Jalil Khatibi and Svaiter, Nami Fux},
	year = {2013},
	note = {arXiv: 1302.6565v1},
	pages = {1--7},
}

@article{sharma_fundamental_nodate,
	title = {Fundamental bound on the reliability of quantum information transmission},
	author = {Sharma, Naresh and Warsi, Naqueeb Ahmad},
	note = {arXiv: 1302.5281v1},
}

@article{ghirardi_about_2013,
	title = {About possible extensions of quantum theory},
	url = {http://arxiv.org/abs/1301.5040},
	abstract = {Recently it has been claimed that no extension of quantum theory can have improved predictive power, the statement following, according to the authors, from the assumptions of free will and of the correctness of quantum predictions concerning the correlations of measurement outcomes. Here we prove that the argument is basically flawed by an inappropriate use of the assumption of free will. In particular, among other implications, the claim, if correct, would imply that Bohmian Mechanics is incompatible with free will. This statement, appearing in the paper, derives from the unjustified identification of free will with the no-signaling constraint and of a purely formal and not physical use of such a constraint.},
	number = {February},
	urldate = {2013-03-03},
	author = {Ghirardi, GianCarlo and Romano, Raffaele},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.5040
Genre: Quantum Physics},
	pages = {5},
}

@article{brasil_how_nodate,
	title = {How much time does a measurement take?},
	author = {Brasil, Carlos Alexandre},
	note = {arXiv: 1109.4613v7},
	pages = {1--17},
}

@article{wang_numerical_2003,
	title = {Numerical method for finding decoherence-free subspaces and its applications},
	author = {Wang, Xiaoting and Byrd, Mark and Jacobs, Kurt},
	year = {2003},
	note = {arXiv: 1212.3839v2},
}

@article{yao_topologically_2013,
	title = {Topologically protected quantum state transfer in a chiral spin liquid},
	volume = {4},
	issn = {2041-1723},
	url = {http://www.nature.com/doifinder/10.1038/ncomms2531},
	doi = {10.1038/ncomms2531},
	urldate = {2013-03-12},
	journal = {Nature Communications},
	author = {Yao, N.Y. and Laumann, C.R. and Gorshkov, a.V. and Weimer, H. and Jiang, L. and Cirac, J.I. and Zoller, P. and Lukin, M.D.},
	month = mar,
	year = {2013},
	note = {Publisher: Nature Publishing Group},
	pages = {1585},
}

@article{Mariantoni2011,
	title = {Implementing the quantum von {Neumann} architecture with superconducting circuits},
	urldate = {2013-07-18},
	journal = {Science},
	author = {Mariantoni, Matteo and Wang, H and Yamamoto, T},
	year = {2011},
	note = {arXiv: 1109.3743v1},
	pages = {1--43},
}

@article{devitt_quantum_2011,
	title = {Quantum {Error} {Correction} for {Beginners}},
	author = {Devitt, Simon J and Nemoto, Kae and Munro, William J},
	year = {2011},
	note = {arXiv: 0905.2794v3},
	pages = {1--38},
}

@article{pachos_non-abelian_1999,
	title = {Non-{Abelian} {Berry} connections for quantum computation {Jiannis}},
	volume = {61},
	url = {http://pra.aps.org/abstract/PRA/v61/i1/e010305},
	urldate = {2013-09-03},
	journal = {Physical Review A},
	author = {Pachos, Jiannis and Zanardi, Paolo and Rasetti, Mario},
	year = {1999},
	pages = {1--4},
}

@article{oreshkov_fault-tolerant_2009,
	title = {Fault-{Tolerant} {Holonomic} {Quantum} {Computation}},
	volume = {102},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.102.070502},
	doi = {10.1103/PhysRevLett.102.070502},
	number = {7},
	urldate = {2013-03-04},
	journal = {Physical Review Letters},
	author = {Oreshkov, Ognyan and Brun, Todd and Lidar, Daniel},
	month = feb,
	year = {2009},
	pages = {070502},
}

@article{whitfield_ground_2012,
	title = {Ground {State} {Spin} {Logic}},
	url = {http://iopscience.iop.org/0295-5075/99/5/57004},
	number = {1},
	urldate = {2014-11-10},
	journal = {EPL (Europhysics Letters)},
	author = {Whitfield, JD and Faccin, M and Biamonte, JD},
	year = {2012},
	note = {arXiv: 1205.1742v1},
}

@article{Lanyon2008,
	title = {Simplifying quantum logic using higher-dimensional {Hilbert} spaces},
	volume = {5},
	issn = {1745-2473},
	url = {http://dx.doi.org/10.1038/nphys1150},
	doi = {10.1038/nphys1150},
	number = {2},
	urldate = {2013-07-18},
	journal = {Nature Physics},
	author = {Lanyon, BP and Barbieri, Marco and Almeida, MP},
	year = {2008},
	note = {Publisher: Nature Publishing Group},
	pages = {134--140},
}

@article{Zhang2009,
	title = {Nongeometric multiqubit conditional phase gates by adiabatic evolution for trapped ions},
	volume = {3},
	url = {http://pra.aps.org/abstract/PRA/v79/i3/e034301},
	doi = {10.1103/PhysRevA.79.034301},
	urldate = {2013-07-29},
	journal = {Physical Review A},
	author = {Zhang, XL and Feng, XL and Wu, Chunfeng and Oh, CH},
	year = {2009},
	pages = {2--5},
}

@article{Tame2009,
	title = {Compact {Toffoli} gate using weighted graph states},
	url = {http://pra.aps.org/abstract/PRA/v79/i2/e020302},
	doi = {10.1103/PhysRevA.79.020302},
	urldate = {2013-05-02},
	journal = {Physical Review A},
	author = {Tame, MS and Özdemir, ŞK and Koashi, M and Imoto, N and Kim, MS},
	year = {2009},
	pages = {1--4},
}

@article{Chen2006,
	title = {Toffoli gate originating from a single resonant interaction with cavity {QED}},
	url = {http://pra.aps.org/abstract/PRA/v73/i6/e064304},
	doi = {10.1103/PhysRevA.73.064304},
	urldate = {2013-07-18},
	journal = {Physical Review A},
	author = {Chen, CY and Feng, Mang and Gao, KL},
	year = {2006},
	pages = {6--9},
}

@article{Kumar2013,
	title = {Direct implementation of an {N}-qubit controlled-unitary},
	doi = {10.1007/s11128-012-0465-9},
	author = {Kumar, Preethika},
	year = {2013},
	keywords = {controlled-unitary, multi-control, multi-coupled, n-qubit, quantum},
	pages = {1201--1223},
}

@article{Lin2006,
	title = {One-step implementation of a multiqubit controlled-phase-flip gate},
	url = {http://pra.aps.org/abstract/PRA/v73/i1/e012323},
	doi = {10.1103/PhysRevA.73.012323},
	urldate = {2013-07-18},
	journal = {Physical Review A},
	author = {Lin, XM and Zhou, ZW and Ye, MY and Xiao, YF and Guo, GC},
	year = {2006},
	pages = {1--7},
}

@article{Deng2007,
	title = {Implementation of a nonlocal {N}-qubit conditional phase gate by single-photon interference},
	url = {http://pra.aps.org/abstract/PRA/v76/i4/e044305},
	doi = {10.1103/PhysRevA.76.044305},
	urldate = {2013-07-18},
	journal = {Physical Review A},
	author = {Deng, ZJ and Zhang, XL and Wei, H and Gao, KL and Feng, M},
	year = {2007},
	pages = {1--4},
}

@article{Xiao2007,
	title = {One-step implementation of an {N}-qubit controlled-phase gate with neutral atoms trapped in an optical cavity},
	volume = {75},
	doi = {10.1103/PhysRevA.75.054303},
	urldate = {2013-07-29},
	journal = {Physical Review A},
	author = {Xiao, Yun-feng and Zou, Xu-bo and Guo, Guang-can},
	year = {2007},
	pages = {054303},
}

@article{Wu2010,
	title = {Implementation of a multiqubit quantum phase gate in a neutral atomic ensemble via the asymmetric {Rydberg} blockade},
	volume = {034307},
	url = {http://pra.aps.org/abstract/PRA/v82/i3/e034307},
	doi = {10.1103/PhysRevA.82.034307},
	urldate = {2013-05-02},
	journal = {Physical Review A},
	author = {Wu, HZ and Yang, ZB and Zheng, SB},
	year = {2010},
	pages = {1--4},
}

@article{Chen2007,
	title = {Toffoli gate made from a single resonant interaction with a trapped ion system},
	volume = {561},
	url = {http://link.springer.com/article/10.1140/epjd/e2006-00250-8},
	doi = {10.1140/epjd/e2006-00250-8},
	urldate = {2013-07-18},
	journal = {The European Physical Journal D},
	author = {Chen, CY and Li, SH},
	year = {2007},
	pages = {557--561},
}

@article{Yang2011,
	title = {A proposal for implementing an n-qubit controlled-rotation gate with three-level superconducting qubit systems in cavity {QED}},
	volume = {225702},
	url = {http://iopscience.iop.org/0953-8984/23/22/225702},
	doi = {10.1088/0953-8984/23/22/225702},
	urldate = {2013-07-29},
	journal = {Journal of Physics: Condensed Matter},
	author = {Yang, CP},
	year = {2011},
}

@article{lidar_adiabatic_2009,
	title = {Adiabatic approximation with exponential accuracy for many-body systems and quantum computation},
	volume = {50},
	issn = {00222488},
	url = {http://link.aip.org/link/JMAPAQ/v50/i10/p102106/s1&Agg=doi},
	doi = {10.1063/1.3236685},
	number = {10},
	urldate = {2013-02-14},
	journal = {Journal of Mathematical Physics},
	author = {Lidar, Daniel a. and Rezakhani, Ali T. and Hamma, Alioscia},
	year = {2009},
	pages = {102106},
}

@article{Zhang2005,
	title = {Generation of quantum logic operations from physical {Hamiltonians}},
	volume = {71},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.71.052317},
	doi = {10.1103/PhysRevA.71.052317},
	number = {5},
	urldate = {2013-02-11},
	journal = {Physical Review A},
	author = {Zhang, Jun and Whaley, K.},
	month = may,
	year = {2005},
	pages = {052317},
}

@article{Zhang2003,
	title = {Geometric theory of nonlocal two-qubit operations},
	volume = {67},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.67.042313},
	doi = {10.1103/PhysRevA.67.042313},
	number = {4},
	urldate = {2013-02-11},
	journal = {Physical Review A},
	author = {Zhang, Jun and Vala, Jiri and Sastry, Shankar and Whaley, K.},
	month = apr,
	year = {2003},
	pages = {042313},
}

@article{Duan2005a,
	title = {Efficient quantum computation with probabilistic quantum gates},
	volume = {080503},
	url = {http://prl.aps.org/abstract/PRL/v95/i8/e080503},
	doi = {10.1103/PhysRevLett.95.080503},
	number = {AUGUST},
	urldate = {2013-07-18},
	journal = {Physical review letters},
	author = {Duan, LM and Raussendorf, R},
	year = {2005},
	pages = {1--4},
}

@article{nest_universal_2013,
	title = {Universal {Quantum} {Computation} with {Little} {Entanglement}},
	volume = {060504},
	doi = {10.1103/PhysRevLett.110.060504},
	number = {February},
	author = {Nest, Maarten Van Den},
	year = {2013},
	pages = {1--4},
}

@article{Geller2009,
	title = {Quantum gate design: {A} perspective},
	volume = {246},
	issn = {03701972},
	url = {http://doi.wiley.com/10.1002/pssb.200881556},
	doi = {10.1002/pssb.200881556},
	number = {5},
	urldate = {2013-02-11},
	journal = {Physica Status Solidi (B)},
	author = {Geller, Michael R. and Pritchett, Emily J. and Galiautdinov, Andrei and Martinis, John M.},
	month = may,
	year = {2009},
	pages = {972--974},
}

@article{pienaar_open_2013,
	title = {Open {Timelike} {Curves} {Violate} {Heisenberg}’s {Uncertainty} {Principle}},
	volume = {110},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.110.060501},
	doi = {10.1103/PhysRevLett.110.060501},
	number = {6},
	urldate = {2013-02-11},
	journal = {Physical Review Letters},
	author = {Pienaar, J. L. and Ralph, T. C. and Myers, C. R.},
	month = feb,
	year = {2013},
	pages = {060501},
}

@article{Duan2005,
	title = {Robust quantum gates on neutral atoms with cavity-assisted photon scattering},
	volume = {72},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.72.032333},
	doi = {10.1103/PhysRevA.72.032333},
	number = {3},
	urldate = {2013-02-02},
	journal = {Physical Review A},
	author = {Duan, L.-M. and Wang, B. and Kimble, H. J.},
	month = sep,
	year = {2005},
	pages = {032333},
}

@article{Chen2012,
	title = {Implementation of a three-qubit {Toffoli} gate in a single step},
	volume = {85},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.85.032326},
	doi = {10.1103/PhysRevA.85.032326},
	number = {3},
	urldate = {2013-02-11},
	journal = {Physical Review A},
	author = {Chen, Ai Min and Cho, Sam Young and Kim, Mun Dae},
	month = mar,
	year = {2012},
	pages = {032326},
}

@article{centre-ville_optimizing_2011,
	title = {Optimizing adiabaticity in quantum mechanics},
	number = {0},
	author = {Centre-ville, Succ},
	year = {2011},
	note = {arXiv: 1109.2565v1},
}

@article{aharonov_quantum_2002,
	title = {Quantum {NP}-a survey},
	url = {http://arxiv.org/abs/quant-ph/0210077},
	urldate = {2013-02-07},
	journal = {arXiv preprint quant-ph/0210077},
	author = {Aharonov, Dorit and Naveh, Tomer},
	year = {2002},
	note = {arXiv: quant-ph/0210077v1},
	pages = {1--23},
}

@article{arora_computational_nodate,
	title = {Computational {Complexity} : {A} {Modern} {Approach}},
	author = {Arora, Sanjeev and Barak, Boaz},
}

@article{chiu_fermionic_2013,
	title = {Fermionic measurement-based quantum computation},
	volume = {87},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.87.012305},
	doi = {10.1103/PhysRevA.87.012305},
	number = {1},
	urldate = {2013-02-01},
	journal = {Physical Review A},
	author = {Chiu, Yu-Ju and Chen, Xie and Chuang, Isaac L.},
	month = jan,
	year = {2013},
	pages = {012305},
}

@article{Yung2003,
	title = {An exact effective two-qubit gate in a chain of three spins},
	url = {http://arxiv.org/abs/quant-ph/0312105},
	urldate = {2013-02-11},
	journal = {arXiv preprint quant-ph/0312105},
	author = {Yung, MH and Leung, DW and Bose, Sougato},
	year = {2003},
	note = {arXiv: quant-ph/0312105v1},
	pages = {1--7},
}

@article{Chiara2005,
	title = {Cloning transformations in spin networks without external control},
	url = {http://pra.aps.org/abstract/PRA/v72/i1/e012328},
	urldate = {2013-07-18},
	journal = {Physical Review A},
	author = {Chiara, Gabriele De and Fazio, Rosario and Macchiavello, Chiara},
	year = {2005},
	note = {arXiv: quant-ph/0410211v2},
	pages = {1--13},
}

@article{wang_numerical_2013,
	title = {Numerical method for finding decoherence-free subspaces and its applications},
	volume = {87},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.87.012338},
	doi = {10.1103/PhysRevA.87.012338},
	number = {1},
	urldate = {2013-02-01},
	journal = {Physical Review A},
	author = {Wang, Xiaoting and Byrd, Mark and Jacobs, Kurt},
	month = jan,
	year = {2013},
	pages = {012338},
}

@article{Jones2012,
	title = {Novel constructions for the fault-tolerant {Toffoli} gate},
	url = {http://arxiv.org/abs/1212.5069},
	urldate = {2013-01-31},
	journal = {arXiv preprint arXiv:1212.5069},
	author = {Jones, Cody},
	year = {2012},
	note = {arXiv: 1212.5069v1},
}

@article{Fredkin1982,
	title = {Conservative {Logic}},
	urldate = {2013-01-31},
	journal = {International Journal of Theoretical Physics},
	author = {Fredkin, Edward and Toffoli, Tommaso},
	year = {1982},
}

@article{Yu2013,
	title = {Five {Two}-{Qubit} {Gates} {Are} {Necessary} for {Implementing} {Toffoli} {Gate}},
	url = {http://arxiv.org/abs/1301.3372},
	urldate = {2013-01-31},
	journal = {arXiv preprint arXiv:1301.3372},
	author = {Yu, Nengkun and Duan, Runyao and Ying, Mingsheng},
	year = {2013},
	note = {arXiv: 1301.3372v1},
}

@article{Yu2013a,
	title = {Optimal simulation of three-qubit gates},
	url = {http://arxiv.org/abs/1301.3727},
	urldate = {2013-01-31},
	journal = {arXiv preprint arXiv:1301.3727},
	author = {Yu, Nengkun and Ying, Mingsheng},
	year = {2013},
	note = {arXiv: 1301.3727v1},
	pages = {1--16},
}

@article{Shi2012,
	title = {One-step implementation of the {Fredkin} gate via quantum {Zeno} dynamics},
	url = {http://arxiv.org/abs/1201.1339},
	urldate = {2013-01-31},
	journal = {arXiv preprint arXiv:1201.1339},
	author = {Shi, ZC and Xia, Yan and Song, Jie},
	year = {2012},
	note = {arXiv: 1201.1339v1},
	keywords = {cavity quantum electrodynamics, fredkin gate, quantum zeno effect},
	pages = {1--22},
}

@article{Gu2005,
	title = {On synthesis of 3 × 3 reversible logic functions},
	volume = {82},
	issn = {0020-7160},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0020716042000301815},
	doi = {10.1080/0020716042000301815},
	number = {4},
	urldate = {2013-01-31},
	journal = {International Journal of Computer Mathematics},
	author = {Gu, Ming and Yang, Guowu and Song, Xiaoyu and Sun, Jiaguang},
	month = apr,
	year = {2005},
	pages = {385--390},
}

@article{Fiurasek2006,
	title = {Linear-optics quantum {Toffoli} and {Fredkin} gates},
	volume = {73},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.73.062313},
	doi = {10.1103/PhysRevA.73.062313},
	number = {6},
	urldate = {2013-01-31},
	journal = {Physical Review A},
	author = {Fiurášek, Jaromír},
	month = jun,
	year = {2006},
	pages = {062313},
}

@article{Gong2008,
	title = {Methods for a linear optical quantum {Fredkin} gate},
	volume = {78},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.78.012305},
	doi = {10.1103/PhysRevA.78.012305},
	number = {1},
	urldate = {2013-01-31},
	journal = {Physical Review A},
	author = {Gong, Yan-Xiao and Guo, Guang-Can and Ralph, Timothy},
	month = jul,
	year = {2008},
	pages = {012305},
}

@article{Mohammadi2009,
	title = {On figures of merit in reversible and quantum logic designs},
	volume = {8},
	issn = {1570-0755},
	url = {http://www.springerlink.com/index/10.1007/s11128-009-0106-0},
	doi = {10.1007/s11128-009-0106-0},
	number = {4},
	urldate = {2013-01-31},
	journal = {Quantum Information Processing},
	author = {Mohammadi, Majid and Eshghi, Mohammad},
	month = feb,
	year = {2009},
	keywords = {constant inputs, delay, garbage outputs, quantum circuit, quantum cost, reversible logic, weighted number of gates},
	pages = {297--318},
}

@article{Fiurasek2008,
	title = {Linear optical {Fredkin} gate based on partial-{SWAP} gate},
	volume = {78},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.78.032317},
	doi = {10.1103/PhysRevA.78.032317},
	number = {3},
	urldate = {2013-01-31},
	journal = {Physical Review A},
	author = {Fiurášek, Jaromír},
	month = sep,
	year = {2008},
	pages = {032317},
}

@article{Chau1995,
	title = {Simple {Realization} of the {Fredkin} {Gate} using a {Series} of {Two}-{Body} {Operators}},
	volume = {75},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.75.748},
	number = {4},
	urldate = {2013-01-31},
	journal = {Physical review letters},
	author = {Chau, HF and Wilczek, F},
	year = {1995},
	pages = {748--750},
}

@article{Smolin1996,
	title = {Five two-bit quantum gates are sufficient to implement the quantum {Fredkin} gate.},
	volume = {53},
	issn = {1050-2947},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9913201},
	number = {4},
	journal = {Physical review. A},
	author = {Smolin, Ja and DiVincenzo, Dp},
	month = may,
	year = {1996},
	pmid = {9913201},
	pages = {2855--2856},
}

@article{Smolin1996a,
	title = {Five {Two}-bit {Quantum} {Gates} are {Sufficient} to {Implement} the {Quantum} {Fredkin} {Gate}},
	urldate = {2013-05-02},
	journal = {Physical Review A},
	author = {Smolin, JA and DiVincenzo, DP},
	year = {1996},
}

@article{Wei2010,
	title = {Synthesis of some three-qubit gates and their implementation in a three spins system coupled with {Ising} interaction},
	volume = {53},
	issn = {1674-7348},
	url = {http://www.springerlink.com/index/10.1007/s11433-010-0165-3},
	doi = {10.1007/s11433-010-0165-3},
	number = {4},
	urldate = {2013-01-31},
	journal = {Science China Physics, Mechanics and Astronomy},
	author = {Wei, HaiRui and Di, YaoMin and Wang, Yan},
	month = apr,
	year = {2010},
	pages = {664--671},
}

@article{Fei2002,
	title = {Realization of the {Fredkin} gate by three transition pulses in a nuclear magnetic resonance quantum information processor},
	volume = {1048},
	url = {http://iopscience.iop.org/0256-307X/19/8/306},
	urldate = {2013-01-31},
	journal = {Chinese physics …},
	author = {Fei, X and Jiang-Feng, D and Ming-Jun, S},
	year = {2002},
}

@article{gisin_bell_2008,
	title = {Bell inequalities: many questions, a few answers},
	author = {Gisin, Nicolas},
	year = {2008},
	note = {arXiv: quant-ph/0702021v2},
	pages = {1--8},
}

@article{hen_how_2013,
	title = {How {Fast} {Can} {Quantum} {Annealers} {Count}?},
	author = {Hen, Itay},
	year = {2013},
	note = {arXiv: 1301.4956v1},
	keywords = {adiabatic quantum computing, grover, ing, phase detection, quantum annealers, quantum count-, s search algorithm},
	pages = {1--6},
}

@book{nielsen_quantum_2010,
	title = {Quantum computation and quantum information},
	url = {http://books.google.com/books?hl=en&lr=&id=-s4DEy7o-a0C&oi=fnd&pg=PR17&dq=Quantum+Computation+and+Quantum+Information&ots=NF8EbkqzYq&sig=dSLrt_DGOdARghWbHTEc2ujnRns},
	urldate = {2013-05-30},
	author = {Nielsen, MA and Chuang, IL},
	year = {2010},
}

@article{gisin_non-realism_2009,
	title = {Non-realism : deep thought or a soft option ?},
	volume = {1},
	number = {1},
	author = {Gisin, Nicolas},
	year = {2009},
	note = {arXiv: 0901.4255v2},
	pages = {1--5},
}

@article{gisin_sundays_1983,
	title = {Sundays in a {Quantum} {Engineer}’s {Life}},
	author = {Gisin, Nicolas},
	year = {1983},
	note = {arXiv: quant-ph/0104140v1},
}

@article{hoyer_quantum_2005,
	title = {Quantum fan-out is powerful},
	volume = {1},
	url = {http://dare.uva.nl/record/191186},
	doi = {10.4086/toc.2005.v001a005},
	urldate = {2013-06-12},
	journal = {Theory of computing},
	author = {Hoyer, P and Spalek, Robert},
	year = {2005},
	keywords = {and phrases, constant depth circuits, fan-out, quantum circuits, quantum computing, quantum fourier transform, threshold circuits},
	pages = {81--103},
}

@article{liu_nuclear_2009,
	title = {Nuclear magnetic resonance implementation of universal quantum gate with constant {Hamiltonian} evolution},
	volume = {94},
	issn = {00036951},
	url = {http://link.aip.org/link/APPLAB/v94/i6/p064103/s1&Agg=doi},
	doi = {10.1063/1.3081022},
	number = {6},
	urldate = {2013-01-16},
	journal = {Applied Physics Letters},
	author = {Liu, Wenzhang and Zhang, Jingfu and Cao, Ye and Huo, Wen Yi and Hao, Liang and Long, Gui Lu and Deng, Zhiwei},
	year = {2009},
	pages = {064103},
}

@article{hu_always_2007,
	title = {Always on non-nearest-neighbour coupling in scalable quantum computing},
	volume = {9},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/9/i=2/a=027?key=crossref.cba1127ed368b590caa19c1c403d0fa4},
	doi = {10.1088/1367-2630/9/2/027},
	number = {2},
	urldate = {2013-01-16},
	journal = {New Journal of Physics},
	author = {Hu, Yong and Zhou, Zheng-Wei and Guo, Guang-Can},
	month = feb,
	year = {2007},
	pages = {27--27},
}

@article{bronken_matlab_nodate,
	title = {{MATLAB} commands in numerical {Python} ( {NumPy} )},
	volume = {0},
	author = {Bronken, Vidar},
	pages = {1--17},
}

@article{kay_computational_2008,
	title = {Computational power of symmetric {Hamiltonians}},
	volume = {78},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.78.012346},
	doi = {10.1103/PhysRevA.78.012346},
	number = {1},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Kay, Alastair},
	month = jul,
	year = {2008},
	pages = {012346},
}

@article{zhou_unified_2006,
	title = {Unified approach for universal quantum gates in a coupled superconducting two-qubit system with fixed always-on coupling},
	volume = {73},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.73.104521},
	doi = {10.1103/PhysRevB.73.104521},
	number = {10},
	urldate = {2013-01-16},
	journal = {Physical Review B},
	author = {Zhou, Zhongyuan and Chu, Shih-I and Han, Siyuan},
	month = mar,
	year = {2006},
	pages = {104521},
}

@article{benjamin_multi-qubit_2004,
	title = {Multi-qubit gates in arrays coupled by  always-on  interactions},
	volume = {6},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/6/i=1/a=061?key=crossref.da52b88cf653c473426ce7fbe93269dd},
	doi = {10.1088/1367-2630/6/1/061},
	urldate = {2012-12-29},
	journal = {New Journal of Physics},
	author = {Benjamin, Simon C},
	month = jun,
	year = {2004},
	pages = {61--61},
}

@article{benjamin_optical_2004,
	title = {Optical quantum computation with perpetually coupled spins},
	volume = {70},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.70.060305},
	doi = {10.1103/PhysRevA.70.060305},
	number = {6},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Benjamin, Simon and Lovett, Brendon and Reina, John},
	month = dec,
	year = {2004},
	pages = {060305},
}

@article{benjamin_quantum_nodate,
	title = {Quantum {Computing} in {Arrays} {Coupled} by ‘ {Always} {On} ’ {Interactions}},
	author = {Benjamin, S C and Bose, S},
	note = {arXiv: quant-ph/0401071v1},
}

@article{benjamin_quantum_2004,
	title = {Quantum computing in arrays coupled by “always-on” interactions},
	volume = {70},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.70.032314},
	doi = {10.1103/PhysRevA.70.032314},
	number = {3},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Benjamin, S. and Bose, S.},
	month = sep,
	year = {2004},
	pages = {032314},
}

@article{leuenberger_quantum_2001,
	title = {Quantum computing in molecular magnets.},
	volume = {410},
	issn = {0028-0836},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/11298441},
	doi = {10.1038/35071024},
	abstract = {Shor and Grover demonstrated that a quantum computer can outperform any classical computer in factoring numbers and in searching a database by exploiting the parallelism of quantum mechanics. Whereas Shor's algorithm requires both superposition and entanglement of a many-particle system, the superposition of single-particle quantum states is sufficient for Grover's algorithm. Recently, the latter has been successfully implemented using Rydberg atoms. Here we propose an implementation of Grover's algorithm that uses molecular magnets, which are solid-state systems with a large spin; their spin eigenstates make them natural candidates for single-particle systems. We show theoretically that molecular magnets can be used to build dense and efficient memory devices based on the Grover algorithm. In particular, one single crystal can serve as a storage unit of a dynamic random access memory device. Fast electron spin resonance pulses can be used to decode and read out stored numbers of up to 105, with access times as short as 10-10 seconds. We show that our proposal should be feasible using the molecular magnets Fe8 and Mn12.},
	number = {6830},
	journal = {Nature},
	author = {Leuenberger, M N and Loss, D},
	month = apr,
	year = {2001},
	pmid = {11298441},
	pages = {789--93},
}

@article{dey_decoherence_nodate,
	title = {Decoherence in an infinite range {Heisenberg} model},
	author = {Dey, A and Lone, M Q and Yarlagadda, S},
	note = {arXiv: 1205.4093v3},
}

@article{coleman_time_nodate,
	title = {Time crystals {Older} but less wise},
	number = {1},
	author = {Coleman, Piers and Newton, Isaac},
	pages = {6--7},
}

@article{operator_letters_1994,
	title = {Letters 4},
	volume = {73},
	number = {1},
	author = {Operator, Discrete Unitary},
	year = {1994},
}

@article{mcclean_h_nodate,
	title = {H clock {\textbar} i},
	author = {Mcclean, Jarrod R and Parkhill, John A},
	note = {arXiv: 1301.2326v1},
	pages = {1--11},
}

@article{kofler_quantum_1814,
	title = {Quantum {Information} and {Randomness}},
	author = {Kofler, Johannes and Zeilinger, Anton},
	year = {1814},
	pages = {1--8},
}

@article{sun_models_2013,
	title = {On models of nonlinear evolution paths in adiabatic quantum algorithms},
	volume = {59},
	number = {1},
	author = {SUN, Jie and Song-feng, L U and Braunstein, S.},
	year = {2013},
	note = {arXiv: 1301.1115v1},
	keywords = {adiabatic evolution, nonlinear evolution paths, quantum computing},
}

@article{sun_speedup_2012,
	title = {Speedup in adiabatic evolution based quantum algorithms},
	volume = {55},
	issn = {1674-7348},
	url = {http://www.springerlink.com/index/10.1007/s11433-012-4854-y},
	doi = {10.1007/s11433-012-4854-y},
	number = {9},
	urldate = {2013-01-09},
	journal = {Science China Physics, Mechanics and Astronomy},
	author = {Sun, Jie and Lu, SongFeng and Liu, Fang},
	month = jul,
	year = {2012},
	pages = {1630--1634},
}

@article{perez_hilbert-space_2009,
	title = {Hilbert-space average method and adiabatic quantum search},
	volume = {79},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.79.012314},
	doi = {10.1103/PhysRevA.79.012314},
	number = {1},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Pérez, a.},
	month = jan,
	year = {2009},
	pages = {012314},
}

@article{mackenzie_optimizing_2012,
	title = {Optimizing adiabaticity in quantum mechanics},
	volume = {191},
	doi = {10.1139/P2012-005},
	number = {February},
	author = {Mackenzie, R and Pineault, M},
	year = {2012},
	pages = {187--191},
}

@article{das_energy_2003,
	title = {Energy and efficiency of adiabatic quantum search algorithms},
	volume = {2839},
	urldate = {2013-01-09},
	journal = {Journal of Physics A: …},
	author = {Das, Saurya and Kobes, Randy and Kunstatter, Gabor},
	year = {2003},
}

@article{nori_feedback-controlled_2013,
	title = {Feedback-controlled adiabatic quantum computation},
	author = {Nori, Franco},
	year = {2013},
	note = {arXiv: 1301.0459v1},
}

@article{burkard_coupled_1999,
	title = {Coupled quantum dots as quantum gates},
	volume = {59},
	number = {3},
	author = {Burkard, Guido and Loss, Daniel and Divincenzo, David P},
	year = {1999},
	pages = {2070--2078},
}

@article{hu_charge-fluctuation-induced_2006,
	title = {Charge-{Fluctuation}-{Induced} {Dephasing} of {Exchange}-{Coupled} {Spin} {Qubits}},
	volume = {96},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.96.100501},
	doi = {10.1103/PhysRevLett.96.100501},
	number = {10},
	urldate = {2012-11-16},
	journal = {Physical Review Letters},
	author = {Hu, Xuedong and Das Sarma, S.},
	month = mar,
	year = {2006},
	pages = {100501},
}

@article{haque_self-similar_2010,
	title = {Self-similar spectral structures and edge-locking hierarchy in open-boundary spin chains},
	volume = {82},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.82.012108},
	doi = {10.1103/PhysRevA.82.012108},
	number = {1},
	urldate = {2013-01-08},
	journal = {Physical Review A},
	author = {Haque, Masudul},
	month = jul,
	year = {2010},
	pages = {012108},
}

@article{kestner_noise-resistant_2013,
	title = {Noise-resistant control for a spin qubit array},
	author = {Kestner, J P and Wang, Xin and Bishop, Lev S and Barnes, Edwin and Sarma, S Das},
	year = {2013},
	note = {arXiv: 1301.0826v1},
	pages = {1--9},
}

@article{andrecut_unstructured_2004,
	title = {Unstructured {Adiabatic} {Quantum} {Search}},
	volume = {43},
	issn = {0020-7748},
	url = {http://link.springer.com/10.1023/B:IJTP.0000048589.20608.aa},
	doi = {10.1023/B:IJTP.0000048589.20608.aa},
	number = {4},
	journal = {International Journal of Theoretical Physics},
	author = {Andrecut, M. and Ali, M. K.},
	month = apr,
	year = {2004},
	keywords = {a computational procedure is, a newer subfield emerged, adiabatic quantum computation model, adiabatic quantum search, by new works addressing, described by, developing quantum algorithms based, farhi et al, in the, on adiabatic evolution, quantum computation, recently, the idea of},
	pages = {925--931},
}

@article{chaitin_gthiel_1982,
	title = {Gthiel ' s {Theorem} and {Information}},
	number = {12},
	author = {Chaitin, Gregory J},
	year = {1982},
	pages = {941--954},
}

@article{fern_gapless_nodate,
	title = {Gapless {Hamiltonians} for the toric code using the {PEPS} formalism},
	author = {Fern, Carlos and Schuch, Norbert and Wolf, Michael M and Cirac, J Ignacio and David, P},
	note = {arXiv: 1111.5817v2},
	pages = {1--8},
}

@article{norgaard_atm_2006,
	title = {{ATM} with {Random} {Access}},
	volume = {1},
	author = {Nørgaard, Hald},
	year = {2006},
	pages = {1--6},
}

@article{Bacon2008,
	title = {Stability of quantum concatenated-code {Hamiltonians}},
	volume = {78},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.78.042324},
	doi = {10.1103/PhysRevA.78.042324},
	number = {4},
	urldate = {2012-12-11},
	journal = {Physical Review A},
	author = {Bacon, Dave},
	month = oct,
	year = {2008},
	pages = {042324},
}

@article{Facchi2011,
	title = {Greenberger-{Horne}-{Zeilinger} {States} and {Few}-{Body} {Hamiltonians}},
	volume = {107},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.107.260502},
	doi = {10.1103/PhysRevLett.107.260502},
	number = {26},
	urldate = {2012-11-02},
	journal = {Physical Review Letters},
	author = {Facchi, Paolo and Florio, Giuseppe and Pascazio, Saverio and Pepe, Francesco},
	month = dec,
	year = {2011},
	pages = {1--5},
}

@article{raussendorf_measurement-based_2003,
	title = {Measurement-based quantum computation on cluster states},
	url = {http://pra.aps.org/abstract/PRA/v68/i2/e022312},
	urldate = {2012-12-28},
	journal = {Physical Review A},
	author = {Raussendorf, Robert and Browne, DE and Briegel, HJ},
	year = {2003},
	note = {arXiv: quant-ph/0301052v2},
}

@article{Zanardi1999,
	title = {Holonomic quantum computation},
	volume = {264},
	urldate = {2012-12-14},
	journal = {Physics Letters A},
	author = {Zanardi, Paolo and Rasetti, Mario},
	year = {1999},
	pages = {94--99},
}

@article{Schollwock2011,
	title = {The density-matrix renormalization group in the age of matrix product states},
	volume = {326},
	issn = {00034916},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0003491610001752},
	doi = {10.1016/j.aop.2010.09.012},
	number = {1},
	urldate = {2012-11-02},
	journal = {Annals of Physics},
	author = {Schollwöck, Ulrich},
	month = jan,
	year = {2011},
	pages = {96--192},
}

@article{Fujii2012,
	title = {Computational power and correlation in a quantum computational tensor network},
	volume = {85},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.85.032338},
	doi = {10.1103/PhysRevA.85.032338},
	number = {3},
	urldate = {2012-12-11},
	journal = {Physical Review A},
	author = {Fujii, Keisuke and Morimae, Tomoyuki},
	month = mar,
	year = {2012},
	pages = {032338},
}

@article{Klagges2012,
	title = {Constraints on {Measurement}-{Based} {Quantum} {Computation} in {Effective} {Cluster} {States}},
	volume = {108},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.108.230508},
	doi = {10.1103/PhysRevLett.108.230508},
	number = {23},
	urldate = {2012-11-16},
	journal = {Physical Review Letters},
	author = {Klagges, Daniel and Schmidt, Kai},
	month = jun,
	year = {2012},
	pages = {230508},
}

@article{VandenNest2008,
	title = {Graph states as ground states of many-body spin-1∕2 {Hamiltonians}},
	volume = {77},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.77.012301},
	doi = {10.1103/PhysRevA.77.012301},
	number = {1},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Van den Nest, M. and Luttmer, K. and Dür, W. and Briegel, H.},
	month = jan,
	year = {2008},
	pages = {012301},
}

@article{Chen2011,
	title = {No-go theorem for one-way quantum computing on naturally occurring two-level systems},
	volume = {83},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.83.050301},
	doi = {10.1103/PhysRevA.83.050301},
	number = {5},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Chen, Jianxin and Chen, Xie and Duan, Runyao and Ji, Zhengfeng and Zeng, Bei},
	month = may,
	year = {2011},
	pages = {050301},
}

@article{Nielsen2006,
	title = {Cluster-state quantum computation},
	url = {http://www.sciencedirect.com/science/article/pii/S0034487706800145},
	urldate = {2012-12-28},
	journal = {Reports on Mathematical Physics},
	author = {Nielsen, MA},
	year = {2006},
	note = {arXiv: quant-ph/0504097v2},
	keywords = {cluster states, one-way quantum computer, quantum computation},
	pages = {1--15},
}

@article{Siu2005,
	title = {From quantum circuits to adiabatic algorithms},
	volume = {71},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.71.062314},
	doi = {10.1103/PhysRevA.71.062314},
	number = {6},
	urldate = {2012-12-11},
	journal = {Physical Review A},
	author = {Siu, M.},
	month = jun,
	year = {2005},
	pages = {062314},
}

@article{Mari2012,
	title = {Positive {Wigner} {Functions} {Render} {Classical} {Simulation} of {Quantum} {Computation} {Efficient}},
	volume = {109},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.109.230503},
	doi = {10.1103/PhysRevLett.109.230503},
	number = {23},
	urldate = {2012-12-07},
	journal = {Physical Review Letters},
	author = {Mari, a. and Eisert, J.},
	month = dec,
	year = {2012},
	pages = {230503},
}

@book{knuth_mathematical_1996,
	title = {Mathematical writing},
	url = {http://books.google.com/books?hl=en&lr=&id=dDOehHMbUMcC&oi=fnd&pg=PA1&dq=Mathematical+writing&ots=8afy58e-fu&sig=dM1tU9mlC9u_Wjkf5E_imO4pkiE},
	urldate = {2012-12-28},
	author = {Knuth, DE and Larrabee, Tracy and Roberts, PM},
	year = {1996},
}

@article{Luttmer2008,
	title = {Graph states as ground states of many-body spin-1/2 {Hamiltonians}},
	volume = {77},
	url = {http://pra.aps.org/abstract/PRA/v77/i1/e012301},
	number = {1},
	urldate = {2012-12-11},
	journal = {Physical Review A},
	author = {Nest, M Van den and Luttmer, K and Dür, W and Briegel, HJ},
	year = {2008},
	note = {arXiv: quant-ph/0612186v1},
	pages = {1--10},
}

@article{Rohrlich,
	title = {Berry's {Phase}},
	author = {Rohrlich, D},
	note = {arXiv: 0708.3749v1},
	pages = {1--6},
}

@article{rezakhani_intrinsic_2010,
	title = {Intrinsic geometry of quantum adiabatic evolution and quantum phase transitions},
	volume = {82},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.82.012321},
	doi = {10.1103/PhysRevA.82.012321},
	number = {1},
	urldate = {2012-11-10},
	journal = {Physical Review A},
	author = {Rezakhani, a. and Abasto, D. and Lidar, D. and Zanardi, P.},
	month = jul,
	year = {2010},
	pages = {012321},
}

@article{Aharonov2008,
	title = {Adiabatic {Quantum} {Computation} {Is} {Equivalent} to {Standard} {Quantum} {Computation}},
	volume = {50},
	number = {4},
	urldate = {2012-12-06},
	journal = {SIAM review},
	author = {Aharonov, Dorit and Dam, W Van and Kempe, Julia and Landau, Z},
	year = {2008},
	pages = {755--787},
}

@article{Karbach2005,
	title = {Spin chains as perfect quantum state mirrors},
	volume = {72},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.72.030301},
	doi = {10.1103/PhysRevA.72.030301},
	number = {3},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Karbach, Peter and Stolze, Joachim},
	month = sep,
	year = {2005},
	pages = {030301},
}

@article{childs_quantum_2008,
	title = {The quantum adiabatic theorem},
	volume = {1},
	number = {4},
	author = {Childs, Andrew},
	year = {2008},
	pages = {1--5},
}

@article{Haselgrove2004,
	title = {Entanglement, correlations, and the energy gap in many-body quantum systems},
	volume = {69},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.69.032303},
	doi = {10.1103/PhysRevA.69.032303},
	number = {3},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Haselgrove, Henry and Nielsen, Michael and Osborne, Tobias},
	month = mar,
	year = {2004},
	pages = {032303},
}

@article{lidar_adiabatic_2009,
	title = {Adiabatic approximation with exponential accuracy for many-body systems and quantum computation},
	volume = {50},
	issn = {00222488},
	url = {http://link.aip.org/link/JMAPAQ/v50/i10/p102106/s1&Agg=doi},
	doi = {10.1063/1.3236685},
	number = {10},
	urldate = {2012-12-06},
	journal = {Journal of Mathematical Physics},
	author = {Lidar, Daniel a. and Rezakhani, Ali T. and Hamma, Alioscia},
	year = {2009},
	pages = {102106},
}

@article{wharton_universe_nodate,
	title = {The {Universe} is not a {Computer}},
	author = {Wharton, Ken},
	note = {arXiv: 1211.7081v1},
}

@article{chancellor_using_2012,
	title = {Using the {J1}-{J2} {Quantum} {Spin} {Chain} as an},
	author = {Chancellor, Nick and Haas, Stephan},
	year = {2012},
	note = {arXiv: 1204.1382v5},
}

@article{albrecht_origin_nodate,
	title = {Origin of probabilities and their application to the multiverse},
	author = {Albrecht, Andreas and Phillips, Daniel},
	note = {arXiv: 1212.0953v1},
	pages = {1--5},
}

@article{Das2008,
	title = {Colloquium: {Quantum} annealing and analog quantum computation},
	volume = {80},
	issn = {0034-6861},
	url = {http://link.aps.org/doi/10.1103/RevModPhys.80.1061},
	doi = {10.1103/RevModPhys.80.1061},
	number = {3},
	urldate = {2012-11-08},
	journal = {Reviews of Modern Physics},
	author = {Das, Arnab and Chakrabarti, Bikas},
	month = sep,
	year = {2008},
	pages = {1061--1081},
}

@article{Carollo,
	title = {Holonomic quantum computation},
	url = {http://www.sciencedirect.com/science/article/pii/S0375960199008038},
	urldate = {2012-12-11},
	author = {Carollo, Angelo C.M. and Vedral, Vlatko},
	note = {arXiv: quant-ph/0504205v1},
}

@article{Amin2009,
	title = {Decoherence in adiabatic quantum computation},
	url = {http://pra.aps.org/abstract/PRA/v79/i2/e022107},
	doi = {10.1103/PhysRevA.79.022107},
	number = {August 2007},
	urldate = {2013-02-07},
	journal = {Physical Review A},
	author = {Amin, MHS and Averin, DV and Nesteroff, JA},
	year = {2009},
	pages = {2007--2010},
}

@article{Amin2008,
	title = {Effect of {Local} {Minima} on {Adiabatic} {Quantum} {Optimization}},
	volume = {100},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.100.130503},
	doi = {10.1103/PhysRevLett.100.130503},
	number = {13},
	urldate = {2012-12-03},
	journal = {Physical Review Letters},
	author = {Amin, M.},
	month = apr,
	year = {2008},
	pages = {130503},
}

@article{Ilievski2010,
	title = {Adiabatic {Quantum} {Computing}},
	url = {http://meeting.aps.org/Meeting/4CF12/Event/180645},
	urldate = {2012-12-03},
	author = {Ilievski, Enej},
	year = {2010},
}

@article{Roland2002,
	title = {Quantum search by local adiabatic evolution},
	volume = {65},
	issn = {1050-2947},
	doi = {10.1103/PhysRevA.65.042308},
	number = {4},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Roland, Jérémie and Cerf, Nicolas},
	month = mar,
	year = {2002},
	pages = {042308},
}

@article{Trifunovic2012,
	title = {Long-{Distance} {Spin}-{Spin} {Coupling} via {Floating} {Gates}},
	volume = {2},
	issn = {2160-3308},
	url = {http://link.aps.org/doi/10.1103/PhysRevX.2.011006},
	doi = {10.1103/PhysRevX.2.011006},
	number = {1},
	urldate = {2012-10-30},
	journal = {Physical Review X},
	author = {Trifunovic, Luka and Dial, Oliver and Trif, Mircea and Wootton, James and Abebe, Rediet and Yacoby, Amir and Loss, Daniel},
	month = jan,
	year = {2012},
	keywords = {nanophysics, quantum information, semiconductor physics},
	pages = {1--13},
}

@article{Aharonov2007,
	title = {Adiabatic {Quantum} {State} {Generation}},
	volume = {37},
	issn = {0097-5397},
	url = {http://epubs.siam.org/doi/abs/10.1137/060648829},
	doi = {10.1137/060648829},
	number = {1},
	journal = {SIAM Journal on Computing},
	author = {Aharonov, Dorit and Ta‐Shma, Amnon},
	month = jan,
	year = {2007},
	keywords = {adiabatic theorem, hamiltonians, markov chains, quantum algorithm, quantum computation, quantum sampling, spectral, state generation, statistical zero knowledge, zeno effect},
	pages = {47--82},
}

@article{Kwapinski2012,
	title = {Quantum wire as a charge-qubit detector},
	volume = {86},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.86.052338},
	doi = {10.1103/PhysRevA.86.052338},
	number = {5},
	urldate = {2012-12-03},
	journal = {Physical Review A},
	author = {Kwapiński, Tomasz and Taranko, Ryszard},
	month = nov,
	year = {2012},
	pages = {052338},
}

@article{Beaudrap,
	title = {Phase map decompositions for unitaries},
	url = {http://arxiv.org/abs/quant-ph/0603266},
	urldate = {2013-02-07},
	journal = {arXiv preprint quant-ph/0603266},
	author = {Beaudrap, Niel De and Danos, Vincent and Kashefi, Elham},
	year = {2006},
	note = {arXiv: quant-ph/0603266v1},
}

@article{Kult2006,
	title = {Noncyclic geometric changes of quantum states},
	url = {http://pra.aps.org/abstract/PRA/v74/i2/e022106},
	doi = {10.1103/PhysRevA.74.022106},
	urldate = {2012-12-04},
	journal = {Physical Review A},
	author = {Kult, David and Åberg, J and Sjöqvist, E},
	year = {2006},
	pages = {1--5},
}

@article{Farhi2001,
	title = {A quantum adiabatic evolution algorithm applied to random instances of an {NP}-complete problem.},
	volume = {292},
	issn = {0036-8075},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/11313487},
	doi = {10.1126/science.1057726},
	abstract = {A quantum system will stay near its instantaneous ground state if the Hamiltonian that governs its evolution varies slowly enough. This quantum adiabatic behavior is the basis of a new class of algorithms for quantum computing. We tested one such algorithm by applying it to randomly generated hard instances of an NP-complete problem. For the small examples that we could simulate, the quantum adiabatic algorithm worked well, providing evidence that quantum computers (if large ones can be built) may be able to outperform ordinary computers on hard sets of instances of NP-complete problems.},
	number = {5516},
	urldate = {2012-11-22},
	journal = {Science (New York, N.Y.)},
	author = {Farhi, E and Goldstone, J and Gutmann, S and Lapan, J and Lundgren, a and Preda, D},
	month = apr,
	year = {2001},
	pmid = {11313487},
	pages = {472--5},
}

@article{Lewis2012,
	title = {Distinct {Quantum} {States} {Can} {Be} {Compatible} with a {Single} {State} of {Reality}},
	volume = {109},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.109.150404},
	doi = {10.1103/PhysRevLett.109.150404},
	number = {15},
	urldate = {2012-10-26},
	journal = {Physical Review Letters},
	author = {Lewis, Peter and Jennings, David and Barrett, Jonathan and Rudolph, Terry},
	month = oct,
	year = {2012},
	pages = {1--5},
}

@article{Gattobigio2012,
	title = {Optically {Guided} {Beam} {Splitter} for {Propagating} {Matter} {Waves}},
	volume = {109},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.109.030403},
	doi = {10.1103/PhysRevLett.109.030403},
	number = {3},
	urldate = {2012-11-05},
	journal = {Physical Review Letters},
	author = {Gattobigio, G. and Couvert, a. and Reinaudi, G. and Georgeot, B. and Guéry-Odelin, D.},
	month = jul,
	year = {2012},
	pages = {1--5},
}

@article{lidar_decoherence-free_2003,
	title = {Decoherence-free subspaces and subsystems},
	urldate = {2013-02-05},
	journal = {Irreversible Quantum Dynamics},
	author = {Lidar, D and Whaley, K Birgitta},
	year = {2003},
	pages = {83--120},
}

@article{Zyczkowski2005,
	title = {Average fidelity between random quantum states},
	volume = {71},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.71.032313},
	doi = {10.1103/PhysRevA.71.032313},
	number = {3},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Życzkowski, Karol and Sommers, Hans-Jürgen},
	month = mar,
	year = {2005},
	pages = {032313},
}

@article{DeBeaudrap2008,
	title = {Finding flows in the one-way measurement model},
	volume = {77},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.77.022328},
	doi = {10.1103/PhysRevA.77.022328},
	number = {2},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {de Beaudrap, Niel},
	month = feb,
	year = {2008},
	pages = {022328},
}

@article{koppens_three-spin_2012,
	title = {Three-spin juggling},
	volume = {8},
	number = {January},
	author = {Koppens, Frank},
	year = {2012},
	pages = {5--6},
}

@article{Danos2006,
	title = {Determinism in the one-way model},
	volume = {74},
	issn = {1050-2947},
	doi = {10.1103/PhysRevA.74.052310},
	number = {5},
	urldate = {2012-11-16},
	journal = {Physical Review A},
	author = {Danos, Vincent and Kashefi, Elham},
	month = nov,
	year = {2006},
	pages = {052310},
}

@article{Browne2007,
	title = {Generalized flow and determinism in measurement-based quantum computation},
	volume = {250},
	url = {http://iopscience.iop.org/1367-2630/9/8/250},
	doi = {10.1088/1367-2630/9/8/250},
	number = {9},
	urldate = {2013-02-21},
	journal = {New Journal of …},
	author = {Browne, DE and Kashefi, Elham and Mhalla, Mehdi and Perdrix, Simon},
	year = {2007},
}

@article{Anders2008,
	title = {How {Much} of {One}-{Way} {Computation} {Is} {Just} {Thermodynamics}?},
	volume = {38},
	issn = {0015-9018},
	number = {6},
	urldate = {2012-11-26},
	journal = {Foundations of Physics},
	author = {Anders, Janet and Hajdušek, Michal and Markham, Damian and Vedral, Vlatko},
	month = mar,
	year = {2008},
	keywords = {3 science drive 2, anders, centre for quantum information, j, national university of singapore, one-way quantum computation, phase transitions, quantum mechanics, technologies, thermodynamics, v, vedral},
	pages = {506--522},
}

@article{Gottesman2008,
	title = {The {Heisenberg} representation of quantum computers},
	volume = {1},
	url = {http://arxiv.org/abs/quant-ph/9807006},
	urldate = {2013-02-07},
	journal = {arXiv preprint quant-ph/9807006},
	author = {Gottesman, Daniel},
	year = {1998},
	note = {arXiv: quant-ph/9807006v1},
	pages = {1--20},
}

@article{Jacobs2012,
	title = {Quantum measurement and the first law of thermodynamics: {The} energy cost of measurement is the work value of the acquired information},
	volume = {86},
	issn = {1539-3755},
	url = {http://link.aps.org/doi/10.1103/PhysRevE.86.040106},
	doi = {10.1103/PhysRevE.86.040106},
	number = {4},
	urldate = {2012-11-08},
	journal = {Physical Review E},
	author = {Jacobs, Kurt},
	month = oct,
	year = {2012},
	pages = {040106},
}

@article{Bacon2010,
	title = {Adiabatic cluster-state quantum computing},
	volume = {82},
	doi = {10.1103/PhysRevA.82.030303},
	number = {3},
	urldate = {2012-11-21},
	journal = {Phys. Rev. A},
	author = {Bacon, Dave and Flammia, Steven T},
	year = {2010},
	note = {arXiv: 0912.2098v2
Publisher: American Physical Society},
	pages = {30303},
}

@article{Aharonov2004,
	title = {Adiabatic quantum computation is equivalent to standard quantum computation},
	issn = {0272-5428},
	urldate = {2012-11-21},
	journal = {SIAM review},
	author = {Aharonov, D and Dam, W Van and Kempe, J and Landau, Z},
	year = {2008},
	keywords = {adiabatic quantum computation, adiabatic simulatio},
	pages = {42--51},
}

@article{Doherty2009,
	title = {Identifying {Phases} of {Quantum} {Many}-{Body} {Systems} {That} {Are} {Universal} for {Quantum} {Computation}},
	volume = {103},
	issn = {0031-9007},
	number = {2},
	urldate = {2012-10-04},
	journal = {Phys. Rev. Lett.},
	author = {Doherty, Andrew C and Bartlett, Stephen D},
	month = jul,
	year = {2009},
	note = {Publisher: American Physical Society},
	pages = {20506},
}

@article{Jordan2008,
	title = {Perturbative gadgets at arbitrary orders},
	volume = {77},
	issn = {1050-2947},
	doi = {10.1103/PhysRevA.77.062329},
	number = {6},
	urldate = {2012-11-16},
	journal = {Phys. Rev. A},
	author = {Jordan, Stephen P and Farhi, Edward},
	month = jun,
	year = {2008},
	note = {Publisher: American Physical Society},
	pages = {62329},
}

@article{Browne2011,
	title = {Computational depth complexity of measurement-based quantum computation},
	volume = {6519},
	urldate = {2012-11-21},
	journal = {Theory of Quantum Computation, Communication, and Cryptography},
	author = {Browne, Dan and Kashefi, Elham and Perdrix, Simon},
	editor = {van Dam, Wim and Kendon, Vivien and Severini, Simone},
	year = {2011},
	note = {Publisher: Springer Berlin / Heidelberg
ISBN: 978-3-642-18072-9},
	pages = {35--46},
}

@article{Farhi,
	title = {Quantum computation by adiabatic evolution},
	url = {http://arxiv.org/abs/quant-ph/0001106},
	urldate = {2012-12-03},
	journal = {arXiv:quant-ph/0001106},
	author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, S and Sipser, M},
	year = {2000},
	note = {arXiv: quant-ph/0001106v1},
}

@article{Anders2009,
	title = {Computational {Power} of {Correlations}},
	volume = {102},
	issn = {0031-9007},
	number = {5},
	urldate = {2012-11-21},
	journal = {Physical Review Letters},
	author = {Anders, Janet and Browne, Dan},
	month = feb,
	year = {2009},
	pages = {050502},
}

@article{Jansen2007,
	title = {Bounds for the adiabatic approximation with applications to quantum computation},
	volume = {48},
	doi = {10.1063/1.2798382},
	number = {10},
	journal = {Journal of Mathematical Physics},
	author = {Jansen, Sabine and Ruskai, Mary-Beth and Seiler, Ruedi},
	year = {2007},
	note = {Publisher: AIP},
	keywords = {approximation theory, estimation theory, quantum c},
	pages = {102111},
}

@article{Dorier2005,
	title = {Quantum compass model on the square lattice},
	volume = {72},
	number = {2},
	journal = {Phys. Rev. B},
	author = {Dorier, Julien and Becca, Federico and Mila, Frédéric},
	year = {2005},
	note = {Publisher: American Physical Society},
	pages = {24448},
}

@article{Samaj2010,
	title = {Introduction to {Integrable} {Many}-{Body} {Systems} {II}},
	volume = {60},
	journal = {Acta Physica Slovaca},
	author = {Šamaj, L},
	year = {2010},
	keywords = {Heisenberg spin chains, Integrable systems, Magnetic properties, Quantum Inverse Scattering method, Thermodynamic Bethe ansatz, Yang-Baxter equation},
	pages = {155--257},
}

@article{Lieb1961,
	title = {No {Title}},
	volume = {16},
	journal = {Annals of Physics},
	author = {E. Lieb T. Schultz, D Mattis},
	year = {1961},
	pages = {407},
}

@article{Bartlett2006,
	title = {Simple nearest-neighbor two-body {Hamiltonian} system for which the ground state is a universal resource for quantum computation},
	volume = {74},
	number = {4},
	journal = {Phys. Rev. A},
	author = {Bartlett, Stephen D and Rudolph, Terry},
	year = {2006},
	note = {Publisher: American Physical Society},
	pages = {40302},
}

@article{BaconAGT,
	title = {Adiabatic {Gate} {Teleportation}},
	volume = {103},
	number = {12},
	journal = {Phys. Rev. Lett.},
	author = {Bacon, Dave and Flammia, Steven T},
	year = {2009},
	note = {Publisher: American Physical Society},
	pages = {120504},
}

@incollection{Browne2011,
	title = {Computational {Depth} {Complexity} of {Measurement}-{Based} {Quantum} {Computation}},
	volume = {6519},
	isbn = {978-3-642-18072-9},
	booktitle = {Theory of {Quantum} {Computation}, {Communication}, and {Cryptography}},
	publisher = {Springer Berlin / Heidelberg},
	author = {Browne, Dan and Kashefi, Elham and Perdrix, Simon},
	editor = {van Dam, Wim and Kendon, Vivien and Severini, Simone},
	year = {2011},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {35--46},
}

@article{You2010,
	title = {The low-energy states and directional long-range order in the two-dimensional quantum compass model},
	volume = {43},
	abstract = {In the present paper, we study the properties of the two-dimensional quantum compass model by a mathematically rigorous approach. Based on the Perron--Fr\{ö\}benius theorem and the reflection positivity method, we show that the low-energy spectrum of this model is of one-dimensional type. Furthermore, its global ground states have a directional long-range order. Our results confirm several previous conclusions obtained by either numerical or perturbation calculations},
	number = {27},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {You, Wen-Long and Tian, Guang-Shan and Lin, Hai-Qing},
	year = {2010},
	pages = {275001},
}

@article{Doherty2009,
	title = {Identifying {Phases} of {Quantum} {Many}-{Body} {Systems} {That} {Are} {Universal} for {Quantum} {Computation}},
	volume = {103},
	number = {2},
	journal = {Phys. Rev. Lett.},
	author = {Doherty, Andrew C and Bartlett, Stephen D},
	year = {2009},
	note = {Publisher: American Physical Society},
	pages = {20506},
}

@article{Oliveira2008,
	title = {The complexity of quantum spin systems on a two-dimensional square lattice},
	volume = {8},
	issn = {1533-7146},
	number = {10},
	journal = {Quantum Info. Comput.},
	author = {Oliveira, Roberto and Terhal, Barbara M},
	year = {2008},
	note = {Publisher: Rinton Press, Incorporated
Place: Paramus, NJ},
	pages = {900--924},
}

@article{Brzezicki2010,
	title = {Quantum compass model on a chain, ladder and finite square clusters},
	volume = {1297},
	number = {1},
	journal = {AIP Conference Proceedings},
	author = {Brzezicki, W},
	editor = {Avella, Adolfo and Mancini, Ferdinando},
	year = {2010},
	note = {Publisher: AIP},
	keywords = {Ising model; localised states; spin-lattice relaxa},
	pages = {407--411},
}

@article{Silvester2000,
	title = {Determinants of {Block} {Matrices}},
	volume = {84},
	issn = {00255572},
	number = {501},
	journal = {The Mathematical Gazette},
	author = {Silvester, John R},
	year = {2000},
	note = {Publisher: The Mathematical Association},
	pages = {pp. 460--467},
}

@book{HornJohnson2,
	title = {Topics in {Matrix} {Analysis}},
	publisher = {Cambridge University Press},
	author = {Horn, R A and Johnson, C R},
	year = {1991},
}

@article{VandenNest2006,
	title = {Universal {Resources} for {Measurement}-{Based} {Quantum} {Computation}},
	volume = {97},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.97.150504},
	number = {15},
	urldate = {2012-11-01},
	journal = {Physical Review Letters},
	author = {Van den Nest, Maarten and Miyake, Akimasa and Dür, Wolfgang and Briegel, Hans},
	month = oct,
	year = {2006},
	pages = {1--4},
}

@article{Pfeuty1970,
	title = {The one-dimensional {Ising} model with a transverse field},
	volume = {57},
	issn = {0003-4916},
	number = {1},
	journal = {Annals of Physics},
	author = {Pfeuty, Pierre},
	year = {1970},
	pages = {79--90},
}

@article{Broadbent2009,
	title = {Parallelizing quantum circuits},
	volume = {410},
	issn = {03043975},
	doi = {10.1016/j.tcs.2008.12.046},
	number = {26},
	urldate = {2012-11-20},
	journal = {Theoretical Computer Science},
	author = {Broadbent, Anne and Kashefi, Elham},
	month = jun,
	year = {2009},
	note = {Publisher: Elsevier B.V.},
	pages = {2489--2510},
}

@article{Danos2007,
	title = {The measurement calculus},
	volume = {54},
	issn = {00045411},
	number = {2},
	urldate = {2012-11-19},
	journal = {Journal of the ACM},
	author = {Danos, Vincent and Kashefi, Elham and Panangaden, Prakash},
	month = apr,
	year = {2007},
	pages = {8--es},
}

@article{Gross2007,
	title = {Novel {Schemes} for {Measurement}-{Based} {Quantum} {Computation}},
	volume = {98},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.98.220503},
	number = {22},
	urldate = {2012-10-26},
	journal = {Physical Review Letters},
	author = {Gross, D. and Eisert, J.},
	month = may,
	year = {2007},
	pages = {1--4},
}

@article{Broadbent2010,
	title = {Measurement-based and universal blind quantum computation},
	urldate = {2012-11-21},
	journal = {Formal Methods for Quantitative …},
	author = {Broadbent, Anne and Fitzsimons, Joseph and Kashefi, Elham},
	year = {2010},
	pages = {43--86},
}

@article{Beaudrap2008,
	title = {Quadratic form expansions for unitaries},
	number = {1},
	urldate = {2012-11-21},
	journal = {Theory of Quantum …},
	author = {Beaudrap, Niel De and Danos, Vincent},
	year = {2008},
	pages = {29--46},
}

@article{Kempe2001,
	title = {Theory of decoherence-free fault-tolerant universal quantum computation},
	url = {http://pra.aps.org/abstract/PRA/v63/i4/e042307},
	urldate = {2012-12-06},
	journal = {Physical Review A},
	author = {Kempe, J and Bacon, D and Lidar, DA and Whaley, KB},
	year = {2001},
	note = {arXiv: quant-ph/0004064v2},
	pages = {1--40},
}

@article{fong_universal_2011,
	title = {Universal quantum computation and leakage reduction in the 3-qubit decoherence free subsystem},
	url = {http://dl.acm.org/citation.cfm?id=2230965},
	urldate = {2013-06-11},
	journal = {Quantum Information \& Computation},
	author = {Fong, BH and Wandzura, SM},
	year = {2011},
	note = {arXiv: 1102.2909v1},
}

@article{Smith,
	title = {The {Measurement} {Based} {Quantum} {Computing} {Search} {Algorithm} is {Faster} than {Grover}'s {Algorithm}},
	urldate = {2012-11-21},
	journal = {arXiv preprint arXiv:1211.3405},
	author = {Smith, AM and Alsing, PM},
	year = {2012},
	note = {arXiv: 1211.3405v1},
	pages = {2--6},
}

@article{Frees,
	title = {Power law scaling for the adiabatic algorithm for search engine ranking},
	number = {i},
	urldate = {2012-11-21},
	journal = {arXiv preprint arXiv: …},
	author = {Frees, Adam and Gamble, JK and Rudinger, Kenneth},
	year = {2012},
	note = {arXiv: 1211.2248v2},
	pages = {1--6},
}

@article{fong_exchange-only_2012,
	title = {Exchange-only dynamical decoupling in the 3-qubit decoherence free subsystem},
	url = {http://meetings.aps.org/link/BAPS.2012.MAR.B30.8},
	urldate = {2013-06-14},
	journal = {Bulletin of the American Physical Society},
	author = {Fong, B and West, J},
	year = {2012},
	note = {arXiv: 1203.4296v1},
}

@article{Mhalla,
	title = {Finding optimal flows efficiently},
	urldate = {2012-12-12},
	journal = {Automata, Languages and Programming},
	author = {Mhalla, Mehdi and Perdrix, Simon},
	year = {2008},
	note = {arXiv: 0709.2670v1},
	keywords = {graph algorithms, quantum computing},
	pages = {1--10},
}

@article{Lidar2008,
	title = {Towards {Fault} {Tolerant} {Adiabatic} {Quantum} {Computation}},
	volume = {100},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.100.160506},
	number = {16},
	urldate = {2012-11-08},
	journal = {Physical Review Letters},
	author = {Lidar, Daniel},
	month = apr,
	year = {2008},
	pages = {160506},
}

@article{Oreshkov2009,
	title = {Scheme for fault-tolerant holonomic computation on stabilizer codes},
	volume = {80},
	issn = {1050-2947},
	doi = {10.1103/PhysRevA.80.022325},
	number = {2},
	urldate = {2012-11-08},
	journal = {Physical Review A},
	author = {Oreshkov, Ognyan and Brun, Todd and Lidar, Daniel},
	month = aug,
	year = {2009},
	pages = {022325},
}

@article{Jordan2006,
	title = {Error-correcting codes for adiabatic quantum computation},
	volume = {74},
	issn = {1050-2947},
	doi = {10.1103/PhysRevA.74.052322},
	number = {5},
	urldate = {2012-11-08},
	journal = {Physical Review A},
	author = {Jordan, Stephen and Farhi, Edward and Shor, Peter},
	month = nov,
	year = {2006},
	pages = {052322},
}

@article{Deift2006,
	title = {Improved {Gap} {Estimates} for {Simulating} {Quantum} {Circuits} by {Adiabatic} {Evolution}},
	volume = {6},
	issn = {1570-0755},
	number = {2},
	urldate = {2012-11-08},
	journal = {Quantum Information Processing},
	author = {Deift, Percy and Ruskai, Mary Beth and Spitzer, Wolfgang},
	month = dec,
	year = {2006},
	keywords = {adiabatic quantum computation, quantum circuit simulation},
	pages = {121--125},
}

@article{Mizel2007,
	title = {Simple {Proof} of {Equivalence} between {Adiabatic} {Quantum} {Computation} and the {Circuit} {Model}},
	volume = {99},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.99.070502},
	number = {7},
	urldate = {2012-11-04},
	journal = {Physical Review Letters},
	author = {Mizel, Ari and Lidar, Daniel and Mitchell, Morgan},
	month = aug,
	year = {2007},
	pages = {1--4},
}

@article{Biamonte2008,
	title = {Realizable {Hamiltonians} for universal adiabatic quantum computers},
	volume = {78},
	issn = {1050-2947},
	number = {1},
	urldate = {2012-11-08},
	journal = {Physical Review A},
	author = {Biamonte, Jacob and Love, Peter},
	month = jul,
	year = {2008},
	pages = {012352},
}

@article{Gaitan2012,
	title = {Ramsey {Numbers} and {Adiabatic} {Quantum} {Computing}},
	volume = {108},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.108.010501},
	number = {1},
	urldate = {2012-11-02},
	journal = {Physical Review Letters},
	author = {Gaitan, Frank and Clark, Lane},
	month = jan,
	year = {2012},
	pages = {6--9},
}

@article{Nagaj2012,
	title = {Universal two-body-{Hamiltonian} quantum computing},
	volume = {85},
	issn = {1050-2947},
	doi = {10.1103/PhysRevA.85.032330},
	number = {3},
	urldate = {2012-11-08},
	journal = {Physical Review A},
	author = {Nagaj, Daniel},
	month = mar,
	year = {2012},
	pages = {1--5},
}

@article{nigg_experimental_nodate,
	title = {experimental test},
	number = {2},
	author = {Nigg, Daniel and Monz, Thomas and Schindler, Philipp and Martinez, Esteban A and Chwalla, Michael and Hennrich, Markus and Blatt, Rainer and Pusey, Matthew F and Rudolph, Terry and Barrett, Jonathan},
	note = {arXiv: 1211.0942v1},
	pages = {1--7},
}

@article{Nagaj2010,
	title = {Fast universal quantum computation with railroad-switch local {Hamiltonians}},
	volume = {51},
	issn = {00222488},
	doi = {10.1063/1.3384661},
	number = {6},
	urldate = {2012-11-08},
	journal = {Journal of Mathematical Physics},
	author = {Nagaj, Daniel},
	year = {2010},
	pages = {062201},
}

@article{wrachtrup_quantum_2001,
	title = {Quantum {Computation} {Using} the 13 {C} {Nuclear} {Spins} {Near} the {Single} {NV} {Defect} {Center} in {Diamond} 1},
	volume = {91},
	number = {3},
	author = {Wrachtrup, J and Kilin, S Ya and Nizovtsev, A P},
	year = {2001},
	pages = {429--437},
}

@article{Jelezko2004,
	title = {Observation of {Coherent} {Oscillations} in a {Single} {Electron} {Spin}},
	volume = {92},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.92.076401},
	doi = {10.1103/PhysRevLett.92.076401},
	number = {7},
	urldate = {2012-11-02},
	journal = {Physical Review Letters},
	author = {Jelezko, F. and Gaebel, T. and Popa, I. and Gruber, a. and Wrachtrup, J.},
	month = feb,
	year = {2004},
	pages = {1--4},
}

@article{nizovtsev_qubits_2005,
	title = {{QUBITS} {AND} {QUGATES} {A} {Quantum} {Computer} {Based} on {NV} {Centers} in {Diamond} : {Optically} {Detected} {Nutations} of {Single} {Electron} and {Nuclear} {Spins}},
	volume = {99},
	number = {2},
	author = {Nizovtsev, A P and Kilin, S Ya and Jelezko, F and Gaebal, T and Popa, I},
	year = {2005},
	pages = {233--244},
}

@article{Fuchs2011,
	title = {A quantum memory intrinsic to single nitrogen–vacancy centres in diamond},
	volume = {7},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys2026},
	doi = {10.1038/nphys2026},
	number = {10},
	urldate = {2012-10-29},
	journal = {Nature Physics},
	author = {Fuchs, G. D. and Burkard, G. and Klimov, P. V. and Awschalom, D. D.},
	month = jun,
	year = {2011},
	note = {Publisher: Nature Publishing Group},
	pages = {789--793},
}

@article{Neumann2010,
	title = {Quantum register based on coupled electron spins in a room-temperature solid},
	volume = {6},
	issn = {1745-2473},
	url = {http://www.nature.com/doifinder/10.1038/nphys1536},
	doi = {10.1038/nphys1536},
	number = {4},
	urldate = {2012-10-29},
	journal = {Nature Physics},
	author = {Neumann, P. and Kolesov, R. and Naydenov, B. and Beck, J. and Rempp, F. and Steiner, M. and Jacques, V. and Balasubramanian, G. and Markham, M. L. and Twitchen, D. J. and Pezzagna, S. and Meijer, J. and Twamley, J. and Jelezko, F. and Wrachtrup, J.},
	month = feb,
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {249--253},
}

@article{Brown2011,
	title = {Coherent {State} {Transfer} between an {Electron} and {Nuclear} {Spin} in {\textasciicircum}\{15\}{N}@{C}\_\{60\}},
	volume = {106},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.106.110504},
	doi = {10.1103/PhysRevLett.106.110504},
	number = {11},
	urldate = {2012-11-06},
	journal = {Physical Review Letters},
	author = {Brown, Richard and Tyryshkin, Alexei and Porfyrakis, Kyriakos and Gauger, Erik and Lovett, Brendon and Ardavan, Arzhang and Lyon, S. and Briggs, G. and Morton, John},
	month = mar,
	year = {2011},
	pages = {2--5},
}

@article{Simon2010,
	title = {Quantum memories},
	volume = {58},
	issn = {1434-6060},
	url = {http://www.springerlink.com/index/10.1140/epjd/e2010-00103-y},
	doi = {10.1140/epjd/e2010-00103-y},
	number = {1},
	urldate = {2012-11-01},
	journal = {The European Physical Journal D},
	author = {Simon, C. and Afzelius, M. and Appel, J. and Boyer de la Giroday, a. and Dewhurst, S. J. and Gisin, N. and Hu, C. Y. and Jelezko, F. and Kröll, S. and Müller, J. H. and Nunn, J. and Polzik, E. S. and Rarity, J. G. and De Riedmatten, H. and Rosenfeld, W. and Shields, a. J. and Sköld, N. and Stevenson, R. M. and Thew, R. and Walmsley, I. a. and Weber, M. C. and Weinfurter, H. and Wrachtrup, J. and Young, R. J.},
	month = apr,
	year = {2010},
	pages = {1--22},
}

@article{Shi2010,
	title = {Room-{Temperature} {Implementation} of the {Deutsch}-{Jozsa} {Algorithm} with a {Single} {Electronic} {Spin} in {Diamond}},
	volume = {105},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.105.040504},
	doi = {10.1103/PhysRevLett.105.040504},
	number = {4},
	urldate = {2012-11-06},
	journal = {Physical Review Letters},
	author = {Shi, Fazhan and Rong, Xing and Xu, Nanyang and Wang, Ya and Wu, Jie and Chong, Bo and Peng, Xinhua and Kniepert, Juliane and Schoenfeld, Rolf-Simon and Harneit, Wolfgang and Feng, Mang and Du, Jiangfeng},
	month = jul,
	year = {2010},
	pages = {2--5},
}

@article{Smeltzer2011,
	title = {13 {C} hyperfine interactions in the nitrogen-vacancy centre in diamond},
	volume = {13},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/13/i=2/a=025021?key=crossref.bdd6956722cde89a36ab3eee44a82724},
	doi = {10.1088/1367-2630/13/2/025021},
	number = {2},
	urldate = {2012-11-06},
	journal = {New Journal of Physics},
	author = {Smeltzer, Benjamin and Childress, Lilian and Gali, Adam},
	month = feb,
	year = {2011},
	pages = {025021},
}

@article{Wang2011,
	title = {Time-optimal rotation of a spin 1/2: {Application} to the {NV} center spin in diamond},
	volume = {84},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.84.045303},
	doi = {10.1103/PhysRevB.84.045303},
	number = {4},
	urldate = {2012-11-06},
	journal = {Physical Review B},
	author = {Wang, Zhi-Hui and Dobrovitski, V.},
	month = jul,
	year = {2011},
	pages = {1--5},
}

@article{Said2009,
	title = {Robust control of entanglement in a nitrogen-vacancy center coupled to a {\textasciicircum}\{13\}{C} nuclear spin in diamond},
	volume = {80},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.80.032303},
	doi = {10.1103/PhysRevA.80.032303},
	number = {3},
	urldate = {2012-11-06},
	journal = {Physical Review A},
	author = {Said, R. and Twamley, J.},
	month = sep,
	year = {2009},
	pages = {1--7},
}

@article{Ardavan2011,
	title = {Quantum control in spintronics.},
	volume = {369},
	issn = {1364-503X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21727123},
	doi = {10.1098/rsta.2011.0009},
	abstract = {Superposition and entanglement are uniquely quantum phenomena. Superposition incorporates a phase that contains information surpassing any classical mixture. Entanglement offers correlations between measurements in quantum systems that are stronger than any that would be possible classically. These give quantum computing its spectacular potential, but the implications extend far beyond quantum information processing. Early applications may be found in entanglement-enhanced sensing and metrology. Quantum spins in condensed matter offer promising candidates for investigating and exploiting superposition and entanglement, and enormous progress is being made in quantum control of such systems. In gallium arsenide (GaAs), individual electron spins can be manipulated and measured, and singlet-triplet states can be controlled in double-dot structures. In silicon, individual electron spins can be detected by ionization of phosphorus donors, and information can be transferred from electron spins to nuclear spins to provide long memory times. Electron and nuclear spins can be manipulated in nitrogen atoms incarcerated in fullerene molecules, which in turn can be assembled in ordered arrays. Spin states of charged nitrogen vacancy centres in diamond can be manipulated and read optically. Collective spin states in a range of materials systems offer scope for holographic storage of information. Conditions are now excellent for implementing superposition and entanglement in spintronic devices, thereby opening up a new era of quantum technologies.},
	number = {1948},
	urldate = {2012-11-04},
	journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
	author = {Ardavan, a and Briggs, G a D},
	month = aug,
	year = {2011},
	pmid = {21727123},
	pages = {3229--48},
}

@article{Wu2005,
	title = {Holonomic {Quantum} {Computation} in {Decoherence}-{Free} {Subspaces}},
	volume = {95},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.95.130501},
	number = {13},
	urldate = {2012-11-06},
	journal = {Physical Review Letters},
	author = {Wu, L.-a. and Zanardi, P. and Lidar, D.},
	month = sep,
	year = {2005},
	pages = {1--4},
}

@article{Xu2012,
	title = {Nonadiabatic {Holonomic} {Quantum} {Computation} in {Decoherence}-{Free} {Subspaces}},
	volume = {109},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.109.170501},
	number = {17},
	urldate = {2012-10-29},
	journal = {Physical Review Letters},
	author = {Xu, G. and Zhang, J. and Tong, D. and Sjöqvist, Erik and Kwek, L.},
	month = oct,
	year = {2012},
	pages = {1--5},
}

@article{meier_quantum_2003,
	title = {Quantum {Computing} with {Spin} {Cluster} {Qubits}},
	volume = {90},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.90.047901},
	doi = {10.1103/PhysRevLett.90.047901},
	number = {4},
	urldate = {2012-11-01},
	journal = {Physical Review Letters},
	author = {Meier, Florian and Levy, Jeremy and Loss, Daniel},
	month = jan,
	year = {2003},
	pages = {1--4},
}

@article{Loss1998,
	title = {Quantum computation with quantum dots},
	volume = {57},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.57.120},
	doi = {10.1103/PhysRevA.57.120},
	number = {1},
	journal = {Physical Review A},
	author = {Loss, Daniel and DiVincenzo, David P.},
	month = jan,
	year = {1998},
	pages = {120--126},
}

@article{sandvik_striped_2002,
	title = {Striped {Phase} in a {Quantum} {XY} {Model} with {Ring} {Exchange}},
	doi = {10.1103/PhysRevLett.89.247201},
	author = {Sandvik, A W and Daul, S and Singh, R R P and Scalapino, D J},
	year = {2002},
	pages = {1--4},
}

@article{laird_coherent_2010,
	title = {Coherent spin manipulation in an exchange-only qubit},
	url = {http://prb.aps.org/abstract/PRB/v82/i7/e075403},
	urldate = {2013-06-14},
	journal = {Physical Review B},
	author = {Laird, EA and Taylor, JM and DiVincenzo, DP},
	year = {2010},
	note = {arXiv: 1005.0273v1},
	pages = {1--7},
}

@article{coello_spin_2010,
	title = {Spin {Filtering} and {Entanglement} {Swapping} through {Coherent} {Evolution} of a {Single} {Quantum} {Dot}},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.105.080502},
	number = {1},
	urldate = {2013-06-14},
	journal = {Physical review letters},
	author = {Coello, JG and Bayat, Abolfazl and Bose, Sougato and Jefferson, JH and Creffield, CE},
	year = {2010},
	note = {arXiv: 1003.0909v2},
	pages = {6--10},
}

@article{crepaldi_giant_2012,
	title = {Giant {Ambipolar} {Rashba} {Effect} in the {Semiconductor} {BiTeI}},
	volume = {109},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.109.096803},
	doi = {10.1103/PhysRevLett.109.096803},
	number = {9},
	urldate = {2012-09-03},
	journal = {Physical Review Letters},
	author = {Crepaldi, a. and Moreschini, L. and Autès, G. and Tournier-Colletta, C. and Moser, S. and Virk, N. and Berger, H. and Bugnon, Ph. and Chang, Y. and Kern, K. and Bostwick, a. and Rotenberg, E. and Yazyev, O. and Grioni, M.},
	month = aug,
	year = {2012},
	pages = {1--5},
}

@article{machnes_comparing_2011,
	title = {Comparing, optimizing, and benchmarking quantum-control algorithms in a unifying programming framework},
	url = {http://pra.aps.org/abstract/PRA/v84/i2/e022305},
	urldate = {2012-12-04},
	journal = {Physical Review A},
	author = {Machnes, S and Sander, U and Glaser, SJ},
	year = {2011},
	note = {arXiv: 1011.4874v3},
	pages = {35--45},
}

@article{Gilchrist2005,
	title = {Distance measures to compare real and ideal quantum processes},
	volume = {71},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.71.062310},
	doi = {10.1103/PhysRevA.71.062310},
	number = {6},
	urldate = {2012-10-10},
	journal = {Physical Review A},
	author = {Gilchrist, Alexei and Langford, Nathan and Nielsen, Michael},
	month = jun,
	year = {2005},
	pages = {1--14},
}

@article{kieling_percolation_2008,
	title = {Percolation, renormalization, and quantum computing with non-deterministic gates},
	author = {Kieling, K and Rudolph, T and Eisert, J},
	year = {2008},
	note = {arXiv: quant-ph/0611140v3},
	pages = {1--5},
}

@article{Danos2008,
	title = {The {Measurement} {Calculus}},
	author = {Danos, Vincent},
	year = {2008},
	note = {arXiv: quant-ph/0412135v1},
}

@article{broxson_kronecker_2006,
	title = {The {Kronecker} {Product}},
	journal = {Spring},
	author = {Broxson, Bobbi Jo},
	year = {2006},
}

@article{Brennen2008,
	title = {Measurement-based quantum computer in the gapped ground state of a two-body {Hamiltonian}},
	urldate = {2012-11-21},
	journal = {Physical review letters},
	author = {Brennen, GK and Miyake, Akimasa},
	year = {2008},
	note = {arXiv: 0803.1478v3},
	pages = {1--5},
}

@article{bacon_analyzing_2008,
	title = {Analyzing {Algebraic} {Quantum} {Circuits} {Using} {Exponential} {Sums}},
	author = {Bacon, Dave and Russell, Alexander},
	year = {2008},
}

@article{kult_noncyclic_2006,
	title = {Noncyclic geometric changes of quantum states},
	url = {http://pra.aps.org/abstract/PRA/v74/i2/e022106},
	number = {1},
	urldate = {2013-05-30},
	journal = {Physical Review A},
	author = {Kult, D and Åberg, J and Sjöqvist, E},
	year = {2006},
	note = {arXiv: quant-ph/0512045v2},
	pages = {1--5},
}

@article{Laforest2004,
	title = {Holonomic {Quantum} {Computation} with {Berry} ’ s {Phases}},
	author = {Fuentes-Guridi, Ivette},
	year = {2004},
	pages = {1--22},
}

@article{Fujii2002,
	title = {Introduction to {Grassmann} manifolds and quantum computation},
	volume = {8},
	number = {October 2001},
	urldate = {2012-11-21},
	journal = {Journal of Applied Mathematics},
	author = {Fujii, Kazuyuki},
	year = {2002},
	pages = {371--405},
}

@article{Bacon,
	title = {Adiabatic {Quantum} {Transistors}},
	urldate = {2012-11-21},
	journal = {arXiv preprint arXiv:},
	author = {Bacon, Dave and Flammia, ST and Crosswhite, GM},
	year = {2012},
	note = {arXiv: quant-ph/1207.2769},
	pages = {1--17},
}

@article{kult_noncyclic_2006-1,
	title = {Noncyclic geometric changes of quantum states},
	url = {http://pra.aps.org/abstract/PRA/v74/i2/e022106},
	number = {1},
	urldate = {2012-12-04},
	journal = {Physical Review A},
	author = {Kult, David and Åberg, J and Sjöqvist, E},
	year = {2006},
	note = {arXiv: quant-ph/0512045v2},
	pages = {1--5},
}

@article{taylor_relaxation_2007,
	title = {Relaxation, dephasing, and quantum control of electron spins in double quantum dots},
	urldate = {2013-01-09},
	journal = {Physical Review B},
	author = {Taylor, JM and Petta, JR and Johnson, AC},
	year = {2007},
	note = {arXiv: cond-mat/0602470v2},
	pages = {1--18},
}

@article{burgarth_local_2009,
	title = {Local {Controllability} of quantum networks},
	url = {http://pra.aps.org/abstract/PRA/v79/i6/e060305},
	urldate = {2012-12-04},
	journal = {Physical Review A},
	author = {Burgarth, Daniel and Bose, Sougato and Bruder, Christoph and Giovannetti, Vittorio},
	year = {2009},
	note = {arXiv: 0805.3975v2},
	pages = {3--6},
}

@article{Bacon2009,
	title = {Adiabatic {Gate} {Teleportation}},
	urldate = {2012-11-21},
	journal = {Physical review letters},
	author = {Bacon, Dave and Flammia, ST},
	year = {2009},
	pages = {1--7},
}

@article{Cui2012,
	title = {Quantum phases with differing computational power},
	urldate = {2012-11-21},
	journal = {Nature …},
	author = {Cui, Jian and Gu, Mile and Kwek, LC and Santos, MF},
	year = {2012},
	note = {arXiv: 1110.3331v4},
	pages = {1--7},
}

@article{Garnerone2012,
	title = {Adiabatic {Quantum} {Algorithm} for {Search} {Engine} {Ranking}},
	volume = {108},
	issn = {0031-9007},
	doi = {10.1103/PhysRevLett.108.230506},
	number = {23},
	urldate = {2012-06-06},
	journal = {Physical Review Letters},
	author = {Garnerone, Silvano and Zanardi, Paolo and Lidar, Daniel},
	month = jun,
	year = {2012},
	pages = {1--6},
}

@article{pusey_reality_2012,
	title = {On the reality of the quantum state},
	volume = {8},
	doi = {10.1038/NPHYS2309},
	number = {June},
	journal = {Nature Physics},
	author = {Pusey, Matthew F and Barrett, Jonathan and Rudolph, Terry},
	year = {2012},
	pages = {475--478},
}

@article{Cui2012a,
	title = {Quantum phases with differing computational power},
	volume = {3},
	issn = {2041-1723},
	number = {may},
	urldate = {2012-05-02},
	journal = {Nature Communications},
	author = {Cui, Jian and Gu, Mile and Kwek, Leong Chuan and Santos, Marcelo França and Fan, Heng and Vedral, Vlatko},
	month = may,
	year = {2012},
	pages = {812},
}

@article{bravyi_classification_2012,
	title = {Classification of topologically protected gates for local stabilizer codes},
	url = {http://arxiv.org/abs/1206.1609},
	urldate = {2012-12-04},
	journal = {arXiv preprint arXiv:1206.1609},
	author = {Bravyi, Sergey and Koenig, R},
	year = {2012},
	note = {arXiv: 1206.1609v1},
	pages = {1--6},
}

@article{Virmani2008,
	title = {Classical simulability, entanglement breaking, and quantum computation thresholds},
	urldate = {2012-11-21},
	journal = {Physical Review A},
	author = {Virmani, S and Huelga, SF and Plenio, MB},
	year = {2005},
	note = {arXiv: quant-ph/0408076v2},
	pages = {1--12},
}

@article{nielsen_simple_2008,
	title = {A simple formula for the average gate fidelity of a quantum dynamical operation},
	number = {4},
	journal = {Direct},
	author = {Nielsen, Michael A},
	year = {2008},
	note = {arXiv: quant-ph/0205035v2},
	pages = {1--3},
}

@article{Childs2001,
	title = {Realization of quantum process tomography in {NMR}},
	volume = {64},
	url = {http://dx.doi.org/10.1103/PhysRevA.64.012314},
	journal = {Phys. Rev. A},
	author = {Childs, A M and Chuang, I L and Leung, D W},
	year = {2001},
	pages = {12314},
}

@article{DiVincenzo2000,
	title = {The physical implementation of quantum computation},
	volume = {48},
	url = {http://dx.doi.org/10.1002/1521-3978(200009)48:9/11<771::AID-PROP771>3.0.CO},
	journal = {Fort. Phys.},
	author = {DiVincenzo, D},
	year = {2000},
	pages = {771--783},
}

@article{steane_quantum_2002,
	title = {Quantum computer architecture for fast entropy extraction},
	volume = {4},
	journal = {Quant. Inf. Comput.},
	author = {Steane, A M},
	year = {2002},
	pages = {297--306},
}

@article{DiVincenzo1998,
	title = {Quantum-channel capacity of very noisy channels},
	volume = {57},
	url = {http://dx.doi.org/10.1103/PhysRevA.57.830},
	journal = {Phys. Rev. A},
	author = {DiVincenzo, D P and Shor, P W and Smolin, J A},
	year = {1998},
	pages = {830--839},
}

@misc{Nielsen2001,
	title = {Quantum {Computation} and {Quantum} {Information}},
	author = {Nielsen, M A and Chuang, I L},
	year = {2001},
}

@article{Knill1998,
	title = {Resilient quantum computation},
	volume = {279},
	url = {http://dx.doi.org/10.1126/science.279.5349.342},
	journal = {Science},
	author = {Knill, E and Laflamme, R and Zurek, W H},
	year = {1998},
	pages = {342--345},
}

@article{Raussendorf2001,
	title = {A one-way quantum computer},
	volume = {86},
	journal = {Phys. Rev. Lett.},
	author = {Raussendorf, R and Briegel, H J},
	year = {2001},
	pages = {5188--5191},
}

@article{knill_algorithmic_2000,
	title = {An algorithmic benchmark for quantum information processing},
	volume = {404},
	url = {http://dx.doi.org/10.1038/35006012},
	journal = {Nature},
	author = {Knill, E and Laflamme, R and Martinez, R and Tseng, C.-H.},
	year = {2000},
	pages = {368--370},
}

@article{Steane1998,
	title = {Space, time, parallelism and noise requirements for reliable quantum computing},
	volume = {46},
	url = {http://dx.doi.org/10.1002/(SICI)1521-3978(199806)46:4/5<443::AID-PROP443>3.0.CO},
	journal = {Fort. Phys.},
	author = {Steane, A},
	year = {1998},
	pages = {443--457},
}

@article{Bennett1993,
	title = {Teleporting an unknown quantum state via dual classical and {Einstein}[ndash]{Podolsky}[ndash]{Rosen} channels},
	volume = {70},
	url = {http://dx.doi.org/10.1103/PhysRevLett.70.1895},
	journal = {Phys. Rev. Lett.},
	author = {Bennett, C H},
	year = {1993},
	pages = {1895--1899},
}

@article{Leibfried2003,
	title = {Experimental demonstration of a robust, high-fidelity geometric two ion-qubit phase gate},
	volume = {422},
	url = {http://dx.doi.org/10.1038/nature01492},
	journal = {Nature},
	author = {Leibfried, D},
	year = {2003},
	pages = {412--415},
}

@article{roos_bell_2004,
	title = {Bell states of atoms with ultralong lifetimes and their tomographic state analysis},
	url = {http://dx.doi.org/10.1103/PhysRevLett.92.220402},
	journal = {Phys. Rev. Lett.},
	author = {Roos, C F},
	year = {2004},
	pages = {220402},
}

@article{Bennett1996,
	title = {Purification of noisy entanglement and faithful teleportation via noisy channels},
	volume = {76},
	url = {http://dx.doi.org/10.1103/PhysRevLett.76.722},
	journal = {Phys. Rev. Lett.},
	author = {Bennett, C H},
	year = {1996},
	pages = {722--725},
}

@article{Zhou2000,
	title = {Methodology for quantum logic gate construction},
	volume = {62},
	url = {http://dx.doi.org/10.1103/PhysRevA.62.052316},
	journal = {Phys. Rev. A},
	author = {Zhou, X and Leung, D W and Chuang, I L},
	year = {2000},
	pages = {52316},
}

@article{Shor1997,
	title = {Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer},
	volume = {26},
	journal = {SIAM J. Comput.},
	author = {Shor, P W},
	year = {1997},
	pages = {1484--1509},
}

@misc{Gottesman1997,
	title = {Stabilizer {Codes} and {Quantum} {Error} {Correction}},
	author = {Gottesman, D},
	year = {1997},
}

@article{Feynman1982,
	title = {Simulating physics with computers},
	volume = {21},
	journal = {Int. J. Theor. Phys.},
	author = {Feynman, R P},
	year = {1982},
	pages = {467--488},
}

@article{Steane2003,
	title = {Overhead and noise threshold of fault-tolerant quantum error correction},
	volume = {68},
	url = {http://dx.doi.org/10.1103/PhysRevA.68.042322},
	journal = {Phys. Rev. A},
	author = {Steane, A M},
	year = {2003},
	pages = {42322},
}

@article{Preskill1998,
	title = {Reliable quantum computers},
	volume = {454},
	url = {http://dx.doi.org/10.1098/rspa.1998.0167},
	journal = {Proc. R. Soc. Lond. A},
	author = {Preskill, J},
	year = {1998},
	pages = {385--410},
}

@misc{Shor1996,
	title = {Proc. 37th {Symp}. {Foundations} of {Computer} {Science} ({FOCS})},
	author = {Shor, P W},
	year = {1996},
}

@article{bollinger_303[thinsp]mhz_1991,
	title = {A 303[thinsp]{MHz} frequency standard based on trapped {Be}+ ions},
	volume = {40},
	journal = {IEEE Trans. Instrum. Meas.},
	author = {Bollinger, J J and Heinzen, D J and Itano, W M and Gilbert, S L and Wineland, D J},
	year = {1991},
	pages = {126--128},
}

@article{Knill1998a,
	title = {Resilient quantum computation: {Error} models and thresholds},
	volume = {454},
	url = {http://dx.doi.org/10.1098/rspa.1998.0166},
	journal = {Proc. R. Soc. Lond. A},
	author = {Knill, E and Laflamme, R and Zurek, W},
	year = {1998},
	pages = {365--384},
}

@article{kitaev_quantum_1997,
	title = {Quantum computations: {Algorithms} and error correction},
	volume = {52},
	journal = {Russ. Math. Surv.},
	author = {Kitaev, A Y},
	year = {1997},
	pages = {1191--1249},
}

@article{Gottesman1999,
	title = {Demonstrating the viability of universal quantum computation using teleportation and single-qubit operations},
	volume = {402},
	url = {http://dx.doi.org/10.1038/46503},
	journal = {Nature},
	author = {Gottesman, D and Chuang, I L},
	year = {1999},
	pages = {390--393},
}

@phdthesis{Bacon2003,
	title = {Decoherence, {Control}, and {Symmetry} in {Quantum} {Computers}},
	urldate = {2012-12-03},
	school = {Caltech},
	author = {Bacon, D},
	year = {2003},
	note = {arXiv: quant-ph/0305025v1
Publication Title: arXiv preprint quant-ph/0305025},
}

@article{Markham2010,
	title = {Measurement {Based} {Quantum} {Computation} on {Fractal} {Lattices} {The} analogy : {Phase} {Transition} and {Measurement} {Based} {Quantum} {Com}-},
	doi = {10.4204/EPTCS.26.10},
	number = {Dcm},
	journal = {Quantum},
	author = {Markham, Damian and Anders, Janet and Hajduˇ, Michal},
	year = {2010},
	pages = {109--115},
}

@article{Trotzky2008,
	title = {Time-resolved observation and control of superexchange interactions with ultracold atoms in optical lattices.},
	volume = {319},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/18096767},
	doi = {10.1126/science.1150841},
	abstract = {Quantum mechanical superexchange interactions form the basis of quantum magnetism in strongly correlated electronic media. We report on the direct measurement of superexchange interactions with ultracold atoms in optical lattices. After preparing a spin-mixture of ultracold atoms in an antiferromagnetically ordered state, we measured coherent superexchange-mediated spin dynamics with coupling energies from 5 hertz up to 1 kilohertz. By dynamically modifying the potential bias between neighboring lattice sites, the magnitude and sign of the superexchange interaction can be controlled, thus allowing the system to be switched between antiferromagnetic and ferromagnetic spin interactions. We compare our findings to predictions of a two-site Bose-Hubbard model and find very good agreement, but are also able to identify corrections that can be explained by the inclusion of direct nearest-neighbor interactions.},
	number = {5861},
	urldate = {2011-07-19},
	journal = {Science (New York, N.Y.)},
	author = {Trotzky, S and Cheinet, P and Fölling, S and Feld, M and Schnorrberger, U and Rey, a M and Polkovnikov, a and Demler, E a and Lukin, M D and Bloch, I},
	month = jan,
	year = {2008},
	pmid = {18096767},
	pages = {295--9},
}

@article{Paredes2008,
	title = {Minimum instances of topological matter in an optical plaquette},
	volume = {77},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.77.023603},
	doi = {10.1103/PhysRevA.77.023603},
	number = {2},
	urldate = {2011-07-25},
	journal = {Physical Review A},
	author = {Paredes, Belén and Bloch, Immanuel},
	month = feb,
	year = {2008},
	pages = {1--8},
}

@article{bloch_many-body_2008,
	title = {Many-body physics with ultracold gases},
	volume = {80},
	issn = {0034-6861},
	url = {http://link.aps.org/doi/10.1103/RevModPhys.80.885},
	doi = {10.1103/RevModPhys.80.885},
	number = {3},
	urldate = {2011-06-14},
	journal = {Reviews of Modern Physics},
	author = {Bloch, Immanuel and Zwerger, Wilhelm},
	month = jul,
	year = {2008},
	pages = {885--964},
}

@article{richert_propagation_2012,
	title = {Propagation of local excitations through strongly correlated quantum chains},
	volume = {407},
	url = {http://www.sciencedirect.com/science/article/pii/S0921452611011963},
	doi = {10.1016/j.physb.2011.12.010},
	urldate = {2013-06-18},
	journal = {Physica B: Condensed Matter},
	author = {Richert, Jean and Khalil, Tarek},
	year = {2012},
	keywords = {amplitude, correlated spin systems, group velocities of spin, propagation of signals in, strongly},
	pages = {729--734},
}

@article{preskill_lecture_2004,
	title = {Lecture {Notes} for {Physics} 219 : {Quantum} {Computation}},
	number = {June},
	journal = {Quantum},
	author = {Preskill, John},
	year = {2004},
}

@article{Jefferson1996,
	title = {Effective charge-spin models for quantum dots.},
	volume = {54},
	issn = {0163-1829},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9986456},
	number = {7},
	journal = {Physical review. B, Condensed matter},
	author = {Jefferson, Jh and Häusler, W},
	month = aug,
	year = {1996},
	pmid = {9986456},
	pages = {4936--4947},
}

@article{Mizel2004,
	title = {Three- and {Four}-{Body} {Interactions} in {Spin}-{Based} {Quantum} {Computers}},
	volume = {92},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.92.077903},
	doi = {10.1103/PhysRevLett.92.077903},
	number = {7},
	urldate = {2011-06-20},
	journal = {Physical Review Letters},
	author = {Mizel, Ari and Lidar, Daniel},
	month = feb,
	year = {2004},
	pages = {1--4},
}

@article{search_spin_1999,
	title = {Spin dynamics simulations of classical ferro- and antiferromagnetic model systems : comparison with theory and experiment {Spin} dynamics simulations of classical ferro- and antiferromagnetic model systems : comparison with theory and experiment},
	volume = {179},
	author = {Search, Home and Journals, Collections and Contact, About and Iopscience, My},
	year = {1999},
}

@misc{noauthor_phase_nodate,
	title = {Phase {Diagram} of {S}=1/2 {Antiferromagnetic}},
}

@article{blechman_mathematics_2007,
	title = {A {Mathematics} {Primer} for {Physics} {Graduate} {Students} ( {Version} 2 . 0 )},
	journal = {Physics},
	author = {Blechman, Andrew E},
	year = {2007},
}

@article{search_my_2001,
	title = {My {IOPscience} {A} two-leg quantum {Ising} ladder : a bosonization study of the {ANNNI} model},
	volume = {305},
	author = {Search, Home and Journals, Collections and Contact, About and Iopscience, My and Address, I P},
	year = {2001},
}

@article{reece_derivation_2006,
	title = {A {Derivation} of the {Quantum} {Mechanical} {Momentum} {Operator} in the {Position} {Representation} {Eigenstates} of {K}},
	volume = {1},
	number = {8},
	author = {Reece, Ryan D},
	year = {2006},
	pages = {1--5},
}

@article{luke_phy2403f_2008,
	title = {{PHY2403F} {Lecture} {Notes}},
	journal = {Notes},
	author = {Luke, Michael},
	year = {2008},
}

@article{boyer_continuous_1967,
	title = {Continuous symmetries and conserved currents},
	url = {http://www.sciencedirect.com/science/article/pii/0003491667901352},
	urldate = {2012-12-04},
	journal = {Annals of Physics},
	author = {Boyer, TH},
	year = {1967},
}

@article{noauthor_+_2010,
	title = {+ µ 2 ; that is, show that � � (∂},
	number = {x},
	journal = {October},
	year = {2010},
	pages = {2403--2403},
}

@article{tong_quantum_2007,
	title = {Quantum {Field} {Theory} : {Example} {Sheet} 1},
	number = {October},
	journal = {Compute},
	author = {Tong, David},
	year = {2007},
	pages = {1--4},
}

@article{tong_quantum_2007-1,
	title = {Quantum {Field} {Theory} : {Example} {Sheet} 2},
	volume = {0},
	number = {October},
	journal = {October},
	author = {Tong, David},
	year = {2007},
	pages = {1--4},
}

@article{noauthor_phy_2010,
	title = {{PHY} {2403F}: {PROBLEM} {SET} \#1 {Due}: {Tuesday}, {Sept}. 28, 2010 1. {A} {Lorentz} transformation x},
	number = {d},
	year = {2010},
	pages = {1--3},
}

@article{dasgupta_introduction_2008,
	title = {An {Introduction} to {Quantum} {Field} {Theory}},
	number = {September},
	journal = {Energy},
	author = {Dasgupta, Mrinal},
	year = {2008},
}

@article{simons_chapter_nodate,
	title = {Chapter 3 {Feynman} {Path} {Integral}},
	journal = {Physics},
	author = {Simons, Ben},
}

@article{tong_quantum_2007-2,
	title = {Quantum {Field} {Theory}},
	journal = {Quantum},
	author = {Tong, David},
	year = {2007},
}

@article{srednicki_quantum_2006,
	title = {Quantum {Field} {Theory}},
	author = {Srednicki, Mark},
	year = {2006},
}

@misc{noauthor_gogolin_bosonization.pdf_nodate,
	title = {Gogolin\_Bosonization.pdf},
}

@article{Kay2006,
	title = {Perfect state transfer: {Beyond} nearest-neighbor couplings},
	volume = {73},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.73.032306},
	doi = {10.1103/PhysRevA.73.032306},
	number = {3},
	urldate = {2011-09-05},
	journal = {Physical Review A},
	author = {Kay, Alastair},
	month = mar,
	year = {2006},
	pages = {1--7},
}

@article{Romero-Isart2007,
	title = {Quantum state transfer in spin-1 chains},
	volume = {75},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.75.050303},
	doi = {10.1103/PhysRevA.75.050303},
	number = {5},
	urldate = {2011-10-10},
	journal = {Physical Review A},
	author = {Romero-Isart, O. and Eckert, K. and Sanpera, a.},
	month = may,
	year = {2007},
	pages = {1--4},
}

@article{Rossignoli2011,
	title = {Even-odd entanglement in boson and spin systems},
	volume = {83},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.83.042328},
	doi = {10.1103/PhysRevA.83.042328},
	number = {4},
	urldate = {2011-10-10},
	journal = {Physical Review A},
	author = {Rossignoli, R. and Canosa, N. and Matera, J.},
	month = apr,
	year = {2011},
	pages = {1--8},
}

@article{Ananikyan2011,
	title = {Magnetic properties and quantum entanglement on decorated zigzag ladder},
	volume = {46},
	issn = {1068-3372},
	url = {http://www.springerlink.com/index/10.3103/S1068337211040104},
	doi = {10.3103/S1068337211040104},
	number = {4},
	urldate = {2011-10-10},
	journal = {Journal of Contemporary Physics (Armenian Academy of Sciences)},
	author = {Ananikyan, L. N. and Lazaryan, H. a.},
	month = jul,
	year = {2011},
	keywords = {bogoliubov inequality, concurrence, exchange interactions, gibbs, heisenberg model, magnetic, properties, quantum entanglement, zigzag ladder},
	pages = {184--190},
}

@article{Nest2007,
	title = {Fundamentals of universality in one-way quantum computation},
	volume = {9},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/9/i=6/a=204?key=crossref.e94a2b46e2ec40ac06a8437f442c786b},
	doi = {10.1088/1367-2630/9/6/204},
	number = {6},
	urldate = {2011-10-10},
	journal = {New Journal of Physics},
	author = {Nest, M Van Den and Dür, W and Miyake, a and Briegel, H J},
	month = jun,
	year = {2007},
	pages = {204--204},
}

@article{makhlin_nonlocal_2002,
	title = {Nonlocal properties of two-qubit gates and mixed states, and the optimization of quantum computations},
	url = {http://link.springer.com/article/10.1023/A:1022144002391},
	urldate = {2015-01-08},
	journal = {Quantum Information Processing},
	author = {Makhlin, Yuriy},
	year = {2002},
	note = {arXiv: quant-ph/0002045v2},
}

@article{search_frustration_2010,
	title = {Frustration and intermediate dimerized phase in spin ladder system ∗},
	volume = {077503},
	journal = {Society},
	author = {Search, Home and Journals, Collections and Contact, About and Iopscience, My},
	year = {2010},
	keywords = {7510j, 7540c, 7540f, dimerized phases, frustration, pacc, spin ladder},
}

@article{wen_quantum_2011,
	title = {Quantum {Phase} {Transitions} and {Dimerized} {Phases} in {Frustrated} {Spin} {Ladder}},
	volume = {55},
	issn = {0253-6102},
	url = {http://stacks.iop.org/0253-6102/55/i=6/a=26},
	doi = {10.1088/0253-6102/55/6/26},
	abstract = {In this paper, we study the phase diagram of a frustrated spin ladder model by applying the  bosonization technique and the density-matrix renormalization-group (DMRG) algorithm. Effect of the  intra-chain next-nearestneighbor (NNN) super-exchange interaction is investigated in detail and the  order parameters are calculated to detect the emergence of the dimerized phases. We find that the  intra-chain NNN interaction plays a key role in inducing dimerized phases.},
	number = {6},
	urldate = {2011-10-10},
	journal = {Communications in Theoretical Physics},
	author = {Wen, Rui and Liu, Guang-Hua and Tian, Guang-Shan},
	month = jun,
	year = {2011},
	pages = {1102--1108},
}

@article{song_entanglement_2011,
	title = {Entanglement entropy from charge statistics: {Exact} relations for noninteracting many-body systems},
	volume = {83},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.83.161408},
	doi = {10.1103/PhysRevB.83.161408},
	number = {16},
	urldate = {2011-10-04},
	journal = {Physical Review B},
	author = {Song, H. and Flindt, Christian and Rachel, Stephan and Klich, Israel and Le Hur, Karyn},
	month = apr,
	year = {2011},
	pages = {2--5},
}

@article{song_bipartite_2011,
	title = {Bipartite {Fluctuations} as a {Probe} of {Many}-{Body} {Entanglement}},
	author = {Song, H Francis and Rachel, Stephan and Flindt, Christian and Klich, Israel and Laflorencie, Nicolas and Hur, Karyn Le},
	year = {2011},
	note = {arXiv: 1109.1001v1},
	pages = {6--19},
}

@article{song_bipartite_2011-1,
	title = {Bipartite {Fluctuations} as a {Probe} of {Many}-{Body} {Entanglement}},
	author = {Song, H Francis and Rachel, Stephan and Flindt, Christian and Klich, Israel and Laflorencie, Nicolas and Hur, Karyn Le},
	year = {2011},
	note = {arXiv: 1109.1001v1},
	pages = {6--19},
}

@article{Sen1996,
	title = {Relaxation in {S}=1/2 quantum spin chains: {The} role of second neighbor interactions.},
	volume = {53},
	issn = {0163-1829},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9983851},
	number = {6},
	journal = {Physical review. B, Condensed matter},
	author = {Sen, S and Hoff, Cn and Kuhl, De and McGrew, Da},
	month = feb,
	year = {1996},
	pmid = {9983851},
	pages = {3398--3408},
}

@book{Preskill2008,
	title = {Chapter 1. {Introduction} and {Overview}},
	volume = {9},
	url = {http://www.tandfonline.com/doi/abs/10.1300/J515v09n03_01},
	author = {Preskill, John},
	month = feb,
	year = {2008},
	doi = {10.1300/J515v09n03_01},
	note = {Issue: 3-4
ISSN: 1934-9637},
}

@article{song_general_2010,
	title = {General relation between entanglement and fluctuations in one dimension},
	volume = {82},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.82.012405},
	doi = {10.1103/PhysRevB.82.012405},
	number = {1},
	urldate = {2011-10-04},
	journal = {Physical Review B},
	author = {Song, H. and Rachel, Stephan and Le Hur, Karyn},
	month = jul,
	year = {2010},
	pages = {3--6},
}

@article{woodworth_few-body_2006,
	title = {Few-body spin couplings and their implications for universal quantum computation},
	volume = {721},
	doi = {10.1088/0953-8984/18/21/S02},
	author = {Woodworth, Ryan and Mizel, Ari and Lidar, Daniel A},
	year = {2006},
}

@article{Kane1998,
	title = {A silicon-based nuclear spin quantum computer},
	url = {http://www.nature.com/nature/journal/v393/n6681/abs/393133a0.html},
	urldate = {2013-07-23},
	journal = {nature},
	author = {Kane, BE},
	year = {1998},
	pages = {133--137},
}

@article{Weinstein2005a,
	title = {Energetic suppression of decoherence in exchange-only quantum computation},
	volume = {72},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.72.022319},
	doi = {10.1103/PhysRevA.72.022319},
	number = {2},
	urldate = {2011-08-30},
	journal = {Physical Review A},
	author = {Weinstein, Yaakov and Hellberg, C.},
	month = aug,
	year = {2005},
	pages = {1--4},
}

@misc{BayatACreffieldC.E.JeffersonJ.H.PepperM.,
	title = {Quantum {Dot} {Spin} {Cellular} {Automata} for {Realising} a {Quantum} {Processor}},
	author = {Bayat A, Creffield C. E., Jefferson J. H., Pepper M., Bose S.},
}

@article{Weinstein2005,
	title = {Quantum-dot cluster-state computing with encoded qubits},
	volume = {72},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.72.020304},
	doi = {10.1103/PhysRevA.72.020304},
	number = {2},
	urldate = {2011-08-30},
	journal = {Physical Review A},
	author = {Weinstein, Yaakov and Hellberg, C. and Levy, Jeremy},
	month = aug,
	year = {2005},
	pages = {1--4},
}

@article{mizel_three-_2004,
	title = {Three- and {Four}-{Body} {Interactions} in {Spin}-{Based} {Quantum} {Computers}},
	volume = {92},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.92.077903},
	doi = {10.1103/PhysRevLett.92.077903},
	number = {7},
	urldate = {2011-06-20},
	journal = {Physical Review Letters},
	author = {Mizel, Ari and Lidar, Daniel},
	month = feb,
	year = {2004},
	pages = {1--4},
}

@article{Bacon2001,
	title = {Coherence-{Preserving} {Quantum} {Bits}},
	volume = {87},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.87.247902},
	doi = {10.1103/PhysRevLett.87.247902},
	number = {24},
	urldate = {2011-07-26},
	journal = {Physical Review Letters},
	author = {Bacon, Dave and Brown, Kenneth and Whaley, K.},
	month = nov,
	year = {2001},
	pages = {1--4},
}

@article{divincenzo_universal_2000,
	title = {Universal quantum computation with the exchange interaction},
	url = {http://www.nature.com/nature/journal/v408/n6810/abs/408339a0.html},
	urldate = {2013-06-11},
	journal = {Nature},
	author = {DiVincenzo, DP and Bacon, D and Kempe, J},
	year = {2000},
	note = {arXiv: quant-ph/0005116v2},
	pages = {1--13},
}

@article{bacon_universal_2000,
	title = {Universal fault-tolerant quantum computation on decoherence-free subspaces},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.85.1758},
	urldate = {2013-05-07},
	journal = {Physical Review Letters},
	author = {Bacon, D and Kempe, J and Lidar, DA and Whaley, KB},
	year = {2000},
	note = {arXiv: quant-ph/9909058v2},
}

@article{bacon_coherence-preserving_2008,
	title = {Coherence-{Preserving} {Quantum} {Bits}},
	volume = {94704},
	number = {i},
	author = {Bacon, Dave and Brown, Kenneth R and Whaley, K Birgitta},
	year = {2008},
	note = {arXiv: quant-ph/0012018v2},
}

@article{weinstein_quantum_2001,
	title = {Quantum {Dot} {Cluster} {State} {Computing} with {Encoded} {Qubits}},
	journal = {Science},
	author = {Weinstein, Yaakov S and Hellberg, C Stephen and Levy, Jeremy},
	year = {2001},
	note = {arXiv: quant-ph/0506032v1},
	pages = {1--5},
}

@article{jiang_preparation_2009,
	title = {Preparation of decoherence-free cluster states with optical superlattices},
	url = {http://pra.aps.org/abstract/PRA/v79/i2/e022309},
	urldate = {2013-07-19},
	journal = {Physical Review A},
	author = {Jiang, Liang and Rey, AM and Romero-Isart, O},
	year = {2009},
	note = {arXiv: 0811.3049v2},
	pages = {1--17},
}

@article{schollw_one-dimensional_1998,
	title = {One-dimensional {Quantum} {Spin} {Systems}},
	number = {June},
	author = {Schollw, Ulrich},
	year = {1998},
}

@article{Garcia-Ripoll2006,
	title = {Time evolution of {Matrix} {Product} {States}},
	volume = {8},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/8/i=12/a=305?key=crossref.1ba36a6744b1adce73f0ed1217be040e},
	doi = {10.1088/1367-2630/8/12/305},
	number = {12},
	urldate = {2011-08-24},
	journal = {New Journal of Physics},
	author = {García-Ripoll, Juan José},
	month = dec,
	year = {2006},
	pages = {305--305},
}

@article{oliver_continuous-time_2011,
	title = {Continuous-{Time} {Quantum} {Walks}: {Models} for {Coherent} {Transport} on {Complex} {Networks}},
	journal = {Transport},
	author = {Oliver, M and Blumen, Alexander},
	year = {2011},
	note = {arXiv: 1101.2572v1},
}

@article{gualdi_perfect_2008,
	title = {Perfect state transfer in long-range interacting spin chains},
	url = {http://pra.aps.org/abstract/PRA/v78/i2/e022325},
	number = {3},
	urldate = {2013-06-13},
	journal = {Physical Review A},
	author = {Gualdi, Giulia and Kostak, Vojtech and Marzoli, Irene and Tombesi, Paolo},
	year = {2008},
	note = {arXiv: 0807.5018v1},
	pages = {1--5},
}

@article{gualdi_entanglement_2009,
	title = {Entanglement generation and perfect state transfer in ferromagnetic qubit chains},
	url = {http://iopscience.iop.org/1367-2630/11/6/063038},
	urldate = {2013-06-13},
	journal = {New Journal of Physics},
	author = {Gualdi, Giulia and Marzoli, Irene and Tombesi, Paolo},
	year = {2009},
	note = {arXiv: 0812.2404v2},
}

@article{paredes_minimum_2008,
	title = {Minimum instances of topological matter in an optical plaquette},
	volume = {77},
	issn = {1050-2947},
	url = {http://arxiv.org/abs/0711.3796},
	doi = {10.1103/PhysRevA.77.023603},
	abstract = {We propose experimental schemes to create and probe minimum forms of different topologically ordered states in a plaquette of an optical lattice: Resonating Valence Bond, Laughlin and string-net condensed states. We show how to create anyonic excitations on top of these liquids and detect their fractional statistics. In addition, we propose a way to design a plaquette ring-exchange interaction, the building block Hamiltonian of a lattice topological theory. Our preparation and detection schemes combine different techniques already demonstrated in experiments with atoms in optical superlattices.},
	number = {2},
	urldate = {2011-07-25},
	journal = {Physical Review A},
	author = {Paredes, Belén and Bloch, Immanuel},
	month = feb,
	year = {2008},
	note = {arXiv: 0711.3796},
	keywords = {Other Condensed Matter},
	pages = {8},
}

@book{SimonsBen,
	title = {{CondensedMatterFieldTheory}},
	author = {{Simons Ben}},
}

@article{oliver_continuous-time_2011-1,
	title = {Continuous-{Time} {Quantum} {Walks}: {Models} for {Coherent} {Transport} on {Complex} {Networks}},
	journal = {Transport},
	author = {Oliver, M and Blumen, Alexander},
	year = {2011},
	note = {arXiv: 1101.2572v1},
}

@article{Cerletti2005,
	title = {Recipes for spin-based quantum computing},
	volume = {16},
	issn = {0957-4484},
	url = {http://stacks.iop.org/0957-4484/16/i=4/a=R01?key=crossref.ffa73be2c017295d2d73db0659d6d44d},
	doi = {10.1088/0957-4484/16/4/R01},
	number = {4},
	urldate = {2011-07-19},
	journal = {Nanotechnology},
	author = {Cerletti, Veronica and Coish, W a and Gywat, Oliver and Loss, Daniel},
	month = apr,
	year = {2005},
	pages = {R27--R49},
}

@misc{noauthor_phys._nodate,
	title = {Phys. {Rev}. {Lett}. 50, 1395 (1983): {Anomalous} {Quantum} {Hall} {Effect}: {An} {Incompressible} {Quantum} {Fluid} with {Fractionally} {Charged} {Excitations}},
	url = {http://prl.aps.org/abstract/PRL/v50/i18/p1395_1},
	urldate = {2011-08-04},
	keywords = {Qubits},
}

@article{Sen1996a,
	title = {Relaxation in {S}= 1/2 quantum spin chains: {The} role of second neighbor interactions},
	url = {http://prb.aps.org/abstract/PRB/v53/i6/p3398_1},
	urldate = {2012-12-13},
	journal = {Physical Review B},
	author = {Sen, S and Hoff, CN and Kuhl, DE and McGrew, DA},
	year = {1996},
	keywords = {Spin Chains},
}

@article{shastry_excitation_1981,
	title = {Excitation spectrum of a dimerized next-neighbor antiferromagnetic chain},
	url = {http://prl.aps.org/abstract/PRL/v47/i13/p964_1},
	urldate = {2013-06-18},
	journal = {Physical Review Letters},
	author = {Shastry, B Sriram and Sutherland, B},
	year = {1981},
	keywords = {Shastry Sutherland},
}

@misc{noauthor_phys._nodate-1,
	title = {Phys. {Rev}. {A} 77, 023603 (2008): {Minimum} instances of topological matter in an optical plaquette},
	url = {http://pra.aps.org/abstract/PRA/v77/i2/e023603},
	urldate = {2011-08-04},
	keywords = {Qubits},
}

@article{janssens_chapter_2004,
	title = {Chapter 1 {Introduction} and overview},
	volume = {42},
	doi = {10.1016/S0166-526X(04)80005-9},
	journal = {Comprehensive Analytical Chemistry},
	author = {Janssens, K},
	year = {2004},
	pages = {1--11},
}

@article{measurement_chapter_nodate,
	title = {Chapter 3 {Foundations} {II} : {Measurement} and {Evolution}},
	journal = {Measurement},
	author = {Measurement, Orthogonal},
}

@article{preskill_chapter_nodate,
	title = {Chapter 4 {Quantum} {Entanglement}},
	journal = {Quantum},
	author = {Preskill, John},
}

@article{preskill_chapter_nodate-1,
	title = {Chapter 2 {Foundations} {I} : {States} and {Ensembles}},
	volume = {0},
	number = {i},
	journal = {Quantum},
	author = {Preskill, John},
	pages = {1--40},
}

@article{preskill_lecture_2001,
	title = {Lecture {Notes} for {Ph219} / {CS219} : {Quantum} {Information} and {Computation} {Chapter} 4},
	journal = {Quantum Information And Computation},
	author = {Preskill, John},
	year = {2001},
}

@article{preskill_chapter_nodate-2,
	title = {Chapter 5 {Quantum} {Information} {Theory}},
	number = {4},
	journal = {I Can},
	author = {Preskill, John},
}

@article{Verstraete2008,
	title = {Matrix product states, projected entangled pair states, and variational renormalization group methods for quantum spin systems},
	url = {http://www.tandfonline.com/doi/abs/10.1080/14789940801912366},
	urldate = {2012-12-13},
	journal = {Advances in Physics},
	author = {Verstraete, F and Murg, V and Cirac, JI},
	year = {2008},
	note = {arXiv: 0907.2796v1},
	pages = {1--99},
}

@article{foini_simulating_2010,
	title = {Simulating {Quantum} {Systems} through {Matrix} {Product} {States}},
	author = {Foini, Laura},
	year = {2010},
}

@article{preskill_chapter_nodate-3,
	title = {Chapter 6 {Quantum} {Computation}},
	journal = {Quantum},
	author = {Preskill, John},
}

@article{preskill_chapter_nodate-4,
	title = {Chapter 7 {Quantum} {Error} {Correction}},
	journal = {Quantum},
	author = {Preskill, John},
}

@article{peschel_computational_2008,
	title = {Computational {Many}-{Particle} {Physics}},
	volume = {739},
	url = {http://www.springerlink.com/index/10.1007/978-3-540-74686-7},
	doi = {10.1007/978-3-540-74686-7},
	urldate = {2011-06-22},
	journal = {Notes},
	author = {Peschel, Ingo and Eisler, Viktor},
	editor = {Fehske, H. and Schneider, R. and Weiße, A.},
	year = {2008},
	note = {Publisher: Springer Berlin Heidelberg
Place: Berlin, Heidelberg
ISBN: 978-3-540-74685-0},
	pages = {581--596},
}

@article{Jozsa,
	title = {An {Introduction} to measurement-based quantum computation},
	urldate = {2012-11-21},
	journal = {NATO Science Series, III: Computer and systems …},
	author = {Jozsa, R},
	year = {2006},
	note = {arXiv: quant-ph/0508124v2},
	pages = {1--22},
}

@misc{noauthor_mermin_ajp_58_731_90.pdf_nodate,
	title = {mermin\_ajp\_58\_731\_90.pdf},
}

@article{clark_strongly_2007,
	title = {Strongly correlated one-dimensional systems of cold atoms in optical lattices},
	urldate = {2013-06-18},
	author = {Clark, SRJF},
	year = {2007},
}

@article{hsieh_explicit_2008,
	title = {An explicit universal gate-set for exchange-only quantum computation},
	journal = {Quantum},
	author = {Hsieh, M and Kempe, J and Myrgren, S and Whaley, K B},
	year = {2008},
	note = {arXiv: quant-ph/0309002v2},
	pages = {1--16},
}

@article{Toffoli1998,
	title = {How much of physics isjustcomputation?},
	volume = {23},
	issn = {07496036},
	doi = {10.1006/spmi.1997.0560},
	number = {3-4},
	journal = {Superlattices and Microstructures},
	author = {Toffoli, T},
	month = mar,
	year = {1998},
	pages = {381--406},
}

@article{hartmann_effective_2008,
	title = {Effective spin systems in coupled micro-cavities},
	author = {Hartmann, Michael J and Brand, Fernando G S L and Plenio, Martin B},
	year = {2008},
	note = {arXiv: 0704.3056v1},
}

@article{eisert_area_nodate,
	title = {Area laws for the entanglement entropy – a review},
	journal = {interactions},
	author = {Eisert, J and Cramer, M and Plenio, M B},
	note = {arXiv: 0808.3773v4},
	pages = {1--28},
}

@article{hartmann_density_2009,
	title = {Density {Matrix} {Renormalization} {Group} in the {Heisenberg} {Picture}},
	number = {1},
	author = {Hartmann, Michael J and Prior, Javier and Clark, Stephen R and Plenio, Martin B},
	year = {2009},
	note = {arXiv: 0808.0666v2},
	pages = {2--6},
}

@article{plenio_physics_nodate,
	title = {The physics of forgetting: {Landauer}’s erasure principle and information theory},
	journal = {Quantum},
	author = {Plenio, M B and Vitelli, V and Section, Optics},
	note = {arXiv: quant-ph/0103108v1},
}

@article{Tsomokos2008,
	title = {State transfer in highly connected networks and a quantum {Babinet} principle},
	volume = {78},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.78.062310},
	doi = {10.1103/PhysRevA.78.062310},
	number = {6},
	urldate = {2011-08-03},
	journal = {Physical Review A},
	author = {Tsomokos, D. and Plenio, M. and de Vega, I. and Huelga, S.},
	month = dec,
	year = {2008},
	pages = {1--5},
}

@article{Farhi1998,
	title = {Analog analogue of a digital quantum computation},
	volume = {57},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.57.2403},
	doi = {10.1103/PhysRevA.57.2403},
	number = {4},
	journal = {Physical Review A},
	author = {Farhi, Edward and Gutmann, Sam},
	month = apr,
	year = {1998},
	pages = {2403--2406},
}

@article{emms_graph_2007,
	title = {Graph {Embedding} {Using} {Quantum} {Commute} {Times}},
	author = {Emms, David and Wilson, Richard C and Hancock, Edwin},
	year = {2007},
	pages = {371--382},
}

@article{Mulken2005,
	title = {Slow transport by continuous time quantum walks},
	volume = {71},
	issn = {1539-3755},
	url = {http://link.aps.org/doi/10.1103/PhysRevE.71.016101},
	doi = {10.1103/PhysRevE.71.016101},
	number = {1},
	urldate = {2011-07-28},
	journal = {Physical Review E},
	author = {Mülken, Oliver and Blumen, Alexander},
	month = jan,
	year = {2005},
	pages = {1--6},
}

@article{gottesman_thesis_2008,
	title = {Thesis by},
	volume = {2008},
	journal = {Energy},
	author = {Gottesman, Daniel},
	year = {2008},
	note = {arXiv: quant-ph/9705052v1},
}

@article{loss_quantum_1998,
	title = {Quantum computation with quantum dots},
	volume = {57},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.57.120},
	doi = {10.1103/PhysRevA.57.120},
	number = {1},
	journal = {Physical Review A},
	author = {Loss, Daniel and DiVincenzo, David P.},
	month = jan,
	year = {1998},
	pages = {120--126},
}

@article{Coldea2001,
	title = {Spin {Waves} and {Electronic} {Interactions} in {La}\_\{2\}{CuO}\_\{4\}},
	volume = {86},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.86.5377},
	doi = {10.1103/PhysRevLett.86.5377},
	number = {23},
	urldate = {2011-06-21},
	journal = {Physical Review Letters},
	author = {Coldea, R. and Hayden, S. M. and Aeppli, G. and Perring, T. G. and Frost, C. D. and Mason, T. E. and Cheong, S.-W. and Fisk, Z.},
	month = jun,
	year = {2001},
	pages = {5377--5380},
}

@article{Katanin2002,
	title = {Spin excitations in {La2CuO4}: {Consistent} description by inclusion of ring exchange},
	volume = {66},
	issn = {0163-1829},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.66.100403},
	doi = {10.1103/PhysRevB.66.100403},
	number = {10},
	urldate = {2011-08-03},
	journal = {Physical Review B},
	author = {Katanin, a. and Kampf, a.},
	month = sep,
	year = {2002},
	pages = {17--20},
}

@article{Song2006,
	title = {Quantum entanglement in the {S}=1∕2 spin ladder with ring exchange},
	volume = {74},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.74.155119},
	doi = {10.1103/PhysRevB.74.155119},
	number = {15},
	urldate = {2011-08-03},
	journal = {Physical Review B},
	author = {Song, Jun-Liang and Gu, Shi-Jian and Lin, Hai-Qing},
	month = oct,
	year = {2006},
	pages = {1--6},
}

@article{dakic_quantum_2010,
	title = {Quantum simulation of a frustrated {Heisenberg} spin system},
	journal = {Quantum},
	author = {Dakic, Borivoje and Naylor, William and Zeilinger, Anton and Walther, Philip},
	year = {2010},
	note = {arXiv: 1008.4116v1},
	pages = {1--8},
}

@article{nakata_extended_nodate,
	title = {Extended {Quantum} {Dimer} {Model} and novel},
	author = {Nakata, Kouki and Totsuka, Keisuke},
	note = {arXiv: 1101.2111v1},
}

@article{brehmer_effects_1999,
	title = {Effects of biquadratic exchange on the spectrum of elementary excitations in spin ladders},
	volume = {60},
	issn = {0163-1829},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.60.329},
	doi = {10.1103/PhysRevB.60.329},
	number = {1},
	journal = {Physical Review B},
	author = {Brehmer, S. and Mikeska, H.-J. and Müller, M. and Nagaosa, N. and Uchida, S.},
	month = jul,
	year = {1999},
	pages = {329--334},
}

@article{Muller2002,
	title = {Perturbation theories for the {S}=12 spin ladder with a four-spin ring exchange},
	volume = {66},
	issn = {0163-1829},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.66.134423},
	doi = {10.1103/PhysRevB.66.134423},
	number = {13},
	urldate = {2011-08-03},
	journal = {Physical Review B},
	author = {Müller, M. and Vekua, T. and Mikeska, H.-J.},
	month = oct,
	year = {2002},
	pages = {1--5},
}

@article{Raman2005,
	title = {{SU}(2)-invariant spin-12 {Hamiltonians} with resonating and other valence bond phases},
	volume = {72},
	issn = {1098-0121},
	url = {http://link.aps.org/doi/10.1103/PhysRevB.72.064413},
	doi = {10.1103/PhysRevB.72.064413},
	number = {6},
	urldate = {2011-08-03},
	journal = {Physical Review B},
	author = {Raman, K. and Moessner, R. and Sondhi, S.},
	month = aug,
	year = {2005},
	pages = {1--15},
}

@article{umber_2d_1994,
	title = {2'"d" =4 =, ' +},
	number = {August},
	journal = {Most},
	author = {Umber, N},
	year = {1994},
	pages = {1--4},
}

@article{schollwock_one-dimensional_1998,
	title = {One-dimensional {Quantum} {Spin} {Systems}},
	url = {http://web.physik.rwth-aachen.de/~scholl/habil.ps.gz},
	number = {June},
	urldate = {2011-08-25},
	author = {Schollwock, Ulrich},
	year = {1998},
}

@article{khajetoorians_realizing_2011,
	title = {Realizing all-spin-based logic operations atom by atom.},
	volume = {332},
	issn = {1095-9203},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21551029},
	doi = {10.1126/science.1201725},
	abstract = {An ultimate goal of spintronic research is the realization of concepts for atomic-scale all-spin-based devices. We combined bottom-up atomic fabrication with spin-resolved scanning tunneling microscopy to construct and read out atomic-scale model systems performing logic operations. Our concept uses substrate-mediated indirect exchange coupling to achieve logical interconnection between individual atomic spins. Combined with spin frustration, this concept enables various logical operations between inputs, such as NOT and OR.},
	number = {6033},
	urldate = {2011-08-03},
	journal = {Science (New York, N.Y.)},
	author = {Khajetoorians, Alexander Ako and Wiebe, Jens and Chilian, Bruno and Wiesendanger, Roland},
	month = may,
	year = {2011},
	pmid = {21551029},
	pages = {1062--4},
}

@article{Hartmann2006,
	title = {Excitation and entanglement transfer versus spectral gap},
	volume = {8},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/8/i=6/a=094?key=crossref.3c68420ed223113e39f24141cd8345c9},
	doi = {10.1088/1367-2630/8/6/094},
	number = {6},
	urldate = {2011-06-29},
	journal = {New Journal of Physics},
	author = {Hartmann, M J and Reuter, M E and Plenio, M B},
	month = jun,
	year = {2006},
	pages = {94--94},
}

@article{Bayat2010,
	title = {Information-transferring ability of the different phases of a finite {XXZ} spin chain},
	volume = {81},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.81.012304},
	doi = {10.1103/PhysRevA.81.012304},
	number = {1},
	urldate = {2011-08-03},
	journal = {Physical Review A},
	author = {Bayat, Abolfazl and Bose, Sougato},
	month = jan,
	year = {2010},
	pages = {1--11},
}

@article{nielsen_fermionic_2005,
	title = {The {Fermionic} canonical commutation relations and the {Jordan}-{Wigner} transform},
	urldate = {2014-08-04},
	journal = {School of Physical Sciences The University of …},
	author = {Nielsen, MA},
	year = {2005},
	pages = {1--8},
}

@article{wichterich_entanglement_2010,
	title = {Entanglement between noncomplementary parts of many-body systems},
	author = {Wichterich, Hannu Christian},
	year = {2010},
}

@article{venegas-andraca_quantum_nodate,
	title = {Quantum {Walk}-based {Generation} of {Entanglement} {Between} {Two} {Walkers}},
	journal = {Compute},
	author = {Venegas-andraca, Salvador E and Bose, Sougato},
	note = {arXiv: 0901.3946v1},
	pages = {1--8},
}

@article{Wichterich2009,
	title = {Exploiting quench dynamics in spin chains for distant entanglement and quantum communication},
	volume = {79},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.79.060302},
	doi = {10.1103/PhysRevA.79.060302},
	number = {6},
	urldate = {2011-08-03},
	journal = {Physical Review A},
	author = {Wichterich, Hannu and Bose, Sougato},
	month = jun,
	year = {2009},
	pages = {1--4},
}

@article{Christandl2004,
	title = {Perfect {State} {Transfer} in {Quantum} {Spin} {Networks}},
	volume = {92},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.92.187902},
	doi = {10.1103/PhysRevLett.92.187902},
	number = {18},
	urldate = {2011-07-29},
	journal = {Physical Review Letters},
	author = {Christandl, Matthias and Datta, Nilanjana and Ekert, Artur and Landahl, Andrew},
	month = may,
	year = {2004},
	pages = {1--4},
}

@article{kempe_quantum_2003,
	title = {Quantum random walks: an introductory overview},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00107151031000110776},
	number = {Section VI},
	urldate = {2013-06-13},
	journal = {Contemporary Physics},
	author = {Kempe, J},
	year = {2003},
	note = {arXiv: quant-ph/0303081v1},
}

@article{nepomechie_spin_nodate,
	title = {A {Spin} {Chain} {Primer} {Heisenberg} spin chain},
	number = {Xxx},
	journal = {Quantum},
	author = {Nepomechie, Rafael I},
}

@article{bose_quantum_2007,
	title = {Quantum communication through spin chain dynamics: an introductory overview},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00107510701342313},
	urldate = {2013-06-18},
	journal = {Contemporary Physics},
	author = {Bose, Sougato},
	year = {2007},
	note = {arXiv: 0802.1224v1},
	pages = {1--17},
}

@article{bose_quantum_2007-1,
	title = {Quantum communication through spin chain dynamics: an introductory overview},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00107510701342313},
	urldate = {2013-06-18},
	journal = {Contemporary Physics},
	author = {Bose, Sougato},
	year = {2007},
	note = {arXiv: 0802.1224v1},
	pages = {1--17},
}

@article{CamposVenuti2006,
	title = {Long-{Distance} {Entanglement} in {Spin} {Systems}},
	volume = {96},
	issn = {0031-9007},
	url = {http://link.aps.org/doi/10.1103/PhysRevLett.96.247206},
	doi = {10.1103/PhysRevLett.96.247206},
	number = {24},
	urldate = {2011-07-20},
	journal = {Physical Review Letters},
	author = {Campos Venuti, L. and Degli Esposti Boschi, C. and Roncaglia, M.},
	month = jun,
	year = {2006},
	pages = {1--4},
}

@article{CamposVenuti2007,
	title = {Long-distance entanglement and quantum teleportation in {XX} spin chains},
	volume = {76},
	issn = {1050-2947},
	url = {http://link.aps.org/doi/10.1103/PhysRevA.76.052328},
	doi = {10.1103/PhysRevA.76.052328},
	number = {5},
	urldate = {2011-08-03},
	journal = {Physical Review A},
	author = {Campos Venuti, L. and Giampaolo, S. and Illuminati, F. and Zanardi, P.},
	month = nov,
	year = {2007},
	pages = {1--9},
}

@article{metlitski_xy_1931,
	title = {The {XY} {Model} in {One} {Dimension}},
	number = {5},
	journal = {Quantum},
	author = {Metlitski, Max A},
	year = {1931},
	pages = {1--15},
}

@misc{the_mendeley_support_team_getting_2011,
	title = {Getting {Started} with {Mendeley}},
	url = {http://www.mendeley.com},
	abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
	publisher = {Mendeley Ltd.},
	author = {{The Mendeley Support Team}},
	year = {2011},
	note = {Pages: 1-16
Publication Title: Mendeley Desktop
Place: London},
	keywords = {Mendeley, how-to, user manual},
}
