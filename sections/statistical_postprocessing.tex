\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

\section{Statistical postprocessing overview}
\label{sec:stat_proc}
Statistical postprocessing techniques cover a wide spectrum, from simple methods such as subtracting the mean bias, to complex statistical and machine learning approaches. We will briefly summarise common approaches to statistical postprocessing to provide context to our approach. Since the forecast data we use as input is not from an ensemble, we ignore techniques for statistical postprocessing of ensembles.

Methods for statistical postprocessing can broadly be split into two categories; those that assume a distribution of the variable to be corrected (`parametric' methods), and methods where no distribution is assumed (`nonparametric' methods). We will summarise some of the methods here but a more comprehensive overview can be found in~\cite{vannitsem_statistical_2021}.

Parametric methods include using models such as linear regression~\citep{gneiting_calibrated_2005} or neural networks~\citep{rasp_neural_2018} to predict the parameters of the output distribution. For example, we may build a model that predicts the mean and variance of a Normal distribution describing the temperature at a particular location. Parametric methods are appealing in that the output distribution is known. However, it is not always easy to find a parametric distribution that models the data well, particularly where there are mixtures of behaviours contributing to the overall distribution~\citep{cavanaugh_probability_2015}, and when the data has multiple spatially correlated variables (such as data observed on a spatial grid).

Thus in many cases, it is preferable to use a nonparametric method instead. Nonparametric methods include methods such as quantile mapping (see discussion in Sec.~\ref{subsec:qm}), or quantile regression where quantiles are predicted based on other variables~\citep{bremnes_probabilistic_2004, meinshausen_quantile_2006}, and copulas~\citep{li_development_2020}. Artificial neural networks that directly adjust the forecast value also fall into this category; for example models have been developed that learn to adjust forecasts of variables such as temperature and precipitation~\citep{gronquist_deep_2021, han_deep_2021, horat_deep_2023}. Note that, some approaches also combine a parametric and nonparametric approach; an example is some types of quantile mapping that assume a particular distribution for extreme values that fall beyond the maximum seen in the training data~\citep{wood_long-range_2002}.


% Another distinction that can be made in the approach to statistical postprocessing is the variables used to perform the correction; here two categories are often discussed, Perfect Prognosis (PP) and Model Output Statistics (MOS). PP is often defined as models where a statistical relationship is learnt between a set of observations and the variable of interest. MOS instead learns a statistical relationship between outputs from a model and the variable of interest~\citep{glahn_use_1972, maraun_statistical_2018}. So for example, quantile mapping would be classified as PP, since it uses observation data only to determine the bias correction, 


In this work we are concerned with producing short term precipitation forecasts that will be useful in applications such as flood modelling, and which achieve the highest resolution possible. For this reason we are particularly interested in statistical postprocessing using a model that can produce a spatially coherent ensemble of predictions, and that account for stochasticity in the rainfall by producing an ensemble from a single deterministic forecast (since deterministic forecasts tend to be higher resolution than ensemble forecasts). Therefore the methods that we have discussed above are not appropriate, as they either do not make predictions with realistic spatial structure, or only produce a single prediction. 

One of the most widely used machine learning models that can learn to sample spatially coherent images from a target distribution is a Generative Adversarial Network or GAN (more discussion on GANs can be found in Sec.~\ref{sec:gans}). Several works have demonstrated the effectiveness of GANs in nowcasting~\citep{ravuri_skilful_2021}, downscaling low resolution observations~\citep{leinonen_stochastic_2020}, downscaling forecast variables~\citep{harris_generative_2022, price_increasing_2022}, estimating precipitation from raw satellite data~\citep{hayatbini_conditional_2019}, and postprocessing precipitation forecasts~\citep{duncan_generative_2022, jeong_correcting_2023,  hess_physically_2022, yang_improving_2023}. Based on these demonstrations it is the approach we take in this work. 


A potentially more straightforward way to produce a generator is to train a model such as a neural network that takes noise as input and is trained with a loss function that measures the dissimilarity between the generated and target distributions. One example is to use Maximum Mean Discrepancy (MMD), which is based on the concept of comparing moments of the target and generated samples~\citep{li_generative_2015, dziugaite_training_2015} but usually framed in terms of kernel functions that effectively perform the comparison using an infinite set of basis functions~\citep{murphy_probabilistic_2022}. This approach has been investigated in relatively few works in the weather forecasting domain using the energy score (e.g.~\cite{pacchiardi_probabilistic_2021}) which is an MMD approach with a particular choice of kernel and corresponds to a proper scoring rule~\citep{gneiting_strictly_2007}. Whilst this approach has desirable properties relative to GANs, such as ease of training and well-calibrated distributions, it isn't clear how coherent the resultant spatial distribution would be using this method. Another promising approach is to use a loss function that evaluates the mean square error in power spectral density between the generated and real samples~\citep{singh_numerical_2019}. Other deep learning approaches that have recently been successfully applied to generating realistic forecasts are diffusion models~\citep{li_seeds_2023, addison_machine_2022, leinonen_latent_2023} and transformer models~\citep{ben-bouallegue_improving_2023}.



% In ~\citep{graubner_calibration_2022} they perturb initial conditions and use estimates of model parameter uncertainty to calibrate a large deterministic weather model; in principle this could be applied to the deterministic models too, although it seems better to try and directly simulate the probability density rather than assuming that this method will correctly replicate the distribution.




% Using Copulas in~\citep{li_development_2020} over Australia, showing that it outperforms quantile mapping (looks like the quantile mapping is an 11-day sliding window with `leave one-year out cross validation')
% \citep{amemiya_application_2023} use recurrent NNs to bias-correct the Lorenz-96 model.

% \citep{sweeney_reducing_2013} compare many different methods over Ireland, including NNs, to bias correct wind speeds.
% Methods using U-Nets etc ~\citep{han_deep_2021, horat_deep_2023}; useful but do not produce a full ensemble.







% Summary of the niche we fill here:
% \begin{itemize}
%     \item Post-processing precipitation, which is typically harder to forecast than temp
%     \item Using a generative model to produce an ensemble
%     \item not point-by-point, but attempting to produce realistic spatial patterns.
%     \item applying it to a specific region for which these post-processing methods do not seem to have been tested on.
% \end{itemize}


% Met Office IMPROVER?


% General problem; how can we get bias correction methods to work for unseen scenarios? This would be the dream of a machine learning model that is able to learn about the dynamics. However, work like ~\citep{hess_physically_2022} suggests this isn't guaranteed.

\ifSubfilesClassLoaded{%
    \bibliographystyle{alpha}
    \bibliography{references_z}

}{}
\end{document}