\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{authblk}
\usepackage{soul}
\usepackage{natbib}
\usepackage{authblk}
\bibliographystyle{custom_abbrvnat}%Import the bibliography file
\setcitestyle{authoryear,open={(},close={)}} 
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\graphicspath{{\subfix{images/}}}

\title{Postprocessing East African rainfall forecasts using a generative machine learning model}

\author[1,2]{Bobby Antonio}
\author[1]{Andrew McRae}
\author[3]{Dave MacLeod}
\author[1]{Fenwick Cooper}
\author[4]{John Marsham}
\author[5]{Laurence Aitchison}
\author[1]{Tim Palmer}
\author[2]{Peter Watson}


\affil[1]{Department of Physics, University of Oxford, Oxford, UK}
\affil[2]{School of Geographical Sciences, University of Bristol, Bristol, UK}
\affil[3]{School of Earth and Environment Sciences, University of Cardiff, Cardiff, UK}
\affil[4]{School of Earth and Environment, University of Leeds, Leeds,  UK}
\affil[5]{Machine Learning and Computational Neuroscience Unit, University of Bristol, Bristol, UK}









\begin{document}

\maketitle

\begin{abstract}
   Existing weather models are known to have poor skill at forecasting rainfall over East Africa, where there are regular threats of drought and floods. Improved precipitation forecasts could reduce the effects of these extreme weather events and provide significant financial benefits to the region. Building on a successful application of a conditional Generative Adversarial Network (cGAN) to postprocess UK precipitation forecasts, we present a novel way to improve precipitation forecasts in East Africa. This addresses the challenge of realistically representing tropical rainfall in this region, where convection dominates and is poorly simulated in conventional global forecast models. We use a cGAN to postprocess short range hourly forecasts made by the European Centre for Medium-Range Weather Forecasts Integrated Forecast System at 6-18h lead times, and $0.1^{\circ}$ resolution. The cGAN predictions and original forecast are further postprocessed using a novel neighbourhood version of quantile mapping, to leverage the strengths of conventional postprocessing methods and provide a strong baseline to compare against. Our results indicate that the cGAN significantly improves the diurnal cycle of rainfall, and improves rainfall predictions up to high ($99.9^{\text{th}}$ percentile) rainfall values. This improvement persists when evaluating against a season with significantly higher average rainfall than seen in training. The cGAN provides a further advantage in its ability to easily generate large ensembles; the spread of this ensemble broadly reflects the predictability of the observations, but is also characterised by a mixture of under- and over-dispersion. Overall our results demonstrate how the strengths of machine learning and conventional postprocessing methods can be combined, and illuminate what benefits machine learning approaches can bring to this region.
\end{abstract}

\section{Introduction}


East Africa experiences both severe droughts that cause famine \citep{gebremeskel_haile_droughts_2019} and extreme rainfall leading to floods that significantly impact those living in the region~\citep{kilavi_extreme_2018,wainwright_extreme_2021}. For example, flooding of the Shabelle river in Somalia in March 2023 affected an estimated 460,000 people~\citep{floodlist_somalia_2023}, and an estimated 3000-5000 people die on Lake Victoria every year as a result of storms that capsize or damage boats~\citep{watkiss_socio-economic_2020, ifrc_world_2014}.

Accurate rainfall forecasts are therefore crucial for improving attempts to mitigate the effects of these events, through enabling more accurate early warning systems, more timely disaster response, and agricultural planning. A recent study as part of the WISER project estimated that their work improving early warning systems brought benefits of £3m/yr on the coast alone, plus additional intangible improvements to the well-being and safety of people living in the area~\citep{watkiss_socio-economic_2021}. Heavy rainfall advisories in Kenya have been demonstrated to be effective at predicting impactful rainfall events, especially over recent years, although there is still a need for increased resolution in the forecasts in order to better enable initiatives such as Forecast-based Financing~\citep{macleod_are_2021}.


Conventional forecast products that use convection parameterization tend to predict too many low intensity rainfall events and perform poorly at predicting heavy rainfall~\citep{woodhams_what_2018, chamberlain_forecasting_2014, vogel_skill_2018, walker_skill_2019, bechtold_representing_2014, haiden_intercomparison_2012}.They also do not model the diurnal cycle of rainfall well~\citep{kim_tropical_2013, macleod_drivers_2021, bechtold_simulation_2004}, which is likely because of the convective parameterisation schemes used in the models~\citep{vogel_skill_2018, marsham_role_2013, bechtold_representing_2014}. 

In recent years, it has become computationally feasible to run `convection permitting' (CP) models at higher resolutions ($\sim4\text{km}$) for which the model can better capture convection processes without using a parameterization scheme. Several studies have investigated how well these CP models correct precipitation bias in East Africa~\citep{finney_implications_2019, cafaro_convection-permitting_2021, woodhams_what_2018, chamberlain_forecasting_2014, kendon_enhanced_2019, senior_convection-permitting_2021}; these studies have found that CP models tend to improve the overall rainfall distribution (at both high and low rainfall). They also tend to produce a more realistic rainfall frequency, as well as making the diurnal cycle more in line with observations. However, the rainfall distribution is not uniformly improved over the region~\citep{senior_convection-permitting_2021}, and many of these works demonstrate a tendency to over-predict rainfall. Some biases also still remain in diurnal cycle and intensity over the Lake Victoria region, and the models do not capture some of the nighttime peaks in areas such as South Sudan~\citep{finney_implications_2019, chamberlain_forecasting_2014}. 


% Existing numerical weather prediction (NWP) forecast products tend to struggle in tropical regions, particularly in capturing the intensity and diurnal cycle of precipitation, associated with the large role of convective rainfall in these regions, which is not well represented by convection parameterisation schemes used in existing global forecast models ~\citep{haiden_intercomparison_2012, vogel_skill_2018, woodhams_what_2018}. Convection-permitting models have demonstrated improved abilities at capturing the timing and intensity of rainfall~\citep{finney_implications_2019, woodhams_what_2018}, although there still appears to be a tendency for these models to over-predict rainfall in this region, and they are computationally expensive to run.

% Recent investigations into model performance also advocate that postprocessing may be required to improve skill in this region~\citep{vogel_skill_2018}.

At the same time machine learning (ML) models have improved dramatically over the last decade at a range of tasks, including generating realistic images~\citep{karras_style-based_2019}. This has inspired several attempts to train machine learning models to predict the weather from large amounts of historical data~\citep{nguyen_climax_2023, bi_pangu-weather_2022,ravuri_skilful_2021, zhang_skilful_2023,lam_graphcast_2022} and for bias correction~\citep{rasp_neural_2018,ben-bouallegue_improving_2023}.

One of the most widely used machine learning models that can learn to sample realistic images from a target distribution is a Generative Adversarial Network or GAN~\citep{goodfellow_generative_2014}. Several works have demonstrated the effectiveness of GANs in a range of tasks; in \cite{leinonen_stochastic_2020} a GAN was trained to downscale low resolution observations.~\cite{harris_generative_2022} and~\cite{price_increasing_2022} then applied the same approach to the problem of downscaling coarse resolution forecasts towards radar observations. There are several works that have use GANs to postprocess precipitation forecasts: e.g.~\cite{duncan_generative_2022} used a GAN to postprocess the output of a machine learning model, and~\cite{jeong_correcting_2023} used a cyclical GAN to perform corrections of precipitation forecasts over South Korea. 

% In addition to GANs, other deep learning approaches have recently been successfully applied to postprocessing climate and weather models, such as diffusion models~\citep{li_seeds_2023, addison_machine_2022, leinonen_latent_2023} and transformer models~\citep{ben-bouallegue_improving_2023}, and training neural networks using the energy score~\citep{pacchiardi_probabilistic_2021}.


However, only a small amount of effort has been applied to developing these techniques in tropical regions, such as regions in Africa, for which the need for improved forecasting is often greater, and which historically have not seen the same forecast skill improvements as mid-latitude areas like Europe and North America~\citep{youds_gcrf_2021}. This raises the question; can we leverage recent advances in machine learning to improve forecasts in tropical regions such as East Africa? In this work we investigate this by training a Generative Adversarial Network to postprocess short range NWP rainfall forecasts in East Africa at lead times of 6-18h, to see whether this can provide an effective means of correcting existing forecasts. We use forecasts from the state-of-the-art European Centre for Medium-Range Weather Forecasts (ECMWF) Integrated Forecast System (IFS) as the NWP product. In contrast to previous work on ML-based forecasting, we consider the challenging problem of forecasting rainfall in a tropical region, and evaluate whether a GAN is able to learn the properties of rainfall in a region where convection is dominant. We also combine the ML with a conventional postprocessing technique (quantile mapping), in order to leverage the strengths of both methods. We compare against ECMWF IFS forecasts, with quantile mapping also applied - this gives a baseline that is stronger than unmodified forecasts as commonly used in previous work on ML-based forecasting (e.g.~\citep{bi_pangu-weather_2022}), giving a stronger test of whether ML can improve upon the state-of-the-art. 

This paper is structured as follows: In Sec.~\ref{sec:methods} we provide details of the region, data, machine learning model, and forecast verification metrics used. In Sec.~\ref{sec:eval_normal} we analyse our approach over a typical year, and in Sec.~\ref{sec:eval_extreme} present an analysis over a particularly heavy Long Rains season over Kenya.

% \subsection{Statistical Postprocessing}

% Statistical postprocessing techniques cover a wide spectrum, from simple methods such as subtracting the mean bias, to complex statistical and machine learning approaches (see~\cite{vannitsem_statistical_2021} for a comprehensive overview). 


% In this work we are concerned with producing short term precipitation forecasts that will be useful in applications such as flood modelling, and which achieve the highest resolution possible. For this reason we are particularly interested in statistical postprocessing techniques that can produce spatially realistic predictions, and that can produce an ensemble from a single deterministic forecast (since deterministic forecasts tend to be higher resolution than ensemble forecasts). Therefore previous approaches that predict the parameters of a known output probability distribution at each grid cell, such as~\citep{rasp_neural_2018}, or those that use deterministic models to correct forecasts~\citep{gronquist_deep_2021, han_deep_2021, horat_deep_2023} are not appropriate.

% One of the most widely used machine learning models that can learn to sample realistic images from a target distribution is a Generative Adversarial Network or GAN~\citep{goodfellow_generative_2014}. Several works have demonstrated the effectiveness of GANs in a range of tasks; in \cite{leinonen_stochastic_2020} a GAN was trained to downscale low resolution observations~\citep{leinonen_stochastic_2020}.~\cite{harris_generative_2022} and~\cite{price_increasing_2022} then applied the same approach to the problem of downscaling coarse resolution forecasts towards radar observations. There are several works that have use GANs to postprocess precipitation forecasts: e.g.~\cite{duncan_generative_2022} have used a GAN to postprocess the output of a machine learning model, and~\cite{jeong_correcting_2023} used a cyclical GAN to perform corrections of precipitation forecasts over South Korea.

% Aside from GANs, other deep learning approaches that have recently been successfully applied to downscaling forecasts are diffusion models~\citep{li_seeds_2023, addison_machine_2022, leinonen_latent_2023} and transformer models~\citep{ben-bouallegue_improving_2023}. Another approach that has been investigated in relatively few works in the weather forecasting domain is using the energy score as loss function to train a network~\cite{pacchiardi_probabilistic_2021}; whilst this approach has desirable properties relative to a GAN, such as ease of training and well-calibrated distributions, it isn't clear how coherent the resultant spatial distribution would be using this method. 


\section{Methods}
\label{sec:methods}

\subsection{Region}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{images/area_range.pdf}
    
    \caption{The region we consider in this study. The filled contours show the orography in land regions in metres.}
    \label{fig:region}
\end{figure}

The region we consider is $12^{\circ}\text{S}-15^{\circ}\text{N}$, $25^{\circ}-51^{\circ}\text{E}$, roughly centred around Kenya and Lake Victoria, sometimes referred to as Equatorial East Africa (see Fig.~\ref{fig:region}). The rainfall characteristics in this region can broadly be divided into two regions; a "summer rainfall" region (containing e.g.~South Sudan, North-Western Ethiopia, Djibouti, and Coastal regions) and an ``equatorial rainfall" region (containing e.g.~Kenya, Uganda, Northern Tanzania and Southern Ethiopia)~\citep{nicholson_climate_2017}. Within the summer rainfall region the rain mainly falls in the boreal summer, whilst the equatorial rainfall region has two rainy seasons; the `long' rains in March-May, and the `short' rains in October-December. The long rains tend to be the wettest season, with less interannual variability but more intraseasonal variability, and is generally regarded as the hardest season to forecast~\citep{nicholson_climate_2017, walker_skill_2019, kilavi_extreme_2018}.

Freshwater lakes, such as Lake Victoria and Lake Tanganyika, amongst the largest freshwater lakes in the world, have a significant impact on the surrounding weather. Around Lake Victoria there is rain for much of the year with storms frequently occurring on or near the lake~\citep{macleod_drivers_2021, chamberlain_forecasting_2014, woodhams_identifying_2019}.

There are also interesting orographic features in the area, such as Mt. Kilimanjaro and Mt. Kenya, and mountains extending along the East African rift from the the Ethiopian highlands down either side of Lake Victoria (which we refer to as the Rift Valley in this work). A gap in this range, the Turkana channel, extending from Northwest Kenya to South Sudan, is another important feature in this area that affects moisture transport via the Turkana jet that flows through it~\citep{nicholson_turkana_2016}.


\subsection{Data}

\label{sec:data}


For observational data, we use the Integrated Multi-satellite Retrievals for GPM (IMERG) V06B dataset from 2016-2021; this is calculated through satellite observations of microwave and infrared by the array of Global Precipitation Measurement (GPM) satellites, with subsequent calibration incorporating rain gauges~\citep{huffman_integrated_2023}. The observations are half-hourly at a resolution of $0.1^{\circ} \times 0.1^{\circ}$ ($11\text{km} \times 11\text{km}$ at the equator), with precipitation rates given in mm/hr representing the average rainfall over the entire half hour period. For our purposes the data is coarsened to hourly resolution.


Whilst the IMERG data does not perfectly represent the true rainfall, it performs reasonably well at capturing the diurnal cycle and distribution of rainfall in East Africa~\citep{dezfuli_validation_2017, roca_comparing_2010, camberlin_major_2018} compared to other alternatives, particularly given the scarcity of reliable rain gauge and radar data in the area.  In~\cite{ageet_validation_2022} a range of satellite rainfall estimating products, including the IMERG product, are compared with rain gauges in an area around Uganda (including parts of Kenya, Tanzania, Sudan and the Democratic Republic of Congo) over 17 years. Based on a combined assessment of quantile-quantile plots, correlation, and skill scores such as hit rate and false alarms, they identify the IMERG product as the best performing at a daily level. However, it still has biases: For example it has a tendency to underestimate the rainfall rate, and over-predict the frequency of rainfall. There are also known issues with similar satellite products in mountainous areas~\citep{dinku_comparison_2010}, which means these observations may be more unreliable over areas such as the Ethiopian Highlands and parts of the Rift Valley. A dry bias has also been observed in several studies (e.g.~\cite{vogel_skill_2018}). Overall, though, it provides a good source of data with a high temporal and spatial resolution over our target region, and it has been used in other studies in this area (e.g.~\cite{woodhams_what_2018, finney_implications_2019, cafaro_convection-permitting_2021}).

The forecast dataset used is the ECMWF IFS HRES deterministic hourly forecast~\citep{ecmwf_operational_2023} as this tends to perform amongst the best compared to similar models~\citep{haiden_intercomparison_2012}. IFS forecasts are provided at 00h and 12h and we use lead times within a 6-18h window, corresponding to short-range weather prediction (however it is expected that the method we use could also equally apply to longer lead times). The data is interpolated from $9\text{km} \times 9\text{km}$ resolution to $0.1^{\circ} \times 0.1^{\circ}$ to match the grid points of the IMERG precipitation.

The dataset was split up as follows:
\begin{itemize}
    \item Training set: March 2016 – February 2018 and July 2018 – Sept 2020 (excluding validation months)
    \item Validation set: Jun 2018, Oct 2018, Jan 2019, March 2019
    \item Test sets: October 2020 - September 2021, March - May 2018
\end{itemize}
The data starts at March 2016, after the increase in horizontal resolution for the IFS with the release of Cycle 41r2. To ensure the precipitation forecasts are consistent, we use data up until September 2021 before the upgrade to the convection parameterization scheme with the release of Cycle 47r3 in October 2021~\citep{ecmwf_changes_2023}. Note that we also keep the 2018 long rains (March-May) as an extreme test set, since this was a season of particularly heavy rainfall~\citep{kilavi_extreme_2018} for which the seasonal rainfall was significantly higher than any years used in training and validation. 

The standard choice of validation set would be the period October 2019 - September 2020, which spans a full year and would sit between the training and test periods. However, we observed that the long rains of March-May 2020 were particularly high, and so to avoid validation over an atypical year we chose to validate over the period of June 2018-May 2019. Rather than use a full year for validation, we also chose to maximise the amount of training data by including a month from each of the different seasons in the validation period.

All evaluation results reported in Secs.~\ref{sec:climatologic} and~\ref{sec:fcst_skill} are evaluated on 4000 unique hours sampled uniformly over all dates from the unseen test period October 2020 - September 2021, with 20 ensemble members used in the example plots. The ensemble calibration results in Sec.~\ref{sec:ens_calib} are assessed over 500 unique hours sampled uniformly from the same period with an ensemble size of 100.

For the extreme rainfall evaluation in Sec.~\ref{sec:eval_extreme}, we analyse all of the hours from March-May 2018. Since much of the anomalous rainfall in this season was concentrated over Kenya, we restrict our analysis to this region ($4.6^{\circ}\text{S}-5.1^{\circ}\text{N}$, $33.2^{\circ}-43.1^{\circ}\text{E}$).


\subsection{Machine Learning Model}




Our model architecture uses the same architecture and code that~\cite{harris_generative_2022} used to postprocess UK rainfall forecasts. This is itself based on~\cite{leinonen_stochastic_2020} and a variant was developed for downscaling tropical cyclone rainfall by \cite{vosper_deep_2023}. A conditional Wasserstein GAN is trained to predict realistic rainfall patterns conditioned on several meteorological inputs together with constant inputs such as orography, using the IMERG data as ground truth. We use the same approach to test whether it will transfer to also perform well at postprocessing forecasts in a tropical domain. 

Both the generator and discriminator of the GAN are deep neural networks, primarily made up of residual blocks that each contain two convolution layers, using square convolutional kernels of width 3 pixels. The generator is composed of 7 residual blocks (each with $f_g$ filters), with a final softplus activation function, giving a total of $2 \times 7 \times f_g$ hidden layers each of size $200 \times 200$. The discriminator is made up of 3 residual blocks (each with $f_d$ filters), and two dense layers, giving a total of $2 \times 3 \times f_d$ hidden layers each of size $200 \times 200$. 
Excluding the output layers, PReLU activation functions were used, where we set the $\alpha$ parameter for the PReLU to 0.2 following~\cite{harris_generative_2022}. The number of noise channels was set to 4, and the learning rates for the generator and discriminator were set equal to $1\times 10^{-5}$, with the discriminator being trained for 5 steps for every 1 step of generator training. The batch size was set to 2 based on hardware memory constraints, and the Adam optimiser was used for trianing.

Following~\cite{harris_generative_2022} we use a Wasserstein GAN~\citep{arjovsky_wasserstein_2017}, which has been demonstrated to improve training stability in many cases~\citep{creswell_generative_2018}. This modifies the GAN discriminator to output low numbers for real samples and high numbers for fake samples, rather than producing a number in the range $[0,1]$, and modifies the loss function to approximate the Wasserstein distance between the generated and true distributions~\citep{gulrajani_improved_2017}. This approximation is parameterised by a gradient penalty parameter $\gamma$ which we set to 10 in line with~\cite{gulrajani_improved_2017}.

On top of the 9 variables used in~\cite{harris_generative_2022}, we used 11 extra fields, including temperature, convective precipitation, vertical velocity, and relative humidity (some of which are at several pressure levels; see Appendix~\ref{app:fcst_vars} for a full table of inputs). Convective inhibition was included, with null values set to 0. We included these extra variables as they contain important information about convective processes, which are critical for forecasting in East Africa. Based on the transformations applied in~\cite{harris_generative_2022}, we normalise the input variables; precipitation variables are log-normalised via $x \to \log_{10}(1 + x)$, whilst others were either divided by the maximum value, or normalised to fall within the maximum and minimum values (see Appendix~\ref{app:fcst_vars}).

 In order to perform shorter experiments to tune hyperparameters, smaller models with $f_g=32, f_d=128$ were trained for $6.4\times 10^4$ iterations, and then larger models with $f_g=64, f_d=256$ were trained for $3.2\times10^5$ steps; thus our largest model was smaller than the model in~\cite{harris_generative_2022} that had $f_g=128, f_d=512$. However, since the model is not being used for downscaling here, our model inputs are much larger and so using a smaller number of channels was required to achieve a reasonable training time. 

 
 Model checkpoints were saved every 3200 steps, and the best model in the last 1/3rd of checkpoints was selected based on judgement of the combined performance on CRPS, RAPSD and mean squared error, plus visual evaluation of the samples produced. Our batch size was limited to 2 due to the need to generate an ensemble to calculate part of the loss function (discussed in the next paragraph). All models were trained on a single Nvidia A100 GPU.

One notable addition by Harris et.~al.~is the inclusion of a `content loss' term, inspired by~\cite{ravuri_skilful_2021}, which penalises GAN predictions that do not have an ensemble mean close to the observed value. Specifically, at each training step the generator produces an ensemble of predictions (set to 8 in this work), and the generator loss function includes a mean-squared error term between the observed image and the ensemble mean of the generated samples.

% \begin{align}
% \label{eq:content_loss}
% \tilde{\mathcal{L}}_G(\mathbf{y}_{\text{real}}, \mathbf{z}; \theta_G) = \mathcal{L}_G(\mathbf{z}; \theta_G) + \frac{\mathcal{\lambda}}{HW} \left\Vert \mathbf{y}_{\text{real}} - \frac{1}{N_E} \sum_{i=1}^{N_E} G(\mathbf{z}_i;\theta_G) \right\Vert_2^2
% \end{align}
% where $N_E$ is the size of the ensemble, $H$ the height of the image, $W$ the width of the image and $\lambda$ the content loss parameter. In~\cite{ravuri_skilful_2021} it was demonstrated that without the content loss term, the predictions tended to perform poorly on CRPS, CSI and power spectral density. Note there are significant differences in the content loss term used in~\cite{harris_generative_2022} to the implementation in~\cite{ravuri_skilful_2021}: Harris et. al. transform $\mathbf{y}_{\text{real}}$ according to $x \to \log_{10}(x+1)$, there is no clipped weighting term, and they used an $l_2$ in place of the $l_1$-norm used in~\cite{ravuri_skilful_2021}. 

During validation of the models, we observed that using log normalisation of the outputs, as done by Harris et.~al., produced a distribution of rainfall that tended to exponentially deviate from the observations at the extreme rainfall values, and so produced very unrealistic values of rainfall. We chose to remove the log normalisation for the output rainfall values, which remedied this. This also removed the need to clip the predicted rainfall values to a given maximum, as done in Harris et. al. This also required modifying the content loss parameter $\lambda$, with $\lambda=100$ appearing to produce the best results according to a joint assessment of CRPS, RALSD, and quantile-quantile plots.

In~\cite{harris_generative_2022}, samples are grouped into predefined bins based on the fraction of pixels exceeding a set threshold, and then at training time samples are drawn more frequently from the high rainfall bins. However for our data, the threshold used by Harris et. al.~for postprocessing UK rainfall was not appropriate, and our attempts at using a similar approach did not give any improvements on the validation set.


To increase the variation in the samples seen during training, we randomly cropped the $270 \times 265$ images to smaller images of $200 \times 200$, as this has been demonstrated to improve the generalisability of deep learning models~\citep{goodfellow_deep_2016}, and produces output similar in size to that in~\cite{harris_generative_2022}.


Similarly to the results in~\cite{harris_generative_2022}, we observed that the model quality during training (as measured by CRPS, RAPSD and visual inspection) was very variable, such that a validation set is required to pick the best model. 

\subsection{Quantile mapping}
\label{subsec:qm}
Since it is known that raw IFS forecasts don't perform as well as postprocessed forecasts in this region~\citep{vogel_skill_2018}, and IFS forecasts aren't specifically tuned to reproduce the properties of IMERG observations, we applied quantile mapping to the IFS forecasts (see e.g.~\cite{maraun_model_2017}) to provide a stronger baseline. 

Additionally, since our GAN predictions tended to overpredict high rainfall values, we produced a variant of our model with quantile mapping applied to the output. In doing so we aimed to combine the strengths of both postprocessing methods to achieve an overall more accurate and realistic forecast. The GAN could be expected to perform well at producing predictions with realistic spatial structure, but not necessarily with realistic point frequency distributions. Quantile mapping can greatly improve the latter.
% A similar method to quantile mapping has been used within the African Monsoon Multidisciplinary Analysis 2050 (AMMA-2050) project, in which a model pipeline was built to produce reliable flood maps for future planning in Burkina Faso~\citep{senior_convection-permitting_2021}.


We used empirical quantile mapping rather than a distribution-based quantile mapping approach, since it has been demonstrated to work well~\citep{gudmundsson_quantile_2012}, and does not require a parametric distribution. Our method is based on the well-used methods outlined in~\cite{boe_statistical_2007}, ~\citet{deque_frequency_2007}, and ~\citet{maraun_model_2017}, in which empirical cumulative density functions are calculated over the training period and used to create a mapping between the forecast quantiles and the observed quantiles. In general this means creating an estimate of the cumulative density functions $F_{f}$ and $F_{o}$ of the forecast and observations respectively, and mapping the forecast values $x_{f}$ to an adjusted value $\tilde{x}_f$ according to:
\begin{align}
    \tilde{x}_f = F^{-1}_o (F_f (x_f))
\end{align}

In~\cite{boe_statistical_2007}, percentiles at 1\% spacing are first calculated on the training set to find an approximation to the quantile distributions. Forecast values are then mapped into quantile values relative to the training data, and then converted into adjusted forecast values using the observed quantile values (using linear interpolation when the quantile falls between the known quantile values). 

Since the East African precipitation is low, there can be multiple quantiles that are 0; therefore if the forecast presents a 0 value there is no way to tell which quantile it belongs to. We follow the method in~\cite{boe_statistical_2007} and pick one of the 0-valued forecast quantiles at random, then assign the value of the matching observational quantiles. In practise, this can lead to low level noise on the corrected forecast, but replicates the high level statistics.

In order to better match the tail of the frequency distribution in our work, the step size between the quantiles was decreased towards the higher quantiles; so a step size of 0.01 was used up to 0.99, a step size of 0.001 used from 0.99 to 0.999, and so on up to the $99.9999^{\text{th}}$ percentile, above which we observed significant sampling variability.

% In previous works, high rainfall values have been dealt with in different ways; for example in~\citep{leinonen_stochastic_2020} all assessments were made after converting the rainfall values to the [0,1] range. 

For data greater than the maximum value observed in training, we follow the additive uplift method~\citep{boe_statistical_2007, deque_frequency_2007} and add the uplift of the highest quantile for the IMERG and IFS data. For example, if $o_{\text{max}},f_{\text{max}}$ are the highest values seen in the training set for the observations and forecast respectively, then for any forecast value in the test data greater than $f_{\text{max}}$ we add the uplift $(o_{\text{max}} - f_{\text{max}})$ to it. 

\begin{figure}

    \centering

        \includegraphics[width=0.6\textwidth]{images/quantile_mapping.drawio.pdf}
      \caption{a) Illustration of the general method for how grid cells are grouped together in order to estimate quantiles. In this example, the spatial domain is split into squares of $4 \times 4$ grid cells, giving 9 separate large square regions, and in each region the quantiles are calculated. b) To calculate a particular quantile for a grid point, filled in black, we perform a weighted sum of the value of this quantile calculated in the square regions nearest to that point. The weighting for each large region is proportional to the number of small squares inside the red dashed square. In this example, the red dashed square covers 25 grid cells and the weightings would be $\frac{12}{25}$, $\frac{3}{25}$, $\frac{8}{25}$, $\frac{2}{25}$.}
     \label{fig:quantiles}
\end{figure}    

From experiments we found that the typical approach of quantile mapping the GAN at each grid point individually was not a robust approach for the highest values. Therefore we aggregated the data into square regions to calculate quantiles (Fig.~\ref{fig:quantiles}(a)). The intuition is that nearby points will have similar distributions and so we can gain accuracy by grouping nearby points together. 

To avoid any artefacts due to the edges of these domains, the quantiles for a given grid cell were calculated as a weighted average of the nearest squares; specifically, the quantiles used to update the values at grid cell $(m,n)$ are calculated as a weighted sum, where the weighting is calculated by drawing a square around $(m,n)$ and counting the number of grid points that fall into each quantile grouping (Fig.~\ref{fig:quantiles}(b)). This is partly motivated by the ease of implementation, as this can be easily done by broadcasting the grouped quantiles to the same dimensions as the original grid, and using square convolutions with reflective padding to calculate the weighted versions of each quantiles. The length scale of the weighting window was chosen to be the same as the length scale of the quantile groupings, as this was empirically observed to produce reasonably smoothed values.

To evaluate the optimal grouping in the spatial domain, the quantile mapping approach described above was trained on the same training data as the cGAN, and then used to perform quantile mapping on forecasts in the validation set. The best parameters were chosen by calculating the quantiles over the whole domain after quantile mapping, and comparing these to the quantiles of the IMERG data over the whole domain using mean-square error (MSE) up to the $99.9999^{\text{th}}$ percentile. Using this method, the cGAN performed best when split into 4 regions, whilst the IFS forecast performed best when split into 9 regions. We denote these quantile mapped models as cGAN-qm and IFS-qm.



\subsection{Assessing Sample Variability}
\label{sec:sample_var}


For many diagnostics, particularly those concerned with high rainfall events, the results can be swayed by the presence or absence of a small number of high rainfall events particular to the test year. To estimate the uncertainty due to these effects, we use bootstrapping along the time dimension~\citep{efron_bootstrap_1986}. To perform bootstrapping for a property of interest $\theta$ calculated over a set of $N$ hourly samples, we sample with replacement from these $N$ samples $M$ times, resulting in $M$ sets of samples of size $N$. Then the mean and standard error of $\theta$ can then be estimated from the mean and standard deviation of $\theta$ calculated on the bootstrap samples. Since the hours are sampled uniformly at random, this method does not take into account the correlation between adjacent hours, and so it is likely that the standard error calculated from this method is an underestimate.



\subsection{Forecast verification measures}
\label{sec:verification}
\subsubsection{Radially Averaged Power Spectral Density (RAPSD)}
\label{sec:rapsd}

In order to assess the spatial realism of the generated forecasts, we use the Radially Averaged Power Spectral Density (RAPSD)~\citep{sinclair_empirical_2005, harris_multiscale_2001}. This is calculated by taking the 2D Fourier transform of the precipitation image, and averaging this around the centre of the image, yielding a one-dimensional series showing the distribution of weights given to different frequencies. For assessing multiple forecast images we take the mean RAPSD over the images, and for situations where we need to summarise the overall similarity of two RAPSD curves we use the Radially Averaged Log Spectral Distance (RALSD) defined in~\cite{harris_generative_2022}.

\subsubsection{Equitable Threat Score (ETS)}
\label{sec:ets}

The Equitable Threat Score (ETS)~\citep{schaefer_critical_1990, wilks_forecast_2019}, measures the balance between the hit rate and false alarm rate, whilst accounting for the probability of random events. This score is used in operational forecast verification~\citep{mittermaier_long-term_2013} and in~\cite{manzato_behaviour_2017} was shown to be one of the most robust metrics with respect to random (unskillful) changes in the forecast. It is defined as:

\begin{align}
\label{eq:ets}
\text{ETS} := \frac{\text{TP} - \text{TP}_{r}}{\text{TP} + \text{FP} + \text{FN} - \text{TP}_{r}}
\end{align}
where TP, FP, FN, are the number of true positives, false positives and false negatives. $\text{TP}_{r}$ accounts for the number of true positives we would expect to achieve by guessing at random, and is often estimated from the data using the formula:
\begin{align}
    \text{TP}_{r} = \frac{(\text{TP} + \text{FP})(\text{TP} + \text{FN})}{N}
\end{align}

\subsubsection{Fractions Skill Score (FSS)}
\label{sec:fss}


Many forecast verification scores, such as mean square error or the ETS, don't always align with the intuition that a human forecaster has when they evaluate a forecast by eye. There have been many different approaches employed to try and remedy this problem (see e.g.~\cite{gilleland_intercomparison_2009}). One approach, usually called the neighbourhood approach, is to smooth the forecasts and observations by averaging the forecast around each grid cell with a particular length scale before applying a forecast metric~\citep{ebert_fuzzy_2008}. A commonly used metric in this class, and one that has been using to evaluate forecasts  the Fractions Skill Score (FSS)~\citep{roberts_assessing_2008, roberts_scale-selective_2008}.

To calculate the FSS, we first choose a threshold rainfall value $r$, and for each grid cell of the forecast and observations we calculate the fractions $F_{tij}$ and $O_{tij}$ of neighbouring cells for which the rainfall exceeds the threshold, where $t,i$ and $j$ index the time, latitude and longitude axes respectively. The FSS is then defined as:
\begin{align}
    \text{FSS}(n, r) := \frac{\sum_{t=1}^{T}\sum_{i=1}^{N_x} \sum_{j=1}^{N_y} 2 F_{tij}O_{tij}}{\sum_{t=1}^{T}\sum_{i=1}^{N_x}\sum_{j=1}^{N_y} F_{tij}^2 + O_{tij}^2 } \label{eq:fss_main}
\end{align}
We use the pySTEPS implementation of the FSS~\citep{pulkkinen_pysteps_2019}, which performs the averaging using square convolutions with zero-padding

The FSS is typically evaluated relative to the `useful' criteria defined in~\cite{roberts_scale-selective_2008}, such that a the neighbourhood length is increased until the score exceeds $\frac{1}{2} + \frac{f_o}{2}$, where $f_o$ is the fraction of pixels exceeding the threshold over the whole observational set. Beyond this length scale the forecast is typically regarded as skilful enough to be useful. However, it is known that there can be significant skill for forecasts that do not exceed this threshold~\citep{nachamkin_applying_2015, mittermaier_long-term_2013}. 

In the limit of large neighbourhood size, the FSS approaches the asymptotic limit $\text{FSS}_{\infty}$ where~\citep{roberts_scale-selective_2008}:
\begin{align}
\text{FSS}_{\infty} = \frac{2 f_of_m}{ f_o^2 + f_m^2}
\end{align}
where $f_m$ is the modelled frequency of exceeding the threshold. Thus the value that the FSS reaches at large neighbourhood sizes indicates the level of bias in the average number of pixels exceeding the threshold, with $\text{FSS}_{\infty}=1$ for no bias.

\subsubsection{Spread error}
\label{sec:spread_err}
In order to assess how well calibrated the probabilities of the generated forecast are, we use a spread-error plot, commonly used to assess ensemble calibration~\citep{leutbecher_ensemble_2008}. For an ensemble of forecasts $\{f_{i,t}\}_{i=1}^M$ with ensemble mean $\mu_t$, and an observation $y_t$, the spread $s_t$ and error $e_t$ are defined as:
\begin{align}
    s_t^2 &= \frac{1}{M} \sum_{i=1}^{M} \left( f_{i,t} - \mu_t \right)^2 \\
    e_t^2 &= ( y_t - \mu_t )^2
\end{align}

To produce a spread-error plot, we first calculate the spread values for each grid cell and each time value. Then we split the paired observations and forecasts into bins (100 in our case) according to this spread value, and for each bin we calculate the root mean squared spread and error values. 
 For a perfect forecast, the spread of the ensemble will equal the average error between the ensemble mean and observations, so that an ideal spread-error plot is a straight line with gradient 1. Note that for a small number of ensemble members $M$, we also need to include a correction to the spread:
\begin{align}
    \tilde{s}_t = \frac{M+1}{M-1} s_t
\end{align}

\subsubsection{Rank histogram}
\label{sec:rank_hist}
Another method to assess the statistical calibration of an ensemble forecast is the use of a rank histogram (also known as a Talagrand diagram)~\citep{wilks_forecast_2019}. To construct a rank histogram, for each sample we rank the observed value relative to the ensemble members, and then average this over all the samples. This gives a frequency of how many times the observations were seen to be in each rank, which can be plotted as a histogram. A perfectly calibrated ensemble produces a flat histogram. For an imperfect ensemble, the spread of the ensemble members may be too wide, such that the observations will rank in the middle most of the time, producing a peak in the histogram. For the reverse scenario, the spread is too narrow leading to a U-shaped histogram indicating the ensemble members are too narrowly spread. A histogram sloping to the left or right is also indicative of conditional under-forecasting or over-forecasting respectively~\citep{hamill_interpretation_2001, wilks_forecast_2019}. In this work we use the pySTEPS implementation of the rank histogram~\citep{pulkkinen_pysteps_2019}.

\subsubsection{Continuous Ranked Probability Score (CRPS)}

The Continuous Ranked Probability Score, or CRPS, is a particularly important score in assessing probabilistic forecasts, as it is a \emph{strictly proper} score~\citep{wilks_statistical_2019}, which means that the score is only maximised when the forecast distribution equals the target distribution. For a cumulative forecast distribution $F(y)$, the CRPS for an observed occurrence of $x$ (e.g. observed rainfall value) is defined as:
\begin{align}
    \text{CRPS}(F, x)= \int_{-\infty}^{\infty} \left[ F(y) - \mathbf{1}_{y\geq x} \right]^2
dy\end{align}
where $\mathbf{1}_{y\geq x}$ is the Heaviside step function. The CRPS is a univariate measure, so does not properly account for spatial correlations. Whilst there is a multivariate generalisation of the CRPS, the energy score~\citep{gneiting_strictly_2007}, the CRPS is more commonly used and has been used in previous related works, so we use it in this work as one of many validation metrics, in order to choose the best model. 

\section{Evaluation on normal test set}

\label{sec:eval_normal}

\subsection{Climatological properties of the forecasts}
\label{sec:climatologic}
\begin{figure}
     \centering
     \includegraphics[width=\textwidth]{images/q-q_hist_final-nologs_217600_all.pdf}

     \caption{(a) Quantile-quantile plot, up to the $99.9999^{\text{th}}$ percentile. Red circles (diamonds) indicate quantiles for the IFS (IFS-qm) model. Blue circles (diamonds) indicate quantiles for the cGAN (cGAN-qm) model. The black dashed line is the line along which a perfectly calibrated forecast would sit. The error bars indicate an estimate of 2 standard deviations from 1000 bootstrap samples (b) A histogram showing the distribution of rainfall values; the vertical dashed blue line indicates the $99.99^{\text{th}}$ percentile of observed rainfall }
     \label{fig:qq_hist}
\end{figure}

\begin{figure}
     \centering
     \includegraphics[width=0.5\textwidth]{images/return_periods_train.pdf}

    \caption{The approximate return periods for different rainfall thresholds to be exceeded at any grid point in the spatial domain in the training set. The dashed lines indicate the values of particular high percentiles.}
    \label{fig:return}
\end{figure}

\begin{figure}
     \centering
     \includegraphics[width=1.05\textwidth]{images/cGAN_samples_IFS_final-nologs_217600.pdf}
     
     \caption{Example precipitation forecasts following postprocessing by the cGAN-qm model, for a selection of hours throughout the test year (first column). The examples are from randomly chosen dates, but filtered to ensure that a diverse range of months in the year are represented, and so that the examples show different behaviours for periods with high and low rainfall. The columns from left to right show: a single member of the cGAN-qm ensemble, the average of 20 cGAN-qm ensemble members, the IMERG observations, the IFS-qm forecast. Each row corresponds to one time value, shown in the title of the IMERG sample.  }
     \label{fig:examples}
\end{figure}

\begin{figure}
     \centering

     \includegraphics[width=0.48\textwidth]{images/rapsd_final-nologs_217600.pdf}
     
     \caption{a) Radially averaged power spectral density for the quantile mapped forecasts. The black line is for the IMERG data, the blue line for cGAN-qm, and the red line for IFS-qm. 
}
     \label{fig:rapsd}
\end{figure}


\begin{figure}
    \includegraphics[width=\textwidth]{images/diurnal_cycle_final-nologs_217600.pdf}
    \centering
     \caption{Evaluation of the diurnal cycle by hour (in local time) over the whole domain in; (a) mean rainfall, (b) $99.9^{\text{th}}$ percentile, c) $99.99^{\text{th}}$ percentile, where percentiles are calculated over all individual grid points. The shaded regions indicate $\pm2$ standard errors about the mean, estimated by bootstrapping with 50 samples. Note that rainfall at the $99.9^{\text{th}}$ percentile and above is mainly concentrated over Lake Victoria and the ocean.}
     \label{fig:diurnal}
\end{figure}

\begin{figure}
\centering
        \includegraphics[width=\textwidth]{images/diurnal_cycle_map_All_final-nologs_217600.pdf}

     \caption{Left panel: Map of peak rainfall hour for IMERG, averaged over the whole domain. Middle panel: Difference in peak rainfall hour between cGAN-qm and IMERG. Right panel: Difference in peak rainfall hour between IFS-qm and IMERG. A 3-hour moving average along the time dimensions is applied to the data in each case before calculating the peak hour, and then spatial smoothing using a uniform filter of width 3 pixels is applied to the mapped results.}
     \label{fig:peak_hour}
\end{figure}


\begin{figure}
    \centering
     \includegraphics[width=\textwidth]{images/bias_final-nologs217600_all.pdf}
    
     \caption{Mean bias (top) and bias in standard deviation (bottom) of the cGAN-qm and IFS-qm forecasts. }
     \label{fig:bias}
\end{figure}



 We first assess how well the forecasts capture the distribution of rainfall, shown by a quantile-quantile plot and a histogram of rainfall distribution for 4000 samples, shown in Fig.~\ref{fig:qq_hist} (a) and (b) respectively. The raw model outputs are shown together with quantile-mapped outputs. From these we can see that the raw cGAN output is an improvement on the raw IFS output up to extremely high levels of rainfall (around 50mm/hr) beyond which point the IFS is closer to the distribution. 
 
 % Note that we wouldn't necessarily expect the GAN to align closely with the observed frequency distribution, since it is not explicitly trained to do so.

After both forecasts have been quantile mapped, they are much closer to the ideal line, with deviations at high quantiles which we attribute to sampling variability. The scale of this sampling variability was quantified by performing 1000 iterations of bootstrapping (see Sec.~\ref{sec:sample_var}) to estimate the standard error of the quantiles. These are shown in Fig.~\ref{fig:qq_hist} (a), where each error bar shows 2 standard errors.


In order to also get a sense of how extreme these quantile values are, we plot the approximate return period in days for a range of thresholds in Fig.~\ref{fig:return}, calculated over all hours in the test period. These are calculated as the average time gap between instances where at least one point in the domain exceeds the threshold. Note that the very high rainfall values (above around the $99.9^{\text{th}}$ percentile, $\sim20\text{mm/hr}$) are mainly concentrated over Lake Victoria and parts of the sea. 

We observed that the quantile mapped models cGAN-qm and IFS-qm performed better across the range of diagnostics considered here, and so for the remaining diagnostics in this work we will focus on these quantile-mapped forecasts.

We plot examples of the samples generated by cGAN-qm in Fig.~\ref{fig:examples}. The generated samples appear to have a realistic spatial structure, and the ensemble mean has substantial rainfall in places where rainfall is observed, indicating that at least some members include rainfall in those locations. In the bottom row example (8am on 8th May 2021) we can see cGAN-qm removing some of the excess low rainfall seen over the land and sea for the IFS-qm model.   

To get a quantitative picture of the spatial realism of these rainfall patterns, we plot the RAPSD (see Sec.~\ref{sec:rapsd}) in Fig.~\ref{fig:rapsd}. cGAN-qm is closer to IMERG observations for low length scales, less than around 80km. The forecasts show opposite biases at the lowest wavenumbers, possibly due to sampling variability.

Biases in the diurnal cycle are not adjusted by the quantile mapping approach that we employ. Summary statistics of the rainfall as a function of hour are plotted in Fig.~\ref{fig:diurnal}, from which it is clear that cGAN-qm is making significant improvements to the diurnal cycle (improvements that are also present in cGAN). The IFS-qm forecast has a clear bias in mean rainfall around midday, which is also prominent for the diurnal cycle of high quantiles (Fig.~\ref{fig:diurnal} (b) and (c)), whilst the cGAN-qm diurnal cycle remains much closer to that of observations. 

We can gain insight into which spatial locations are particularly driving this improvement from the plots of average peak rainfall hour in Fig.~\ref{fig:peak_hour}, which shows the peak rainfall hour for observations and forecasts in the top row, and biases in peak rainfall hour for cGAN-qm and IFS-qm on the bottom row. Peak rainfall is calculated for these plots by first smoothing along the time axis using a centred moving average of length 3, in order to remove noise and make it easier to see any patterns in the data. The peak rainfall hour for cGAN-qm is fairly spatially noisy, and overall has a positive bias in the peak rainfall position whilst the IFS-qm has an overall negative bias. Improvements made by the cGAN-qm can mainly be seen to occur on land, and both models have biases over areas that are difficult to model the diurnal cycle for, such as over the Ethiopian highlands ($10^{\circ}\text{N}
$, $40^{\circ}\text{E}$), in the Turkana channel ($5^{\circ}\text{N}
$, $35^{\circ}\text{E}$) and the Congo basin ($0^{\circ}\text{N}
$, $25^{\circ}\text{E}$).


The plot of average bias in Fig.~\ref{fig:bias} (a) shows that on average the cGAN-qm under-predicts rainfall on the test set whilst IFS-qm over-predicts. When averaged over the whole domain, the overall mean bias is -0.004 (0.026) for cGAN-qm (IFS-qm), and the overall bias in standard deviation is 0.016 (0.027) for cGAN-qm (IFS-qm).


% cGAN-qm also reduces the most prominent average biases of IFS-qm occurring over the Ethiopian highlands, Lake Victoria and the Western side of the East African rift, near Rwanda and Burundi. The effects on the bias in the standard deviation in Fig.~\ref{fig:bias} (b) are less obvious, there is some reduction in the sharpness of the peaks around the Ethiopian highlands, coastal regions and over the ocean, but there are also areas of increased negative bias in other parts of the ocean and in the western part of the domain over the Democratic Republic of Congo. 






\subsection{Forecast skill assessment}
\label{sec:fcst_skill}

In this subsection we investigate the forecasting skill of both models. 

\begin{figure}
\centering

    \centering
     \includegraphics[width=\textwidth]{images/scatter_mean_final-nologs_217600.pdf}
     \caption{Scatter plots of predicted versus observed domain-average rainfall, for the cGAN-qm forecasts (left) and IFS-qm forecasts (right). The dashed black lines represents perfect forecasts. The Pearson correlation coefficient is 0.74 for cGAN-qm and 0.61 for IFS-qm.}
     \label{fig:scatter}
\end{figure}

The scatter plot in Fig.~\ref{fig:scatter} shows how each model captures the domain-averaged rainfall; the cGAN-qm model is more tightly clustered around the ideal diagonal line, with IFS-qm more spread out and tending to over-predict. This is reflected in the Pearson correlation coefficients; 0.74 for the cGAN-qm and 0.61 for the IFS-qm. This suggests that cGAN-qm has improved the accuracy of predicting domain-average rainfall on at least some days.


\begin{figure}[t]
    \centering\includegraphics[width=\textwidth]{images/fss_final-nologs_217600.pdf}
     \caption{Fractions Skill Score results for different quantile thresholds (a) $90^{\text{th}}$ percentile (b) $99^{\text{th}}$ percentile (c) $99.9^{\text{th}}$ percentile (d) $99.99^{\text{th}}$ percentile (e) $99.999^{\text{th}}$ percentile. The red dashed lines indicate the `useful' threshold of 0.5 that is typically used to interpret this score. Error bars indicate $\pm2$ standard errors estimated from bootstrapping with 50 samples. Note that rainfall at the $99.9^{\text{th}}$ percentile and above is mainly concentrated over Lake Victoria and over the ocean. }
     \label{fig:fss}
\end{figure}

We now look at skill at predicting specific rainfall events, focusing on occurrences of rainfall above a certain threshold. We start with the Fractions Skill Score (FSS; see Sec.~\ref{sec:fss}). In Fig.~\ref{fig:fss} we show plots of FSS for different quantile thresholds, where the quantiles are calculated over the whole domain rather than for each grid cell individually.

The value that the FSS reaches at the maximum neighbourhood size indicates the bias in the frequency with which forecasts exceed the particular rainfall threshold (see Sec.~\ref{sec:fss}), and so we can see that, up to thresholds around the $99.9^{\text{th}}$ percentile, cGAN-qm has lower overall bias. Above this threshold, whilst IFS-qm has higher FSS on average, the effects of sampling variability are significant (especially given that these error bars are likely an underestimate). 

For thresholds up to around the $99.9^{\text{th}}$ percentile, cGAN-qm is consistently higher than the IFS for most neighbourhood sizes. We can see that cGAN-qm crosses the `useful' threshold line at slightly lower neighbourhood sizes for thresholds up to around the $99.9^{\text{th}}$ percentile. However, we can see that for the $99^{\text{th}}$ and $99.9^{\text{th}}$ percentiles the IFS-qm achieves higher scores at low neighbourhood sizes, which suggests improved skill at fine resolutions. 

Overall then, these results suggest that, for up to around the $99.9^{\text{th}}$ percentile cGAN-qm scores better. Beyond this, the IFS-qm shows better performance on average, although the level of skill for both models is low.



\begin{figure}[ht!]
    \centering
     \includegraphics[width=\textwidth]{images/ets_final-nologs_217600.pdf}
    
     \caption{ (a) Equitable threat score (b) false alarm rate and (c) hit rate for IFS-qm (red) and cGAN-qm (blue), for forecasts at individual grid points, plotted against rainfall threshold. Here an event is defined as occurrence of rainfall exceeding the threshold. Note that rainfall at the $99.9^{\text{th}}$ percentile and above is mainly concentrated over Lake Victoria and over the sea.}
     \label{fig:ets}
\end{figure}


The ETS for several thresholds over the whole domain is shown in Fig.~\ref{fig:ets} (a), for rainfall at individual grid points. This shows that IFS-qm tends to perform better by this metric, which is in agreement with IFS=qm having higher FSS scores at smaller length scales. In Fig~\ref{fig:ets} (b) the Hit Rate (HR) and False Alarm Rate (FAR) are shown, from which we can see that the difference in ETS score is driven by both a slightly lower False Alarm Rate and a slightly higher Hit Rate.

        
\subsection{Assessment of ensemble calibration}
\label{sec:ens_calib}



    
\begin{figure}
    \centering
     \includegraphics[width=0.4\textwidth]{images/spread_error_final-nologs_217600.pdf}
     \caption{Spread error, where hourly ensemble forecasts are divided into 100 bins with different ensemble spreads. Calculated using 500 samples from cGAN-qm with 100 ensemble members and evaluated at individual grid points. }
     \label{fig:spread_error}
\end{figure}

\begin{figure}[t]
    \centering\includegraphics[width=\textwidth]{images/rank_hist_final-nologs_217600_all.pdf}
     
    
     \caption{Rank historgrams for (a) all data, (b) grid points where the cGAN-qm ensemble mean is $>0.1\text{mm/hr}$ and (c) grid points where the cGAN-qm ensemble mean is $\leq 0.1\text{mm/hr}$. The dashed black lines represent the result for a perfectly calibrated ensemble. }
     \label{fig:rank_hist}
\end{figure}


We now focus on the probabilistic calibration of the cGAN-qm ensemble, in order to evaluate how suitable cGAN-qm samples are for use as an ensemble. Note that, there is not an ensemble NWP forecast of equivalent resolution covering the test period to compare with. These results are assessed on 500 samples drawn randomly from the test year, each with 100 ensemble members. 

Conceptually, the probabilistic skill depends on variability of both the deterministic and stochastic components of the forecasts. The deterministic component of cGAN-qm is identified with the population ensemble mean and the stochastic component with the distribution of individual members about the mean.

In Fig.~\ref{fig:spread_error} we show the spread error diagnostic (see Sec.~\ref{sec:spread_err}), where the forecasts are binned into 100 different intervals of the forecast ``spread'', which equals the standard deviation of the forecasts. This specifically tests the calibration of the stochastic component of the predictions alone, which has not been examined in previous studies of ML-based forecasts, and it is of interest how well a GAN can learn to represent this.

From the figure we can see that that the spreads of forecasts from the cGAN-qm ensemble cover a substantial range and are closely correlated with the RMSE, showing that cGAN-qm skilfully distinguishes between situations with higher and lower predictability. The ensemble tends to be under-dispersed (i.e.~over-confident) in relatively predictable situations (low ensemble spread), and over-dispersed (i.e.~under-confident) in relatively less predictable situations (high ensemble spread). Whilst the ensemble isn't calibrated perfectly, it is also reassuring to see that cGAN-qm has not fallen into the common failure mode of predominantly producing predictions close to the most likely result~\citep{arjovsky_towards_2016}, which would produce forecasts with far too low ensemble spread.

Fig.~\ref{fig:rank_hist}(a) shows a rank histogram of forecasts for rainfall at individual grid points. This depends on both the deterministic and stochastic components of the predictions. The ensemble deviates from being perfectly calibrated, and appears to be made up of a mixture of behaviours. In general it is hard to uniquely attribute forecast behaviour from a rank histogram~\citep{hamill_interpretation_2001}: A U-shaped distribution can be indicative of under-dispersion of the ensemble, but can also be a mixture of over- and under-forecasting biases. 

In order to shed light on this, we also calculate rank histograms conditioned on whether the cGAN-qm ensemble mean indicates relatively wet or dry weather. We condition on the mean because its sampling variability is lower, and so using this as the conditioning variable greatly reduces the selection bias that would occur if we conditioned on the observed rainfall. In Fig.~\ref{fig:rank_hist} (b) and (c) we show rank histograms conditioned on the cGAN-qm ensemble mean being $>0.1\text{mm/hr}$ and $\leq 0.1\text{mm/hr}$, respectively. In Fig.~\ref{fig:rank_hist} (b) we can see a clear U-shape for the wet hours, indicating under-dispersion, and the histogram is skewed to the left indicating a tendency to predict higher than the observations. From Fig.~\ref{fig:rank_hist} (c) we can see a slight peak at low ranks plus sharper peaks at both ends, and the histogram has a negative gradient. This suggests that at low rainfall intensity the dominant behaviour is over-prediction and under-dispersion, mixed with a weaker over-dispersive behaviour.


% There is also an apparent contradiction between the spread-error plot result and the rank histogram: The spread-error plot appears to show over-dispersion with high rainfall values, while the rank histogram appears to show under-dispersion at high rainfall values. An explanation for this could be down to the fact that the rank histogram measures ordering and is insensitive to the particular error values, unlike the spread-error. Then potentially the spread-error is being skewed by a small number of forecasts that produce very high predictions, which doesn't show up on the rank histograms.

\section{Evaluation on extreme test set}
\label{sec:eval_extreme}

In this section we evaluate the performance of the cGAN-qm model on the extreme test set (see Sec.~\ref{sec:data}). A quantile-quantile plot is shown in Fig.~\ref{fig:quantiles_extreme} showing that the raw cGAN forecast is reasonably close to the ideal line, as are the quantile-mapped models, modulo some sampling variability. The scatter plots in~\ref{fig:scatter_extreme} show that, whilst the results show larger deviations from the diagonal than in Fig.~\ref{fig:scatter} (which may also be due to the smaller domain size and number of samples used here), the cGAN-qm results are more tightly clustered around the ideal diagonal line than IFS-qm. Finally we show the FSS in Fig.~\ref{fig:fss_extreme}. In contrast to the results for the normal evaluation period shown in Fig.~\ref{fig:fss}, the cGAN-qm achieves higher scores up to the $99.99^{\text{th}}$ percentile, although both models have low scores beyond the $99.9^{\text{th}}$ percentile. Overall, these results demonstrate that cGAN-qm has extrapolated well in this case to a rainfall regime outside that which it was trained on. 





\begin{figure}
     \centering
     \includegraphics[width=0.5\textwidth]{images/q-q_hist_final-nologs-mam2018_217600_kenya.pdf}
     
     \caption{
     }
     \label{fig:quantiles_extreme}
\end{figure}

\begin{figure}
     \centering
     \includegraphics[width=1.05\textwidth]{images/scatter_mean_final-nologs-mam2018_217600_kenya.pdf}
     
     \caption{
     }
     \label{fig:scatter_extreme}
\end{figure}

\begin{figure}
     \centering
     \includegraphics[width=\textwidth]{images/fss_final-nologs-mam2018_217600_kenya.pdf}
     
     \caption{
     }
     \label{fig:fss_extreme}
\end{figure}

\section{Conclusions}


In this work we have investigated the use of a conditional Generative Adversarial Network to postprocess the ECMWF IFS HRES forecast over equatorial East Africa and to generate an ensemble forecast. Quantile mapping was applied to the IFS forecast (labelled ``IFS-qm'') to provide a strong baseline, and also to the cGAN output (labelled ``cGAN-qm'') in order to combine the strengths of both conventional postprocessing methods and machine learning.

The rainfall distribution of cGAN-qm and IFS-qm were both comparable, with small biases which we attribute to sampling variability. cGAN-qm postprocessing substantially improved the diurnal cycle, which is known to be particularly problematic for conventional NWP models to capture. The cGAN-qm model demonstrated a substantial correction to the timing of peak mean rainfall over the whole domain, which persisted when looking at the diurnal cycle of high quantiles. However, there is substantial spatial noise in the cGAN-qm peak rainfall hour. Using time as an input variable may be one way to improve the learnt relationships.

The cGAN-qm shows generally higher Fractions Skill Scores up to a high percentile ($99.9^{\text{th}}$), particularly at larger neighbourhood sizes (above $\sim500\text{km}$). For higher percentiles the IFS-qm forecast demonstrated a higher score. The IFS-qm model also achieved higher ETS at the grid scale at all thresholds, although the scores were nevertheless quite low.

Both models were also evaluated on the 2018 Long Rains, which were significantly wetter than any Long Rains season seen in training. The results were consistent with those for the normal evaluation period, demonstrating the ability of the cGAN-qm model to perform reasonable extrapolation for unseen extremes. 

An important advantage that generative machine learning models provide over other non-generative models, is the ability to create an ensemble of predictions from a single forecast. It is therefore important to assess how well this ensemble correlates with the observed variability. Our assessment indicates that the ensemble tends to be under-dispered when predictability is high, and over-dispersed when predictability is low, but is predominantly under-dispersed.

Many studies on the performance of machine learning models compare the output of a model to the raw IFS forecast, or similar (e.g.~\cite{bi_pangu-weather_2022}). In contrast, by evaluating our model against a strong baseline of quantile mapping, our results highlight the subtlety in identifying the improvements a machine learning model can bring. Whilst the cGAN improves on the raw IFS forecast rainfall distribution, compared to the quantile mapping baseline the improvements are mainly in the diurnal cycle and the scores for rainfall events at low to medium rainfall intensities.

There are several future directions for improving this model. There may be additional variables that could be included; for example, we implicitly assume that the IFS forecast captures all of the useful information about phenomena such as the Indian Ocean Dipole and Madden-Julian oscillations; it would be insightful to use indexes of these drivers and/or sea surface temperatures as additional input. It would be interesting as well to include the initial state of the forecast, if available, to see whether the machine learning model is able to correct for errors in how the IFS evolves this initial state.


There are also other promising machine learning models that would be interesting to compare with, to see if performance improvements can be made. Particularly promising approaches would be diffusion models~\citep{addison_machine_2022, leinonen_latent_2023}, since they appear to perform well and are easier to train, and models trained directly on a suitable loss function such as the energy score.

% One limitation we have in this work is the lack of a baseline ensemble to compare probabilistic forecasts against; towards the end of this project, the ECMWF IFS released ensemble forecast data at the same resolution, so going forward it would be ideal to compare our ensemble predictions to these. 
 
% Since the focus of this work is on short term forecasting, which could aid with applications such as flood warnings, it would be insightful to evaluate whether using the cGAN-qm predictions improves flood forecasts. Over Lake Victoria, one of the main hazards is high winds, and whilst rainfall can serve as a proxy for when these winds may happen, it would be interesting to see how the cGAN would perform at postprocessing wind speeds as well, perhaps with multivariate output.




% There are many possible standard machine learning methods that would be good to try, such as spectral normalisation or batch normalisation after the convolutions as used in~\cite{ravuri_skilful_2021}, or a more nuanced scheme to weight the rainier days more heavily, which given the particularly dry climate seen here may well make significant improvements if performed properly. There are also additional terms we could use in the loss function, such as RAPSD, pooled spatial values, or Fractions Skill Score, that we may expect to help produce well-calibrated forecasts.





\section*{Acknowledgments}

Funding etc

Bristol uni cluster.

\bibliography{references_z}

\appendix

\section{Forecast variables used}\label{app:fcst_vars}
The IFS variables and constant fields used to train the model are shown in Table~\ref{tab:vars}, definitions taken from~\citep{ecmwf_parameter_2023}.

The preprocessing methods mentioned in the table are as follows, using the year 2017 as the reference period:
\begin{itemize}
    \item Minmax: calculate the minimum $d_{\text{min}}$ and maximum $d_{\text{max}}$ over the reference period, and then transform each value $v$ according to $(v - d_{\text{min}}) / (d_{\text{max}} - d_{\text{min}})$.
    \item Max: calculate the and maximum $d_{\text{max}}$ over the reference period, and then transform each value $v$ according to $v  / d_{\text{max}}$.
    \item Log: Transform each value $v$ according to $\log_{10}(1+v)$.
\end{itemize}
\begin{table}[ht!]
\centering
\begin{tabular}{c | c | c } 
 \hline
 Variable name & Symbol & Pre-processing applied \\ [0.5ex] 
 \hline\hline
 2 metre temperature &2t & Minmax  \\
 Convective available potential energy &cape & Log \\
 Convective inhibition &cin & Max \\
Convective precipitation &cp & Log \\
Surface pressure & sp & Minmax  \\
Total column cloud liquid water &tclw & Max \\
Total column vertically-integrated water vapour&tcwv & Log \\
Top of atmosphere incident solar radiation&tisr & Max \\
Total precipitation &tp & Log \\
Relative humidity at 200hPa  &r200 & Max \\
Relative humidity at 700hPa  &r700 & Max \\
Relative humidity at 950hPa  &r950 & Max \\
Temperature at 200hPa &t200 & Minmax \\
Temperature at 700hPa  &t700 & Minmax \\
Eastward component of wind at 200hPa &u200 & Max \\
Eastward component of wind at 700hPa &u700 & Max \\
Northward component of wind at 200hPa&v200 & Max \\
Northward component of wind at 700hPa &v700 & Max \\
Vertical velocity at 200hPa &w200 & Max \\
Vertical velocity at 500hPa &w500 & Max \\
Vertical velocity at 700hPa &w700 & Max \\
Orography & h & Max \\
Land-sea mask & lsm & N/A \\
 \hline
\end{tabular}

\caption{IFS variables used to train the model, as well as the normalisation applied to each variable. See text for description of the different preprocessing types.}
\label{tab:vars}
\end{table}


\end{document}
